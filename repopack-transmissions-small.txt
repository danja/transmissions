This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-09T14:43:22.660Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
- Code comments have been removed.

Additional Info:
----------------
User Provided Header:
-----------------------
Transmissions source code

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
src/
  api/
    ApplicationManager.js
    CommandUtils.js
    ModuleLoader.js
    ModuleLoaderFactory.js
    WebRunner.js
  applications/
    file-pipeline/
      processors-config.ttl
      transmissions.ttl
    string-pipeline/
      processors-config.ttl
      transmissions.ttl
    test_fork/
      about.md
      processors-config.ttl
      transmissions.ttl
  engine/
    AbstractProcessorFactory.js
    Connector.js
    Transmission.js
    TransmissionBuilder.js
    TransmissionRunner.js
    WorkerPool.js
  processors/
    base/
      Processor.js
      ProcessProcessor.js
      SinkProcessor.js
      SourceProcessor.js
    flow/
      FlowProcessorsFactory.js
      ForEach.js
    util/
      CaptureAll.js
      DeadEnd.js
      Fork.js
      Halt.js
      NOP.js
      ShowConfig.js
      ShowMessage.js
      ShowTransmission.js
      Stash.js
      Unfork.js
      UtilProcessorsFactory.js
      WhiteboardToMessage.js
  utils/
    footpath.js
    GrapoiHelpers.js
    Logger.js
    ns.js
    t2j.js
    test_runner.js
package.json
README.md
run.js

================================================================
Repository Files
================================================================

================
File: src/api/ApplicationManager.js
================
import path from 'path'
import fs from 'fs/promises'
import logger from '../utils/Logger.js'

class ApplicationManager {
    constructor(appsDir) {
        this.appsDir = appsDir
    }

    async listApplications() {
        try {
            const entries = await fs.readdir(this.appsDir, { withFileTypes: true })
            const subdirChecks = entries
                .filter(dirent => dirent.isDirectory())
                .map(async (dirent) => {
                    const subdirPath = path.join(this.appsDir, dirent.name)
                    const files = await fs.readdir(subdirPath)
                    return files.includes('about.md') ? dirent.name : null
                })

            const validApps = (await Promise.all(subdirChecks)).filter(Boolean)
            return validApps
        } catch (err) {
            logger.error('Error listing applications:', err)
            return []
        }
    }


    resolveApplicationPath(appName) {
        logger.debug('appName = ' + appName)
        if (appName.startsWith('..')) {

            return path.resolve(process.cwd(), appName)
        }

        return path.join(this.appsDir, appName)
    }

    async getApplicationConfig(appPath) {
        logger.debug('appPath = ' + appPath)

        return {
            transmissionsFile: path.join(appPath, 'transmissions.ttl'),
            processorsConfigFile: path.join(appPath, 'processors-config.ttl'),
            modulePath: path.join(appPath, 'processors')
        }
    }
}

export default ApplicationManager

================
File: src/api/CommandUtils.js
================
import path from 'path'
import fs from 'fs/promises'
import logger from '../utils/Logger.js'

import TransmissionRunner from '../engine/TransmissionRunner.js'
import ApplicationManager from './ApplicationManager.js'

class CommandUtils {
    constructor(appsDir) {
        this.appManager = new ApplicationManager(appsDir)
        this.runner = new TransmissionRunner()
    }

    static splitName(fullPath) {
        const parts = fullPath.split(path.sep)
        const lastPart = parts[parts.length - 1]
        if (lastPart.includes('.')) {
            const [name, task] = lastPart.split('.')
            return { first: name, second: task }
        }
        return { first: lastPart, second: false }
    }

    async run(application, target, message = {}) {
        logger.setLogLevel('debug')
        logger.debug('\nCommandUtils.run()')
        logger.debug('CommandUtils.run, process.cwd() = ' + process.cwd())
        logger.debug('CommandUtils.run, application = ' + application)
        logger.debug('CommandUtils.run, target = ' + target)

        const normalizedAppPath = path.normalize(application)
        logger.debug('CommandUtils.run, normalizedAppPath = ' + normalizedAppPath)

        const isRemoteModule = normalizedAppPath.includes('/')


        const appSplit = CommandUtils.splitName(normalizedAppPath)
        const appName = appSplit.first
        const subtask = appSplit.second

        const transmissionsDir = isRemoteModule
            ? normalizedAppPath
            : this.appManager.resolveApplicationPath(appName)

        logger.debug('CommandUtils.run, transmissionsDir = ' + transmissionsDir)
        const appPath = path.join(transmissionsDir, appName)

        logger.debug('CommandUtils.run,  normalizedAppPath = ' + normalizedAppPath)



        const config = await this.appManager.getApplicationConfig(transmissionsDir)

        logger.debug('config.modulePath = ' + config.modulePath)

        await this.runner.initialize(config.modulePath)

        const defaultDataDir = path.join(transmissionsDir, '/data')
        message = {
            ...message,
            dataDir: defaultDataDir,
            rootDir: target || application,
            applicationRootDir: target || application
        }

        return await this.runner.run({
            ...config,
            message,
            subtask
        })
    }

    async listApplications() {
        return await this.appManager.listApplications()
    }

    static async parseOrLoadContext(contextArg) {
        logger.debug(`CommandUtils.parseOrLoadContext(), contextArg = ${contextArg}`)
        let message = {}
        try {
            message.payload = JSON.parse(contextArg)
        } catch (err) {
            logger.debug('*** Loading JSON from file...')
            const filePath = path.resolve(contextArg)
            const fileContent = await fs.readFile(filePath, 'utf8')
            message.payload = JSON.parse(fileContent)
        }
        return message
    }
}

export default CommandUtils

================
File: src/api/ModuleLoader.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import logger from '../utils/Logger.js'

class ModuleLoader {
    constructor(classpath) {
        if (!Array.isArray(classpath)) {
            throw new TypeError('Classpath must be an array')
        }


        this.classpath = classpath.map(p => {
            if (typeof p !== 'string') {
                throw new TypeError('All classpath entries must be strings')
            }
            return path.normalize(p)
        })

        this.moduleCache = new Map()
        logger.debug('ModuleLoader initialized with paths:', this.classpath)
    }

    async loadModule(moduleName) {
        if (typeof moduleName !== 'string') {
            throw new TypeError('Module name must be a string')
        }

        logger.debug(`Attempting to load module: ${moduleName}`)


        if (this.moduleCache.has(moduleName)) {
            logger.debug(`Retrieved ${moduleName} from cache`)
            return this.moduleCache.get(moduleName)
        }


        for (const basePath of this.classpath) {
            try {
                const fullPath = path.join(basePath, `${moduleName}.js`)
                logger.debug(`Trying path: ${fullPath}`)

                const module = await import(fullPath)
                this.moduleCache.set(moduleName, module)
                logger.debug(`Successfully loaded ${moduleName} from ${fullPath}`)
                return module
            } catch (error) {
                logger.debug(`Failed to load from ${basePath}: ${error.message}`)
                continue
            }
        }

        throw new Error(`Module ${moduleName} not found in paths: ${this.classpath.join(', ')}`)
    }

    clearCache() {
        this.moduleCache.clear()
    }

    addPath(newPath) {
        if (typeof newPath !== 'string') {
            throw new TypeError('Path must be a string')
        }
        this.classpath.push(path.normalize(newPath))
    }
}
export default ModuleLoader

================
File: src/api/ModuleLoaderFactory.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import logger from '../utils/Logger.js'
import ModuleLoader from './ModuleLoader.js'

class ModuleLoaderFactory {
    static instance = null;

    static createModuleLoader(additionalPaths = []) {
        const __filename = fileURLToPath(import.meta.url)
        const __dirname = path.dirname(__filename)


        const corePath = path.resolve(__dirname, '../processors')


        const classpath = [corePath, ...additionalPaths].map(p => path.normalize(p.toString()))

        logger.debug('Creating ModuleLoader with paths:', classpath)


        if (!ModuleLoaderFactory.instance) {
            ModuleLoaderFactory.instance = new ModuleLoader(classpath)
        }

        return ModuleLoaderFactory.instance
    }

















    static clearInstance() {
        ModuleLoaderFactory.instance = null
    }
}
export default ModuleLoaderFactory

================
File: src/api/WebRunner.js
================
import express from 'express'
import TransmissionRunner from '../engine/TransmissionRunner.js'
import ApplicationManager from './ApplicationManager.js'
import logger from '../utils/Logger.js'

class WebRunner {
    constructor(appsDir, port = 7247) {
        this.appManager = new ApplicationManager(appsDir)
        this.runner = new TransmissionRunner()
        this.app = express()
        this.port = port

        this.setupRoutes()
    }

    setupRoutes() {
        this.app.use(express.json())

        this.app.get('/applications', async (req, res) => {
            const apps = await this.appManager.listApplications()
            res.json(apps)
        })

        this.app.post('/run/:application', async (req, res) => {
            const { application } = req.params
            const { target, message } = req.body

            try {
                const config = await this.appManager.getApplicationConfig(application)
                await this.runner.initialize(config.modulePath)

                const result = await this.runner.run({
                    ...config,
                    message,
                    target
                })

                res.json(result)
            } catch (error) {
                logger.error('Error running application:', error)
                res.status(500).json({
                    success: false,
                    error: error.message
                })
            }
        })
    }

    start() {
        this.app.listen(this.port, () => {
            logger.log(`Web interface running on port ${this.port}`)
        })
    }
}

export default WebRunner

================
File: src/applications/file-pipeline/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:FilePipelineMap a trm:DataMap ;
    trm:sourceFile "input.txt" ;
    trm:destinationFile "output.txt" .

================
File: src/applications/file-pipeline/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:file_pipeline a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4) .

:s1 a :FileSource .
:s2 a :AppendProcess .
:s3 a :AppendProcess .
:s4 a :FileSink .

================
File: src/applications/string-pipeline/processors-config.ttl
================
### NOT USED

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dc: <http://purl.org/dc/terms/> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:StringPipeline dc:title "Hello" .

================
File: src/applications/string-pipeline/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:stringpipe a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4) .

:s1 a :StringSource .
:s2 a :AppendProcess .
:s3 a :AppendProcess .
:s4 a :StringSink .

================
File: src/applications/test_fork/about.md
================
# Test Fork/Unfork

```
./run test_fork | grep 's2 a NOP'
```

should show the number of forks + 1 (for `message.done`)

```
./run test_fork | grep s1.s2.s10.s11.s12.s13
```

should show just one

================
File: src/applications/test_fork/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:dirWalkerPosts a trm:ServiceConfig ;
    trm:key t:files .

t:postTemplateMap a trm:ReMap ;
   trm:rename (t:rn1) . # consider using blank nodes
     t:rn1    trm:pre     "content" ;
            trm:post    "template"  .

t:postSaver a trm:ReMap ;
    trm:rename (t:rn2) . 
    t:rn2   trm:pre     "targetFilename" ;
            trm:post    "filename" .

================
File: src/applications/test_fork/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:test_fork a :Transmission ;
   trm:contains :pipeA .

:pipeA a trm:Pipeline ;
trm:pipe (:p10 :SM ) .

:p10 a :Fork .

# :s10 a :Unfork .
:s11 a :NOP .

================
File: src/engine/AbstractProcessorFactory.js
================
import SystemProcessorsFactory from '../processors/system/SystemProcessorsFactory.js'
import TestProcessorsFactory from '../processors/test/TestProcessorsFactory.js'
import FsProcessorsFactory from '../processors/fs/FsProcessorsFactory.js'
import MarkupProcessorsFactory from '../processors/markup/MarkupProcessorsFactory.js'
import UtilProcessorsFactory from '../processors/util/UtilProcessorsFactory.js'
import TextProcessorsFactory from '../processors/text/TextProcessorsFactory.js'
import ProtocolsProcessorsFactory from '../processors/protocols/ProtocolsProcessorsFactory.js'
import RDFProcessorsFactory from '../processors/rdf/RDFProcessorsFactory.js'
import PostcraftProcessorsFactory from '../processors/postcraft/PostcraftProcessorsFactory.js'
import FlowProcessorsFactory from '../processors/flow/FlowProcessorsFactory.js'
import StagingProcessorsFactory from '../processors/staging/StagingProcessorsFactory.js'
import GitHubProcessorsFactory from '../processors/github/GitHubProcessorsFactory.js'
import JSONProcessorsFactory from '../processors/json/JSONProcessorsFactory.js'

class AbstractProcessorFactory {




    static createProcessor(type, config) {


        var processor = TestProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = UtilProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = MarkupProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = TextProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = ProtocolsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = RDFProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = PostcraftProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = SystemProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FlowProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = GitHubProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = StagingProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = JSONProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        throw new Error("Unknown processor type: " + type.value)
    }
}

export default AbstractProcessorFactory

================
File: src/engine/Connector.js
================
import { EventEmitter } from 'events'
import logger from '../utils/Logger.js'
import footpath from '../utils/footpath.js'

class Connector extends EventEmitter {


    constructor(fromName, toName) {
        super()
        this.fromName = fromName
        this.toName = toName
    }

    connect(processors) {
        logger.log(`Connector.connect this.fromName = ${this.fromName} this.toName =  ${this.toName}`)
        let fromProcessor = processors[this.fromName]
        let toProcessor = processors[this.toName]

        if (!fromProcessor) {
            throw new Error(`\nMissing processor : ${this.fromName}, going to ${this.toName} \n(check for typos in transmissions.ttl)\n`)
        }

        fromProcessor.on('message', (message) => {
            var tags = ''
            //     if (toProcessor.message) {
            tags = ' (' + fromProcessor.message.tags + ') '
            toProcessor.tags = tags

            const thisTag = footpath.urlLastPart(this.toName)
            logger.log("| Running : " + tags + thisTag + " a " + toProcessor.constructor.name)

            toProcessor.receive(message)
        })

    }


}

export default Connector

================
File: src/engine/Transmission.js
================
import logger from '../utils/Logger.js'
import Connector from './Connector.js'

class Transmission {
  constructor() {
    this.processors = {}
    this.connectors = []

  }

  register(processorName, instance) {
    this.processors[processorName] = instance

  }

  get(processorName) {
    return this.processors[processorName]
  }

  connect(fromProcessorName, toProcessorName) {
    logger.log(`Transmission.connect from ${fromProcessorName} to ${fromProcessorName}`)
    let connector = new Connector(fromProcessorName, toProcessorName)
    this.connectors.push(connector)
    connector.connect(this.processors)
  }




  async process(message) {
    logger.log('\n+ ***** Execute Transmission : ' + this.label + ' <' + this.id + '>')
    const processorName = this.connectors[0]?.fromName || Object.keys(this.processors)[0]
    let processor = this.get(processorName)
    if (processor) {
      logger.log("| Running : " + processorName + " a " + processor.constructor.name)
      await processor.receive(message)
    } else {
      logger.error("No valid processor found to execute")
    }
  }






















  toString() {
    let description = 'Transmission Structure:\n';


    description += 'Processors:\n';
    Object.keys(this.processors).forEach(processorName => {
      description += `  - ${processorName}\n`;
    });


    description += 'Connectors:\n';
    this.connectors.forEach((connector, index) => {
      description += `  - Connector ${index + 1}: ${connector.fromName} -> ${connector.toName}\n`;
    })

    return description
  }
}

export default Transmission

================
File: src/engine/TransmissionBuilder.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'

import ns from '../utils/ns.js'
import GrapoiHelpers from '../utils/GrapoiHelpers.js'
import logger from '../utils/Logger.js'

import ModuleLoader from '../api/ModuleLoader.js'
import AbstractProcessorFactory from "./AbstractProcessorFactory.js"
import Transmission from './Transmission.js'
import ModuleLoaderFactory from '../api/ModuleLoaderFactory.js'



class TransmissionBuilder {

  constructor(moduleLoader) {
    this.moduleLoader = moduleLoader
  }

  static async build(transmissionConfigFile, processorsConfigFile, appPath) {
    const transmissionConfig = await TransmissionBuilder.readDataset(transmissionConfigFile)
    const processorsConfig = await TransmissionBuilder.readDataset(processorsConfigFile)



    const builder = new TransmissionBuilder(this.moduleLoader)
    return builder.buildTransmissions(transmissionConfig, processorsConfig)
  }

  async buildTransmissions(transmissionConfig, processorsConfig) {
    const poi = grapoi({ dataset: transmissionConfig })
    const transmissions = []

    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.trm.Pipeline)) {
        const pipelineID = q.subject

        transmissions.push(await this.constructTransmission(transmissionConfig, pipelineID, processorsConfig))
      }
    }
    return transmissions
  }

  async constructTransmission(transmissionConfig, pipelineID, processorsConfig) {
    processorsConfig.whiteboard = {}

    const transmission = new Transmission()
    transmission.id = pipelineID.value
    transmission.label = ''

    const transPoi = grapoi({ dataset: transmissionConfig, term: pipelineID })

    // TODO has grapoi got a first/single property method?
    for (const quad of transPoi.out(ns.rdfs.label).quads()) {
      transmission.label = quad.object.value
    }
    logger.log('\n+ ***** Construct Transmission : ' + transmission.label + ' <' + transmission.id + '>')

    let previousName = "nothing"

    // grapoi probably has a built-in for all this
    const pipenodes = GrapoiHelpers.listToArray(transmissionConfig, pipelineID, ns.trm.pipe)
    await this.createNodes(transmission, pipenodes, transmissionConfig, processorsConfig) // was await, bad Claude
    //    this.createNodes(transmission, pipenodes, transmissionConfig, processorsConfig); // was await, bad Claude
    this.connectNodes(transmission, pipenodes)
    return transmission
  }

  async createNodes(transmission, pipenodes, transmissionConfig, processorsConfig) {
    for (let i = 0; i < pipenodes.length; i++) {
      let node = pipenodes[i]
      let processorName = node.value

      if (!transmission.get(processorName)) {
        let np = rdf.grapoi({ dataset: transmissionConfig, term: node })
        let processorType = np.out(ns.rdf.type).term
        let processorConfig = np.out(ns.trm.configKey).term

        try {
          let name = ns.getShortname(processorName)
          let type = ns.getShortname(processorType.value)

          logger.log("| Create processor :" + name + " of type :" + type)
          let processor = await this.createProcessor(processorType, processorsConfig) // was await
          processor.id = processorName
          processor.type = processorType
          processor.transmission = transmission

          //    logger.log("| processorConfig :" + processorConfig)
          //  logger.reveal(processorConfig)
          if (processorConfig) {
            processor.configKey = processorConfig
          }
          transmission.register(processorName, processor)
        } catch (err) {
          logger.error('-> Can\'t resolve ' + processorName + ' (check transmission.ttl for typos!)\n')
          logger.error(err)
        }
      }
    }
  }

  async connectNodes(transmission, pipenodes) {
    for (let i = 0; i < pipenodes.length - 1; i++) {
      let leftNode = pipenodes[i]
      let leftProcessorName = leftNode.value
      let rightNode = pipenodes[i + 1]
      let rightProcessorName = rightNode.value
      logger.log("  > Connect #" + i + " [" + ns.getShortname(leftProcessorName) + "] => [" + ns.getShortname(rightProcessorName) + "]")
      transmission.connect(leftProcessorName, rightProcessorName)
    }
  }

  async createProcessor(type, config) {
    try {
      const coreProcessor = AbstractProcessorFactory.createProcessor(type, config)
      if (coreProcessor) {
        return coreProcessor
      }
    } catch (error) {
      logger.debug(`TransmissionBuilder, core processor not found for ${type.value}. Trying remote module loader...`)
    }

    try {
      const shortName = type.value.split('/').pop()
      logger.debug(`Loading module: ${shortName}`)
      const ProcessorClass = await this.moduleLoader.loadModule(shortName)
      logger.debug(`Module loaded successfully: ${shortName}`)
      return new ProcessorClass.default(config)
    } catch (error) {

      logger.error(`TransmissionBuilder, failed to load processor ${type.value}`)
      process.exit(1)
    }
  }







  static async readDataset(filename) {
    const stream = fromFile(filename)
    const dataset = await rdf.dataset().import(stream)
    return dataset
  }

  static async writeDataset(dataset, filename) {
    await toFile(dataset.toStream(), filename)
  }


}

export default TransmissionBuilder

================
File: src/engine/TransmissionRunner.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import ModuleLoaderFactory from '../api/ModuleLoaderFactory.js'
import TransmissionBuilder from './TransmissionBuilder.js'
import logger from '../utils/Logger.js'

class TransmissionRunner {
    constructor() {
        this.moduleLoader = null
    }

    async initialize(modulePath) {
        if (typeof modulePath !== 'string') {
            throw new TypeError('Module path must be a string')
        }
        this.moduleLoader = ModuleLoaderFactory.createModuleLoader([modulePath])
    }

    async run(options) {
        const {
            transmissionsFile,
            processorsConfigFile,
            message = {},
            rootDir = '',
            applicationRootDir
        } = options

        logger.debug('\nTransmissionRunner.run()')
        logger.debug('transmissionsFile =' + transmissionsFile)
        logger.debug('processorsConfigFile =' + processorsConfigFile)

        try {
            if (!this.moduleLoader) {
                throw new Error('ModuleLoader not initialized. Call initialize() first.')
            }

            const transmissions = await TransmissionBuilder.build(
                transmissionsFile,
                processorsConfigFile,
                this.moduleLoader
            )

            if (!message.rootDir) {
                message.rootDir = rootDir
            }
            if (!message.applicationRootDir) {
                message.applicationRootDir = applicationRootDir
            }

            for (const transmission of transmissions) {
                if (!options.subtask || options.subtask === transmission.label) {
                    await transmission.process(message)
                }
            }

            return { success: true }
        } catch (error) {
            logger.error('Error in TransmissionRunner:', error)
            throw error
        }
    }
}

export default TransmissionRunner

================
File: src/engine/WorkerPool.js
================
import { Worker } from 'worker_threads'

class WorkerPool {
    constructor(module, size) {
        this.workers = [];
        this.queue = [];
        for (let i = 0; i < size; i++) {
            const worker = new Worker(module);
            worker.on('message', () => {

                this.markWorkerIdle(worker);
            });
            this.workers.push({ worker, busy: false });
        }
    }

    enqueueMessage(message) {
        this.queue.push(message);
        this.dispatch();
    }

    dispatch() {
        const idleWorkerWrapper = this.workers.find(wrapper => !wrapper.busy);
        if (idleWorkerWrapper && this.queue.length) {
            const message = this.queue.shift();
            idleWorkerWrapper.busy = true;
            idleWorkerWrapper.worker.postMessage(message);
        }
    }

    markWorkerIdle(workerWrapper) {
        workerWrapper.busy = false;
        this.dispatch();
    }
}

================
File: src/processors/base/Processor.js
================
import logger from '../../utils/Logger.js'
import { EventEmitter } from 'events'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'





class Processor extends EventEmitter {





    constructor(config) {
        super()
        this.config = config
        this.messageQueue = []
        this.processing = false
        this.done = false
        this.outputs = []
    }

    preProcess(message) {
        return







        const processorPoi = rdf.grapoi({ dataset: this.config, term: this.configKey })
        logger.log('this.configKey = ' + this.configKey.value)
        logger.poi(processorPoi)

        logger.log('describe Desc')
        if (this.configKey.value === ns.trm.describe.value) {
            this.describe()
        }



    }

    describe() {
        logger.log('describe')
        const inputs = this.getInputKeys()
        const outputs = this.getOutputKeys()
        for (var input of inputs) {
            logger.log('input = ' + input)
            logger.log(this.message[input] + ' = ' + this.message[input])
        }
        for (var output of outputs) {
            logger.log('output = ' + output)
            logger.log(this.message[output] + ' = ' + this.message[output])
        }
    }





















    getMyConfigNode() {
        const dataset = this.config
        const configNode = grapoi({ dataset, term: this.configKey }).in()
        return configNode.term
    }

    getMyPoi() {
        const dataset = this.config
        const myConfigNode = this.getMyConfigNode()
        const poi = grapoi({ dataset: dataset, term: myConfigNode })
        return poi
    }

    async addPropertyToMyConfig(predicate, value) {
        logger.log('addPropertyToMyConfig predicate = ' + predicate)
        logger.log('addPropertyToMyConfig value = ' + value)
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('addPropertyToMyConfig  myConfigNode.value = ' + myConfigNode.value)
        const dataset = this.config
        dataset.add(myConfigNode, predicate, value)
        this.config = dataset
    }

    showMyConfig() {
        const poi = this.getMyPoi()
        logger.log('POI = ')
        logger.poi(poi)
    }

    getPropertyFromMyConfig(property) {

        if (this.config.simples) {
            const shortName = property.value.split('/').pop()
            logger.debug(`Processor (simples), property = ${shortName}`)
            const value = this.config[shortName]
            logger.debug(`Processor (simples), value = ${value}`)
            return value
        }
        const poi = this.getMyPoi()
        try {
            return poi.out(property).term.value
        } catch (err) {
            logger.warn('* Warn : Processor.getPropertyFromMyConfig(), property not defined : ' + property)
            return rdf.literal('undefined')
        }
    }

    async deletePropertyFromMyConfig(predicate, value) {
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('DELETING FROM ' + s)
        const dataset = this.config
        dataset.delete(myConfigNode, predicate, value)
        this.config = dataset
    }






    async receive(message) {
        await this.enqueue(message)
    }






    async enqueue(message) {
        this.messageQueue.push({ message })
        if (!this.processing) {
            this.executeQueue()
        }
    }

    cloneContext(baseContext) {
        const message = structuredClone(baseContext)
        if (baseContext.dataset) {


            message.dataset = baseContext.dataset
        }
        return message
    }





    async executeQueue() {
        this.processing = true
        while (this.messageQueue.length > 0) {
            let { message } = this.messageQueue.shift()

            message = this.cloneContext(message)
            this.message = message




            this.addTag(message)

            await this.process(message)
        }
        this.processing = false
    }

    addTag(message) {
        const tag = this.getTag()
        if (!message.tags) {
            message.tags = tag
            return
        }
        message.tags = message.tags + '.' + tag

    }

    getTag() {
        return footpath.urlLastPart(this.id)
    }







    async process(message) {
        throw new Error('execute method not implemented')
    }







    async doEmit(message) {
        this.emit(message)
    }

















    emit(event, message) {



        super.emit(event, message)
        return message


    }

    getOutputs() {
        const results = this.outputs
        this.outputs = []
        return results
    }
}

export default Processor

================
File: src/processors/base/ProcessProcessor.js
================
import logger from '../../utils/Logger.js'
import Processor from './Processor.js'

class ProcessProcessor extends Processor {
    constructor(config) {
        super(config)
    }





}

export default ProcessProcessor

================
File: src/processors/base/SinkProcessor.js
================
import Processor from './Processor.js'

class SinkProcessor extends Processor {
    constructor(config) {
        super(config)
    }

    async process(message) {

    }
}

export default SinkProcessor

================
File: src/processors/base/SourceProcessor.js
================
import Processor from './Processor.js'

class SourceProcessor extends Processor {
    constructor(config) {
        super(config);
    }







}

export default SourceProcessor

================
File: src/processors/flow/FlowProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import ForEach from './ForEach.js'

class FlowProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.ForEach)) {
            logger.debug('FlowProcessorsFactory: Creating ForEach processor')
            return new ForEach(config)
        }

        return false
    }
}

export default FlowProcessorsFactory

================
File: src/processors/flow/ForEach.js
================
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class ForEach extends ProcessProcessor {
    constructor(config) {
        super(config)
    }

    async process(message) {
        logger.setLogLevel('debug')
        logger.debug('ForEach execute method called')

        if (!message.foreach || !Array.isArray(message.foreach)) {
            logger.error('ForEach: Invalid or missing foreach array in message')
            message.foreach = ["testing-testing", "one", "two", "three"]

        }

        for (const item of message.foreach) {
            const clonedMessage = structuredClone(message)
            clonedMessage.currentItem = item
            delete clonedMessage.foreach

            logger.debug(`ForEach: Emitting message for item: ${item}`)
            this.emit('message', clonedMessage)
        }

        logger.debug('ForEach: Finished processing all items')
    }
}
export default ForEach

================
File: src/processors/util/CaptureAll.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class CaptureAll extends Processor {

    constructor(config) {
        if (!config.whiteboard) {
            config.whiteboard = []
        }
        super(config)


        if (CaptureAll.singleInstance) {
            return CaptureAll.singleInstance
        }
        CaptureAll.singleInstance = this
    }


    async process(message) {
        logger.log('CaptureAll at (' + message.tags + ') ' + this.getTag())
        this.config.whiteboard.push(message)
        return this.emit('message', message)
    }


}

export default CaptureAll

================
File: src/processors/util/DeadEnd.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class DeadEnd extends Processor {

    async process(message) {
        logger.log('DeadEnd  at (' + message.tags + ') ' + this.getTag())
    }

}
export default DeadEnd

================
File: src/processors/util/Fork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'




class Fork extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        const nForks = message.nForks || 2

        logger.debug('forks = ' + nForks)

        for (let i = 0; i < nForks; i++) {
            var messageClone = structuredClone(message)
            messageClone.forkN = i
            logger.debug('--- emit --- ' + i)
            return this.emit('message', messageClone)
        }

        message.done = true

        return this.emit('message', message)
        return this.getOutputs()
    }

}

export default Fork

================
File: src/processors/util/Halt.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class Halt extends Processor {

    process(message) {
        logger.log('\n************************************************************************')
        logger.log('*** << Thou Hast Summoned HALT, the Mighty Stopper of All Things  >> ***')
        logger.log('*** <<                   ~~~ ALL IS GOOD ~~~                      >> ***')
        logger.log('*** <<                     Have a nice day!                       >> ***')
        logger.log('************************************************************************\n')
        logger.log('*** Pipeline was : ' + message.tags)
        logger.log('*** Context now : ')
        logger.reveal(message)
        process.exit()
    }
}

export default Halt

================
File: src/processors/util/NOP.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'

class NOP extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        logger.log('NOP at (' + message.tags + ') ' + this.getTag())
        return this.emit('message', message)
    }
}
export default NOP

================
File: src/processors/util/ShowConfig.js
================
import logger from '../../utils/Logger.js'
import SinkProcessor from '../base/SinkProcessor.js'

class ShowConfig extends SinkProcessor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async process(message) {



        if (this.verbose) logger.log("\n***  Show Config ***")


        logger.log("***************************")
        logger.log("***  Config")
        logger.reveal(this.config)
        logger.log("***************************")




        return this.emit('message', message)
    }
}

export default ShowConfig

================
File: src/processors/util/ShowMessage.js
================
import logger from '../../utils/Logger.js'
import SinkProcessor from '../base/SinkProcessor.js'

class ShowMessage extends SinkProcessor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async process(message) {



        if (this.verbose) logger.log("\n***  Show Message ***")

        logger.log("***************************")
        logger.log("***  Message")
        logger.reveal(message)
        logger.log("***************************")




        return this.emit('message', message)
    }
}

export default ShowMessage

================
File: src/processors/util/ShowTransmission.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ShowTransmission extends Processor {

    async process(message) {
        logger.log(this.transmission.toString())
        return this.emit('message', message)
    }
}

export default ShowTransmission

================
File: src/processors/util/Stash.js
================
import rdf from 'rdf-ext'
import { fromFile, toFile } from 'rdf-utils-fs'
import SourceProcessor from '../base/SourceProcessor.js'












class Stash extends SourceProcessor {





    constructor(config) {
        super(config)
    }






    async process(message) {
        const manifestFilename = rootDir + '/manifest.ttl'
        const stream = fromFile(manifestFilename)


        message.rootDir = rootDir
        message.dataset = await rdf.dataset().import(stream)
        return this.emit('message', message)
    }
}
export default Stash

================
File: src/processors/util/Unfork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import DeadEnd from './DeadEnd.js'






class Unfork extends Processor {

    constructor(config) {
        super(config)
        logger.setLogLevel("info")






    }

    async process(message) {

        if (message.done) {
            logger.debug(' - Unfork passing message')
            message.done = false
            return this.emit('message', message)
        } else {
            logger.debug(' - Unfork terminating pipe')
        }
    }

}

export default Unfork

================
File: src/processors/util/UtilProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import NOP from './NOP.js'
import DeadEnd from './DeadEnd.js'
import ShowMessage from './ShowMessage.js'
import Halt from './Halt.js'
import Restructure from '../json/Restructure.js'
import Unfork from './Unfork.js'
import Fork from './Fork.js'
import ShowTransmission from './ShowTransmission.js'
import CaptureAll from './CaptureAll.js'
import ShowConfig from './ShowConfig.js'
import WhiteboardToMessage from './WhiteboardToMessage.js'

class UtilProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.NOP)) {
            return new NOP(config)
        }
        if (type.equals(ns.t.DeadEnd)) {
            return new DeadEnd(config)
        }
        if (type.equals(ns.t.ShowMessage)) {
            return new ShowMessage(config)
        }
        if (type.equals(ns.t.Halt)) {
            return new Halt(config)
        }
        if (type.equals(ns.t.Restructure)) {
            return new Restructure(config)
        }
        if (type.equals(ns.t.Fork)) {
            return new Fork(config)
        }
        if (type.equals(ns.t.Unfork)) {
            return new Unfork(config)
        }
        if (type.equals(ns.t.ShowTransmission)) {
            return new ShowTransmission(config)
        }
        if (type.equals(ns.t.CaptureAll)) {
            return new CaptureAll(config)
        }
        if (type.equals(ns.t.ShowConfig)) {
            return new ShowConfig(config)
        }
        if (type.equals(ns.t.WhiteboardToMessage)) {
            return new WhiteboardToMessage(config)
        }

        return false

    }
}

export default UtilProcessorsFactory

================
File: src/processors/util/WhiteboardToMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class WhiteboardToMessage extends Processor {

    constructor(config) {
        super(config);
    }
    async process(message) {

        logger.log('WhiteboardToMessage at (' + message.tags + ') ' + this.getTag())

        const originalArray = this.config.whiteboard

        message.whiteboard = Object.keys(originalArray).reduce((acc, key) => {
            const value = originalArray[key];
            if (value !== undefined && value !== null) {
                Object.keys(value).forEach((prop) => {
                    if (!acc[prop]) {
                        acc[prop] = [];
                    }
                    acc[prop].push(value[prop]);
                });
            }
            return acc;
        }, {});

        return super.handle(message)

    }
}

export default WhiteboardToMessage

================
File: src/utils/footpath.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import logger from './Logger.js'







let footpath = {}

footpath.resolve = function footpath(here, relative, start) {

    const loggy = false
    if (loggy) {
        logger.debug("\n*** start footpath.resolve ***")
        logger.debug("process.cwd() = " + process.cwd())
        logger.debug("here = " + here)
        logger.debug("relative = " + relative)
        logger.debug("start = " + start)
    }

    const __filename = fileURLToPath(here)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, relative)
    const filePath = path.join(rootDir, start)

    if (loggy) {
        logger.debug("__filename = " + __filename)
        logger.debug("__dirname = " + __dirname)
        logger.debug("rootDir = " + rootDir)
        logger.debug("filePath = " + filePath)
        logger.debug("*** end footpath.resolve ***\n")
    }

    return filePath
}

footpath.urlLastPart = function footpath(url = 'http://example.org/not-a-url') {


    const urlObj = new URL(url);
    const hash = urlObj.hash;
    const path = urlObj.pathname;
    const lastPart = hash ? hash.replace(/^#/, '') : path.split('/').pop();
    // } catch {
    //  return 'not-a-url'

    return lastPart;
}

export default footpath

================
File: src/utils/GrapoiHelpers.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'
import ns from './ns.js'
import logger from './Logger.js'



class GrapoiHelpers {


    static async readDataset(filename) {
        const stream = fromFile(filename)
        const dataset = await rdf.dataset().import(stream)
        return dataset
    }

    static async writeDataset(dataset, filename) {
        await toFile(dataset.toStream(), filename)
    }


    static listToArray(dataset, term, property) {
        const poi = rdf.grapoi({ dataset: dataset, term: term })
        const first = poi.out(property).term

        let p = rdf.grapoi({ dataset, term: first })
        let object = p.out(ns.rdf.first).term

        const result = [object]

        while (true) {
            let restHead = p.out(ns.rdf.rest).term
            let p2 = rdf.grapoi({ dataset, term: restHead })
            let object = p2.out(ns.rdf.first).term

            if (restHead.equals(ns.rdf.nil)) break
            result.push(object)
            p = rdf.grapoi({ dataset, term: restHead })
        }
        return result
    }





    static listObjects(dataset, subjectList, predicate) {
        const objects = []
        for (const subject of subjectList) {
            logger.log("subject = " + subject.value)
            let p = rdf.grapoi({ dataset, term: subject })
            let object = p.out(predicate).term
            logger.log("object = " + object.value)
            objects.push(object)
        }
        return objects
    }
}
export default GrapoiHelpers

================
File: src/utils/Logger.js
================
import fs from 'fs'


let logger = {}

logger.logfile = 'latest.log'



const LOG_LEVELS = [
    "debug",
    "info",
    "log",
    "warn",
    "error",
]
const logComponent = "api.logger"

logger.appendLogToFile = function (message) {
    if (logger.logfile) {
        fs.appendFileSync(logger.logfile, message + '\n', 'utf8')
    }
}

logger.setLogLevel = function (logLevel = "warn") {

    console[logLevel]('', logComponent, logLevel)
    logger.currentLogLevel = logLevel
}

logger.timestampISO = function () {
    let now = new Date()
    return now.toISOString()
}

logger.log = function (msg, level = "log") {
    const currentLevelIndex = LOG_LEVELS.indexOf(logger.currentLogLevel)
    const messageLevelIndex = LOG_LEVELS.indexOf(level)

    if (messageLevelIndex >= currentLevelIndex) {
        console[level](msg)
        const logMessage = `[${logger.timestampISO()}] [${level.toUpperCase()}] - ${msg}`
        logger.appendLogToFile(logMessage)
    }
}

logger.reveal = function (instance) {
    const serialized = {}

    logger.log('***    hidden keys :  ')
    for (const key in instance) {

        if (key.startsWith('_')) {


            logger.log(`       ${key}`)
            continue
        } else {
            if (instance.hasOwnProperty(key)) {
                let kiki = instance[key]

                if (kiki) {
                    if (Buffer.isBuffer(kiki)) {
                        kiki = kiki.toString()

                    }
                    if (kiki.length > 100) {
                        try {
                            kiki = kiki.substring(0, 100) + '...'
                        } catch (e) {
                            kiki = kiki.slice(0, 99)
                        }
                    }
                    serialized[key] = kiki
                } else {
                    serialized[key] = '[no key]'
                }
            }
        }
    }
    const props = JSON.stringify(serialized, null, 2)
    if (!instance) {
        throw new Error(` no instance defined`)
        return
    }
    logger.log(`Instance of ${instance.constructor.name} with properties - \n${props}`)

}

logger.debug = function (msg) {
    logger.log(msg, "debug")
}

logger.info = function (msg) {
    logger.log(msg, "info")
}

logger.warn = function (msg) {
    logger.log(msg, "warn")
}

logger.error = function (msg) {
    logger.log(msg, "error")
}

logger.poi = function exploreGrapoi(grapoi, predicates, objects, subjects) {

    console.log('Properties of the Grapoi object:')
    for (const prop in grapoi) {
        console.log(`\t${prop}: ${grapoi[prop]}`)
    }


    console.log('\nPath:')
    const path = grapoi.out(predicates, objects).in(predicates, subjects)
    for (const quad of path.quads()) {
        console.log(`\t${quad.predicate.value}: ${quad.object.value}`)
    }
}

function handleExit(options, exitCode) {
    if (options.cleanup) {

    }
    if (exitCode || exitCode === 0) console.log(exitCode)
    if (options.exit) process.exit()
}


process.on('exit', handleExit.bind(null, { cleanup: true }))
process.on('SIGINT', handleExit.bind(null, { exit: true }))
process.on('SIGUSR1', handleExit.bind(null, { exit: true }))
process.on('SIGUSR2', handleExit.bind(null, { exit: true }))
process.on('uncaughtException', handleExit.bind(null, { exit: true }))

export default logger

================
File: src/utils/ns.js
================
import rdf from 'rdf-ext'

const ns = {
    rdf: rdf.namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#'),
    rdfs: rdf.namespace('http://www.w3.org/2000/01/rdf-schema#'),
    dc: rdf.namespace('http://purl.org/dc/terms/'),
    schema: rdf.namespace('http://schema.org/'),
    xsd: rdf.namespace('http://www.w3.org/2001/XMLSchema#'),
    trm: rdf.namespace('http://purl.org/stuff/transmission/'),
    t: rdf.namespace('http://hyperdata.it/transmissions/'),
    fs: rdf.namespace('http://purl.org/stuff/filesystem/'),
    pc: rdf.namespace('http://purl.org/stuff/postcraft/')
}





ns.getShortname = function (url) {

    if (!url) return
    const lastSlashIndex = url.lastIndexOf('/');
    const lastHashIndex = url.lastIndexOf('#');
    const path = url.slice(lastSlashIndex + 1);
    return path.split('#')[0].split('?')[0];
}
export default ns

================
File: src/utils/t2j.js
================
import { Readable } from 'readable-stream'
import rdf from '@rdfjs/data-model'
import SerializerJsonld from '@rdfjs/serializer-jsonld'
import Serializer from '@rdfjs/serializer-turtle'
import N3Parser from '@rdfjs/parser-n3'
import { fromFile } from 'rdf-utils-fs'
import { toFile } from 'rdf-utils-fs'

const testTurtle = `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <https://hyperdata.it/transmissions/> . # for custom terms & instances

:simplepipe a trm:PipelineTransmission ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a trm:StringSource .
:s2 a trm:AppendProcess .
:s3 a trm:StringSink .
`
export class Turtle2JSONLD {
    static async convert(turtle) {

        let parser = new N3Parser({ factory: rdf })



        const input = Readable.from(turtle)

        const output = parser.import(input)

        const serializerJsonld = new SerializerJsonld()
        const jsonStream = serializerJsonld.import(output)





        const outputJson = await Turtle2JSONLD.streamToString(jsonStream)
        return outputJson
    }

    static stringToStream(str) {
        const stream = new Readable();
        stream.push(str);
        stream.push(null);
        return stream;
    }

    static streamToString(stream) {
        const chunks = [];
        return new Promise((resolve, reject) => {
            stream.on('data', (chunk) => {
                chunks.push(Buffer.from(chunk))
                console.log('chunk:', chunk)
            }
            );
            stream.on('error', (err) => reject(err));
            stream.on('end', () => {
                const result = Buffer.concat(chunks).toString('utf8')
                resolve(result)
                console.log('****************** result:', result)
            });
        })
    }
}



const testJson = await Turtle2JSONLD.convert(testTurtle)
console.log('')
console.log(testJson)

================
File: src/utils/test_runner.js
================
import fs from 'fs';
import path from 'path';

const testFiles = fs.readdirSync(__dirname).filter(file => file.startsWith('test_'));

testFiles.forEach(testFile => {
    console.log(`Running ${testFile}`);
    require(path.join(__dirname, testFile));
});

================
File: package.json
================
{
  "type": "module",
  "version": "1.0.0",
  "description": "Transmissions",
  "name": "transmissions",
  "scripts": {
    "test": "jasmine --config=jasmine.json --reporter=tests/helpers/reporter.js",
    "docs": "jsdoc -c jsdoc.json",
    "build": "webpack --mode=production --node-env=production",
    "build:dev": "webpack --mode=development",
    "build:prod": "webpack --mode=production --node-env=production",
    "rp": "repopack -c repopack.config-small.json . && repopack -c repopack.config-large.json . ",
    "watch": "webpack --watch",
    "serve": "webpack serve"
  },
  "devDependencies": {
    "@babel/core": "^7.23.7",
    "@babel/preset-env": "^7.23.8",
    "autoprefixer": "^10.4.17",
    "babel-loader": "^9.1.3",
    "chai": "^5.0.3",
    "css-loader": "^6.9.1",
    "html-webpack-plugin": "^5.6.0",
    "jasmine": "^5.1.0",
    "jasmine-browser-runner": "^2.3.0",
    "jasmine-core": "^5.1.1",
    "jasmine-spec-reporter": "^7.0.0",
    "jsdoc": "^4.0.2",
    "mini-css-extract-plugin": "^2.7.7",
    "postcss": "^8.4.33",
    "postcss-loader": "^8.0.0",
    "prettier": "^3.2.4",
    "style-loader": "^3.3.4",
    "webpack": "^5.90.0",
    "webpack-cli": "^5.1.4",
    "webpack-dev-server": "^4.15.1",
    "workbox-webpack-plugin": "^7.0.0"
  },
  "dependencies": {
    "@dotenvx/dotenvx": "^1.14.2",
    "@rdfjs/formats": "^4.0.0",
    "axios": "^1.6.8",
    "cheerio": "^1.0.0-rc.12",
    "d3": "^7.9.0",
    "jsdom": "^25.0.0",
    "marked": "^12.0.1",
    "marked-code-format": "^1.1.6",
    "marked-custom-heading-id": "^2.0.10",
    "marked-footnote": "^1.2.4",
    "markmap-lib": "^0.17.0",
    "markmap-render": "^0.17.0",
    "markmap-toolbar": "^0.17.0",
    "markmap-view": "^0.17.0",
    "nunjucks": "^3.2.4",
    "queue": "^7.0.0",
    "rdf-ext": "^2.5.2",
    "rdf-utils-fs": "^3.0.0",
    "yargs": "^17.7.2"
  }
}

================
File: README.md
================
# transmissions

After _No Code_ and _Lo Code_ comes _Marginally Less Code_

**Transmissions** is a micro-framework intended to simplify construction of small pipeliney data processing applications in JavaScript (assuming you are already familiar with JavaScript and RDF).

The code is in active development, ie. **not stable**, subject to arbitrary changes.

A bit like `make` or a `package.json` builder. But much harder work (and fun).

Applications are defined in several places, the bits of interest are eg. Postcraft's [transmissions.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/transmissions.ttl) and [services.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/services.ttl).
The former defines the flow, the latter config of the services (under [src/services](https://github.com/danja/transmissions/tree/main/src/services)). The runtime instance of the application is given in the target [manifest.ttl](https://github.com/danja/postcraft/blob/main/danny.ayers.name/manifest.ttl).

### Installation etc.

This is not ready yet. But if you really must...

Make a fresh dir. Clone this repo and [Postcraft](https://github.com/danja/postcraft) into it.

```
cd transmissions
npm i
```

This may or may not work :

```
npm run test
```

Then if you do :

```
./trans postcraft /home/danny/github-danny/postcraft/danny.ayers.name
```

it may build a site (my blog - this is dogfooding to the max) under `public/home`

```
./trans
```

on its own should list the applications available. Most of these won't work, the code has been shapeshifting a lot.

### Status

**2024-09-02** Getting used as a serrrrriously over-engineered, feature-lacking static site builder, proof of concept is [Postcraft](https://github.com/danja/postcraft), as evinced by my [blog](https://danny.ayers.name/) (where, for now at least you will find update on this). But it mostly works as intended. Docs lagging. But now I have a documentation engine...

Documentation will be lagging behind code, be incomplete and out of date.

**2024-03-24** : a couple of simple data processing pipelines working and wired up as Jasmine e2e tests in place; started to develop actually useful pipelines for [foaf-archive](https://github.com/danja/foaf-archive) project

## Motivation

This was prompted by something of a ~~perfect storm~~ serendipitous convergence of ideas, requirements and environment.

- I had the realization that much of the software I want to write in the near future has the same basic high level **architecture** : pipeline-like, ie. a series of steps each of which transform data from one form into another ([Wikipedia](<https://en.wikipedia.org/wiki/Pipeline_(software)>)).
- **Pipelines** are approachable using concepts from Design Patterns like [Dependency Injection](https://en.wikipedia.org/wiki/Dependency_injection), [Inversion of control](https://en.wikipedia.org/wiki/Inversion_of_control) etc. A key benefit of such patterns is that, because the components are loosely coupled, they can largely be developed in isolation as small standalone units before integration into a larger application.
- **Large Language Models** currently struggle with large codebases and complex architectures (mostly due to [context window](https://en.wikipedia.org/wiki/Large_language_model#Prompt_engineering,_attention_mechanism,_and_context_window) limitations). But they are very good at building small, simple, self-contained units of code.

reused in different contexts.

_I'm writing this with GitHub Copilot enabled for markdown, which can be helpful, but sometimes it goes off on one... After the first point above it offered :_

```
This is true of a blog engine, a personal knowledge management system, a chatbot, a mail archive miner, a web scraper, a data visualizer, a data importer/exporter, a data transformer, a data validator, a data filter, a data aggregator, a data splitter, a data joiner, a data normalizer, a data summarizer, a data classifier, a data indexer, a data search engine, a data storage system, a data retrieval system, a data synchronization system, a data backup system, a data migration system, a data versioning system, a data sharing system, a data access control system, a data privacy system, a data security system, a data integrity system, a data quality system, a data governance system, a data provenance system, a data lineage system, a data lifecycle system, a data archiving system, a data retention system, a data deletion system, a data recovery system, a data discovery system, a data exploration system, a data analysis system, a data visualization system, a data reporting system, a data monitoring system, a data alerting system, a data forecasting system, a data prediction system, a data recommendation system, a data personalization system, a data optimization system, a data simulation system, a data modeling system, a data inference system, a data learning system, a data reasoning system, a data decision system, a data planning system, a data execution system, a data monitoring system, a data feedback system, a data control system, a data automation system, a data orchestration system, a data integration system, a data federation system, a data interoperability system, a data exchange system, a data transformation system, a data migration system, a data synchronization system, a data replication system, a data federation system, a data virtualization system, a data streaming system, a data batch system, a data real-time system, a data near-real-time system, a data historical system, a data temporal system, a data spatial system, a data geospatial system, a data network system, a data graph system, a data tree system, a data list system, a data set system, a data table system, a data document system, a data record system, a data entity system, a data attribute system, a data value system, a data type system, a data format system, a data encoding system, a data serialization
```

_... which is a little over the top, but it's a fair list of things that might be helped by a micro-framework like this. Remarkably the first 5 are pretty much my immediate intended uses for this._

**Docs below need updating**

for later :

The [Community Solid Server](https://github.com/CommunitySolidServer/CommunitySolidServer) uses [Components.js](https://componentsjs.readthedocs.io/en/latest/) to specify how modules and components need to be wired together at runtime. Components.js is a dependency injection framework for JavaScript applications.

## What it is

A low-level system for data processing pipelines/networks. Wherever convenient functionality will be defined declaratively with JSON-LD configuration files.

Dependency injection is used internally to allow loose coupling of components.

## What it isn't

There are several sophisticated frameworks for building interfaces between software applications and creating data processing networks. NodeRed, NoFlo etc. This is not one of them. This is much more basic and bare bones, down in the details.

See also [David Booth](https://github.com/dbooth-boston)'s [RDF Pipeline Framework](https://github.com/rdf-pipeline)

_I do eventually want to use this with NodeRed or whatever, but the entities created by transmissions will be at the level of nodes in such networks, not the network itself._

## Motivation

I'm in the process of writing yet another blog engine (Postcraft). I've also started working on a playground for interconnecting intelligent agents in an XMPP multiuser chat environment (Kia). I'm also revising a system for managing a personal knowledge base in the world of LLMs (HKMS). These all share functionality around connectivity to external data/messaging systems and internal data transformation. Might as well write this bit once only, and avoid thinking about software architecture more than I have to.

### Goals

To facilate :

- rapid development of small applications
- reuse of components in a loosely-couple environment
- versatility

### Soft Goals

- performance - low on the list
- scalability - ditto
- security - ditto

================
File: run.js
================
import yargs from 'yargs'
import { hideBin } from 'yargs/helpers'
import CommandUtils from './src/api/CommandUtils.js'
import WebRunner from './src/api/WebRunner.js'

const applicationsDir = './src/applications'
const commandUtils = new CommandUtils(applicationsDir)

async function main() {
    await yargs(hideBin(process.argv))
        .usage('Usage: ./trans <application>[.subtask] [options] [target]')
        .option('payload', {
            alias: 'P',
            describe: 'message.payload as a JSON string or a path to a JSON file',
            type: 'string',
        })
        .option('web', {
            alias: 'w',
            describe: 'Start web interface',
            type: 'boolean',
        })
        .option('port', {
            alias: 'p',
            describe: 'Port for web interface',
            type: 'number',
            default: 3000
        })
        .command('$0 [application] [target]', 'runs the specified application', (yargs) => {
            return yargs
                .positional('application', {
                    describe: 'the application to run'
                })
                .positional('target', {
                    describe: 'the target of the application'
                })
        }, async (argv) => {
            if (argv.web) {
                const webRunner = new WebRunner(applicationsDir, argv.port)
                webRunner.start()
                return
            }

            if (!argv.application) {
                console.log('Available applications:')
                const apps = await commandUtils.listApplications()
                console.log(apps.join('\n'))
                return
            }

            let message = {}
            if (argv.payload) {
                message = await CommandUtils.parseOrLoadContext(argv.payload)
            }

            await commandUtils.run(argv.application, argv.target, message)
        })
        .help('h')
        .alias('h', 'help')
        .argv
}

main().catch(console.error)
