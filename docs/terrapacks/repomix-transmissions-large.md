This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document.
Generated by Repomix on: 2025-04-01T17:30:03.273Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*
- Files matching these patterns are excluded: src/applications/claude-json-converter/data, **/_*/**, **/.env, **/old, docs, **/*repopack*, **/*repomix*, .git, node_modules, *.log, **/data/*, **/*copy.js, **/conversations.json
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------
User Provided Header:
-----------------------
Transmissions source code

================================================================
Directory Structure
================================================================
config/
  jasmine.json
  jsdoc.json
  terrapack.config.json
  webpack.config.js
src/
  api/
    cli/
      about.md
      run.js
    common/
      CommandUtils.js
    http/
      client/
        css/
          client.css
        js/
          client.js
          TransmissionsClient.js
        test-client.html
      server/
        WebRunner.js
      about.md
      openapi-spec.yaml
    about.md
  applications/
    claude-json-converter/
      about.md
      config.ttl
      transmissions copy.ttl
      transmissions.ttl
    example-application/
      about.md
      about.ttl
      config.ttl
      transmissions.ttl
    md-to-sparqlstore/
      data/
        input/
          input.md
          input2.md
      docs/
        handover-doc.md
        handover.ttl
        sparql-processors-docs.md
        test-app-docs.md
      examples/
        blog-post-rdf.txt
        sparql-queries.md
      templates/
        update-article.njk
      about.md
      config.ttl
      endpoint.json
      python-test.py
      test-queries.sh
      transmissions.ttl
    postcraft-statics/
      data/
        public/
          file1.md
          file2.md
        raw/
          file1.md
          file2.md
      about.md
      about.ttl
      config.ttl
      transmissions.ttl
    sparqlstore-to-html/
      system/
        sparql-templates/
          save-html-chunk.njk
          select-articles.njk
        endpoints.json
      templates/
        article-content.njk
        article-page.njk
        atom_template.njk
        index-page_template.njk
      about.md
      config.ttl
      transmissions.ttl
    sparqlstore-to-site-indexes/
      system/
        sparql-templates/
          recent-articles.njk
          select-articles.njk
        endpoints.json
      templates/
        article-content.njk
        article-page.njk
        index-atom.njk
        index-html.njk
      about.md
      config.ttl
      transmissions.ttl
    terrapack/
      data/
        input/
          subdir/
            subby.md
          2023-10-27_hello.md
          2025-01-08_hello-again.md
          exclude.me
      about.md
      config.ttl
      file-container-test-application.txt
      transmissions.ttl
    test/
      accumulate/
        about.md
        about.ttl
        config.ttl
        transmissions.ttl
      config-setting/
        about.md
        config.ttl
        simple.js
        transmissions.ttl
      config-setting-manifest/
        about.md
        config.ttl
        simple.js
        transmissions.ttl
      config-settings/
        about.md
        config.ttl
        test-settings-simple.js
        transmissions.ttl
      dirwalker/
        data/
          subdir/
            about-subdir.md
        about.md
        config.ttl
        transmissions.ttl
      echo/
        about.md
        config.ttl
        transmissions.ttl
      file-remove-copy/
        data/
          public/
            file1.md
            file2.md
          raw/
            file1.md
            file2.md
        about.md
        about.ttl
        config.ttl
        transmissions.ttl
      file-to-sparqlstore/
        data/
          input/
            input.md
        diamonds/
          select-blogposting.njk
          update-blogposting.njk
        docs/
          handover-doc.md
          handover.ttl
          sparql-processors-docs.md
          test-app-docs.md
        examples/
          blog-post-rdf.txt
          sparql-queries.md
        about.md
        config.ttl
        endpoint.json
        python-test.py
        test-queries.sh
        transmissions.ttl
      filereader/
        data/
          input/
            input.md
        about.md
        config.ttl
        transmissions.ttl
      foreach/
        about.md
        config.ttl
        transmissions.ttl
      restructure/
        data/
          input/
            input-01.json
          output/
            output-01.json
            output-02.json
            required-01.json
            required-02.json
        manifest/
          manifest.ttl
        about.md
        config.ttl
        simple.js
        transmissions.ttl
      stringops/
        about.md
        about.ttl
        config.ttl
        transmissions.ttl
    about.md
  engine/
    AbstractProcessorFactory.js
    ApplicationManager.js
    AppResolver.js
    ModuleLoader.js
    ModuleLoaderFactory.js
    ProcessorSettings.js
    TransmissionBuilder.js
    WorkerPool.js
  model/
    Application.js
    Connector.js
    Processor.js
    SlowableProcessor.js
    Transmission.js
    Whiteboard.js
  processors/
    example-group/
      ExampleProcessor.js
      ExampleProcessorsFactory.js
    flow/
      Accumulate.js
      DeadEnd.js
      FlowProcessorsFactory.js
      ForEach.js
      Fork.js
      Halt.js
      NOP.js
      Ping.js
      Unfork.js
    fs/
      DirWalker.js
      FileCopy.js
      FilenameMapper.js
      FileReader.js
      FileRemove.js
      FileWriter.js
      FsProcessorsFactory.js
    github/
      GitHubList_no-pag.js
      GitHubList.js
      GitHubProcessorsFactory.js
    http/
      services/
        MetricsService.js
        ShutdownService.js
      HttpClient.js
      HttpProcessorsFactory.js
      HttpProxy.js
      HttpServer.js
      HttpServerWorker.js
    json/
      Blanker.js
      JSONProcessorsFactory.js
      JsonRestructurer.js
      JSONWalker.js
      Restructure.js
      StringOps.js
    markup/
      LinkFinder.js
      MarkdownToHTML.js
      MarkupProcessorsFactory.js
      MetadataExtractor.js
    mcp/
      McpClient.js
      McpProcessorsFactory.js
      McpServer.js
    postcraft/
      MakeEntry.js
      PostcraftProcessorsFactory.js
      PrepareArticle.js
    protocols/
      HttpGet.js
      ProtocolsProcessorsFactory.js
    rdf/
      ConfigMap.js
      DatasetReader.js
      RDFConfig.js
      RDFProcessorsFactory.js
    sparql/
      bad-sparql.sparql
      config.js
      custom-predicates.js
      SessionEnvironment.js
      SPARQLProcessorsFactory.js
      SPARQLSelect.js
      SPARQLUpdate.js
      validator.js
    staging/
      MarkdownFormatter.js
      StagingProcessorsFactory.js
      TurtleFormatter.js
    system/
      EnvLoader.js
      SystemProcessorsFactory.js
    terrapack/
      comment-stripper.js
      CommentStripper.js
      file-container.js
      FileContainer.js
      terrapack-factory.js
      TerrapackProcessorsFactory.js
    test/
      TestProcessorsFactory.js
      TestSetting.js
      TestSettings.js
    text/
      LineReader.js
      StringFilter.js
      StringMerger.js
      StringReplace.js
      Templater.js
      TextProcessorsFactory.js
    unsafe/
      _RunCommand.spec.js
      BashCommand.js
      ExampleProcessor.js
      UnsafeProcessorsFactory.js
    util/
      CaptureAll.js
      SetMessage.js
      ShowConfig.js
      ShowMessage.js
      ShowSettings.js
      ShowTransmission.js
      Stash.js
      UtilProcessorsFactory.js
      WhiteboardToMessage.js
    xmpp/
      XmppClient.js
      XmppProcessorsFactory.js
    about.md
  simples/
    env-loader/
      about.md
      env-loader.js
    nop/
      nop.js
      simple-runner.js
    set-message/
      set-message.js
  tools/
    node-flow/
      editor-html.html
      ProcessorNodePublisher.js
      README.txt
      TransmissionEditor.js
      TransmissionsExporter.js
      TransmissionsGraphBuilder.js
      TransmissionsLoader.js
  utils/
    cache.js
    footpath.js
    GrapoiHelpers.js
    JSONUtils.js
    Logger.js
    MockApplicationManager.js
    ns.js
    RDFUtils.js
    StringUtils.js
    SysUtils.js
    t2j.js
    test_runner.js
    text-utils.js
staging/
  _transmissions.config.json
  schema-documentation.md
  template-cli.js
  template-generator.js
  template-tool-docs.md
  transmissions-prompt-template.md
  transmissions-template-schema.json
  transmissions-template-turtle.txt
  transmissions-testing-template.md
tests/
  applications/
    config-setting-manifest/
      manifest.ttl
    about.md
    ApplicationRunner.spec.js
    applications.json
  examples/
    test-data-usage.js
  tests-support/
    helpers/
      file-test-helper.js
      reporter.js
      test-data-generator.js
    jasmine-browser.json
  about.md
tests_pending/
  integration/
    app-context.spec.js
    application-manager.spec.js
    configmap.spec.js
    file-container-integration-test.js
    filename-mapper.spec.js
    fork.spec.js
    fs-rw_simple.spec.js
    fs-rw.spec.js
    http-server.spec.js
    markmap.spec.js
    restructure_simple.spec.js
    restructure.spec.js
    run-command.spec.js
    string-filter.spec.js
    test_apps.spec.js
    test-data-generator_string-filter.js
    test-settings-integration.js
  tests-pending/
    test_env-loader/
      2024-11-28T17-44-11.419Z/
        test-output.json
      2024-11-28T17-46-20.677Z/
        test-output.json
      2024-11-28T18-31-38.300Z/
        test-output.json
      2024-11-28T18-34-16.177Z/
        test-output.json
    test_http-server/
      2024-11-30T12-30-16.673Z/
        test-output.json
  unit/
    Application.spec.js
    file-container-unit-test.js
    filename-mapper.spec.js
    http-server_MetricsService.spec.js
    http-server_ShutdownService.spec.js
    markmap.spec..js
    NOP.spec.js
    PostcraftPrep.spec.js
    ProcessorSettings.spec.js
    RunCommand.spec.js
    StringFilter.spec.js
    StringReplace.spec.js
    test.settings.spec.js
    updated-shutdown-test.js
types/
  grapoi.d.ts
  processor.d.ts
_README.md
.babelrc
.gitignore
.npmignore
delete-triples.sh
jsconfig.json
LICENSE
package.json
postcraft.sh
README.md
tbox.sh
trans

================================================================
Files
================================================================

================
File: config/jasmine.json
================
{
    "spec_dir": "tests",
    "spec_files": [
        "**/*[sS]pec.js"
    ],
    "helpers": [
        "helpers/reporter.js"
    ],
    "stopSpecOnExpectationFailure": true,
    "random": false
}

================
File: config/jsdoc.json
================
{
    "source": {
        "include": [
            "../src"
        ],
        "exclude": [
            "../src/node_modules"
        ],
        "includePattern": ".+\\.js(doc|x)?$",
        "excludePattern": "(^|\\/|\\\\)_"
    },
    "opts": {
        "verbose": true,
        "recurse": true,
        "destination": "../docs/jsdoc"
    },
    "plugins": [
        "plugins/markdown"
    ]
}

================
File: config/terrapack.config.json
================
{
  "output": {
    "filePath": "./docs/terrapacks/terrapack-transmissions.json",
    "format": "text/plain",
    "removeComments": true,
    "summary": true
  },
  "filters": {
    "include": [
      "*.js",
      "*.jsx",
      "*.ts",
      "*.tsx",
      "*.md",
      "*.ttl",
      "*.json"
    ],
    "exclude": [
      "node_modules",
      ".git",
      "dist",
      "build",
      "coverage",
      "**/test/*",
      "**/*.test.*",
      "**/*.spec.*",
      "**/*copy*",
      "**/_*"
    ]
  }
}

================
File: config/webpack.config.js
================
// Generated using webpack-cli https://github.com/webpack/webpack-cli

const path = require('path');
const HtmlWebpackPlugin = require('html-webpack-plugin');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');
const WorkboxWebpackPlugin = require('workbox-webpack-plugin');

const isProduction = process.env.NODE_ENV == 'production';


const stylesHandler = MiniCssExtractPlugin.loader;



const config = {
    entry: './src/index.js',
    output: {
        path: path.resolve(__dirname, 'dist'),
    },
    devServer: {
        open: true,
        host: 'localhost',
    },
    plugins: [
        new HtmlWebpackPlugin({
            template: 'index.html',
        }),

        new MiniCssExtractPlugin(),

        // Add your plugins here
        // Learn more about plugins from https://webpack.js.org/configuration/plugins/
    ],
    module: {
        rules: [
            {
                test: /\.(js|jsx)$/i,
                loader: 'babel-loader',
            },
            {
                test: /\.css$/i,
                use: [stylesHandler, 'css-loader', 'postcss-loader'],
            },
            {
                test: /\.(eot|svg|ttf|woff|woff2|png|jpg|gif)$/i,
                type: 'asset',
            },

            // Add your rules for custom modules here
            // Learn more about loaders from https://webpack.js.org/loaders/
        ],
    },
};

module.exports = () => {
    if (isProduction) {
        config.mode = 'production';
        
        
        config.plugins.push(new WorkboxWebpackPlugin.GenerateSW());
        
    } else {
        config.mode = 'development';
    }
    return config;
};

================
File: src/api/cli/about.md
================
# About : CLI

`src/api/cli/*`

The CLI entry point `./trans` calls `src/api/cli/run.js` which uses [yargs](https://yargs.js.org/) - _tee hee_, they say it best :

> Yargs be a node.js library fer hearties tryin' ter parse optstrings.

`src/api/cli/run.js` then calls `src/api/common/CommandUtils.js`. That does a little bit of path-splitting and simple logic, calling on `src/core/ApplicationManager.js` to get things going.

================
File: src/api/cli/run.js
================
import yargs from 'yargs'
import { hideBin } from 'yargs/helpers'
import CommandUtils from '../common/CommandUtils.js'
import WebRunner from '../http/server/WebRunner.js'
import chalk from 'chalk'
import { readFileSync } from 'fs'
import { dirname, join } from 'path'
import { fileURLToPath } from 'url'

const __dirname = dirname(fileURLToPath(import.meta.url))
const packageJson = JSON.parse(readFileSync(join(__dirname, '../../../package.json')))
const buildInfo = process.env.BUILD_INFO || 'dev'
const version = `${packageJson.version} (${buildInfo})`

const banner = `
  _____
 |_   _| __ __ _ _ __  ___
   | || '__/ _\` | '_ \\/ __|
   | || | | (_| | | | \\__ \\
   |_||_|  \\__,_|_| |_|___/
             ${version.padStart(10).padEnd(20)}
         ${new Date().toISOString().split('T')[0]}
`

async function main() {
    console.log(chalk.cyan(banner))
    const commandUtils = new CommandUtils()

    const yargsInstance = yargs(hideBin(process.argv))
        .usage(chalk.cyan('Usage: ./trans [application][.subtask] [options] [target]\n  Run without arguments to list available applications.'))
        .option('verbose', {
            alias: 'v',
            describe: chalk.yellow('Enable verbose output'),
            type: 'boolean'
        })
        .option('silent', {
            alias: 's',
            describe: chalk.yellow('Suppress all output'),
            type: 'boolean'
        })
        .option('message', {
            alias: 'm',
            describe: chalk.yellow('Input message as JSON'),
            type: 'string',
            coerce: JSON.parse
        })
        .option('test', {
            alias: 't',
            describe: chalk.yellow('Run in test mode'),
            type: 'boolean',
            default: false
        })
        .option('web', {
            alias: 'w',
            describe: chalk.yellow('Start web interface'),
            type: 'boolean'
        })
        .option('port', {
            alias: 'p',
            describe: chalk.yellow('Port for web interface'),
            type: 'number',
            default: 4200
        })

    yargsInstance.command('$0 [application] [target]', chalk.green('runs the specified application\n'), (yargs) => {
        return yargs
            .positional('application', {
                describe: chalk.yellow('the application to run')
            })
            .positional('target', {
                describe: chalk.yellow('the target of the application')
            })
    }, async (argv) => {


        if (!argv.application) {
            console.log(chalk.cyan('Available applications:'))
            const apps = await commandUtils.listApplications()
            console.log(chalk.green(`\t${apps.join('\n\t')}\n`))

            yargsInstance.showHelp()
            return
        }
        const flags = { "web": argv.web, "port": argv.port, "verbose": argv.verbose, "silent": argv.silent, "test": argv.test }
        await commandUtils.begin(argv.application, argv.target, argv.message, flags)
    })

    await yargsInstance.argv
}

main().catch(console.error)

================
File: src/api/common/CommandUtils.js
================
// src/api/CommandUtils.js

import path from 'path'
import fs from 'fs/promises'
import logger from '../../utils/Logger.js'

import ApplicationManager from '../../engine/ApplicationManager.js'
import WebRunner from '../http/server/WebRunner.js'

class CommandUtils {

    #appManager

    constructor() {
        this.#appManager = new ApplicationManager()
    }

    async begin(application, target, message = {}, flags = {}) {

        var debugLevel = (flags.verbose || flags.test) ? "debug" : "info"
        if (!flags.verbose) logger.silent = flags.silent
        logger.setLogLevel(debugLevel)

        logger.debug('\nCommandUtils.begin')
        logger.debug('CommandUtils.begin, process.cwd() = ' + process.cwd())
        logger.debug('CommandUtils.begin, flags = ' + flags)
        // logger.reveal(flags)
        logger.debug('CommandUtils.begin, application = ' + application)
        logger.debug('CommandUtils.begin, target = ' + target)
        logger.debug(`CommandUtils.begin, message = ${message}`)

        // dir containing manifest
        if (target && !target.startsWith('/')) {
            target = path.join(process.cwd(), target)
        }

        var { appName, appPath, subtask } = CommandUtils.splitName(application)
        // short name or path (TODO or URL)

        logger.trace(`\n
    after split :
    appName = ${appName}
    appPath = ${appPath}
    subtask = ${subtask}
    target = ${target}`)

        this.#appManager = await this.#appManager.initialize(appName, appPath, subtask, target, flags)

        if (flags.web) {
            const webRunner = new WebRunner(this.#appManager, flags.port)
            await webRunner.start()
            return
        }

        return await this.#appManager.start(message)
    }

    static splitName(fullPath) {
        logger.debug(`\nCommandUtils.splitName, fullPath  = ${fullPath}`)
        const parts = fullPath.split(path.sep)
        logger.debug(`\nCommandUtils.splitName, parts  = ${parts}`)
        var lastPart = parts[parts.length - 1]

        var task = false
        if (lastPart.includes('.')) {
            const split = lastPart.split('.')
            task = split[1]
            lastPart = split[0]
        }
        var appPath = parts.slice(0, parts.length - 1).join(path.sep)
        appPath = path.join(appPath, lastPart)
        //  logger.debug(`\nCommandUtils.splitName, parts.slice(0, parts.length - 1) = ${parts.slice(0, parts.length - 1)}`)

        // const appPath = parts.join(path.sep)
        logger.debug(`CommandUtils.splitName, appName:${lastPart}, appPath:${appPath}, task:${task},`)

        return { appName: lastPart, appPath: appPath, task: task }
    }

    async listApplications() {
        return await this.#appManager.listApplications()
    }

    // TODO appears to be unused
    static async parseOrLoadContext(contextArg) { // TODO rename context -> message
        logger.debug(`CommandUtils.parseOrLoadContext(), contextArg = ${contextArg}`)
        let message = {}
        try {
            message.payload = JSON.parse(contextArg)
        } catch (err) {
            logger.debug('*** Loading JSON from file...')
            const filePath = path.resolve(contextArg)
            const fileContent = await fs.readFile(filePath, 'utf8')
            message.payload = JSON.parse(fileContent)
        }
        return message
    }
}

export default CommandUtils

================
File: src/api/http/client/css/client.css
================
body {
    font-family: system-ui, -apple-system, sans-serif;
    max-width: 800px;
    margin: 2rem auto;
    padding: 0 1rem;
    line-height: 1.5;
}

.container {
    display: grid;
    gap: 1rem;
}

.form-group {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}

label {
    font-weight: 500;
}

input,
textarea {
    padding: 0.5rem;
    border: 1px solid #ccc;
    border-radius: 4px;
    font-size: 14px;
    font-family: monospace;
}

textarea {
    min-height: 120px;
    resize: vertical;
}

button {
    padding: 0.75rem 1rem;
    background: #0066cc;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    font-weight: 500;
    transition: background-color 0.2s;
}

button:hover:not(:disabled) {
    background: #0052a3;
}

button:disabled {
    background: #ccc;
    cursor: not-allowed;
}

pre {
    background: #f5f5f5;
    padding: 1rem;
    overflow-x: auto;
    border-radius: 4px;
    margin: 0;
    font-size: 14px;
}

.status {
    padding: 1rem;
    margin: 0;
    border-radius: 4px;
    font-size: 14px;
}

.status.error {
    background: #fff5f5;
    color: #c53030;
    border: 1px solid #feb2b2;
}

.status.success {
    background: #f0fff4;
    color: #276749;
    border: 1px solid #9ae6b4;
}

.status.info {
    background: #ebf8ff;
    color: #2c5282;
    border: 1px solid #90cdf4;
}

.metrics {
    display: grid;
    grid-template-columns

================
File: src/api/http/client/js/client.js
================
import TransmissionsClient from './TransmissionsClient.js'

class TestClientUI {
    constructor() {
        this.elements = {
            baseUrl: document.getElementById('baseUrl'),
            application: document.getElementById('application'),
            message: document.getElementById('message'),
            sendButton: document.getElementById('sendButton'),
            status: document.getElementById('status'),
            response: document.getElementById('response'),
            metrics: document.getElementById('metrics')
        }

        // Ensure baseUrl has a value
        if (!this.elements.baseUrl.value) {
            this.elements.baseUrl.value = 'http://localhost:4200/api'
        }

        this.initialize()
        this.bindEvents()
    }

    initialize() {
        try {
            this.client = new TransmissionsClient(this.elements.baseUrl.value)
            this.updateStatus(true)
            this.startMetricsUpdate()
        } catch (err) {
            console.error('Initialization error:', err)
            this.handleError(err)
        }
    }

    bindEvents() {
        this.elements.sendButton.addEventListener('click', () => this.sendRequest())
        this.elements.baseUrl.addEventListener('change', () => {
            try {
                this.client.setBaseUrl(this.elements.baseUrl.value)
                this.updateStatus(true)
            } catch (err) {
                this.handleError(err)
            }
        })
    }

    updateStatus(isOnline) {
        const { status: statusEl, sendButton } = this.elements
        if (isOnline) {
            statusEl.className = 'status success'
            statusEl.textContent = 'Server online'
            sendButton.disabled = false
        } else {
            statusEl.className = 'status error'
            statusEl.textContent = 'Server offline'
            sendButton.disabled = true
        }
    }

    handleError(error) {
        const { status: statusEl } = this.elements
        statusEl.className = 'status error'
        statusEl.textContent = `Error: ${error.message}`
        console.error('Client error:', error)
    }

    startMetricsUpdate() {
        setInterval(() => {
            if (this.client) {
                const metrics = this.client.getMetrics()
                this.updateMetricsDisplay(metrics)
            }
        }, 1000)
    }

    updateMetricsDisplay(metrics) {
        if (!metrics) return

        this.elements.metrics.innerHTML = `
            <div class="metric-card">
                <div>Requests</div>
                <div class="metric-value">${metrics.requests}</div>
            </div>
            <div class="metric-card">
                <div>Errors</div>
                <div class="metric-value">${metrics.errors}</div>
            </div>
            <div class="metric-card">
                <div>Uptime</div>
                <div class="metric-value">${metrics.uptime}s</div>
            </div>
        `
    }

    async sendRequest() {
        const { application, message, response: responseEl, status: statusEl, sendButton } = this.elements

        try {
            let messageData
            try {
                messageData = JSON.parse(message.value)
            } catch (err) {
                throw new Error('Invalid JSON in message field')
            }

            statusEl.className = 'status info'
            statusEl.textContent = 'Sending request...'
            sendButton.disabled = true

            const result = await this.client.runApplication(application.value, messageData)

            if (result.success) {
                statusEl.className = 'status success'
                statusEl.textContent = 'Request successful'
                responseEl.textContent = JSON.stringify(result.data, null, 2)
            } else {
                throw new Error(result.error || 'Request failed')
            }
        } catch (err) {
            statusEl.className = 'status error'
            statusEl.textContent = `Error: ${err.message}`
            responseEl.textContent = ''
            console.error('Request error:', err)
        } finally {
            sendButton.disabled = false
        }
    }
}

// Initialize the UI when the page loads
const ui = new TestClientUI()

// Handle global errors
window.addEventListener('unhandledrejection', event => {
    console.error('Unhandled promise rejection:', event.reason)
    if (ui) {
        ui.handleError({
            message: 'Unhandled error: ' + event.reason.message
        })
    }
})

================
File: src/api/http/client/js/TransmissionsClient.js
================
class TransmissionsClient {
    constructor(baseUrl) {
        if (!baseUrl) {
            throw new Error('Base URL is required')
        }
        this.baseUrl = baseUrl
        this.metrics = {
            requests: 0,
            errors: 0,
            startTime: Date.now()
        }
    }

    async runApplication(application, message = {}) {
        if (!application) {
            throw new Error('Application name is required')
        }

        try {
            this.metrics.requests++
            console.log('Sending request:', {
                url: `${this.baseUrl}/${application}`,
                body: message
            })

            const response = await fetch(`${this.baseUrl}/${application}`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(message)
            })

            if (!response.ok) {
                this.metrics.errors++
                const error = await response.json()
                throw new Error(error.message || 'Request failed')
            }

            return await response.json()
        } catch (err) {
            this.metrics.errors++
            throw err
        }
    }

    getMetrics() {
        return {
            requests: this.metrics.requests,
            errors: this.metrics.errors,
            uptime: Math.floor((Date.now() - this.metrics.startTime) / 1000)
        }
    }

    setBaseUrl(url) {
        if (!url) {
            throw new Error('Base URL is required')
        }
        this.baseUrl = url
    }
}

export default TransmissionsClient

================
File: src/api/http/client/test-client.html
================
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Transmissions API Test Client</title>
    <link rel="icon" href="data:,">
    <link rel="stylesheet" href="css/client.css">
</head>

<body>
    <h1>Transmissions API Test Client</h1>

    <div class="container">
        <div class="form-group">
            <label for="baseUrl">Base URL:</label>
            <input type="url" id="baseUrl" />
        </div>

        <div class="form-group">
            <label for="application">Application:</label>
            <input type="text" id="application" value="echo" />
        </div>

        <div class="form-group">
            <label for="message">Message (JSON):</label>
            <textarea id="message">{
    "message": "Hello from test client"
}</textarea>
        </div>

        <button id="sendButton">Send Request</button>

        <div id="status" class="status info">Initializing client...</div>

        <div class="metrics" id="metrics">
        </div>

        <div class="form-group">
            <label>Response:</label>
            <pre id="response"></pre>
        </div>
    </div>

    <script type="module" src="js/client.js"></script>
</body>

</html>

================
File: src/api/http/server/WebRunner.js
================
import express from 'express'
import cors from 'cors'
import path from 'path'
import ApplicationManager from '../../../engine/ApplicationManager.js'
import logger from '../../../utils/Logger.js'

class WebRunner {
    constructor(appManager, port = 4000, basePath = '/api') {
        this.appManager = appManager
        this.app = express()
        this.port = port
        this.basePath = basePath
        this.setupMiddleware()
        this.setupRoutes()
        this.requestCount = 0
    }

    setupMiddleware() {
        // CORS setup
        const corsOptions = {
            origin: (origin, callback) => {
                if (!origin || origin.startsWith('http://localhost')) {
                    callback(null, true)
                } else {
                    callback(new Error('Not allowed by CORS'))
                }
            },
            methods: ['GET', 'POST', 'OPTIONS'],
            allowedHeaders: ['Content-Type'],
            credentials: true
        }
        this.app.use(cors(corsOptions))

        // JSON body parsing
        this.app.use(express.json({
            limit: '10mb',
            strict: false,
            verify: (req, res, buf) => {
                try {
                    JSON.parse(buf)
                } catch (e) {
                    logger.error('Invalid JSON:', e)
                }
            }
        }))
    }

    setupRoutes() {
        const router = express.Router()

        router.post('/:application', async (req, res) => {
            const requestId = Math.random().toString(36).substring(7)
            const { application } = req.params
            const message = req.body || {}

            logger.info(`[${requestId}] Running application: ${application}`)
            logger.debug(`[${requestId}] Message payload:`, message)

            try {
                if (!this.appManager) {
                    throw new Error('Application manager not initialized')
                }
                logger.debug(`[${requestId}] Initializing application ${application}`)
                await this.appManager.initialize(application)
                message.requestId = requestId
                logger.debug(`[${requestId}] Starting application with message:`, message)
                const result = await this.appManager.start(message)
                if (!result) {
                    throw new Error('Application returned no result')
                }
                logger.debug(`[${requestId}] Application result:`, result)
                const response = {
                    success: true,
                    requestId: requestId,

                    data: result
                    /*result.whiteboard ?
                        result.whiteboard[result.whiteboard.length - 1] :
                        { message: "Echo response" }
                */
                }
                logger.info(`[${requestId}] Application ${application} completed successfully`)
                res.json(response)

            } catch (error) {
                const errorResponse = {
                    success: false,
                    requestId: requestId,
                    error: error.message,
                    details: error.stack,
                    application: application
                }
                logger.log(`error = `)
                logger.reveal(error)
                logger.log(`errorResponse = `)
                logger.reveal(errorResponse)
                logger.error(`[${requestId}] Error running application ${application}:`, error)
                logger.error(`[${requestId}] Stack:`, error.stack)
                logger.debug(`[${requestId}] Context:`, {
                    application,
                    message,
                    headers: req.headers
                })

                res.status(500).json(errorResponse)
            }
        })

        this.app.use(this.basePath, router)
    }

    async start() {
        return new Promise((resolve, reject) => {
            try {
                this.server = this.app.listen(this.port, () => {
                    const endpoint = `http://localhost:${this.port}${this.basePath}`
                    const msg = `Transmissions API server running at ${endpoint}`
                    logger.info('\n' + '='.repeat(msg.length))
                    logger.info(msg)
                    logger.info('='.repeat(msg.length) + '\n')
                    resolve()
                })

                this.server.on('error', (error) => {
                    logger.error('Server error:', error)
                    reject(error)
                })
            } catch (error) {
                logger.error('Failed to start server:', error)
                reject(error)
            }
        })
    }

    async stop() {
        return new Promise((resolve, reject) => {
            if (this.server) {
                logger.info('Shutting down server...')
                this.server.close((err) => {
                    if (err) {
                        logger.error('Error shutting down server:', err)
                        reject(err)
                    } else {
                        logger.info('Server shutdown complete')
                        resolve()
                    }
                })
            } else {
                resolve()
            }
        })
    }
}

export default WebRunner

================
File: src/api/http/about.md
================
```sh
cd ~/hyperdata/transmissions/
./trans --verbose --web --port 4200 echo

./trans -v -w -p 4200 echo

./trans -v -w -t -p 4200 echo
```

compare with :

```sh
./trans  -v echo
```

#:todo move echo and all the test\_ to applications/system

================
File: src/api/http/openapi-spec.yaml
================
openapi: 3.0.0
info:
  title: Transmissions API
  version: '1.0.0'
  description: API for running Transmissions applications

servers:
  - url: http://localhost:4200/api
    description: Local development server

paths:
  /:
    get:
      summary: Get server status
      responses:
        '200':
          description: Server status information
          content:
            application/json:
              schema:
                type: object
                properties:
                  service:
                    type: string
                    example: 'Transmissions API'
                  version:
                    type: string
                    example: '1.0.0'
                  status:
                    type: string
                    example: 'running'

  /applications:
    get:
      summary: List available applications
      responses:
        '200':
          description: List of available applications
          content:
            application/json:
              schema:
                type: object
                properties:
                  success:
                    type: boolean
                  applications:
                    type: array
                    items:
                      type: string
                example:
                  success: true
                  applications: ['system/echo', 'test/example']
        '500':
          $ref: '#/components/responses/Error'

  /{application}:
    post:
      summary: Run a Transmissions application
      parameters:
        - name: application
          in: path
          required: true
          schema:
            type: string
          description: Application identifier
          example: 'system/echo'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              description: Application-specific message payload
      responses:
        '200':
          description: Successful application execution
          content:
            application/json:
              schema:
                type: object
                properties:
                  success:
                    type: boolean
                  data:
                    type: object
                    description: Application-specific response data
                example:
                  success: true
                  data:
                    message: "Echo response"
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                type: object
                properties:
                  success:
                    type: boolean
                    example: false
                  error:
                    type: string
                    example: 'Invalid JSON payload'
        '500':
          $ref: '#/components/responses/Error'

components:
  responses:
    Error:
      description: Server error
      content:
        application/json:
          schema:
            type: object
            properties:
              success:
                type: boolean
                example: false
              error:
                type: string
                example: 'Internal server error'

================
File: src/api/about.md
================
# transmissions/src/api/

Interfaces for running transmissions.

================
File: src/applications/claude-json-converter/about.md
================
```sh
cd ~/hyperdata/transmissions/

./trans -v claude-json-converter -m '{"sourceFile":"/home/danny/github-danny/hyperdata/docs/chat-archives/data-2025-02-02-12-23-16/conversations.json"}'

./trans claude-json-converter
```

---

```turtle
####  testing only
:nop a trn:Transmission ;
    rdfs:label "nop" ;
    rdfs:comment "NOP for testing" ;
trn:pipe (:n10) .

:n10 a :NOP .

# testing only - FileWriter will save message
:cb a trn:Transmission ;
     rdfs:label "cb" ;
     rdfs:comment "Claude blanker" ;
     trn:pipe (:ccc10   :cb10 :cb20 :cb30) .

:cb10 a :SetMessage ;
     trn:settings :setDump .

:cb20 a :FileWriter .

:cb30 a :Blanker ; # clear values
     trn:settings :blankContent .

##################
##################### only for testing
:bContent a :ConfigSet ;
    rdfs:label "Root node in JSON object for blanker" ;
    :settings :blankContent  ;
    :pointer "content"  ;
    :preserve "content.payload.test.third" .

:setDump a :ConfigSet ;
    :setValue (:sv0)  . # consider using blank nodes
    :sv0   :key    "dump" ;
            :value  "true"  .
#########################################################################
```

#####################################

After `FileReader` (and `Blanker`):

```
{
    // system message bits,

    "content": [
        {
            "uuid": "",
            "name": "",
            "created_at": "",
            "updated_at": "",
            "account": {
                "uuid": ""
            },
            "chat_messages": [
                {
                    "uuid": "",
                    "text": "",
                    "content": [
                        {
                            "type": "",
                            "text": ""
                        }
                    ],
                    "sender": "",
                    "created_at": "",
                    "updated_at": "",
                    "attachments": [],
                    "files": [
                        {
                            "file_name": ""
                        }
                    ]
                },
                {
                    ...
                }
            ]
        }
}
```

`JSONWalker` fires off a message per-conversation.

These need `Restructure` to split off the common metadata as `message.content`, and move `chat_messages` to `message.content`, ready for -

`JSONWalker` fires off a message per-conversation.

================
File: src/applications/claude-json-converter/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .




# should really be in a manifest.ttl
:ReadFile a :ConfigSet ;
    rdfs:label "Read file" ;
    :settings :readFile ;
    :sourceFile "input/conversations.json" ;
  #  :sourceFile "input/input-01.json" ;
    :mediaType "application/json" .


:ConversationsWalker a :ConfigSet ;
# :die "true" ;
    :pointer "content" .


:retreeConvsssssssnot a :ConfigSet ;
    :rename (:pp100 :pp101 :pp102  :pp103) .
    :pp100     :pre     "content.uuid" ;
                :post    "meta.conv_uuid"  .
    :pp101     :pre     "content.name" ;
                :post    "meta.conv_name"  .
    :pp102     :pre     "content.updated_at" ;
                :post    "meta.updated_at"  .
    :pp103     :pre     "content.chat_messages" ;
                :post    "content"  .

:retreeConvs a :ConfigSet ;
    :rename (:pp100 :pp101 :pp102  :pp103) .
    :pp100     :pre     "content.uuid" ;
                :post    "meta.conv_uuid"  .
    :pp101     :pre     "content.name" ;
                :post    "meta.conv_name"  .
    :pp102     :pre     "content.updated_at" ;
                :post    "meta.updated_at"  .
    :pp103     :pre     "content.chat_messages" ;
                :post    "content"  .

  :MessagesWalker a :ConfigSet ;
      :pointer "content" .

# unused
:retreeMsgs a :ConfigSet ;
    :rename (:pp200 :pp201 :pp202) .

    :pp200     :pre     "content.item.chat_messages" ;
                :post    "channel"  .

    :pp201     :pre     "content.item.uuid" ;
                :post    "filename"  .

    :pp202     :pre     "content.item.name" ;
                :post    "title"  .

:Writer a :ConfigSet ;
    :destinationFile "DESTINATION" .

================
File: src/applications/claude-json-converter/transmissions copy.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trn: <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one

# testing only - FileWriter will save message
:cb a trn:Transmission ;
     rdfs:label "cb" ;
     rdfs:comment "Claude blanker" ;
     trn:pipe (:cb10 :cb20 :cb30) .

:cb10 a :SetMessage ;
     trn:settings :setDump .

:cb20 a :FileWriter .

:cb30 a :Blanker ; # clear values
     trn:settings :blankContent .

# :UF :SD :FW :DE
####################################

:nop a trn:Transmission ;
    rdfs:label "nop" ;
    rdfs:comment "NOP for testing" ;
trn:pipe (:n10) .

:n10 a :NOP .

####### The thing

:ccc a trn:Transmission ;
    rdfs:label "ccc" ;
    rdfs:comment "Claude conversations.json converter" ;
     trn:pipe (:ccc10 :ccc20 :ccc30 :ccc40 :ccc50  :ccc60) .

# Start

:ccc10 a :FileReader ; # Claude conversations.json
       trn:settings :readFile .

# Separates into conversations
:ccc20 a :JSONWalker ;
     trn:settings :conversationsConfig .


:ccc30 a :Restructure ;
     trn:settings :retreeConvs .

# Separates into messages
:ccc40 a :JSONWalker ;
     trn:settings :messagesConfig .

#:p50 a :Restructure ;
 #    trn:settings :retreeMsgs .

:ccc50 a :MarkdownFormatter .

:ccc60 a :FileWriter .

================
File: src/applications/claude-json-converter/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trn: <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one


####### The thing

:ccc a trn:Transmission ;
     rdfs:label "ccc" ;
     rdfs:comment "Claude conversations.json converter" ;
     trn:pipe (:ccc10 :ccc20 :ccc30 :ccc40 :ccc50 :ccc60) .

# Start

:ccc10 a :FileReader ; # Claude conversations.json
       trn:settings :readFile .

# Separates into conversations
:ccc20 a :JSONWalker ;
     trn:settings :ConversationsWalker .


:ccc30 a :Restructure ;
     trn:settings :retreeConvs .

# Separates into messages
  :ccc40 a :JSONWalker ;
       trn:settings :MessagesWalker .

#:p50 a :Restructure ;
 #    trn:settings :retreeMsgs .

:ccc50 a :MarkdownFormatter .

:ccc60 a :FileWriter ;
     trn:settings :Writer .

================
File: src/applications/example-application/about.md
================
`src/applications/example-application/about.md`

# Example Application `about.md`

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans example-application
```

## Description

---

================
File: src/applications/example-application/about.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

================
File: src/applications/example-application/config.ttl
================
# src/applications/example-application/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:exampleSettings1 a :ConfigSet ;
    :me "exampleSettings1" ;
    :common "common from 1"  ;
    :something1 "something1 from config" .

:exampleSettings2 a :ConfigSet ;
    :me "exampleSettings2" ;
    :common "common from 2" ;
    :something2 "something2 from config" ;
    :added " added from exampleSettings2" .

================
File: src/applications/example-application/transmissions.ttl
================
# src/applications/example-application/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:SM a :ShowMessage . # verbose report, continues pipe            #
:SC a :ShowConfig . # verbose report, continues pipe             #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:example a :Transmission ;
    :pipe (:p10 :N :p20 :p30) .

:p10 a :ExampleProcessor ;
     :settings :exampleSettings1 .

:p20 a :ExampleProcessor ;
     :settings :exampleSettings2 .

:p30 a :ShowMessage .

================
File: src/applications/md-to-sparqlstore/data/input/input.md
================
`src/applications/md-to-sparqlstore/data/input/input.md`

# First Article

This is a test blog post that will be converted to RDF and stored in the SPARQL database.

## Content

Lorem ipsum dolor sit amet, consectetur adipiscing elit.

================
File: src/applications/md-to-sparqlstore/data/input/input2.md
================
`src/applications/md-to-sparqlstore/data/input/input2.md`

# Second Article

This is a test blog post that will be converted to RDF and stored in the SPARQL database.

## Content

Lorem ipsum

================
File: src/applications/md-to-sparqlstore/docs/handover-doc.md
================
# SPARQL Integration Handover Document

## New Components Added

### 1. SPARQL Processors
- **SPARQLSelect.js**: Query processor with template support
- **SPARQLUpdate.js**: Update processor with template support
- **SPARQLProcessorsFactory.js**: Factory for processor instantiation

### 2. Test Application
- Location: src/applications/test_file-to-sparqlstore/
- Purpose: End-to-end testing of SPARQL integration
- Integration with FileReader for markdown processing

### 3. Configuration Files
- endpoint.json: SPARQL endpoint configuration
- Test data and templates under diamonds/
- SPARQL query/update templates

## Key Technical Details

### Authentication
- Uses Basic Auth
- Credentials in endpoint.json
- Separate configs for query/update endpoints

### Data Model
- Uses schema.org vocabulary
- BlogPosting as primary type
- Nested author information
- Timestamps for created/modified

### Error Handling
- Network failures
- Authentication errors
- Query validation
- Template rendering errors

## Testing

### Automated Tests
- Unit tests for processors
- Integration tests for pipeline
- Template validation

### Manual Testing
- Test scripts in bash/Python
- Example queries
- Curl commands for direct testing

## Dependencies
- axios for HTTP
- nunjucks for templates
- rdf-ext for RDF handling

## Known Issues/TODOs
1. Template caching not implemented
2. Bulk operations not optimized
3. Add transaction support
4. Enhance error reporting

================
File: src/applications/md-to-sparqlstore/docs/handover.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix prj: <http://purl.org/stuff/project/> .
@prefix trn: <http://purl.org/stuff/transmissions/> .
@prefix code: <http://purl.org/stuff/code/> .

# Project Component Description
trn:SPARQLIntegration a prj:Component ;
    dcterms:title "SPARQL Integration for Transmissions" ;
    dcterms:description "SPARQL query and update processors with test application" ;
    dcterms:created "2024-01-16"^^xsd:date ;
    prj:status "Testing" ;
    prj:version "1.0.0" ;
    prj:maintainer <http://danny.ayers.name> ;
    prj:documentation trn:SPARQLDocs .

# Documentation
trn:SPARQLDocs a prj:Documentation ;
    prj:hasSection trn:ProcessorDocs, trn:TestAppDocs, trn:ConfigDocs .

trn:ProcessorDocs a prj:DocumentationSection ;
    dcterms:title "SPARQL Processors" ;
    prj:covers trn:SPARQLSelect, trn:SPARQLUpdate ;
    prj:location "/src/processors/sparql/" .

# Components
trn:SPARQLSelect a code:Processor ;
    dcterms:title "SPARQL Select Processor" ;
    code:implements trn:QueryExecution ;
    code:dependsOn trn:EndpointConfig ;
    code:hasTest trn:SelectTests .

trn:SPARQLUpdate a code:Processor ;
    dcterms:title "SPARQL Update Processor" ;
    code:implements trn:UpdateExecution ;
    code:dependsOn trn:EndpointConfig ;
    code:hasTest trn:UpdateTests .

# Test Application
trn:TestApp a code:Application ;
    dcterms:title "SPARQL Store Test" ;
    code:location "/src/applications/test_file-to-sparqlstore/" ;
    code:uses trn:SPARQLSelect, trn:SPARQLUpdate ;
    code:hasConfig trn:EndpointConfig .

# Configuration
trn:EndpointConfig a code:Configuration ;
    dcterms:title "SPARQL Endpoint Configuration" ;
    code:format "JSON" ;
    code:location "endpoint.json" ;
    code:template [
        code:field "type" ;
        code:required true ;
        code:allowedValues "query", "update"
    ], [
        code:field "url" ;
        code:required true ;
        rdfs:comment "SPARQL endpoint URL"
    ] .

# Known Issues
trn:Issues a prj:IssueList ;
    prj:hasIssue [
        a prj:TODO ;
        dcterms:title "Template Caching" ;
        prj:priority "Medium"
    ], [
        a prj:TODO ;
        dcterms:title "Transaction Support" ;
        prj:priority "High"
    ] .

================
File: src/applications/md-to-sparqlstore/docs/sparql-processors-docs.md
================
# SPARQL Processors Documentation

## Overview
The SPARQL processors provide functionality for interacting with SPARQL endpoints through the Transmissions pipeline framework. Two main processors are provided:
- SPARQLSelect: Executes SELECT queries
- SPARQLUpdate: Executes UPDATE operations

## Configuration
Configuration is managed through endpoint.json:
```json
{
    "name": "local query",
    "type": "query|update",
    "url": "http://localhost:3030/dataset/query",
    "credentials": {
        "user": "username",
        "password": "password"
    }
}
```

## SPARQLSelect Processor
Executes templated SELECT queries against a SPARQL endpoint.

### Usage
```turtle
:query a :Transmission ;
    :pipe (:p10) .

:p10 a :SPARQLSelect ;
    :settings [
        :templateFilename "queries/select.njk" ;
        :endpointSettings "endpoint.json"
    ] .
```

### Template Variables
- startDate: ISO datetime for filtering
- Additional variables from message object

## SPARQLUpdate Processor
Executes templated UPDATE operations against a SPARQL endpoint.

### Usage
```turtle
:update a :Transmission ;
    :pipe (:p10) .

:p10 a :SPARQLUpdate ;
    :settings [
        :templateFilename "queries/update.njk" ;
        :endpointSettings "endpoint.json"
    ] .
```

### Template Variables
- id: Auto-generated UUID
- title: From message.meta.title
- content: From message.content
- published/modified: Current timestamp
- author: From message.meta.author

## Error Handling
- Connection failures throw network errors
- Authentication failures throw 401/403 errors
- Invalid queries throw 400 errors
- All errors include detailed messages in logs

================
File: src/applications/md-to-sparqlstore/docs/test-app-docs.md
================
# SPARQL Store Test Application

## Purpose
Tests complete pipeline functionality for reading files, converting to RDF, storing in a SPARQL database, and verifying storage through queries.

## Quick Start
1. Configure SPARQL endpoint in endpoint.json
2. Place test markdown in data/input/input.md
3. Run application:
```bash
./trans test_file-to-sparqlstore
```

## Components
1. FileReader processor:
   - Reads input markdown
   - Extracts metadata and content

2. SPARQLUpdate processor:
   - Converts markdown to RDF using schema.org vocabulary
   - Stores in SPARQL database

3. SPARQLSelect processor:
   - Queries stored data
   - Verifies successful storage

## Testing
### Manual Testing
Use provided test scripts:
```bash
# Using bash script
./test-queries.sh

# Using Python script
python3 test-queries.py
```

### Example Queries
```sparql
# Find recently added posts
SELECT ?post ?title WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?title ;
        schema:datePublished ?date .
  FILTER(?date >= "2024-01-01T00:00:00Z"^^xsd:dateTime)
}
```

## Configuration
1. endpoint.json: SPARQL endpoint details
2. config.ttl: Transmission configuration
3. transmissions.ttl: Pipeline definition
4. diamonds/*.njk: Query templates

## Error Cases Handled
- Missing input files
- SPARQL endpoint connection failures
- Authentication errors
- Invalid markdown format
- Failed data verification

================
File: src/applications/md-to-sparqlstore/examples/blog-post-rdf.txt
================
@prefix schema: <http://schema.org/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

<http://example.com/posts/fb5e0595-2e98-4c5c-9876-7f402c6439a2> 
    a schema:BlogPosting ;
    schema:headline "Test Blog Post" ;
    schema:description """Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor 
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis 
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.""" ;
    schema:datePublished "2024-01-16T15:55:43.049Z"^^xsd:dateTime ;
    schema:dateModified "2024-01-16T15:55:43.049Z"^^xsd:dateTime ;
    schema:author [
        a schema:Person ;
        schema:name "Test User" ;
        schema:email "test@example.com"
    ] ;
    schema:articleBody """# Test Blog Post

This is a test blog post that will be converted to RDF and stored in the SPARQL database.

## Metadata
- Title: Test Blog Post  
- Author: Test User
- Email: test@example.com

## Content
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor 
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis 
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.""" .

================
File: src/applications/md-to-sparqlstore/examples/sparql-queries.md
================
# Query all blog posts
PREFIX schema: <http://schema.org/>
SELECT ?post ?title ?date ?author WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?title ;
        schema:datePublished ?date ;
        schema:author/schema:name ?author .
} ORDER BY DESC(?date)

# Query posts by specific author
PREFIX schema: <http://schema.org/>
SELECT ?post ?title ?date WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?title ;
        schema:datePublished ?date ;
        schema:author [ schema:name "Test User" ] .
} ORDER BY DESC(?date)

# Query posts in date range
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
SELECT ?post ?title ?date WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?title ;
        schema:datePublished ?date .
  FILTER(?date >= "2024-01-01T00:00:00Z"^^xsd:dateTime && 
         ?date <= "2024-12-31T23:59:59Z"^^xsd:dateTime)
} ORDER BY DESC(?date)

# Update/Insert new blog post
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
INSERT DATA {
  <http://example.com/posts/test-123> a schema:BlogPosting ;
    schema:headline "Test Post" ;
    schema:description "Test content" ;
    schema:datePublished "2024-01-16T10:00:00Z"^^xsd:dateTime ;
    schema:dateModified "2024-01-16T10:00:00Z"^^xsd:dateTime ;
    schema:author [
      a schema:Person ;
      schema:name "Test User" ;
      schema:email "test@example.com"
    ] .
}

# Delete a blog post
PREFIX schema: <http://schema.org/>
DELETE WHERE {
  <http://example.com/posts/test-123> ?p ?o .
  OPTIONAL { ?o ?p2 ?o2 }
}

================
File: src/applications/md-to-sparqlstore/templates/update-article.njk
================
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX : <http://purl.org/stuff/transmissions/>

# https://schema.org/BlogPosting
# https://schema.org/Article

INSERT DATA {
  <{{uri}}> a schema:Article ;
      :sourcePath "{{sourcePath}}" ;
      :relPath "{{relPath}}" ;
      :slug "{{slug}}" ;
      schema:headline "{{title}}" ;
      schema:dateCreated "{{dates.created}}"^^xsd:dateTime ;
      schema:dateModified "{{dates.read}}"^^xsd:dateTime ;
      schema:articleBody """{{content}}""" ;
      schema:creator <{{creator.uri}}> .
  <{{creator.uri}}> a schema:Person ;
      schema:name "{{creator.name}}" .
}

================
File: src/applications/md-to-sparqlstore/about.md
================
`src/applications/md-to-sparqlstore/about.md`

# Application 'md-to-sparqlstore'

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans md-to-sparqlstore
```

## Description

1. Reads a file from fs
2. Templates it using nunjucks into a SPARQL UPDATE query
3. POSTs this to the specified endpoint
4. Does a SPARQL SELECT query (based on date) to retrieve data
5. Compares this with the original content to ensure it is in the store

Data looks something like :

```turtle
@prefix schema: <http://schema.org/> .

<http://example.com/posts-one> a schema:BlogPosting ;
    schema:headline "Post one" ;
    schema:url <http://example.com/posts-one> ;
    schema:description "Post one content." ;
    schema:datePublished "2023-05-22T13:00:00Z"^^xsd:dateTime ;
    schema:dateModified "2023-05-22T15:00:00Z"^^xsd:dateTime ;
    schema:author [
        a schema:Person ;
        schema:name "John Doe" ;
        schema:email "johndoe@example.com"
    ] .
```

## Notes

TODO complete -

src/applications/test_file-to-sparqlstore
├── about.md
├── config.ttl
├── data
│   ├── input
│   │   └── input.md
│   └── output
├── diamonds
│   ├── select-blogposting.njk
│   └── update-blogposting.njk
├── endpoint.json
└── transmissions.ttl

src/processors/sparql
├── about.md
├── SPARQLProcessorsFactory.js
├── SPARQLSelect.js
└── SPARQLUpdate.js

================
File: src/applications/md-to-sparqlstore/config.ttl
================
# src/applications/test_file-to-sparqlstore/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .


:mts10-dirWalker a :ConfigSet .
 # :sourceDir "." .

:readerSet a :ConfigSet ;
    :metaField "meta" .

:entryExtras a :ConfigSet ;
    :relMap "blog" ; # relPath goes to this
    :baseURI <http://danny.ayers.name> ;
    :creatorName "Danny" ;
    :creatorURI <http://danny.ayers.name/me> .

# need sensible/test defaults
#:mts40-storeArticle a :ConfigSet ;
 #   :dataBlock "contentBlocks" ;
 #   :templateFilename "layouts/base/templates/store-article.njk" ;
  #  :endpointSettings "endpoint.json" .

================
File: src/applications/md-to-sparqlstore/endpoint.json
================
[
    {
        "name": "local query",
        "type": "query",
        "url": "http://localhost:4030/semem/query",
        "credentials": {
            "user": "admin",
            "password": "admin123"
        }
    },
    {
        "name": "local update",
        "type": "update",
        "url": "http://localhost:4030/semem/update",
        "credentials": {
            "user": "admin",
            "password": "admin123"
        }
    }
]

================
File: src/applications/md-to-sparqlstore/python-test.py
================
#!/usr/bin/env python3
import requests
import json
from base64 import b64encode
from datetime import datetime

# Configuration
ENDPOINT_QUERY = "http://localhost:3030/test/query"
ENDPOINT_UPDATE = "http://localhost:3030/test/update" 
AUTH_USER = "admin"
AUTH_PASS = "admin123"

# Authentication header
auth_string = b64encode(f"{AUTH_USER}:{AUTH_PASS}".encode()).decode()
HEADERS = {
    'Authorization': f'Basic {auth_string}',
    'Accept': 'application/json'
}

def run_query(query):
    """Execute a SPARQL query and return results"""
    headers = {**HEADERS, 'Content-Type': 'application/sparql-query'}
    response = requests.post(ENDPOINT_QUERY, 
                           headers=headers,
                           data=query)
    response.raise_for_status()
    return response.json()

def run_update(update):
    """Execute a SPARQL update"""
    headers = {**HEADERS, 'Content-Type': 'application/sparql-update'}
    response = requests.post(ENDPOINT_UPDATE,
                           headers=headers,
                           data=update)
    response.raise_for_status()
    return response.status_code

def test_queries():
    # Insert test data
    insert_query = """
    PREFIX schema: <http://schema.org/>
    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
    INSERT DATA {
      <http://example.com/posts/test-123> a schema:BlogPosting ;
        schema:headline "Python Test Post" ;
        schema:description "Test content from Python" ;
        schema:datePublished "2024-01-16T10:00:00Z"^^xsd:dateTime ;
        schema:dateModified "2024-01-16T10:00:00Z"^^xsd:dateTime ;
        schema:author [
          a schema:Person ;
          schema:name "Python Test User" ;
          schema:email "python@example.com"
        ] .
    }
    """
    print("Inserting test data...")
    status = run_update(insert_query)
    print(f"Insert status: {status}")

    # Query all posts
    select_query = """
    PREFIX schema: <http://schema.org/>
    SELECT ?post ?title ?date ?author 
    WHERE {
      ?post a schema:BlogPosting ;
            schema:headline ?title ;
            schema:datePublished ?date ;
            schema:author/schema:name ?author .
    } ORDER BY DESC(?date)
    """
    print("\nQuerying all posts...")
    results = run_query(select_query)
    print(json.dumps(results, indent=2))

    # Delete test data
    delete_query = """
    PREFIX schema: <http://schema.org/>
    DELETE WHERE {
      <http://example.com/posts/test-123> ?p ?o .
      OPTIONAL { ?o ?p2 ?o2 }
    }
    """
    print("\nDeleting test data...")
    status = run_update(delete_query)
    print(f"Delete status: {status}")

if __name__ == "__main__":
    try:
        test_queries()
    except requests.exceptions.RequestException as e:
        print(f"Error during SPARQL operations: {e}")

================
File: src/applications/md-to-sparqlstore/test-queries.sh
================
#!/bin/bash

# Configuration
ENDPOINT_QUERY="http://localhost:3030/test/query"
ENDPOINT_UPDATE="http://localhost:3030/test/update"
AUTH_USER="admin"
AUTH_PASS="admin123"
AUTH_HEADER="Authorization: Basic $(echo -n ${AUTH_USER}:${AUTH_PASS} | base64)"

# Function to URL encode query
urlencode() {
  local string="${1}"
  echo "${string}" | curl -Gso /dev/null -w %{url_effective} --data-urlencode @- "" | cut -c 3-
}

# Test Query - Get all posts
echo "Testing: Get all posts"
curl -X POST $ENDPOINT_QUERY \
  -H "Content-Type: application/sparql-query" \
  -H "Accept: application/json" \
  -H "$AUTH_HEADER" \
  --data-raw 'PREFIX schema: <http://schema.org/>
  SELECT ?post ?title ?date ?author 
  WHERE {
    ?post a schema:BlogPosting ;
          schema:headline ?title ;
          schema:datePublished ?date ;
          schema:author/schema:name ?author .
  } ORDER BY DESC(?date)'

# Test Query - Get posts by author
echo -e "\nTesting: Get posts by author"
curl -X POST $ENDPOINT_QUERY \
  -H "Content-Type: application/sparql-query" \
  -H "Accept: application/json" \
  -H "$AUTH_HEADER" \
  --data-raw 'PREFIX schema: <http://schema.org/>
  SELECT ?post ?title ?date 
  WHERE {
    ?post a schema:BlogPosting ;
          schema:headline ?title ;
          schema:datePublished ?date ;
          schema:author [ schema:name "Test User" ] .
  } ORDER BY DESC(?date)'

# Test Update - Insert new post
echo -e "\nTesting: Insert new post"
curl -X POST $ENDPOINT_UPDATE \
  -H "Content-Type: application/sparql-update" \
  -H "$AUTH_HEADER" \
  --data-raw 'PREFIX schema: <http://schema.org/>
  PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
  INSERT DATA {
    <http://example.com/posts/test-123> a schema:BlogPosting ;
      schema:headline "Test Post" ;
      schema:description "Test content" ;
      schema:datePublished "2024-01-16T10:00:00Z"^^xsd:dateTime ;
      schema:dateModified "2024-01-16T10:00:00Z"^^xsd:dateTime ;
      schema:author [
        a schema:Person ;
        schema:name "Test User" ;
        schema:email "test@example.com"
      ] .
  }'

# Test Update - Delete post
echo -e "\nTesting: Delete post"
curl -X POST $ENDPOINT_UPDATE \
  -H "Content-Type: application/sparql-update" \
  -H "$AUTH_HEADER" \
  --data-raw 'PREFIX schema: <http://schema.org/>
  DELETE WHERE {
    <http://example.com/posts/test-123> ?p ?o .
    OPTIONAL { ?o ?p2 ?o2 }
  }'

================
File: src/applications/md-to-sparqlstore/transmissions.ttl
================
# src/applications/test/file-to-sparqlstore/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:SC a :ShowConfig . # verbose report, continues pipe             #
:SM a :ShowMessage . # verbose report, continues pipe            #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:md-to-sparqlstore a :Transmission ;
   :pipe (:mts10 :mts20  :mts30 :mts40) .

:mts10 a :DirWalker ;
   :settings :mts10-dirWalker .

:mts20 a :FileReader ;
     :settings :readerSet .

:mts30 a :MakeEntry ;
     :settings :entryExtras .

:mts40 a :SPARQLUpdate ;
     :settings :mts40-storeArticle .

================
File: src/applications/postcraft-statics/data/public/file1.md
================
One new

================
File: src/applications/postcraft-statics/data/public/file2.md
================
Two

================
File: src/applications/postcraft-statics/data/raw/file1.md
================
One new

================
File: src/applications/postcraft-statics/data/raw/file2.md
================
Two

================
File: src/applications/postcraft-statics/about.md
================
# About 

`src/applications/postcraft-statics/about.md`

# Example Application `about.md`

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans postcraft-statics ~/sites/strandz.it/postcraft
```

```sh
sudo systemctl stop tbox
cd ~/hyperdata/tbox # my local dir
docker-compose down
docker-compose up -d

cd ~/hyperdata/transmissions # my local path

./trans md-to-sparqlstore ~/sites/strandz.it/postcraft
./trans postcraft-statics ~/sites/strandz.it/postcraft
./trans sparqlstore-to-html ~/sites/strandz.it/postcraft
./trans sparqlstore-to-site-indexes ~/sites/strandz.it/postcraft

```


## Description

---

================
File: src/applications/postcraft-statics/about.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

================
File: src/applications/postcraft-statics/config.ttl
================
# src/applications/example-application/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:removePublic a :ConfigSet ;
    :target "public" .

:copyLayout a :ConfigSet ;
    :source "raw" ;
    :destination "public" .

================
File: src/applications/postcraft-statics/transmissions.ttl
================
# src/applications/example-application/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:SM a :ShowMessage . # verbose report, continues pipe            #
:SC a :ShowConfig . # verbose report, continues pipe             #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:cleanTarget a :Transmission ;
    rdfs:label "clear public" ;
    rdfs:comment "directory cleaner" ;
    :pipe (:ct10 :ct20) . # :ct10 not needed right now

:ct10 a :FileRemove ;
    :settings :removePublic .

:ct20 a :FileCopy ;
    :settings :copyLayout .

================
File: src/applications/sparqlstore-to-html/system/sparql-templates/save-html-chunk.njk
================
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX : <http://purl.org/stuff/transmissions/>

# https://schema.org/BlogPosting
# https://schema.org/Article

INSERT DATA {
  <{{uri}}> :htmlContent """{{content}}"""
}

================
File: src/applications/sparqlstore-to-html/system/sparql-templates/select-articles.njk
================
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX : <http://purl.org/stuff/transmissions/>

SELECT DISTINCT * WHERE {

  ?article a schema:Article ;
    :sourcePath ?sourcePath ;
    :relPath ?relPath ;
    :slug ?slug ;
    schema:headline ?title ;
#    schema:dateModified ?modified ;
    schema:articleBody ?content ;
    schema:creator ?creator .

  ?creator a schema:Person ;
    schema:name ?creatorName .

OPTIONAL {
    ?article schema:datePublished ?published ;
}
#   FILTER(?published >= "{{startDate}}"^^xsd:dateTime)
}
ORDER BY DESC(?published)

================
File: src/applications/sparqlstore-to-html/system/endpoints.json
================
[
    {
        "name": "local query",
        "type": "query",
        "url": "http://localhost:4030/semem/query",
        "credentials": {
            "user": "admin",
            "password": "admin123"
        }
    },
    {
        "name": "local update",
        "type": "update",
        "url": "http://localhost:4030/semem/update",
        "credentials": {
            "user": "admin",
            "password": "admin123"
        }
    }
]

================
File: src/applications/sparqlstore-to-html/templates/article-content.njk
================
<!-- ARTICLE CONTENT -->

<article class="post-content">
        <a href="{{link}}">#</a>
    {{content}}
</article>
<p class="post-title h-cinzel">
    <a href="{{link}}">
        {{title}}
    </a>
</p> <em>{{updated}}</em>

================
File: src/applications/sparqlstore-to-html/templates/article-page.njk
================
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <link rel="stylesheet" href="/css/fonts.css" type="text/css"/>
        <link rel="stylesheet" href="/css/grid-columns.css" type="text/css"/>
        <link rel="stylesheet" href="/css/style.css" type="text/css"/>
        <title>{{title}}</title>
    </head>
    <!-- POST PAGE TEMPLATE -->
    <body>
        <header id="entry-header">
            <h1 class="post-title h-cinzel">
                {{header}}
            </h1>
        </header>
        {{content}}
        <div class="entry-footer">
            <h2>About</h2>
            {{footer}}
        </div>
    </body>
</html>

================
File: src/applications/sparqlstore-to-html/templates/atom_template.njk
================
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title></title>
    
    <link href=""/>
    <updated></updated>
    <id></id>
    <author>
        <name></name>
        
    </author>

    
</feed>

================
File: src/applications/sparqlstore-to-html/templates/index-page_template.njk
================
<!DOCTYPE html>
<html lang="en">

<head>
    <title>Rawer</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- #:todo remove when stable -->
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">

    <link rel="stylesheet" href="/css/fonts.css" type="text/css" />
    <link rel="stylesheet" href="/css/grid-columns.css" type="text/css" />
    <link rel="stylesheet" href="/css/style.css" type="text/css" />
    <link rel="stylesheet" href="/css/menu.css" type="text/css" />

</head>

<body>
    <header id="main-header">
        <h1 class="h-title">
           Raw<em>er</em>
        </h1>
    </header>
    <div class="grid-container">
        <div class="main-grid-item directory">
            <p><strong>Under Construction</strong></p>
            <p><em>there are many to-dos</em></p>
        </div>
        <div class="main-grid-item articles">
            <article>
                {{content}}
            </article>
        </div>
        <div class="main-grid-item about">
            <!--
            <h2>About</h2>
            {{footer}}
            -->
        </div>
    </div>
    <script src="js/menu.js"></script>
</body>

</html>

================
File: src/applications/sparqlstore-to-html/about.md
================
`src/applications/sparqlstore-to-html/about.md`

# sparqlstore-to-html

~/sites/danny.ayers.name/public

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans sparqlstore-to-html
```

```sh
./trans sparqlstore-to-html -v ~/sites/danny.ayers.name/postcraft

./trans md-to-sparqlstore -v ~/sites/strandz.it/postcraft
./trans sparqlstore-to-html -v ~/sites/strandz.it/postcraft

cd ~/hyperdata/transmissions # my local path
./trans sparqlstore-to-site-indexes  -v ~/sites/strandz.it/postcraft
```

## Prerequisites

```sh
sudo systemctl stop tbox
cd ~/hyperdata/tbox # my local dir
docker-compose down
docker-compose up -d

cd ~/hyperdata/transmissions # my local path

./trans md-to-sparqlstore -v ~/sites/strandz.it/postcraft
./trans sparqlstore-to-html -v ~/sites/strandz.it/postcraft

./trans sparqlstore-to-site-indexes  -v ~/sites/strandz.it/postcraft

```

## Description

First pass :

Query SPARQL store for a list of schema:Article

---

```sparql
PREFIX schema: <http://schema.org/>

SELECT ?subject ?predicate ?object
WHERE {
  ?subject a schema:Article .
}
LIMIT 25
```

================
File: src/applications/sparqlstore-to-html/config.ttl
================
# src/applications/example-application/config.ttl

# BEWARE OF NAMED CRUFT!!!

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

# #:todo needs sensible/test defaults
:selectArticles a :ConfigSet ;
    :templateFilename "system/sparql-templates/select-articles.njk" ;
    :endpointSettings "system/endpoints.json" .

:resultIterator a :ConfigSet ;
  :forEach "queryResults.results.bindings" ;
  :remove "queryResults" .

:prepArticle a :ConfigSet  ;
    :remove "queryResults" ;
    :rename (:c1 :c2 :c3 :u1)  .
        # for content
        :c1   :pre    "currentItem.title.value" ;
              :post "contentBlocks.title"  .
        :c2   :pre    "currentItem.content.value" ;
              :post  "contentBlocks.content"  .
        :c3   :pre    "appRunStart" ;
              :post  "contentBlocks.published"  .
        # for SPARQL update
        :u1     :pre    "currentItem.article.value" ;
                :post   "contentBlocks.uri"  .

:mdHTML a :ConfigSet ;
    :outputField "contentBlocks.content" .

:articleContentTemplate a :ConfigSet ;
    :templateFilename "system/sparql-templates/article-content.njk" .

:articleHTML a :ConfigSet ;
    :dataBlock "contentBlocks" ;
    :templateFilename "system/sparql-templates/save-html-chunk.njk" ;
    :endpointSettings "system/endpoints.json" .

:articlePageTemplate a :ConfigSet ;
    :templateFilename "templates/article-page.njk" .

:filenameConstructor a :ConfigSet ;
  :asPath true ;
  :targetField "temp" ;
  :values (:fn1 :fn2 :fn3 :fn4) .
  :fn1 :field "targetPath" .
  :fn2 :string "public" .
  :fn3 :field "currentItem.relative.value" .
  :fn4 :field "currentItem.slug.value" .

:filenameExtension a :ConfigSet ;
    :asPath false ;
    :targetField "destinationFile" ;
    :values (:a2 :b2 ) .
    :a2 :field "temp" .
    :b2 :string ".html" .

:recordRendering a :ConfigSet ;
    :dataBlock "contentBlocks" ;
    :templateFilename "system/sparql-templates/record-rendering.njk" ;
    :endpointSettings "system/endpoints.json" .

================
File: src/applications/sparqlstore-to-html/transmissions.ttl
================
# src/applications/example-application/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:SM a :ShowMessage . #  verbose report, continues pipe           #
:SM1 a :ShowMessage .                                            #
:SM2 a :ShowMessage .                                            #
:SC a :ShowConfig . # verbose report, continues pipe             #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:sparqlstore-to-html a :Transmission ;
   :pipe (:p10 :p20 :p30  :p40 :p50 :p55  :p60 :p70
          :p80 :p90 :p100) .

:p10 a :SPARQLSelect ;
     :settings :selectArticles .

:p20 a :ForEach ;
    :settings :resultIterator .

:p30 a :Restructure ;
    :settings :prepArticle .


:p40 a :MarkdownToHTML ;
    :settings :mdHTML .

:p50 a :Templater ;
    :settings :articleContentTemplate .

# store the HTML snippet
:p55 a :SPARQLUpdate ;
     :settings :articleHTML .

# make the page
:p60 a :Templater ;
    :settings :articlePageTemplate .

:p70 a :StringOps ;
  :settings :filenameConstructor .

:p80 a :StringOps ;
  :settings :filenameExtension .

:p90 a :FileWriter ;
  :settings :htmlField .

:p100 a :SPARQLUpdate ;
     :settings :recordRendering .

================
File: src/applications/sparqlstore-to-site-indexes/system/sparql-templates/recent-articles.njk
================
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX : <http://purl.org/stuff/transmissions/>

SELECT DISTINCT * WHERE {
  ?article a schema:Article ;
    :sourcePath ?sourcePath ;
    :relPath ?relPath ;
    :slug ?slug ;
    schema:headline ?title ;
    schema:articleBody ?content ;
    schema:creator ?creator .

  ?creator a schema:Person ;
    schema:name ?creatorName .

  OPTIONAL {
    ?article schema:datePublished ?published ;
  }
}
ORDER BY DESC(?published)
LIMIT 5

================
File: src/applications/sparqlstore-to-site-indexes/system/sparql-templates/select-articles.njk
================
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX : <http://purl.org/stuff/transmissions/>

SELECT DISTINCT * WHERE {

  ?article a schema:Article ;
    :sourcePath ?sourcePath ;
    :relPath ?relPath ;
    :slug ?slug ;
    schema:headline ?title ;
#    schema:dateModified ?modified ;
    schema:articleBody ?content ;
    schema:creator ?creator .

  ?creator a schema:Person ;
    schema:name ?creatorName .

OPTIONAL {
    ?article schema:datePublished ?published ;
}
#   FILTER(?published >= "{{startDate}}"^^xsd:dateTime)
}
ORDER BY DESC(?published)

================
File: src/applications/sparqlstore-to-site-indexes/system/endpoints.json
================
[
    {
        "name": "local query",
        "type": "query",
        "url": "http://localhost:4030/semem/query",
        "credentials": {
            "user": "admin",
            "password": "admin123"
        }
    },
    {
        "name": "local update",
        "type": "update",
        "url": "http://localhost:4030/semem/update",
        "credentials": {
            "user": "admin",
            "password": "admin123"
        }
    }
]

================
File: src/applications/sparqlstore-to-site-indexes/templates/article-content.njk
================
<!-- ARTICLE CONTENT -->

<article class="post-content">
        <a href="{{link}}">#</a>
    {{content}}
</article>
<p class="post-title h-cinzel">
    <a href="{{link}}">
        {{title}}
    </a>
</p> <em>{{updated}}</em>

================
File: src/applications/sparqlstore-to-site-indexes/templates/article-page.njk
================
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <link rel="stylesheet" href="/css/fonts.css" type="text/css"/>
        <link rel="stylesheet" href="/css/grid-columns.css" type="text/css"/>
        <link rel="stylesheet" href="/css/style.css" type="text/css"/>
        <title>{{title}}</title>
    </head>
    <!-- POST PAGE TEMPLATE -->
    <body>
        <header id="entry-header">
            <h1 class="post-title h-cinzel">
                {{header}}
            </h1>
        </header>
        {{content}}
        <div class="entry-footer">
            <h2>About</h2>
            {{footer}}
        </div>
    </body>
</html>

================
File: src/applications/sparqlstore-to-site-indexes/templates/index-atom.njk
================
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title></title>
    
    <link href=""/>
    <updated></updated>
    <id></id>
    <author>
        <name></name>
        
    </author>

    
</feed>

================
File: src/applications/sparqlstore-to-site-indexes/templates/index-html.njk
================
<!DOCTYPE html>
<html lang="en">

<head>
    <title>Rawer</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- #:todo remove when stable -->
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">

    <link rel="stylesheet" href="/css/fonts.css" type="text/css" />
    <link rel="stylesheet" href="/css/grid-columns.css" type="text/css" />
    <link rel="stylesheet" href="/css/style.css" type="text/css" />
    <link rel="stylesheet" href="/css/menu.css" type="text/css" />

</head>

<body>
    <header id="main-header">
        <h1 class="h-title">
           Raw<em>er</em>
        </h1>
    </header>
    <div class="grid-container">
        <div class="main-grid-item directory">
            <p><strong>Under Construction</strong></p>
            <p><em>there are many to-dos</em></p>
        </div>
        <div class="main-grid-item articles">
            <article>
                {{content}}
            </article>
        </div>
        <div class="main-grid-item about">
            <!--
            <h2>About</h2>
            {{footer}}
            -->
        </div>
    </div>
    <script src="js/menu.js"></script>
</body>

</html>

================
File: src/applications/sparqlstore-to-site-indexes/about.md
================
`src/applications/sparqlstore-to-site-indexes/about.md`

# sparqlstore-to-site-indexes Application `about.md`

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans sparqlstore-to-site-indexes  ~/sites/strandz.it/postcraft
```

```sh
sudo systemctl stop tbox
cd ~/hyperdata/tbox # my local dir
docker-compose down
docker-compose up -d

cd ~/hyperdata/transmissions # my local path

./trans md-to-sparqlstore -v ~/sites/strandz.it/postcraft
./trans sparqlstore-to-html -v ~/sites/strandz.it/postcraft
./trans sparqlstore-to-site-indexes  -v ~/sites/strandz.it/postcraft

```

## Description

1. query store for most 5 recent entries
2. template as `index.html`
3. save to site root dir
4. template as `atom.xml`
5. save to site root dir

---

================
File: src/applications/sparqlstore-to-site-indexes/config.ttl
================
# src/applications/sparqlstore-to-site-indexes/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:recentArticles a :ConfigSet ;
    :templateFilename "system/sparql-templates/recent-articles.njk" ;
    :endpointSettings "system/endpoints.json" .

:indexResultIterator a :ConfigSet ;
  :forEach "queryResults.results.bindings" .

:prepRecents a :ConfigSet  ;
    :remove "queryResults" ;
    :rename (:c1 :c2 :c3 :u1)  .
        # for content
        :c1   :pre    "currentItem.title.value" ;
              :post "contentBlocks.title"  .
        :c2   :pre    "currentItem.content.value" ;
              :post  "contentBlocks.content"  .
        :c3   :pre    "appRunStart" ;
              :post  "contentBlocks.published"  .
        # for SPARQL update
        :u1     :pre    "currentItem.article.value" ;
                :post   "contentBlocks.uri"  .

:accumulator a :ConfigSet ;
    :targetField "accumulator" .

:htmlIndex a :ConfigSet ;
   :templateFilename "templates/index-html.njk" .

:htmlFilepath a :ConfigSet ;
  :asPath true ;
  :targetField "destinationFile" ;
  :values (:ip1 :ip2 :ip3) .
  :ip1 :field "targetPath" .
  :ip2 :string "public" .
  :ip3 :string "index.html" .

:contentLocation a :ConfigSet ;
    :contentField "accumulator" .

:prepHTML a :ConfigSet  ;
    :rename (:ph1)  .
        # for content
        :ph1   :pre    "accumulator" ;
              :post "contentBlocks.content"  .

:atomIndex a :ConfigSet ;
    :templateFilename "templates/index-atom.njk" .

:atomFilepath a :ConfigSet ;
  :asPath true ;
  :targetField "destinationFile" ;
  :values (:ap1 :ap2 :ap3) .
  :ap1 :field "targetPath" .
  :ap2 :string "public" .
  :ap3 :string "atom.xml" .


:contentLocationB a :ConfigSet ;
    :contentField "accumulator" .

  :prepAtom a :ConfigSet  ;
    :rename :pa1  .
        # for content
        :pa1   :pre    "accumulator" ;
              :post "contentBlocks.content"  .

================
File: src/applications/sparqlstore-to-site-indexes/transmissions.ttl
================
# src/applications/sparqlstore-to-site-indexes/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
# watch out for duplicates, they cause loops                     #
:SM a :ShowMessage . # verbose report, continues pipe            #
:SM1 a :ShowMessage . # verbose report, continues pipe           #
:SM2 a :ShowMessage . # verbose report, continues pipe           #
:SC a :ShowConfig . # verbose report, continues pipe             #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:sparqlstore-to-indexes a :Transmission ;
  :pipe ( :p10  :p20 :p30    :p40 :p50 # Query call
          :p90 :p100 :p110    :p120 # HTML rendering
          :p130 :p200 :p210 :p220) . # Atom rendering

# most recent 5 (+pinned?)
:p10 a :SPARQLSelect ;
     :settings :recentArticles .

:p20 a :ForEach ;
    :settings :indexResultIterator .

:p30 a :Restructure ;
    :settings :prepRecents .

:p40 a :Accumulate ;
    :settings :accumulator .

#####

:p50 a :Unfork .

# HTML
:p90 a :Restructure ;
    :settings :prepHTML .

:p100 a :Templater ;
     :settings :htmlIndex .

# builds the path
:p110 a :StringOps ;
  :settings :htmlFilepath .

:p120 a :FileWriter .
# :settings :contentLocation .

# Atom
:p130 a :Restructure ;
    :settings :prepAtom .

:p200 a :Templater ;
     :settings :atomIndex .

:p210 a :StringOps ;
  :settings :atomFilepath .

:p220 a :FileWriter .
 # :settings :contentLocationB .

================
File: src/applications/terrapack/data/input/subdir/subby.md
================
this is src/applications/terrapack/data/input/subdir/subby.md

================
File: src/applications/terrapack/data/input/2023-10-27_hello.md
================
<h1>Hello World! (again)</h1>
<p>lorem etc.</p>

================
File: src/applications/terrapack/data/input/2025-01-08_hello-again.md
================
<h1>Hello World! (again)</h1>
<p>lorem etc.</p>

================
File: src/applications/terrapack/data/input/exclude.me
================
this is src/applications/packer/data/input/exclude.me

================
File: src/applications/terrapack/about.md
================
# terrapack Application

**Packs sets of text/code content into a single file to provide grounding for AI**

_repopack/repomix equiv_

```sh
cd ~/hyperdata/transmissions # my local dir

./trans terrapack ./src/applications/terrapack/data

./trans terrapack ./src/applications/terrapack/data

./trans terrapack path/to/repo

./trans terrapack ./


./trans terrapack
```

from Claude

# terrapack Application

_repopack/repomix equiv_

```sh
./trans terrapack path/to/repo

./trans terrapack ./

./trans terrapack
```

Walks repository directory according to configured patterns, combines files into single AI-friendly document with:

- Directory structure outline
- File content with metadata
- Comment stripping option
- Configurable include/exclude patterns
- Output format optimized for LLMs

## Flow

1. DirWalker scans repository with filters
2. FileReader loads content and metadata
3. FileContainer accumulates and formats data
4. FileWriter generates single combined output

================
File: src/applications/terrapack/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .

:dirWalker a :ConfigSet ;
    :sourceDir "." ;
    :includeExtension ".md", ".js", ".ttl" .

:filterConfig a :ConfigSet ;
    :settings :filterDefaults ;
    :includePattern "*.txt", "*.md", "*.js", "*.jsx", "*.ts", "*.tsx", "*.json", "*.html", "*.css" ;
    :excludePattern
        "node_modules/*",
        "dist/*",
        "build/*",
        ".git/*"
    .

:readConfig a :ConfigSet ;
    :mediaType "text/plain" .

:containerConfig a :ConfigSet ;
    :destination "terrapack.config.json" .

:writeConfig a :ConfigSet ;
    :destinationFile "terrapack-output.txt" .

================
File: src/applications/terrapack/file-container-test-application.txt
================
import fs from 'fs/promises';
import path from 'path';
import FileContainer from '../../processors/terrapack/FileContainer.js';

const config = {
    whiteboard: [],
    destination: "container-output.json"
};

async function testFileContainer() {
    const container = new FileContainer(config);

    // Test with multiple files
    const message1 = {
        filepath: "/test/file1.js",
        content: "console.log('test1')",
        rootDir: "/test"
    };

    const message2 = {
        filepath: "/test/dir/file2.js",
        content: "console.log('test2')",
        rootDir: "/test"
    };

    await container.process(message1);
    await container.process(message2);

    // Send done message to get output
    const finalMessage = { done: true };
    await container.process(finalMessage);

    console.log('Container test complete');
}

testFileContainer().catch(console.error);

================
File: src/applications/terrapack/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trn: <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:terrapack a trn:Transmission ;
    rdfs:label "Repository terrapack" ;
    trn:pipe (:p10 :p20 :p30 :p40 :p50 :SM :DE :p70 :p80) .

:p10 a :DirWalker ;
    trn:settings :dirWalker .

:p20 a :StringFilter ;
    trn:settings :filterConfig .

:p30 a :FileReader ;
    trn:settings :readConfig .

:p40 a :FileContainer ;
    trn:settings :containerConfig .

:p50  a :Accumulate ;
    :settings :accumulator .

:p60 a :WhiteboardToMessage .

:p70 a :Unfork .

:p80 a :FileWriter ;
    trn:settings :writeConfig .

================
File: src/applications/test/accumulate/about.md
================
`src/applications/example-application/about.md`

# Example Application `about.md`

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans accumulate -m '{"foreach": ["first TEST_PASSED", "second TEST_PASSED", "third TEST_PASSED"]}'
```

## Description

---

================
File: src/applications/test/accumulate/about.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

================
File: src/applications/test/accumulate/config.ttl
================
# src/applications/example-application/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:forEachTarget a :ConfigSet ;
    :forEach  "foreach" .

:accumulator a :ConfigSet ;
    :label "test" .

================
File: src/applications/test/accumulate/transmissions.ttl
================
# src/applications/example-application/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:SM a :ShowMessage . # verbose report, continues pipe            #
:SC a :ShowConfig . # verbose report, continues pipe             #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:accumulator_test a :Transmission ;
    :pipe (:at10 :at20 :UF :SM) .

:at10 a :ForEach ;
    :settings :forEachTarget .

:at20 a :Accumulate ;
    :settings :accumulator .

================
File: src/applications/test/config-setting/about.md
================
# App Template

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans -v config-setting

./trans config-setting --message '{"theSettingProperty": "the setting value from message TEST_PASSEDTEST_PASSEDTEST_PASSED" }'
```

================
File: src/applications/test/config-setting/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .
@base <http://purl.org/stuff/path/> .

:theSettingsNode a :ConfigSet ;
    :theSettingProperty "the setting value from config TEST_PASSEDTEST_PASSEDTEST_PASSED" .

================
File: src/applications/test/config-setting/simple.js
================
import TestSetting from '../../../processors/test/TestSetting.js'
import logger from '../../../utils/Logger.js'
import ns from '../../../utils/ns.js'
import rdf from 'rdf-ext'

logger.setLogLevel('debug')
// Create test dataset with configuration
const dataset = rdf.dataset()
const settingsNode = rdf.namedNode('http://purl.org/stuff/transmissions/theSettingsNode')
const config = { dataset }

/*
:ts10 a :TestSetting ;
     :settings :theSettingsNode .

     :theSettingsNode a :ConfigSet ;
    :theSettingProperty "the setting value from config" .

     */

// Add test settings to dataset
dataset.add(rdf.quad(
    settingsNode,
    ns.rdf.type,
    ns.trn.ConfigSet
))

dataset.add(rdf.quad(
    settingsNode,
    ns.trn.theSettingProperty,
    rdf.literal('the setting value from config')
))

logger.log(dataset)

// Create instance with config
const testSetting = new TestSetting(config)
testSetting.settingsNode = settingsNode

// Test message for processing
const message = { value: '42' }

async function runTest() {
    // Process message through TestSettings
    const result = await testSetting.process(message)
    logger.log('Name from config:', testSetting.getProperty(ns.trn.theSettingProperty))
    logger.reveal(result)
}

runTest().catch(console.error)

================
File: src/applications/test/config-setting/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:testSetting a :Transmission ;
    :pipe (:ts10) .

:ts10 a :TestSetting ;
     :settings :theSettingsNode .

================
File: src/applications/test/config-setting-manifest/about.md
================
# config-setting-manifest

## Description

It has a setting in `tests/applications/config-setting-manifest/manifest.ttl` that takes precendence over the setting in `config.ttl`.

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans -v config-setting-manifest tests/applications/config-setting-manifest

# ./trans config-setting --verbose --message '{"theSettingProperty": "the setting value from message" }'
```

================
File: src/applications/test/config-setting-manifest/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .
@base <http://purl.org/stuff/path/> .

:theSettingsNode a :ConfigSet ;
    :theSettingProperty "the setting value from CONFIG" .

================
File: src/applications/test/config-setting-manifest/simple.js
================
import TestSetting from '../../../processors/test/TestSetting.js'
import logger from '../../../utils/Logger.js'
import ns from '../../../utils/ns.js'
import rdf from 'rdf-ext'

logger.setLogLevel('debug')
// Create test dataset with configuration
const dataset = rdf.dataset()
const settingsNode = rdf.namedNode('http://purl.org/stuff/transmissions/theSettingsNode')
const config = { dataset }

/*
:ts10 a :TestSetting ;
     :settings :theSettingsNode .

     :theSettingsNode a :ConfigSet ;
    :theSettingProperty "the setting value from config" .

     */

// Add test settings to dataset
dataset.add(rdf.quad(
    settingsNode,
    ns.rdf.type,
    ns.trn.ConfigSet
))

dataset.add(rdf.quad(
    settingsNode,
    ns.trn.theSettingProperty,
    rdf.literal('the setting value from config')
))

logger.log(dataset)

// Create instance with config
const testSetting = new TestSetting(config)
testSetting.settingsNode = settingsNode

// Test message for processing
const message = { value: '42' }

async function runTest() {
    // Process message through TestSettings
    const result = await testSetting.process(message)
    logger.log('Name from config:', testSetting.getProperty(ns.trn.theSettingProperty))
    logger.reveal(result)
}

runTest().catch(console.error)

================
File: src/applications/test/config-setting-manifest/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:testSetting a :Transmission ;
    :pipe (:N :ts10) .

:ts10 a :TestSetting ;
     :settings :theSettingsNode .

================
File: src/applications/test/config-settings/about.md
================
# App Template

./trans filereader --verbose --message '{"sourceFile": "test.txt"}'

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans config-settings
```

================
File: src/applications/test/config-settings/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .
@base <http://purl.org/stuff/path/> .

:settingsUseMessage a :ConfigSet ;
    :me ":settingsUseMessage" .

:settingsSingle a :ConfigSet ;
    :me ":settingsSingle" ;
    :name "Alice TEST_PASSED" .  # Added TEST_PASSED marker to make it 3 instances

:settingsURI a :ConfigSet ;
    :me ":settingsURI" ;
    :uri <http://example.org> .  # regular URI

[] :todo "need to check ontology for this" .
# #:todo needs something like :path a :Path .
:settingsPath a :ConfigSet ;
    :me ":settingsPath" ;
    :path <dirA> .  # subdirectory path

:settingsMulti a :ConfigSet ;
    :me ":settingsMulti" ;
    :name "BobTEST_PASSED" ;
    :uri <dirB> .

:settingsLists a :ConfigSet ;
  # :loglevel 'debug' ; TODO #:todo MOVE TO TRANSMISSION
    :me ":settingsLists" ;
    :aSetting  "settingA1", "settingA2", "settingA3" ;
    :bSetting  "settingB1", "settingB2TEST_PASSED", "settingB3" .

:settingsKeyValue a :ConfigSet ;
    :me ":settingsKeyValue" ;
    :setters (:setter1)  . # consider using blank nodes
        :setter1    :key    "myKey" ;
                    :value  "myValueTEST_PASSED"  .

:settingsCollection a :ConfigSet ;
    :me ":settingsCollection" ;
    :items  (:sc1 :sc2 :sc3 :sc4) .
  #  :items  a :collectionProperty .
  :sc1 :value "value1" .
  :sc2 :value "value2" .
  :sc3 :value "value3" .
  :sc4 :value "value4" .

================
File: src/applications/test/config-settings/test-settings-simple.js
================
import TestSettings from '../../../processors/test/TestSettings.js'
import logger from '../../../utils/Logger.js'
import ns from '../../../utils/ns.js'
import rdf from 'rdf-ext'

// Create test dataset with configuration
const dataset = rdf.dataset()
const settingsNode = rdf.namedNode('http://example.org/settings')
const config = { dataset }

// Add test settings to dataset
dataset.add(rdf.quad(
    settingsNode,
    ns.rdf.type,
    ns.trn.ConfigSet
))

dataset.add(rdf.quad(
    settingsNode,
    ns.trn.name,
    rdf.literal('Test Name')
))

dataset.add(rdf.quad(
    settingsNode,
    ns.trn.path,
    rdf.namedNode('http://example.org/test/path')
))

// Create instance with config
const testSettings = new TestSettings(config)
testSettings.settingsNode = settingsNode

// Test message for processing
const message = { value: '42' }

async function runTest() {
    // Process message through TestSettings
    const result = await testSettings.process(message)
    logger.log('Name from config:', testSettings.getProperty(ns.trn.name))
    logger.log('Path from config:', testSettings.getProperty(ns.trn.path))
    logger.reveal(result)
}

runTest().catch(console.error)

================
File: src/applications/test/config-settings/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:testSettings a :Transmission ;
    :pipe (:ts0 :ts10 :ts20 :ts30 :ts40 :ts50 :ts60 :ts70 :ts80 ) .


:ts0 a :TestSettings  .

:ts10 a :TestSettings ;
     :settings :settingsNotAValue .

:ts20 a :TestSettings ;
     :settings :settingsUseMessage .

:ts30 a :TestSettings ;
     :settings :settingsSingle .

:ts40 a :TestSettings ;
     :settings :settingsURI .

:ts50 a :TestSettings ;
     :settings :settingsMulti .

:ts60 a :TestSettings ;
     :settings :settingsLists .

:ts70 a :SetMessage ;
     :settings :settingsKeyValue .

:ts80 a :TestSettings ;
     :settings :settingsCollection .

================
File: src/applications/test/dirwalker/data/subdir/about-subdir.md
================
TEST_PASSED

================
File: src/applications/test/dirwalker/about.md
================
# DirWalker

## Runner

```sh
cd ~/hyperdata/transmissions  # my local path

./trans dirwalker
```

Defaults to `dataDir`, which defaults to `src/applications/test/dirwalker/data`

```sh
 ./trans -v dirwalker |grep message.filepath
                        message.filepath: about.md
                        message.filepath: config.ttl
                        message.filepath: data/about-data.md
                        message.filepath: data/subdir/about-subdir.md
                        message.filepath: transmissions.ttl
```

## Description

---

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/test/dirwalker/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .



:dirWalker a :ConfigSet ;
    :sourcePath "." .

================
File: src/applications/test/dirwalker/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage .
:SM2 a :ShowMessage .
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:dirwalker a :Transmission ;
    :pipe (:d1 :d2 :d3) .

:d1 a :DirWalker ;
 :settings :dirWalker . # specify in config.ttl

:d2 a :FileReader .

:d3 a :ShowMessage .

================
File: src/applications/test/echo/about.md
================
`src/applications/system/echo/about.md`

src/applications/system/echo

# Example Application `about.md`

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans system/echo -m '{"message":"Hello, World, TEST_PASSED!"}'
```

## Description

---

================
File: src/applications/test/echo/config.ttl
================
# src/applications/example-application/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:exampleSettings1 a :ConfigSet ;
    :me "exampleSettings1" ;
    :common "common from 1"  ;
    :something1 "something1 from config" .

:exampleSettings2 a :ConfigSet ;
    :me "exampleSettings2" ;
    :common "common from 2" ;
    :something2 "something2 from config" ;
    :added " added from exampleSettings2" .

================
File: src/applications/test/echo/transmissions.ttl
================
# src/applications/example-application/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
# :DE a :DeadEnd . # ends the current pipe quietly
# :H  a :Halt . # kills everything
# :SC a :ShowConfig . # verbose report, continues pipe
# :SM a :ShowMessage . # verbose report, continues pipe
# :N  a :NOP . # no operation (except for showing stage in pipe)
# :UF a :Unfork . # collapses all pipes but one
#############################################################

:echo a :Transmission ;
    :pipe (:p10 :p20) .

:p10  a :NOP .
:p20 a :ShowMessage .

================
File: src/applications/test/file-remove-copy/data/public/file1.md
================
One new

================
File: src/applications/test/file-remove-copy/data/public/file2.md
================
Two

================
File: src/applications/test/file-remove-copy/data/raw/file1.md
================
One new

================
File: src/applications/test/file-remove-copy/data/raw/file2.md
================
Two

================
File: src/applications/test/file-remove-copy/about.md
================
# About 

`src/applications/postcraft-statics/about.md`

# Example Application `about.md`

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans -v postcraft-statics
```

## Description

---

================
File: src/applications/test/file-remove-copy/about.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

================
File: src/applications/test/file-remove-copy/config.ttl
================
# src/applications/example-application/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:removePublic a :ConfigSet ;
    :target "public" .

:copyLayout a :ConfigSet ;
    :source "raw" ;
    :destination "public" .

================
File: src/applications/test/file-remove-copy/transmissions.ttl
================
# src/applications/example-application/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:SM a :ShowMessage . # verbose report, continues pipe            #
:SC a :ShowConfig . # verbose report, continues pipe             #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:cleanTarget a :Transmission ;
    rdfs:label "clear public" ;
    rdfs:comment "directory cleaner" ;
    :pipe (:ct10 :ct20 :SM ) .

:ct10 a :FileRemove ;
    :settings :removePublic .

:ct20 a :FileCopy ;
    :settings :copyLayout .

================
File: src/applications/test/file-to-sparqlstore/data/input/input.md
================
# Test Blog Post

This is a test blog post that will be converted to RDF and stored in the SPARQL database.

## Metadata

- Title: Test Blog Post
- Author: Test User
- Email: test@example.com

## Content

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.

================
File: src/applications/test/file-to-sparqlstore/diamonds/select-blogposting.njk
================
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

SELECT ?post ?headline ?content ?published ?modified ?author
WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?headline ;
        schema:description ?content ;
        schema:datePublished ?published ;
        schema:dateModified ?modified ;
        schema:author/schema:name ?author .
  FILTER(?published >= "{{startDate}}"^^xsd:dateTime)
}
ORDER BY DESC(?published)
LIMIT 1

================
File: src/applications/test/file-to-sparqlstore/diamonds/update-blogposting.njk
================
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

INSERT DATA {
  <http://example.com/posts/{{id}}> a schema:BlogPosting ;
    schema:headline "TEST_PASSED" ;
    schema:url <http://example.com/posts/{{id}}> ;
    schema:description """{{content}}""" ;
    schema:datePublished "{{published}}"^^xsd:dateTime ;
    schema:dateModified "{{modified}}"^^xsd:dateTime ;
    schema:author [
      a schema:Person ;
      schema:name "{{author.name}}" ;
      schema:email "{{author.email}}"
    ] .
}

================
File: src/applications/test/file-to-sparqlstore/docs/handover-doc.md
================
# SPARQL Integration Handover Document

## New Components Added

### 1. SPARQL Processors
- **SPARQLSelect.js**: Query processor with template support
- **SPARQLUpdate.js**: Update processor with template support
- **SPARQLProcessorsFactory.js**: Factory for processor instantiation

### 2. Test Application
- Location: src/applications/test_file-to-sparqlstore/
- Purpose: End-to-end testing of SPARQL integration
- Integration with FileReader for markdown processing

### 3. Configuration Files
- endpoint.json: SPARQL endpoint configuration
- Test data and templates under diamonds/
- SPARQL query/update templates

## Key Technical Details

### Authentication
- Uses Basic Auth
- Credentials in endpoint.json
- Separate configs for query/update endpoints

### Data Model
- Uses schema.org vocabulary
- BlogPosting as primary type
- Nested author information
- Timestamps for created/modified

### Error Handling
- Network failures
- Authentication errors
- Query validation
- Template rendering errors

## Testing

### Automated Tests
- Unit tests for processors
- Integration tests for pipeline
- Template validation

### Manual Testing
- Test scripts in bash/Python
- Example queries
- Curl commands for direct testing

## Dependencies
- axios for HTTP
- nunjucks for templates
- rdf-ext for RDF handling

## Known Issues/TODOs
1. Template caching not implemented
2. Bulk operations not optimized
3. Add transaction support
4. Enhance error reporting

================
File: src/applications/test/file-to-sparqlstore/docs/handover.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix prj: <http://purl.org/stuff/project/> .
@prefix trn: <http://purl.org/stuff/transmissions/> .
@prefix code: <http://purl.org/stuff/code/> .

# Project Component Description
trn:SPARQLIntegration a prj:Component ;
    dcterms:title "SPARQL Integration for Transmissions" ;
    dcterms:description "SPARQL query and update processors with test application" ;
    dcterms:created "2024-01-16"^^xsd:date ;
    prj:status "Testing" ;
    prj:version "1.0.0" ;
    prj:maintainer <http://danny.ayers.name> ;
    prj:documentation trn:SPARQLDocs .

# Documentation
trn:SPARQLDocs a prj:Documentation ;
    prj:hasSection trn:ProcessorDocs, trn:TestAppDocs, trn:ConfigDocs .

trn:ProcessorDocs a prj:DocumentationSection ;
    dcterms:title "SPARQL Processors" ;
    prj:covers trn:SPARQLSelect, trn:SPARQLUpdate ;
    prj:location "/src/processors/sparql/" .

# Components
trn:SPARQLSelect a code:Processor ;
    dcterms:title "SPARQL Select Processor" ;
    code:implements trn:QueryExecution ;
    code:dependsOn trn:EndpointConfig ;
    code:hasTest trn:SelectTests .

trn:SPARQLUpdate a code:Processor ;
    dcterms:title "SPARQL Update Processor" ;
    code:implements trn:UpdateExecution ;
    code:dependsOn trn:EndpointConfig ;
    code:hasTest trn:UpdateTests .

# Test Application
trn:TestApp a code:Application ;
    dcterms:title "SPARQL Store Test" ;
    code:location "/src/applications/test_file-to-sparqlstore/" ;
    code:uses trn:SPARQLSelect, trn:SPARQLUpdate ;
    code:hasConfig trn:EndpointConfig .

# Configuration
trn:EndpointConfig a code:Configuration ;
    dcterms:title "SPARQL Endpoint Configuration" ;
    code:format "JSON" ;
    code:location "endpoint.json" ;
    code:template [
        code:field "type" ;
        code:required true ;
        code:allowedValues "query", "update"
    ], [
        code:field "url" ;
        code:required true ;
        rdfs:comment "SPARQL endpoint URL"
    ] .

# Known Issues
trn:Issues a prj:IssueList ;
    prj:hasIssue [
        a prj:TODO ;
        dcterms:title "Template Caching" ;
        prj:priority "Medium"
    ], [
        a prj:TODO ;
        dcterms:title "Transaction Support" ;
        prj:priority "High"
    ] .

================
File: src/applications/test/file-to-sparqlstore/docs/sparql-processors-docs.md
================
# SPARQL Processors Documentation

## Overview
The SPARQL processors provide functionality for interacting with SPARQL endpoints through the Transmissions pipeline framework. Two main processors are provided:
- SPARQLSelect: Executes SELECT queries
- SPARQLUpdate: Executes UPDATE operations

## Configuration
Configuration is managed through endpoint.json:
```json
{
    "name": "local query",
    "type": "query|update",
    "url": "http://localhost:3030/dataset/query",
    "credentials": {
        "user": "username",
        "password": "password"
    }
}
```

## SPARQLSelect Processor
Executes templated SELECT queries against a SPARQL endpoint.

### Usage
```turtle
:query a :Transmission ;
    :pipe (:p10) .

:p10 a :SPARQLSelect ;
    :settings [
        :templateFilename "queries/select.njk" ;
        :endpointSettings "endpoint.json"
    ] .
```

### Template Variables
- startDate: ISO datetime for filtering
- Additional variables from message object

## SPARQLUpdate Processor
Executes templated UPDATE operations against a SPARQL endpoint.

### Usage
```turtle
:update a :Transmission ;
    :pipe (:p10) .

:p10 a :SPARQLUpdate ;
    :settings [
        :templateFilename "queries/update.njk" ;
        :endpointSettings "endpoint.json"
    ] .
```

### Template Variables
- id: Auto-generated UUID
- title: From message.meta.title
- content: From message.content
- published/modified: Current timestamp
- author: From message.meta.author

## Error Handling
- Connection failures throw network errors
- Authentication failures throw 401/403 errors
- Invalid queries throw 400 errors
- All errors include detailed messages in logs

================
File: src/applications/test/file-to-sparqlstore/docs/test-app-docs.md
================
# SPARQL Store Test Application

## Purpose
Tests complete pipeline functionality for reading files, converting to RDF, storing in a SPARQL database, and verifying storage through queries.

## Quick Start
1. Configure SPARQL endpoint in endpoint.json
2. Place test markdown in data/input/input.md
3. Run application:
```bash
./trans test_file-to-sparqlstore
```

## Components
1. FileReader processor:
   - Reads input markdown
   - Extracts metadata and content

2. SPARQLUpdate processor:
   - Converts markdown to RDF using schema.org vocabulary
   - Stores in SPARQL database

3. SPARQLSelect processor:
   - Queries stored data
   - Verifies successful storage

## Testing
### Manual Testing
Use provided test scripts:
```bash
# Using bash script
./test-queries.sh

# Using Python script
python3 test-queries.py
```

### Example Queries
```sparql
# Find recently added posts
SELECT ?post ?title WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?title ;
        schema:datePublished ?date .
  FILTER(?date >= "2024-01-01T00:00:00Z"^^xsd:dateTime)
}
```

## Configuration
1. endpoint.json: SPARQL endpoint details
2. config.ttl: Transmission configuration
3. transmissions.ttl: Pipeline definition
4. diamonds/*.njk: Query templates

## Error Cases Handled
- Missing input files
- SPARQL endpoint connection failures
- Authentication errors
- Invalid markdown format
- Failed data verification

================
File: src/applications/test/file-to-sparqlstore/examples/blog-post-rdf.txt
================
@prefix schema: <http://schema.org/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

<http://example.com/posts/fb5e0595-2e98-4c5c-9876-7f402c6439a2> 
    a schema:BlogPosting ;
    schema:headline "Test Blog Post" ;
    schema:description """Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor 
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis 
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.""" ;
    schema:datePublished "2024-01-16T15:55:43.049Z"^^xsd:dateTime ;
    schema:dateModified "2024-01-16T15:55:43.049Z"^^xsd:dateTime ;
    schema:author [
        a schema:Person ;
        schema:name "Test User" ;
        schema:email "test@example.com"
    ] ;
    schema:articleBody """# Test Blog Post

This is a test blog post that will be converted to RDF and stored in the SPARQL database.

## Metadata
- Title: Test Blog Post  
- Author: Test User
- Email: test@example.com

## Content
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor 
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis 
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.""" .

================
File: src/applications/test/file-to-sparqlstore/examples/sparql-queries.md
================
# Query all blog posts
PREFIX schema: <http://schema.org/>
SELECT ?post ?title ?date ?author WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?title ;
        schema:datePublished ?date ;
        schema:author/schema:name ?author .
} ORDER BY DESC(?date)

# Query posts by specific author
PREFIX schema: <http://schema.org/>
SELECT ?post ?title ?date WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?title ;
        schema:datePublished ?date ;
        schema:author [ schema:name "Test User" ] .
} ORDER BY DESC(?date)

# Query posts in date range
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
SELECT ?post ?title ?date WHERE {
  ?post a schema:BlogPosting ;
        schema:headline ?title ;
        schema:datePublished ?date .
  FILTER(?date >= "2024-01-01T00:00:00Z"^^xsd:dateTime && 
         ?date <= "2024-12-31T23:59:59Z"^^xsd:dateTime)
} ORDER BY DESC(?date)

# Update/Insert new blog post
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
INSERT DATA {
  <http://example.com/posts/test-123> a schema:BlogPosting ;
    schema:headline "Test Post" ;
    schema:description "Test content" ;
    schema:datePublished "2024-01-16T10:00:00Z"^^xsd:dateTime ;
    schema:dateModified "2024-01-16T10:00:00Z"^^xsd:dateTime ;
    schema:author [
      a schema:Person ;
      schema:name "Test User" ;
      schema:email "test@example.com"
    ] .
}

# Delete a blog post
PREFIX schema: <http://schema.org/>
DELETE WHERE {
  <http://example.com/posts/test-123> ?p ?o .
  OPTIONAL { ?o ?p2 ?o2 }
}

================
File: src/applications/test/file-to-sparqlstore/about.md
================
`src/applications/test/file-to-sparqlstore/about.md`

NOT IN INTEGRATION TESTS

# Application 'file-to-sparqlstore'

**Checks interaction with SPARQL store**

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans test_file-to-sparqlstore
```

## Description

1. Reads a file from fs
2. Templates it using nunjucks into a SPARQL UPDATE query
3. POSTs this to the specified endpoint
4. Does a SPARQL SELECT query (based on date) to retrieve data
5. Compares this with the original content to ensure it is in the store

Data looks something like :

```turtle
@prefix schema: <http://schema.org/> .

<http://example.com/posts-one> a schema:BlogPosting ;
    schema:headline "Post one" ;
    schema:url <http://example.com/posts-one> ;
    schema:description "Post one content." ;
    schema:datePublished "2023-05-22T13:00:00Z"^^xsd:dateTime ;
    schema:dateModified "2023-05-22T15:00:00Z"^^xsd:dateTime ;
    schema:author [
        a schema:Person ;
        schema:name "John Doe" ;
        schema:email "johndoe@example.com"
    ] .
```

## Notes

TODO complete -

src/applications/test_file-to-sparqlstore
├── about.md
├── config.ttl
├── data
│   ├── input
│   │   └── input.md
│   └── output
├── diamonds
│   ├── select-blogposting.njk
│   └── update-blogposting.njk
├── endpoint.json
└── transmissions.ttl

src/processors/sparql
├── about.md
├── SPARQLProcessorsFactory.js
├── SPARQLSelect.js
└── SPARQLUpdate.js

================
File: src/applications/test/file-to-sparqlstore/config.ttl
================
# src/applications/test_file-to-sparqlstore/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:readerSet a :ConfigSet ;
    :sourceFile  "input/input.md" ;
    :metaField "meta" .

:sparqlUpdate a :ConfigSet ;
    :templateFilename "diamonds/update-blogposting.njk" ;
    :endpointSettings "endpoint.json" .

:sparqlSelect a :ConfigSet ;
    :templateFilename "diamonds/select-blogposting.njk" ;
    :endpointSettings "endpoint.json" .

================
File: src/applications/test/file-to-sparqlstore/endpoint.json
================
[
    {
        "name": "local query",
        "type": "query",
        "url": "http://localhost:4030/test-mem/query",
        "credentials": {
            "user": "admin",
            "password": "admin123"
        }
    },
    {
        "name": "local update",
        "type": "update",
        "url": "http://localhost:4030/test-mem/update",
        "credentials": {
            "user": "admin",
            "password": "admin123"
        }
    }
]

================
File: src/applications/test/file-to-sparqlstore/python-test.py
================
#!/usr/bin/env python3
import requests
import json
from base64 import b64encode
from datetime import datetime

# Configuration
ENDPOINT_QUERY = "http://localhost:3030/test/query"
ENDPOINT_UPDATE = "http://localhost:3030/test/update" 
AUTH_USER = "admin"
AUTH_PASS = "admin123"

# Authentication header
auth_string = b64encode(f"{AUTH_USER}:{AUTH_PASS}".encode()).decode()
HEADERS = {
    'Authorization': f'Basic {auth_string}',
    'Accept': 'application/json'
}

def run_query(query):
    """Execute a SPARQL query and return results"""
    headers = {**HEADERS, 'Content-Type': 'application/sparql-query'}
    response = requests.post(ENDPOINT_QUERY, 
                           headers=headers,
                           data=query)
    response.raise_for_status()
    return response.json()

def run_update(update):
    """Execute a SPARQL update"""
    headers = {**HEADERS, 'Content-Type': 'application/sparql-update'}
    response = requests.post(ENDPOINT_UPDATE,
                           headers=headers,
                           data=update)
    response.raise_for_status()
    return response.status_code

def test_queries():
    # Insert test data
    insert_query = """
    PREFIX schema: <http://schema.org/>
    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
    INSERT DATA {
      <http://example.com/posts/test-123> a schema:BlogPosting ;
        schema:headline "Python Test Post" ;
        schema:description "Test content from Python" ;
        schema:datePublished "2024-01-16T10:00:00Z"^^xsd:dateTime ;
        schema:dateModified "2024-01-16T10:00:00Z"^^xsd:dateTime ;
        schema:author [
          a schema:Person ;
          schema:name "Python Test User" ;
          schema:email "python@example.com"
        ] .
    }
    """
    print("Inserting test data...")
    status = run_update(insert_query)
    print(f"Insert status: {status}")

    # Query all posts
    select_query = """
    PREFIX schema: <http://schema.org/>
    SELECT ?post ?title ?date ?author 
    WHERE {
      ?post a schema:BlogPosting ;
            schema:headline ?title ;
            schema:datePublished ?date ;
            schema:author/schema:name ?author .
    } ORDER BY DESC(?date)
    """
    print("\nQuerying all posts...")
    results = run_query(select_query)
    print(json.dumps(results, indent=2))

    # Delete test data
    delete_query = """
    PREFIX schema: <http://schema.org/>
    DELETE WHERE {
      <http://example.com/posts/test-123> ?p ?o .
      OPTIONAL { ?o ?p2 ?o2 }
    }
    """
    print("\nDeleting test data...")
    status = run_update(delete_query)
    print(f"Delete status: {status}")

if __name__ == "__main__":
    try:
        test_queries()
    except requests.exceptions.RequestException as e:
        print(f"Error during SPARQL operations: {e}")

================
File: src/applications/test/file-to-sparqlstore/test-queries.sh
================
#!/bin/bash

# Configuration
ENDPOINT_QUERY="http://localhost:3030/test/query"
ENDPOINT_UPDATE="http://localhost:3030/test/update"
AUTH_USER="admin"
AUTH_PASS="admin123"
AUTH_HEADER="Authorization: Basic $(echo -n ${AUTH_USER}:${AUTH_PASS} | base64)"

# Function to URL encode query
urlencode() {
  local string="${1}"
  echo "${string}" | curl -Gso /dev/null -w %{url_effective} --data-urlencode @- "" | cut -c 3-
}

# Test Query - Get all posts
echo "Testing: Get all posts"
curl -X POST $ENDPOINT_QUERY \
  -H "Content-Type: application/sparql-query" \
  -H "Accept: application/json" \
  -H "$AUTH_HEADER" \
  --data-raw 'PREFIX schema: <http://schema.org/>
  SELECT ?post ?title ?date ?author 
  WHERE {
    ?post a schema:BlogPosting ;
          schema:headline ?title ;
          schema:datePublished ?date ;
          schema:author/schema:name ?author .
  } ORDER BY DESC(?date)'

# Test Query - Get posts by author
echo -e "\nTesting: Get posts by author"
curl -X POST $ENDPOINT_QUERY \
  -H "Content-Type: application/sparql-query" \
  -H "Accept: application/json" \
  -H "$AUTH_HEADER" \
  --data-raw 'PREFIX schema: <http://schema.org/>
  SELECT ?post ?title ?date 
  WHERE {
    ?post a schema:BlogPosting ;
          schema:headline ?title ;
          schema:datePublished ?date ;
          schema:author [ schema:name "Test User" ] .
  } ORDER BY DESC(?date)'

# Test Update - Insert new post
echo -e "\nTesting: Insert new post"
curl -X POST $ENDPOINT_UPDATE \
  -H "Content-Type: application/sparql-update" \
  -H "$AUTH_HEADER" \
  --data-raw 'PREFIX schema: <http://schema.org/>
  PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
  INSERT DATA {
    <http://example.com/posts/test-123> a schema:BlogPosting ;
      schema:headline "Test Post" ;
      schema:description "Test content" ;
      schema:datePublished "2024-01-16T10:00:00Z"^^xsd:dateTime ;
      schema:dateModified "2024-01-16T10:00:00Z"^^xsd:dateTime ;
      schema:author [
        a schema:Person ;
        schema:name "Test User" ;
        schema:email "test@example.com"
      ] .
  }'

# Test Update - Delete post
echo -e "\nTesting: Delete post"
curl -X POST $ENDPOINT_UPDATE \
  -H "Content-Type: application/sparql-update" \
  -H "$AUTH_HEADER" \
  --data-raw 'PREFIX schema: <http://schema.org/>
  DELETE WHERE {
    <http://example.com/posts/test-123> ?p ?o .
    OPTIONAL { ?o ?p2 ?o2 }
  }'

================
File: src/applications/test/file-to-sparqlstore/transmissions.ttl
================
# src/applications/test/file-to-sparqlstore/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .

####################################################################
# Utility Processors : insert into pipe for debugging              #
#                                                                  #
# :DE a :DeadEnd . # ends the current pipe quietly                 #
# :H  a :Halt . # kills everything                                 #
# :SC a :ShowConfig . # verbose report, continues pipe             #
# :SM a :ShowMessage . # verbose report, continues pipe            #
# :N  a :NOP . # no operation (except for showing stage in pipe)   #
# :UF a :Unfork . # collapses all pipes but one                    #
####################################################################

:SM1 a :ShowMessage .
:SM2 a :ShowMessage .

:file-to-sparqlstore a :Transmission ;
:pipe (:p10 :SM1 :p20 :p30 :SM2) .

:p10 a :FileReader ;
     :settings :readerSet .

:p20 a :SPARQLUpdate ;
     :settings :sparqlUpdate .

:p30 a :SPARQLSelect ;
     :settings :sparqlSelect .

================
File: src/applications/test/filereader/data/input/input.md
================
This is the content of file input.md TEST_PASSED

================
File: src/applications/test/filereader/about.md
================
# Example Application `about.md`

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans test_filereader
```

## Description

---

================
File: src/applications/test/filereader/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:readerSet a :ConfigSet ;
    :sourceFile  "input/input.md" ;
    :metaField "meta" .

================
File: src/applications/test/filereader/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .


##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:SC a :ShowConfig . # verbose report, continues pipe             #
:SM a :ShowMessage . # verbose report, continues pipe            #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:filereader a :Transmission ;
:pipe (:p10 :p20 :p30) .

:p10 a :ShowMessage .

:p20 a :FileReader ;
     :settings :readerSet .

:p30 a :ShowMessage .

================
File: src/applications/test/foreach/about.md
================
# ForEach processor module for Transmissions

./trans foreach -m '{"foreach": ["first TEST_PASSED", "second TEST_PASSED", "third TEST_PASSED"]}'

## Prompts

```sh
./trans test_fork
```

Your Goal is to write a processor module for Transmissions that will initiate multiple processing pipelines based on a list provided in the incoming message. First review these instructions as a whole, and then identify the subgoals. Then, taking each subgoal in turn, break it down into a concrete series of tasks. Carry out the sequence of tasks.
You have plenty of time, so don't rush, try to be as careful in understanding and operation as possible.
Existing source code may be found in the Project Knowledge files.

Two modules are required -

1. `ForEach` located in :

```sh
./transmissions/src/processors/flow/ForEach.js
```

modeled on :

```sh
./transmissions/src/processors/templates/ProcessorTemplate.js
```

2. `FlowProcessorsFactory` located in

```sh
./transmissions/src/processors/flow/FlowProcessorsFactory.js
```

modeled on :

```sh
/transmissions/src/processors/templates/TemplateProcessorsFactory.js
```

The input message will contain the list to be processed in the form of this example :

```json
{
  "foreach": ["item1", "item2", "item3"]
}
```

The behavior will be to emit the message to a subsequent processor using the existing engine infrastructure, like a simpler version of :

```sh
transmissions/src/processors/fs/DirWalker.js
```

Each message emitted will be a structuredClone of the input message.

Once this code is completed, create application definitions in the form of these examples :

```sh
transmissions/src/applications/test_fork/transmissions.ttl
transmissions/src/applications/test_fork/processors-config.ttl
```

After you have finished all these, re-read the high level Goal and taking each of your derived subgoals in turn, review your code to ensure that it fulfils the requirements.
Show me the full source of the implementations.

---

/home/danny/github-danny/postcraft/danny.ayers.name/content-raw/entries/2024-09-27_lively-distractions.md

https://github.com/github/rest-api-description

================
File: src/applications/test/foreach/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trn: <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> .

:forEachTarget a :ConfigSet ;
    :forEach  "foreach" .

:removo a :ConfigSet ;
    :remove  "foreach" .

================
File: src/applications/test/foreach/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trn: <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:SM1 a :ShowMessage . # verbose report, continues pipe            #
:SM2 a :ShowMessage .
:SC a :ShowConfig . # verbose report, continues pipe             #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################


:foreach_test a trn:Transmission ;
    trn:pipe (:fe10 :fe15 :SM1) .

:fe10 a :ForEach ;
    :settings :forEachTarget .

:fe15 a :Restructure ;
    :settings :removo .

================
File: src/applications/test/restructure/data/input/input-01.json
================
{
    "item": {
        "uuid": "convo1",
        "name": "Name of this convo",
        "created_at": "2024-10-29T17:57:50.229169Z",
        "chat_messages": [
            {
                "uuid": "id1",
                "text": "Text one"
            },
            {
                "uuid": "id2",
                "text": "Text two TEST_PASSED"
            }
        ]
    }
}

================
File: src/applications/test/restructure/data/output/output-01.json
================
{"channel":[{"uuid":"id1","text":"Text one"},{"uuid":"id2","text":"Text two TEST_PASSED"}],"filename":"convo1","title":"Name of this convo"}

================
File: src/applications/test/restructure/data/output/output-02.json
================
{"TEST_PASSED":[{"uuid":"id1","text":"Text one"},{"uuid":"id2","text":"Text two TEST_PASSED"}],"filename":"convo1","title":"Name of this convo"}

================
File: src/applications/test/restructure/data/output/required-01.json
================
{
    "channel": [
        {
            "uuid": "id1",
            "text": "Text one"
        },
        {
            "uuid": "id2",
            "text": "Text two TEST_PASSED"
        }
    ],
    "filename": "convo1",
    "title": "Name of this convo"
}

================
File: src/applications/test/restructure/data/output/required-02.json
================
{
    "TEST_PASSED": [
        {
            "uuid": "id1",
            "text": "Text one"
        },
        {
            "uuid": "id2",
            "text": "Text two TEST_PASSED"
        }
    ],
    "filename": "convo1",
    "title": "Name of this convo"
}

================
File: src/applications/test/restructure/manifest/manifest.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix dcterms: <http://purl.org/dc/terms/> .

@prefix : <http://purl.org/stuff/transmissions/> .

:reader a :ConfigSet ;
   :sourceFile "../data/input/input-01.json" .

:retree a :ConfigSet ;
    :rename (:pp1 :pp2 :pp3) . # consider using blank nodes
    :pp1   :pre     "content.item.chat_messages" ;
            :post    "content.TEST_PASSED"  .
    :pp2   :pre     "content.item.uuid" ;
            :post    "content.filename"  .
    :pp3   :pre     "content.item.name" ;
            :post    "content.title"  .


:writer a :ConfigSet ;
    :destinationFile "../data/output/output-02.json" .

================
File: src/applications/test/restructure/about.md
================
# Application : test_restructure

Run with :

```sh
cd ~/hyperdata/transmissions/ # local path of repo
./trans restructure

./trans  restructure src/applications/test/restructure/manifest

```

#:todo make this into something like processor signature
#:todo make Turtle version

## Description

Reads :

```sh
src/applications/test_restructure/data/output/input-01.json
```

as a message, restructures it according to config, then writes the result to :

```sh
src/applications/test_restructure/data/output/output-01.json
```

the tests compare the new file with :

```sh
src/applications/test_restructure/data/output/required-01.json
```

```sh
cd ~/hyperdata/transmissions/ # my local path

# run as application
./trans test_restructure

# run as simples
node src/applications/test_restructure/simple.js

## Tests in tests/integration

# test as application
npm test -- --filter="restructure test"

# test as simples
npm test -- --filter="restructure_simple test"
```

---

```sh
cd ~/hyperdata/transmissions/
./trans test_restructure -P ./src/applications/test_restructure/input/input-01.json
```

---

./trans claude-json-converter -P ./conversations.json

```turtle
:s40 a :Restructure ;
    trm:configKey :walkPrep .

...

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
```

================
File: src/applications/test/restructure/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> .

:reader a :ConfigSet ;
    :sourceFile "input/input-01.json" ;
    :mediaType "application/json" .

:retree a :ConfigSet ;
    :rename (:pp1 :pp2 :pp3) . # consider using blank nodes
    :pp1   :pre     "content.item.chat_messages" ;
            :post    "content.channel"  .
    :pp2   :pre     "content.item.uuid" ;
            :post    "content.filename"  .
    :pp3   :pre     "content.item.name" ;
            :post    "content.title"  .


:writer a :ConfigSet ;
    :destinationFile "output/output-01.json" .

================
File: src/applications/test/restructure/simple.js
================
// src/applications/test_restructure/simple.js

import FileReader from '../../../processors/fs/FileReader.js'
import Restructure from '../../../processors/json/Restructure.js'
import FileWriter from '../../../processors/fs/FileWriter.js'

const config = {
    "simples": "true",
    "sourceFile": "input/input-01.json",
    "destinationFile": "output/output-01.json",
    "mediaType": "application/json",
    "rename": [{
        "pre": "content.item.chat_messages",
        "post": "content.channel"
    }, {
        "pre": "content.item.uuid",
        "post": "content.filename"
    }, {
        "pre": "content.item.name",
        "post": "content.title"
    }]
}

var message = { "workingDir": "src/applications/test_restructure/data" }

const read = new FileReader(config)
message = await read.process(message)

const restructure = new Restructure(config)
message = await restructure.process(message)

const write = new FileWriter(config)
await write.process(message)

================
File: src/applications/test/restructure/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trn: <http://purl.org/stuff/transmissions/> .
@prefix : <http://purl.org/stuff/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:cjc a trn:Transmission ;


trn:pipe (:read :restructure :SM   :write) .

:read a :FileReader ;
     trn:settings :reader .

:restructure a :Restructure ;
     trn:settings :retree .

:write a :FileWriter ;
     trn:settings :writer .

================
File: src/applications/test/stringops/about.md
================
`src/applications/test/stringops/about.md`

# Test Transmissions for StringOps Processor

## Runner

```sh
cd ~/hyperdata/transmissions # my local path
./trans stringops -m '{"fields": {"fieldB" : "TEST","fieldC":"_PASSED"}}'
```

## Tests

## Description

---

TEST_PASSED

================
File: src/applications/test/stringops/about.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

================
File: src/applications/test/stringops/config.ttl
================
# src/applications/example-application/config.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> . # for custom terms & instances

:filenameConstructor a :ConfigSet ;
  :asPath true ;
  :targetField "aFilepath" ;
  :values (:a1 :b1 :c1 :d1) .
  :a1 :string "TEST_PASSED" .
  :b1 :field "fields.fieldB" .
  :c1 :field "fields.fieldC" .
  :d1 :string "TEST_PASSED" .

  :stringConstructor a :ConfigSet ;
  :asPath false ;
  :targetField "aString" ;
  :values (:a2 :b2 :c2 :d2) .
  :a2 :string "TEST_PASSED" .
  :b2 :field "fields.fieldB" .
  :c2 :field "fields.fieldC" .
  :d2 :string ".html" .

================
File: src/applications/test/stringops/transmissions.ttl
================
# src/applications/example-application/transmissions.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix : <http://purl.org/stuff/transmissions/> .

##################################################################
# Utility Processors : insert into pipe for debugging            #
#                                                                #
:SM a :ShowMessage . # verbose report, continues pipe            #
:SC a :ShowConfig . # verbose report, continues pipe             #
:DE a :DeadEnd . # ends the current pipe quietly                 #
:H  a :Halt . # kills everything                                 #
:N  a :NOP . # no operation (except for showing stage in pipe)   #
:UF a :Unfork . # collapses all pipes but one                    #
##################################################################

:stringops a :Transmission ;
    :pipe (:pFilename :pString :SM) .

:pFilename a :StringOps ;
  :settings :filenameConstructor .

:pString a :StringOps ;
  :settings :stringConstructor .

================
File: src/applications/about.md
================
# About src/applications

- example-application
- pending : hold here until test criteria fulfilled

================
File: src/engine/AbstractProcessorFactory.js
================
import logger from '../utils/Logger.js'

// Import processor groups
import SystemProcessorsFactory from '../processors/system/SystemProcessorsFactory.js'
import TestProcessorsFactory from '../processors/test/TestProcessorsFactory.js'
import FsProcessorsFactory from '../processors/fs/FsProcessorsFactory.js'
import MarkupProcessorsFactory from '../processors/markup/MarkupProcessorsFactory.js'
import UtilProcessorsFactory from '../processors/util/UtilProcessorsFactory.js'
import TextProcessorsFactory from '../processors/text/TextProcessorsFactory.js'
import ProtocolsProcessorsFactory from '../processors/protocols/ProtocolsProcessorsFactory.js'
import RDFProcessorsFactory from '../processors/rdf/RDFProcessorsFactory.js'
import PostcraftProcessorsFactory from '../processors/postcraft/PostcraftProcessorsFactory.js'
import FlowProcessorsFactory from '../processors/flow/FlowProcessorsFactory.js'
import StagingProcessorsFactory from '../processors/staging/StagingProcessorsFactory.js'
import GitHubProcessorsFactory from '../processors/github/GitHubProcessorsFactory.js'
import JSONProcessorsFactory from '../processors/json/JSONProcessorsFactory.js'
import TerrapackProcessorsFactory from '../processors/terrapack/TerrapackProcessorsFactory.js' // 2025-01-01

// added 2024-11-28
import UnsafeProcessorsFactory from '../processors/unsafe/UnsafeProcessorsFactory.js'
import HttpProcessorsFactory from '../processors/http/HttpProcessorsFactory.js'
import McpProcessorsFactory from '../processors/mcp/McpProcessorsFactory.js'
import XmppProcessorsFactory from '../processors/xmpp/XmppProcessorsFactory.js'

// added 2025-01-14 : Happy Birthday to me!
import ExampleProcessorsFactory from '../processors/example-group/ExampleProcessorsFactory.js'

// 2025-01-16 : finally getting around to it
import SPARQLProcessorsFactory from '../processors/sparql/SPARQLProcessorsFactory.js'

class AbstractProcessorFactory {

    // looks until it finds

    static createProcessor(type, config) {
        if (!type) {
            throw new Error(`Processor type undefined (typo in 'transmission.ttl'..?)`)
        }
        logger.trace(`\nAbstractProcessorFactory.createProcessor : type.value = ${type.value}`)
        logger.trace(`AbstractProcessorFactory.createProcessor : config = ${config}`)

        var processor = ExampleProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = UnsafeProcessorsFactory.createProcessor(type, config)
        if (processor) return processor
        var processor = HttpProcessorsFactory.createProcessor(type, config)
        if (processor) return processor
        var processor = McpProcessorsFactory.createProcessor(type, config)
        if (processor) return processor
        var processor = XmppProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = TestProcessorsFactory.createProcessor(type, config)
        if (processor) return processor
        var processor = UtilProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = MarkupProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = TextProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = ProtocolsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = RDFProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = PostcraftProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = SystemProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FlowProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = GitHubProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = StagingProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = JSONProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = TerrapackProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = SPARQLProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

    }
}

export default AbstractProcessorFactory

================
File: src/engine/ApplicationManager.js
================
// ApplicationManager.js
import path from 'path'
import fs from 'fs/promises'
import _ from 'lodash'
// import rdf from 'rdf-ext'

// import ns from '../utils/ns.js'
import logger from '../utils/Logger.js'
import RDFUtils from '../utils/RDFUtils.js'

import Application from '../model/Application.js'
import MockApplicationManager from '../utils/MockApplicationManager.js'
import TransmissionBuilder from './TransmissionBuilder.js'
import ModuleLoaderFactory from './ModuleLoaderFactory.js'
import AppResolver from './AppResolver.js'

class ApplicationManager {
    constructor() {
        this.appResolver = new AppResolver()
        this.moduleLoader = null
        this.app = new Application()
        //   this.dataset = rdf.dataset()
    }

    async initialize(appName, appPath, subtask, target, flags) {
        logger.trace(`ApplicationManager.initialize, appName=${appName}, appPath=${appPath}, subtask=${subtask}, target=${target}`)

        if (flags && flags.test) {
            const mock = new MockApplicationManager()
            await mock.initialize(appName, appPath, subtask, target, flags)
            return mock
        }

        await this.appResolver.initialize(appName, appPath, subtask, target)
        this.moduleLoader = ModuleLoaderFactory.createApplicationLoader(this.appResolver.getModulePath())

        // TODO refactor more
        // Add to config before building transmissions
        await this.app.initDataset(appName)
        //    logger.log(`this.appResolver.dataset = ${this.appResolver.dataset}`)
        //  logger.log(`this.app.dataset = ${this.app.dataset}`)

        //     await this.app.mergeIn(this.appResolver.dataset)
        this.app.dataset = this.appResolver.dataset
        //   logger.log(`TYPEOF this.app.dataset = ${typeof this.app.dataset}`)
        //  logger.reveal(this.app.dataset)
        return this
    }

    async buildTransmissions(transmissionConfigFile, processorsConfigFile, moduleLoader, app) {
        logger.trace(`\nApplicationManager.build ****************************************`)

        const builder = new TransmissionBuilder(this.moduleLoader, this.appResolver)
        const transmissionConfig = await RDFUtils.readDataset(this.appResolver.getTransmissionsPath())
        const processorsConfig = await RDFUtils.readDataset(this.appResolver.getConfigPath())


        // Merge with app dataset
        /*
            for (const quad of app.dataset) {
              transmissionConfig.add(quad)
              processorsConfig.add(quad)
            }
        */
        return await builder.buildTransmissions(transmissionConfig, processorsConfig)
    }

    async start(message = {}) {
        logger.trace(`\n||| ApplicationManager.start`)
        message.app = this.app

        logger.debug(`
            transmissionsFile=${this.appResolver.getTransmissionsPath()}
            configFile=${this.appResolver.getConfigPath()}
            subtask=${this.appResolver.subtask}`)

        const transmissions = await this.buildTransmissions()

        // TODO this is wrong
        logger.debug(`Transmissions has length ${transmissions.length}`)

        // Get application context
        const contextMessage = this.appResolver.toMessage()

        // Modify the input message in place
        _.merge(message, contextMessage)
        message.appRunStart = (new Date()).toISOString()
        logger.trace('**************** Message with merged context:', message)

        /*
        for (const transmission of transmissions) {
            if (!this.appResolver.subtask || this.appResolver.subtask === transmission.label) {
                await transmission.process(message)
            }
        }
*/
        //       message.app = this.appResolver
        message.sessionNode = this.appResolver.sessionNode

        for (const transmission of transmissions) {
            logger.trace(`transmission = \n${transmission}`)
            if (!this.appResolver.subtask || this.appResolver.subtask === transmission.label) {
                //     await transmission.process(message)
                message = await transmission.process(message)
            }
        }
        message.success = true
        //     logger.reveal(message)
        return message //{ success: true }
    }

    async listApplications() {
        try {
            const entries = await fs.readdir(this.appResolver.appsDir, { withFileTypes: true })
            const subdirChecks = entries
                .filter(dirent => dirent.isDirectory())
                .map(async (dirent) => {
                    const subdirPath = path.join(this.appResolver.appsDir, dirent.name)
                    const files = await fs.readdir(subdirPath)
                    return files.includes('about.md') ? dirent.name : null
                })

            const validApps = (await Promise.all(subdirChecks)).filter(Boolean)
            return validApps
        } catch (err) {
            logger.error('Error listing applications:', err)
            return []
        }
    }
}

export default ApplicationManager

================
File: src/engine/AppResolver.js
================
import path from 'path'
import { fromFile } from 'rdf-utils-fs'
import fs from 'fs/promises'
import rdf from 'rdf-ext'
import logger from '../utils/Logger.js'
import RDFUtils from '../utils/RDFUtils.js'

class AppResolver {
    constructor(options = {}) {
        // Core paths
        this.appsDir = 'src/applications'
        this.transmissionFilename = 'transmissions.ttl'
        this.configFilename = 'config.ttl'
        this.moduleSubDir = 'processors'
        this.dataSubDir = 'data'
        this.manifestFilename = 'manifest.ttl'

        // Application identity
        this.appName = options.appName || null
        this.appPath = options.appPath || null
        this.subtask = options.subtask || null

        // Runtime paths
        this.rootDir = options.rootDir || null
        this.workingDir = options.workingDir || null
        this.targetPath = options.targetPath || null

        // RDF dataset from manifest
        this.dataset = options.dataset || null
    }

    async initialize(appName, appPath, subtask, target, flags = {}) {
        logger.debug(`AppResolver.initialize,
            appName : ${appName}
            appPath : ${appPath}
            subtask : ${subtask}
            target : ${target}`)

        this.appName = appName
        this.appPath = await this.resolveApplicationPath(appName)
        this.subtask = subtask
        this.targetPath = target

        if (target) {
            this.manifestFilename = path.join(target, this.manifestFilename)
            logger.debug(`AppResolver, found manifest : ${this.manifestFilename}`)
            try {
                this.dataset = await RDFUtils.readDataset(this.manifestFilename)
            } catch (e) {
                this.dataset = rdf.dataset()
            }
        }
    }

    async findInDirectory(dir, targetName, depth = 0) {
        if (depth > 3) return null

        try {
            const entries = await fs.readdir(dir, { withFileTypes: true })

            for (const entry of entries) {
                if (!entry.isDirectory()) continue

                const fullPath = path.join(dir, entry.name)

                // Check if this directory matches
                if (entry.name === targetName) {
                    const transmissionsFile = path.join(fullPath, this.transmissionFilename)
                    try {
                        await fs.access(transmissionsFile)
                        return fullPath
                    } catch {
                        // Has matching name but no transmissions.ttl
                    }
                }

                // Recurse into subdirectories
                const found = await this.findInDirectory(fullPath, targetName, depth + 1)
                if (found) return found
            }
        } catch (err) {
            logger.debug(`Error scanning directory ${dir}: ${err.message}`)
        }

        return null
    }

    async resolveApplicationPath(appName) {
        if (!appName) {
            throw new Error('Application name is required')
        }

        const baseDir = path.join(process.cwd(), this.appsDir)
        const appPath = await this.findInDirectory(baseDir, appName)

        if (!appPath) {
            throw new Error(`Could not find application ${appName} with transmissions.ttl in any subdirectory`)
        }

        return appPath
    }

    /*
    async loadManifest() {
        try {
            logger.debug(`AppResolver.loadManifest, loading: ${this.manifestFilename}`)
            const stream = fromFile(this.manifestFilename)
            this.dataset = await rdf.dataset().import(stream)
            return this.dataset
        } catch (err) {
            logger.debug(`AppResolver.loadManifest, ${this.manifestFilename} not found, creating empty dataset`)
            this.dataset = rdf.dataset()
            return this.dataset
        }
    }
*/
    getTransmissionsPath() {
        return path.join(this.appPath, this.transmissionFilename)
    }

    getConfigPath() {
        return path.join(this.appPath, this.configFilename)
    }

    getModulePath() {
        logger.debug(`AppResolver.getModulePath,\nthis.appPath : ${this.appPath}\nthis.moduleSubDir : ${this.moduleSubDir}`)
        return path.join(this.appPath, this.moduleSubDir)
    }

    resolveDataDir() {
        if (this.targetPath){
            this.workingDir = this.targetPath
        }
        if (!this.workingDir) {
            this.workingDir = path.join(this.appPath, this.dataSubDir)
        }
        return this.workingDir
    }

    toMessage() {
        return {
            appName: this.appName,
            appPath: this.appPath,
            subtask: this.subtask,
            rootDir: this.rootDir || this.appPath,
            workingDir: this.resolveDataDir(),
            targetPath: this.targetPath,
            dataset: this.dataset
        }
    }
}

export default AppResolver

================
File: src/engine/ModuleLoader.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import logger from '../utils/Logger.js'

class ModuleLoader {
    constructor(classpath) {
        if (!Array.isArray(classpath)) {
            throw new Error('Classpath must be an array of paths')
        }
        this.classpath = classpath.map(p => path.normalize(p))
        this.moduleCache = new Map()
        logger.debug(`ModuleLoader initialized with paths:\n${this.classpath.join('\n')}`)
    }

    async loadModule(moduleName) {
        logger.debug(`ModuleLoader.loadModule attempting to load: ${moduleName}`)

        if (this.moduleCache.has(moduleName)) {
            logger.debug(`Retrieved ${moduleName} from cache`)
            return this.moduleCache.get(moduleName)
        }

        const errors = []
        for (const basePath of this.classpath) {
            try {
                const fullPath = path.join(basePath, `${moduleName}.js`)
                logger.debug(`Trying path: ${fullPath}`)
                const module = await import(fullPath)
                this.moduleCache.set(moduleName, module)
                logger.debug(`Successfully loaded ${moduleName} from ${fullPath}`)
                return module
            } catch (error) {
                errors.push(`${basePath}: ${error.message}`)
                continue
            }
        }

        const errorMsg = `Failed to load module '${moduleName}' from paths:\n${errors.join('\n')}`
        throw new Error(errorMsg)
    }

    clearCache() {
        this.moduleCache.clear()
    }

    addPath(newPath) {
        if (typeof newPath !== 'string') {
            throw new TypeError('Path must be a string')
        }
        const normalizedPath = path.normalize(newPath)
        if (!this.classpath.includes(normalizedPath)) {
            this.classpath.push(normalizedPath)
            logger.debug(`Added path to classpath: ${normalizedPath}`)
        }
    }
}

export default ModuleLoader

================
File: src/engine/ModuleLoaderFactory.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import logger from '../utils/Logger.js'
import ModuleLoader from './ModuleLoader.js'

class ModuleLoaderFactory {
    static instance = null;

    static createModuleLoader(classpath) {
        logger.debug(`\nModuleLoaderFactory.createModuleLoader, classpath =\n ${classpath}`)
        const __filename = fileURLToPath(import.meta.url)
        const __dirname = path.dirname(__filename)

        if (!ModuleLoaderFactory.instance) {
            ModuleLoaderFactory.instance = new ModuleLoader(classpath)
        }

        return ModuleLoaderFactory.instance
    }

    static createApplicationLoader(appPath) {
        logger.debug(`\nModuleLoaderFactory.createApplicationLoader called with ${appPath}`)
        if (!appPath) {
            throw new Error('Application path is required')
        }

        // TODO revisit
        const appProcessorsPath = appPath
        // const appProcessorsPath = path.join(appPath, 'processors')

        const corePath = path.resolve(process.cwd(), 'src/processors')

        logger.debug(`ModuleLoaderFactory creating loader with paths:
      App: ${appProcessorsPath}
      Core: ${corePath}`)

        return this.createModuleLoader([appProcessorsPath, corePath])
    }

    static clearInstance() {
        ModuleLoaderFactory.instance = null
    }
}
export default ModuleLoaderFactory

================
File: src/engine/ProcessorSettings.js
================
// import Dataset from '@rdfjs/dataset/DatasetCore.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import ns from '../utils/ns.js'
import logger from '../utils/Logger.js'
import GrapoiHelpers from '../utils/GrapoiHelpers.js'

class ProcessorSettings {
    constructor(config) {
        this.config = config
    }

    // rename...to what?
    getProperty(settingsNode, property, fallback = undefined) {
        logger.trace(`\nProcessorSettings.getProperty looking for ${property}`)
        logger.trace(`settingsNode = ${settingsNode}`)
        this.settingsNode = settingsNode
        //    if (this.settingsNode) logger.trace(`this.settingsNode = ${this.settingsNode.value}`)
        //   this.settee.settingsNode = settingsNode

        const values = this.getValues(settingsNode, property)
        if (values.length == 0) {
            return fallback
        }
        return values.length == 1 ? values[0] : values

        /*
        const value = this.getValue(property, fallback)

        logger.trace(`Processor.getProperty, value = ${value}`)
        return value
        */
    }

    /*
    getValues(property, fallback) {
        logger.trace(`ProcessorSettings.getValues,
                looking for ${property}`)

        var value = this.propertyInMessage(property)
        if (value) {
            return [value]
        }

        this.settee.settingsNode = this.settingsNode
        var values = this.settee.valuesFromDataset(this.app.dataset, property)
        if (!values) {
            values = this.settee.getValues(property, fallback)
        }
        logger.trace(`Processor.getValues values = ${values}`)
        return values
    }
        */

    valuesFromDataset(dataset, property) {
        if (!dataset) return undefined
        const ptr = grapoi({ dataset, term: this.settingsNode })
        //      logger.trace(`ProcessorSettings.valuesFromDataset,  this.settingsNode = ${this.settingsNode.value}`)
        //   logger.trace(`valuesFromDataset, this.settingsNode = ${this.settingsNode.value}`)
        //    logger.trace(`valuesFromDataset, property = ${property}`)
        //     logger.reveal(ptr)
        //  logger.sh(dataset)
        //
        // Special handling for rename lists

        if (property.equals(ns.trn.rename)) {
            try {
                return GrapoiHelpers.listToArray(dataset, this.settingsNode, property)
            } catch (err) {
                logger.error(`Error extracting list values for ${property}: ${err}`)
                return []
            }
        }

        // Regular property handling
        try {

           const value1 = ptr.out(property)
            logger.trace(`\n\nvalue1 = ${value1.value}`)

            // Check if property exists but doesn't have value1
            if (value1.terms.length === 0) {
                return []
            }

            const first = this.tryFirst(dataset, value1)
            if (first) {
                const arr = GrapoiHelpers.listToArray(dataset, this.settingsNode, property)
                //      logger.log(`\narr = ${arr}`)
                for (var i = 0; i < arr.length; i++) {
                    //        logger.log(`\narr[i] = `)
             //       logger.reveal(arr[i])
                }
                //    return arr
            }
            //////////////////////

            // Process value based on type
            if (value1.terms.length === 1) {
                return [value1.value]
            } else {
                var values = value1.terms.map(term => term.value)
                logger.reveal(values)
                return values
            }
        } catch (e) {
            logger.error(`Error getting values for ${property}: ${e}`)
            return []
        }
    }

    /** looks to see if exists
        <value1> rdf:first ?o
    */
    tryFirst(dataset, maybeList) {
        try {
            const maybe = grapoi({ dataset, term: maybeList }).out(ns.rdf.first)
            //  logger.log(`OUT = ${maybe.value}`)
            return maybe
        } catch (e) {
            return false
        }
    }

    getValues(settingsNode, property, fallback) {
        this.settingsNode = settingsNode
        logger.trace(`\n\nProcessorSettings.getValues, property = ${property.value}`)

        if (!this.settingsNode) {
            return fallback ? [fallback] : []
        }

        /////////////////////////////////////////////////////////////////////////////////////////////////
        // logger.trace(`settingsNode = ${this.settingsNode.value}`)

        logger.trace(`\n\n   *** ProcessorSettings.getValues, looking for ${property} in APP dataset`)
        var dataset = this.app?.dataset

        if (dataset) {
            // logger.trace('------------------------------------')
            //logger.log(dataset)
            // logger.trace('------------------------------------')
            var values = this.valuesFromDataset(dataset, property)
            if (values && values.length > 0) {
                logger.trace(`   ProcessorSettings.getValues, found in APP dataset (manifest.ttl): ${values}`)
                return values
            }
        }

        logger.trace(`*** ProcessorSettings.getValues, looking for ${property} in CONFIG dataset (config.ttl)`)
        dataset = this.config

        /*
        logger.trace('------------------------------------')
        logger.reveal(dataset)
        logger.trace('------------------------------------')
        */

        values = this.valuesFromDataset(dataset, property)
        if (values && values.length > 0) {
            return values
        }

        return fallback ? [fallback] : []
    }

    /*
    getValue(property, fallback) {
        const values = this.getValues(property, fallback)

        logger.trace(`All values2: ${values}`)
        if (values.length == 0) {
            return undefined
        }
        return values.length == 1 ? values[0] : values // TODO DEPRECATED
    }
        */
}

export default ProcessorSettings

================
File: src/engine/TransmissionBuilder.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'

import ns from '../utils/ns.js'
import GrapoiHelpers from '../utils/GrapoiHelpers.js'
import logger from '../utils/Logger.js'

import AbstractProcessorFactory from "./AbstractProcessorFactory.js"
import Transmission from '../model/Transmission.js'
import Whiteboard from '../model/Whiteboard.js'

class TransmissionBuilder {
  constructor(moduleLoader, app) {
    this.moduleLoader = moduleLoader
    this.app = app
    this.transmissionCache = new Map()
    this.MAX_NESTING_DEPTH = 10
    this.currentDepth = 0
  }

  async buildTransmissions(transmissionConfig, processorsConfig) {
    logger.debug(`\nTransmissionBuilder.buildTransmissions`)
    logger.trace(`transmissionConfig = \n${transmissionConfig}`)
    const poi = grapoi({ dataset: transmissionConfig })
    const transmissions = []

    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.trn.Transmission)) {
        const transmissionID = q.subject
        logger.debug(`\ntransmissionID = ${transmissionID.value}`)

        const transmission = await this.constructTransmission(
          transmissionConfig,
          transmissionID,
          processorsConfig
        )
        transmissions.push(transmission)
      }
    }
    return transmissions
  }

  async constructTransmission(transmissionConfig, transmissionID, processorsConfig) {
    logger.debug(`\nTransmissionBuilder.constructTransmission`)

    if (++this.currentDepth > this.MAX_NESTING_DEPTH) {
      throw new Error(`Maximum transmission nesting depth of ${this.MAX_NESTING_DEPTH} exceeded`)
    }

    if (this.transmissionCache.has(transmissionID.value)) {
      return this.transmissionCache.get(transmissionID.value)
    }

    const transmission = new Transmission()
    transmission.id = transmissionID.value
    transmission.app = this.app

    transmission.whiteboard = new Whiteboard()

    //  processorsConfig.whiteboard = {}
    transmission.label = ''

    const transPoi = grapoi({ dataset: transmissionConfig, term: transmissionID })
    const pipenodes = GrapoiHelpers.listToArray(transmissionConfig, transmissionID, ns.trn.pipe)

    for (const quad of transPoi.out(ns.rdfs.label).quads()) {
      transmission.label = quad.object.value
    }
    logger.log('\n+ ***** Construct Transmission : ' + transmission.label + ' <' + transmission.id + '>')

    await this.createNodes(transmission, pipenodes, transmissionConfig, processorsConfig)
    this.connectNodes(transmission, pipenodes)

    this.currentDepth--
    return transmission
  }

  async createNodes(transmission, pipenodes, transmissionConfig, processorsConfig) {
    for (const node of pipenodes) {
      //  node.value is either the name of a processor or a nested transmission

      if (!transmission.get(node.value)) {
        const np = rdf.grapoi({ dataset: transmissionConfig, term: node })
        const processorType = np.out(ns.rdf.type).term

        const settingsNode = np.out(ns.trn.settings).term
     //   const settingsNodeName = settingsNode ? settingsNode.value : undefined

        logger.debug(`Creating processor:
          Node: :${ns.shortName(node?.value)}
          Type: :${ns.shortName(processorType?.value)}
          SettingsNode: :${ns.shortName(settingsNode?.value)}
        `)
        //    Config: \n${processorsConfig}
        // Check if node is a nested transmission transmissionConfig
        // if (processorType && this.isTransmissionReference(processorType)) {
        if (processorType && this.isTransmissionReference(transmissionConfig, processorType)) {
          const nestedTransmission = await this.constructTransmission(
            transmissionConfig,
            processorType, // is used?
            processorsConfig
          )
          transmission.register(node.value, nestedTransmission)
        } else {
          // Regular processor handling
          const processor = await this.createProcessor(processorType, processorsConfig)
          processor.id = node.value
          processor.type = processorType
          if (settingsNode) {
            processor.settingsNode = settingsNode
          }
          processor.transmission = transmission
          processor.whiteboard = transmission.whiteboard // feels redundant...
          transmission.register(node.value, processor)
        }
      }
    }
  }

  isTransmissionReference(transmissionConfig, processorType) {
    const processorPoi = grapoi({ dataset: transmissionConfig, term: processorType })
    return processorPoi.out(ns.rdf.type).terms.some(t => t.equals(ns.trn.Transmission))
  }

  /*
    isTransmissionReference(processorType) {
    const processorPoi = grapoi({ dataset: this.app.dataset, term: processorType })
    return processorPoi.out(ns.rdf.type).terms.some(t => t.equals(ns.trn.Transmission))
  }
    */

  getPipeNodes(transmissionConfig, transmissionID) {
    const transPoi = grapoi({ dataset: transmissionConfig, term: transmissionID })
    return transPoi.out(ns.trn.pipe).terms
  }

  async connectNodes(transmission, pipenodes) {
    for (let i = 0; i < pipenodes.length - 1; i++) {
      let leftNode = pipenodes[i]
      let leftProcessorName = leftNode.value
      let rightNode = pipenodes[i + 1]
      let rightProcessorName = rightNode.value
      logger.log("  > Connect #" + i + " [" + ns.getShortname(leftProcessorName) + "] => [" + ns.getShortname(rightProcessorName) + "]")
      transmission.connect(leftProcessorName, rightProcessorName)
    }
  }

  async createProcessor(type, config) {
    logger.trace(`\n\nTransmissionBuilder.createProcessor, config = ${config}`)

    const coreProcessor = AbstractProcessorFactory.createProcessor(type, config)
    if (coreProcessor) {
      return coreProcessor
    }

    logger.debug(`TransmissionBuilder, core processor not found for ${type?.value}. Trying remote module loader...`)

    try {
      const shortName = type.value.split('/').pop()
      logger.debug(`TransmissionBuilder, loading module: ${shortName}`)
      const ProcessorClass = await this.moduleLoader.loadModule(shortName)

      logger.debug(`Module loaded successfully: ${shortName}`)
      return new ProcessorClass.default(config)
    } catch (error) {
      logger.error(`TransmissionBuilder.createProcessor, failed to load ${type?.value} : ${error.message}`)
      process.exit(1)
    }
  }


}

export default TransmissionBuilder

================
File: src/engine/WorkerPool.js
================
import { Worker } from 'worker_threads'

class WorkerPool {
    constructor(module, size) {
        this.workers = [];
        this.queue = [];
        for (let i = 0; i < size; i++) {
            const worker = new Worker(module);
            worker.on('message', () => {
                // Handle completion, possibly dispatching next message
                this.markWorkerIdle(worker);
            });
            this.workers.push({ worker, busy: false });
        }
    }

    enqueueMessage(message) {
        this.queue.push(message);
        this.dispatch();
    }

    dispatch() {
        const idleWorkerWrapper = this.workers.find(wrapper => !wrapper.busy);
        if (idleWorkerWrapper && this.queue.length) {
            const message = this.queue.shift();
            idleWorkerWrapper.busy = true;
            idleWorkerWrapper.worker.postMessage(message);
        }
    }

    markWorkerIdle(workerWrapper) {
        workerWrapper.busy = false;
        this.dispatch(); // Check if there's more work to do
    }
}

================
File: src/model/Application.js
================
import rdf from 'rdf-ext'

import ns from '../utils/ns.js'
import logger from '../utils/Logger.js'
// import Transmission from './Transmission.js'

class Application {
    constructor() {
        this.dataset = rdf.dataset()
        this.dummy = 'dummy'
        this.dataset.add(rdf.quad(
            rdf.namedNode(`http://purl.org/stuff/transmissions/dummy`),
            ns.rdf.type,
            ns.trn.Application
        ))
    }

    async initDataset(appName, sessionNode = rdf.blankNode()) {

        // TODO validate syntax of appName
        this.appNode = rdf.namedNode(`http://purl.org/stuff/transmissions/${appName}`)
        this.sessionNode = sessionNode

        this.dataset.add(rdf.quad(
            this.appNode,
            ns.rdf.type,
            ns.trn.Application
        ))

        this.dataset.add(rdf.quad(
            this.sessionNode,
            ns.rdf.type,
            ns.trn.ApplicationSession
        ))

        this.dataset.add(rdf.quad(
            this.sessionNode,
            ns.trn.application,
            this.appNode
        ))
    }

    async mergeIn(dataset) {
        /*
        logger.log('--------------------MERGEIN----------------')
        logger.log(dataset)
        logger.reveal(dataset)
        logger.log(this.dataset)
        logger.reveal(this.dataset)
        logger.log('--------------^^^^^^^^^^^^^^^^^^^^-')
        */
        this.dataset.addAll(dataset)
        /*
        logger.log('--------------------MERGEDDDDDDDDDDDDD----------------')
        logger.log(this.dataset)
        logger.reveal(this.dataset)
        logger.log('---------------^^^^^^^^^^^^^^D----------------')
    */
    }

    toString() {
        return this.dataset.toString()
    }
}
export default Application

================
File: src/model/Connector.js
================
import ns from '../utils/ns.js'
import { EventEmitter } from 'events'
import logger from '../utils/Logger.js'
import Transmission from './Transmission.js'

class Connector extends EventEmitter {

    constructor(fromName, toName) {
        super()
        this.fromName = fromName
        this.toName = toName
    }

    connect(processors) {
        logger.trace(`Connector.connect this.fromName = ${this.fromName} this.toName =  ${this.toName}`)
        const fromProcessor = processors[this.fromName]
        const toProcessor = processors[this.toName]
        if (fromProcessor instanceof Transmission) {
            // Connect last node of nested transmission
            const lastNode = fromProcessor.getLastNode()
            lastNode.on('message', async (message) => {
                await toProcessor.receive(message)
            })
        } else if (toProcessor instanceof Transmission) {
            // Connect to first node of nested transmission
            fromProcessor.on('message', async (message) => {
                const firstNode = toProcessor.getFirstNode()
                await firstNode.receive(message)
            })
        } else {
            if (!fromProcessor) {
                throw new Error(`\nMissing processor : ${this.fromName}, going to ${this.toName} \n(check for typos in transmissions.ttl)\n`)
            }

            fromProcessor.on('message', async (message) => {
                var tags = fromProcessor.message?.tags ? ` [${fromProcessor.message.tags}] ` : ''
                toProcessor.tags = tags
                logger.log(`|-> ${tags}-> ${ns.shortName(toProcessor.id)} a ${toProcessor.constructor.name}`)
                await toProcessor.receive(message)
            })
        }

    }


}

export default Connector

================
File: src/model/Processor.js
================
import { EventEmitter } from 'events'
import logger from '../utils/Logger.js'
import ns from '../utils/ns.js'
import SysUtils from '../utils/SysUtils.js'
import ProcessorSettings from '../engine/ProcessorSettings.js'

class Processor extends EventEmitter {
    constructor(config) {
        super()
        this.config = config
        this.settee = new ProcessorSettings(this.config)
        this.messageQueue = []
        this.processing = false
        this.outputs = []
    }

    // TODO needed?
    getAppPath(relativePath) {
        if (!this.app?.rootDir) {
            throw new Error('Application context not available')
        }
        return path.join(this.app.rootDir, relativePath)
    }

    // TODO tidy up
    getValues(property, fallback) {
        return this.settee.getValues(this.settingsNode, property, fallback)
    }

    getProperty(property, fallback = undefined) {
        logger.trace(`\nProcessor.getProperty looking for ${property}`)
        // first check if the property is in the message
        var value = this.propertyInMessage(property)
        if (value) {
            logger.trace(`property found in message : ${value}`)
            return value
        }
        logger.trace(`\nProcessor.getProperty this.settingsNode = ${this.settingsNode}`)
        logger.trace(`\nProcessor.getProperty    typeof this.settingsNode = ${typeof this.settingsNode}`)

        // Get values from settings
        const values = this.settee.getValues(this.settingsNode, property, fallback)

        // If it's a single value, return it directly, otherwise return the array
        if (values && Array.isArray(values)) {
            if (values.length === 1) {
                return values[0]
            } else if (values.length > 1) {
                return values
            }
        }

        return fallback
    }

    propertyInMessage(property) {
        const shortName = ns.getShortname(property)
        if (this.message && this.message[shortName]) {
            logger.trace(`Found in message: ${this.message[shortName]}`)
            return this.message[shortName]
        }
        return undefined
    }

    async preProcess(message) {

        this.app = message.app
        // this.config.app = this.app // ??????????
        this.settee.app = this.app
        //    logger.log(`THIS APP = ${this.app}`)

        if (message.onProcess) { // Claude
            message.onProcess(this, message)
        }

        this.previousLogLevel = logger.getLevel()


        const debug = this.getProperty(ns.trn.debug)

        if (debug) {
            logger.setLogLevel('debug')
        }
        logger.debug(`Processor.preProcess, debug = ${debug}`)

        /* TODO uncomment after config sorted
        const messageType = this.getProperty(ns.trn.messageType)
        if (messageType) {
            if (messageType.value) {
                message.messageType = messageType.value
            } else {
                message.messageType = messageType
            }
        }
            */
        this.message = message
    }

    /*
async process(message) {
    throw new Error('process method not implemented')
}
*/

    /* cLAUDE
    // is useful?
    async process(message) {
        if (message.onProcess) {
            message.onProcess(this, message)
        }
        await this.emit('message', message)
    }
*/

    async postProcess(message) {
        logger.setLogLevel(this.previousLogLevel)
        this.previousLogLevel = null
    }

    async receive(message) {
        await this.enqueue(message)
    }

    async enqueue(message) {
        this.messageQueue.push({ message })
        if (!this.processing) {
            this.executeQueue()
        }
    }

    async executeQueue() {
        this.processing = true
        while (this.messageQueue.length > 0) {
            let { message } = this.messageQueue.shift()

            /* structuredClone makes a deep copy of the message object
            *  so that the original message is not modified
            *  except its depth doesn't appear to cover internal objects
            *  so here the app.dataset is passed between messages
            *  (which is fine)
            */
            //            const dataset = message.app.dataset
            //          message = structuredClone(message)
            //        message.app.dataset = dataset
            message = SysUtils.copyMessage(message)

            this.addTag(message)
            logger.trace(`BEFORE PRE`)
            await this.preProcess(message)
            logger.trace(`AFTER PRE`)
            await this.process(message)
            await this.postProcess(message)
        }
        this.processing = false
    }

    addTag(message) {
        const tag = this.getTag()
        if (!message.tags) {
            message.tags = tag
            return
        }
        message.tags = message.tags + '.' + tag
    }

    getTag() {
        return ns.shortName(this.id)
    }

    async emit(event, message) {
        await new Promise(resolve => {
            super.emit(event, message)
            resolve()
        })
        return message
    }

    getOutputs() {
        const results = this.outputs
        this.outputs = []
        return results
    }

    toString() {
        logger.reveal(this.settings)
        const settingsNodeValue = this.settingsNode ? this.settingsNode.value : 'none'
        return `
        *** Processor ${this.constructor.name}
                id = ${this.id}
                label = ${this.label}
                type = ${this.type}
                description = ${this.description}
                settingsNodeValue = ${settingsNodeValue}
                settings = ${this.settings}
       `
    }
}

export default Processor

================
File: src/model/SlowableProcessor.js
================
import { EventEmitter } from 'events'
import Processor from './Processor.js'
import logger from '../utils/Logger.js'
import ns from '../utils/ns.js'
import SysUtils from '../utils/SysUtils.js'

class SlowableProcessor extends Processor {

    constructor(config) {
        super(config)
    }

    async preProcess(message) {
        const delay = super.getProperty(ns.trn.delay, '0') // 100mS is default if unspecified elsewhere
        logger.trace(`SPEEPIES`)
        await SysUtils.sleep(delay)
        return super.preProcess(message)
    }
}
export default SlowableProcessor

================
File: src/model/Transmission.js
================
import logger from '../utils/Logger.js'
import Connector from './Connector.js'
import ns from '../utils/ns.js'

class Transmission {
  constructor() {
    this.processors = {}
    this.connectors = []
    this.parent = null
    this.children = new Set()
    this.path = []
  }

  async process(message) {
    logger.debug(`\nTransmission.process`)
    logger.log(`\n+ Run Transmission : ${this.label} <${this.id}>`)

    try {
      const processorName = this.connectors[0]?.fromName || Object.keys(this.processors)[0]
      let processor = this.get(processorName)
      if (processor) {
        logger.log(`|-> ${ns.shortName(processorName)} a ${processor.constructor.name}`)
        await processor.receive(message)
      } else {
        throw new Error("No valid processor found to execute")
      }
    } catch (error) {
      error.transmissionStack = error.transmissionStack || []
      error.transmissionStack.push(this.id)
      throw error
    }
    return message
  }

  register(processorName, instance) {
    let processor = this.get(processorName)
    if (processor instanceof Transmission) {
      processor.parent = this
      processor.path = [...this.path, processorName]
      this.children.add(processor)
    }
    this.processors[processorName] = instance
  }

  get(processorName) {
    return this.processors[processorName]
  }

  connect(fromProcessorName, toProcessorName) {
    logger.trace(`Transmission.connect from ${fromProcessorName} to ${fromProcessorName}`)
    let connector = new Connector(fromProcessorName, toProcessorName)
    this.connectors.push(connector)
    connector.connect(this.processors)
  }

  getFirstNode() { // used for nested transmissions
    logger.log(this)
    logger.log(this.processors[0])
    return this.processors[0]
  }

  // is used?
  handleError(error) {
    logger.error(`Error in transmission ${this.id}:`, error)
    logger.error('Transmission stack:', error.transmissionStack)
    // Optionally attempt recovery
    if (this.parent) {
      this.parent.handleError(error)
    }
  }

  getTransmissionInfo() {
    return {
      id: this.id,
      path: this.path,
      depth: this.path.length,
      children: Array.from(this.children).map(c => c.id)
    }
  }

  toString() {
    const info = this.getTransmissionInfo()
    let description = `Transmission : ${info.id}
      path: ${info.path},
      depth: ${info.path.length},
      children: ${info.children}`
    // Describe processors
    description += 'Processors:\n'
    Object.keys(this.processors).forEach((processorName) => {
      description += `  - ${ns.shortName(processorName)} a ${this.processors[processorName]} \n`
    })

    // Describe connectors
    description += 'Connectors:\n'
    this.connectors.forEach((connector, index) => {
      description += `  - Connector ${index + 1}: ${ns.shortName(connector.fromName)} -> ${ns.shortName(connector.toName)} \n`
    })

    return description
  }
}

export default Transmission

================
File: src/model/Whiteboard.js
================
import logger from '../utils/Logger.js'

class Whiteboard {
    constructor() {
        this.accumulators = {}
        if (Whiteboard.singleInstance) {
            return Whiteboard.singleInstance
        }
        Whiteboard.singleInstance = this
    }

    accumulate(label, value) {
        var acc = this.accumulators[label]
        switch (typeof acc) {
            case 'object':
                this.accumulators[label].push(value)
                break
            default:
                this.accumulators[label] = `${acc}${value}`
        }
        return this.accumulators[label]
    }

    getAccumulator(label, type) {
        if (!this.accumulators[label]) {
            switch (type) {
                case 'object':
                    this.accumulators[label] = []
                    break
                default:
                    this.accumulators[label] = ''
            }
        }
        return this.accumulators[label]
    }

    toString() {
        var string = '--- Whiteboard ---'
        for (key in accumulators.keys()) {
            string = `${string}\n${key} = ${accumulators[key]}`
        }
        return `${string}\n------------------`
    }
}

export default Whiteboard

================
File: src/processors/example-group/ExampleProcessor.js
================
// src/processors/example-group/ExampleProcessor.js
/**
 * @class ExampleProcessor
 * @extends Processor
 * @classdesc
 * **a Basic Transmissions Processor**
 *
 * Provides a template for creating new processors, demonstrates use of config settings.
 *
 * #### __*Input*__
 * * **`message.common`** - addressed by all instances of this ExampleProcessor (optional, default undefined)
 * * **`message.something1`** - Template string (used if templateFilename is not provided)
 * * **`message.something2`** - Object with properties for template rendering (e.g., title, body)
 * * **`message.notavalue`** - Object with properties for template rendering (e.g., title, body)
 *
 * #### __*Output*__
 * * **`message.content`** - The rendered template content
 *
 * #### __*Processing*__
 * * Uses Nunjucks to render templates
 * * Can render from a template file or a template string
 * * Applies content from message.contentBlocks to the template
 *
* #### __*Side Effects*__
 *
 * #### __Tests__
 * *
 *
  * #### __*ToDo*__
 * * Add test information here
 * * Cache templates - cache in utils?
 */

import { readFile } from 'node:fs/promises'
import { access, constants } from 'node:fs'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'


class ExampleProcessor extends Processor {
    constructor(config) {
        super(config)
    }

    /**
      * Does something with the message and emits a 'message' event with the processed message.
      * @param {Object} message - The message object.
      */
    async process(message) {
        logger.debug(`\n\nExampleProcessor.process`)

        // TODO figure this out better
        // may be needed if preceded by a spawning processor, eg. fs/DirWalker
        if (message.done) {
            return this.emit('message', message)
            // or simply return
        }

        // message is processed here :

        // property values pulled from message | config settings | fallback
        const me = await this.getProperty(ns.trn.me)
        logger.log(`\nI am ${me}`)

        message.common = await this.getProperty(ns.trn.common)
        message.something1 = await this.getProperty(ns.trn.something1)

        message.something2 = await this.getProperty(ns.trn.something2)

        var added = await this.getProperty(ns.trn.added, '')
        message.something1 = message.something1 + added

        message.notavalue = await this.getProperty(ns.trn.notavalue, 'fallback value')

        // message forwarded
        return this.emit('message', message)
    }
}
export default ExampleProcessor

================
File: src/processors/example-group/ExampleProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import ExampleProcessor from './ExampleProcessor.js'
/*
   a ref to this should go in `src/processors/base/AbstractProcessorFactory.js`
*/


// import OtherProcessor from './ExampleProcessor.js'

class ExampleProcessorsFactory {

    static createProcessor(type, config) {

        if (type.equals(ns.trn.ExampleProcessor)) {
            return new ExampleProcessor(config)
        }

        /** Other processors in the group follow the same pattern

        if (type.equals(ns.trn.OtherProcessor)) {
            return new OtherProcessor(config)
        }
            ...
        */

        return false
    }
}
export default ExampleProcessorsFactory

================
File: src/processors/flow/Accumulate.js
================
// TODO something about src/processors/util/CaptureAll.js

import { readFile } from 'node:fs/promises'
import { access, constants } from 'node:fs'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'
import JSONUtils from '../../utils/JSONUtils.js'

class Accumulate extends Processor {
    constructor(config) {
        super(config)
    }

    /**
      * Does something with the message and emits a 'message' event with the processed message.
      * @param {Object} message - The message object.
      */
    async process(message) {
        logger.trace(`\n\nAccumulate.process, done = ${message.done}`)

        const label = super.getProperty(ns.trn.label, 'test')
        const type = super.getProperty(ns.trn.accumulatorType, 'string')
        const acc = this.whiteboard.getAccumulator(label, type)



        if (message.done) {
            const targetField = super.getProperty(ns.trn.targetField, "accumulate")
            logger.trace(`targetField = ${targetField}`)
            message[targetField] = acc
            logger.trace(`full acc = ${acc}`)
            return this.emit('message', message)
        }

        const sourceField = super.getProperty(ns.trn.sourceField, "currentItem")
        const sourceValue = JSONUtils.get(message, sourceField)
        this.whiteboard.accumulate(label, sourceValue)
        logger.trace(`partial acc = ${acc}`)
        return this.emit('message', message)
    }
}
export default Accumulate

================
File: src/processors/flow/DeadEnd.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class DeadEnd extends Processor {

    async process(message) {
        logger.log('DeadEnd at [' + message.tags + '] ' + this.getTag())
    }

}
export default DeadEnd

================
File: src/processors/flow/FlowProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import ForEach from './ForEach.js'
import Ping from './Ping.js'
import NOP from '../flow/NOP.js'
import DeadEnd from '../flow/DeadEnd.js'
import Halt from '../flow/Halt.js'
import Unfork from '../flow/Unfork.js'
import Fork from '../flow/Fork.js'
import Accumulate from '../flow/Accumulate.js'

class FlowProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.ForEach)) {
            logger.debug('FlowProcessorsFactory: Creating ForEach processor')
            return new ForEach(config)
        }
        if (type.equals(ns.trn.Ping)) {
            logger.debug('FlowProcessorsFactory: Creating Ping processor')
            return new Ping(config)
        }
        if (type.equals(ns.trn.NOP)) {
            return new NOP(config)
        }
        if (type.equals(ns.trn.DeadEnd)) {
            return new DeadEnd(config)
        }
        if (type.equals(ns.trn.Halt)) {
            return new Halt(config)
        }
        if (type.equals(ns.trn.Fork)) {
            return new Fork(config)
        }
        if (type.equals(ns.trn.Unfork)) {
            return new Unfork(config)
        }
        if (type.equals(ns.trn.Accumulate)) {
            return new Accumulate(config)
        }
        return false
    }
}

export default FlowProcessorsFactory

================
File: src/processors/flow/ForEach.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import SysUtils from '../../utils/SysUtils.js'
import SlowableProcessor from '../../model/SlowableProcessor.js'
import JSONUtils from '../../utils/JSONUtils.js'

class ForEach extends SlowableProcessor {
    constructor(config) {
        super(config)
        this.eachCounter = 0
    }

    async process(message) {
        logger.trace('ForEach execute method called')

        // TODO default?
        const forEach = super.getProperty(ns.trn.forEach)

        // TODO add suport for removeOrigin - see Restructure, RDFUtils

        const remove = super.getProperty(ns.trn.remove, false)
        //  logger.warn(`REMOVE = ${remove}`)
        const split = forEach.split('.')

        // TODO is similar in 'processors/json/JsonRestructurer.js' - move to utils?
        const reduced = split.reduce((acc, part) => acc[part], message)

        logger.trace(`ForEach, reduced.length = ${reduced.length}`)

        //  logger.reveal(reduced)

        const delay = super.getProperty(ns.trn.delay, '100')

        message.done = false
        for (const item of reduced) {
            //  const clonedMessage = structuredClone(message)

            var clonedMessage = SysUtils.copyMessage(message)
            if (remove) {
                //      logger.warn(`REMOVE ORIGIN ${remove}`)
                clonedMessage = JSONUtils.remove(clonedMessage, remove)
            }
            clonedMessage.currentItem = item
            //    delete clonedMessage.foreach // Remove the original array to prevent infinite loops TODO needed?

            logger.trace(`ForEach: Emitting message for item: ${item}`)
            clonedMessage.eachCounter = this.eachCounter++
            this.emit('message', clonedMessage)
            //  await SysUtils.sleep(delay)
        }
        message.done = true
        /////////////////// TODO put back in
        this.emit('message', message)
        logger.trace('ForEach: Finished processing all items')
    }
}
export default ForEach

================
File: src/processors/flow/Fork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'


// rough, only for system testing

class Fork extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        //   logger.setLogLevel('debug')
        const nForks = message.nForks || 2

        logger.debug('forks = ' + nForks)

        for (let i = 0; i < nForks; i++) {
            var messageClone = structuredClone(message)
            messageClone.forkN = i
            logger.debug('--- emit --- ' + i)
            this.emit('message', messageClone)
        }

        message.done = true // one extra to flag completion

        return this.emit('message', message)
        //   return this.getOutputs()
    }

}

export default Fork

================
File: src/processors/flow/Halt.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class Halt extends Processor {

    process(message) {
        logger.log('\n************************************************************************')
        logger.log('*** << Thou Hast Summoned HALT, the Mighty Stopper of All Things  >> ***')
        logger.log('*** <<                   ~~~ ALL IS GOOD ~~~                      >> ***')
        logger.log('*** <<                     Have a nice day!                       >> ***')
        logger.log('************************************************************************\n')
        logger.log('*** Transmission was : ' + message.tags)
        logger.log('*** Context now : ')
        logger.reveal(message)
        process.exit() // all good
    }
}

export default Halt

================
File: src/processors/flow/NOP.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'

class NOP extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        const done = message.done ? `DONE` : `NOT DONE`
        logger.log(`\nNOP at [${message.tags}] ${this.getTag()} (${done})`)

        return this.emit('message', message)
    }

    double(string) {
        return string + string
    }
}
export default NOP

================
File: src/processors/flow/Ping.js
================
import { Worker } from 'worker_threads'
import path from 'path'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'

class Ping extends Processor {
    constructor(config) {
        super(config)
        this.worker = null
        this.pingConfig = {
            interval: this.getPropertyFromMyConfig(ns.trn.interval) || 5000,
            count: this.getPropertyFromMyConfig(ns.trn.count) || 0,
            payload: this.getPropertyFromMyConfig(ns.trn.payload) || 'ping',
            killSignal: this.getPropertyFromMyConfig(ns.trn.killSignal) || 'STOP',
            retryAttempts: this.getPropertyFromMyConfig(ns.trn.retryAttempts) || 3,
            retryDelay: this.getPropertyFromMyConfig(ns.trn.retryDelay) || 1000
        }
    }

    async process(message) {
        try {
            // Check for kill signal in incoming message
            if (message.kill === this.pingConfig.killSignal) {
                await this.shutdown()
                return this.emit('message', {
                    ...message,
                    pingStatus: 'stopped',
                    timestamp: Date.now()
                })
            }

            if (this.worker) {
                logger.warn('Ping worker already running, ignoring start request')
                return
            }

            let retryCount = 0
            const startWorker = async () => {
                try {
                    this.worker = new Worker(
                        path.join(process.cwd(), 'src/processors/flow/PingWorker.js')
                    )

                    this.worker.on('message', (msg) => {
                        switch (msg.type) {
                            case 'ping':
                                this.emit('message', {
                                    ...message,
                                    ping: {
                                        count: msg.count,
                                        timestamp: msg.timestamp,
                                        payload: msg.payload,
                                        status: 'running'
                                    }
                                })
                                break
                            case 'complete':
                                this.emit('message', {
                                    ...message,
                                    pingComplete: true,
                                    timestamp: Date.now()
                                })
                                break
                            case 'error':
                                this.handleWorkerError(msg.error, startWorker, retryCount)
                                break
                        }
                    })

                    this.worker.on('error', (error) => {
                        this.handleWorkerError(error, startWorker, retryCount)
                    })

                    this.worker.on('exit', (code) => {
                        if (code !== 0) {
                            this.handleWorkerError(
                                new Error(`Worker stopped with exit code ${code}`),
                                startWorker,
                                retryCount
                            )
                        }
                        this.worker = null
                    })

                    this.worker.postMessage({
                        type: 'start',
                        config: this.pingConfig
                    })

                } catch (error) {
                    this.handleWorkerError(error, startWorker, retryCount)
                }
            }

            await startWorker()

            return new Promise((resolve) => {
                this.worker.on('exit', () => {
                    resolve(message)
                })
            })

        } catch (error) {
            logger.error(`Failed to start ping processor: ${error}`)
            throw error
        }
    }

    async handleWorkerError(error, retryFn, retryCount) {
        logger.error(`Ping worker error: ${error}`)

        if (retryCount < this.pingConfig.retryAttempts) {
            retryCount++
            logger.info(`Retrying ping worker (attempt ${retryCount}/${this.pingConfig.retryAttempts})`)
            setTimeout(retryFn, this.pingConfig.retryDelay)
        } else {
            logger.error('Max retry attempts reached, stopping ping worker')
            this.emit('error', error)
            await this.shutdown()
        }
    }

    async shutdown() {
        if (this.worker) {
            this.worker.postMessage({ type: 'stop' })
            this.worker = null
        }
    }
}

export default Ping

================
File: src/processors/flow/Unfork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import DeadEnd from './DeadEnd.js'
/*
TODO move to ./flow

only accept the first call
*/

class Unfork extends Processor {

    constructor(config) {
        super(config)

        /* NOPE
        if (Unfork._instance) {
            return new DeadEnd(config)
        }
        Unfork._instance = this;
*/
    }

    async process(message) {
        //     logger.setLogLevel("debug")
        logger.debug(`Unfork got message with done=${message.done}, tags=${message.tags}`)

        logger.debug('Unfork ----')
        if (message.done) {
            logger.debug(' - Unfork passing message >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')
            message.done = false // in case it's needed later

            /*
                        await new Promise(resolve => {
                            //    super.emit(event, message)
                            return this.emit('message', message)
                            resolve()
                            logger.log(`after resolve has ${message.done}`)
                        })
            */
            return this.emit('message', message)
        } else {
            logger.debug(' - Unfork terminating pipe')
            return
        }
    }
}
export default Unfork

================
File: src/processors/fs/DirWalker.js
================
import { readdir } from 'fs/promises'
import path from 'path'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import SysUtils from '../../utils/SysUtils.js'
import Processor from '../../model/Processor.js'
import StringUtils from '../../utils/StringUtils.js'

class DirWalker extends Processor {
    constructor(config) {
        super(config)
        this.count = 0
    }



    async process(message) {
        logger.trace('\nDirWalker.process')
        logger.trace(`\nDirWalker.process, this = ${this}`)
        message.done = false

        var sourceDir = this.getProperty(ns.trn.sourceDir)
        logger.trace(`--------------- DirWalker sourceDir from config = ${sourceDir}`)

        if (!message.sourceDir) {
            message.sourceDir = sourceDir
        }

        if (!sourceDir) {
            sourceDir = message.workingDir
        }

        this.includePatterns = this.getProperty(ns.trn.includePattern, ['*.md', '*.js', '*.json', '*.ttl'])
        this.excludePatterns = this.getProperty(ns.trn.excludePattern, ['*.', '.git', 'node_modules'])

        logger.trace('\n\nDirWalker, message.targetPath = ' + message.targetPath)
        logger.trace('DirWalker, message.rootDir = ' + message.rootDir)
        logger.trace('DirWalker, message.sourceDir = ' + message.sourceDir)

        //    logger.log(`DirWalker.sourceDir = ${sourceDir}`)
        //  logger.reveal(sourceDir)
        let dirPath
        if (path.isAbsolute(sourceDir)) {
            dirPath = sourceDir
        } else {
            if (message.targetPath) {
                dirPath = path.join(message.targetPath, sourceDir)
            } else {
                dirPath = path.join(message.rootDir, sourceDir)
            }
        }
        logger.trace(`DirWalker resolved dirPath = ${dirPath}`)

        await this.walkDirectory(dirPath, message)

        //   const finalMessage = structuredClone(message)
        const finalMessage = SysUtils.copyMessage(message)
        finalMessage.done = true
        finalMessage.count = this.count
        logger.trace("DirWalker emitting final done=true message")
        return this.emit('message', finalMessage)
    }

    // move to util.js ?
    // const markdownFiles = files.filter(file => matchesPattern(file, '*.md'));

    matchPatterns(str, patterns) {
        return StringUtils.matchPatterns(str, patterns)

        /*
        const matches = patterns.filter(pattern => this.matchesPattern(str, pattern))
        if (matches.length > 0) {
            return matches
        }
        return false
   */
    }
    /*
        matchesPattern(str, pattern) {
            return StringUtils.matchesPattern(str, pattern)

            // Convert glob pattern to regex
            const regexPattern = pattern
                .replace(/\./g, '\\.')   // Escape dots
    */
    //       .replace(/\*/g, '.*')   // Convert * to .*
    //    const regex = new RegExp(`^${regexPattern}$`)
    //  return regex.test(str)
    //}


    async walkDirectory(dir, baseMessage) {
        logger.trace(`DirWalker.walkDirectory, dir = ${dir}`)
        //   logger.reveal(this.message)
        const entries = await readdir(dir, { withFileTypes: true })

        for (const entry of entries) {
            const fullPath = path.join(dir, entry.name)
            const targetPath = super.getProperty(ns.trn.targetPath, this.message.workingDir)
            //   if (entry.isDirectory() && !this.excludePatterns.includes(entry.name[0])) {

            // should be dir? what about added includes?
            if (entry.isDirectory() && !this.matchPatterns(fullPath, this.excludePatterns)) {
                await this.walkDirectory(fullPath, baseMessage)
            } else if (entry.isFile()) {

                if (!this.matchPatterns(fullPath, this.excludePatterns) &&
                    this.matchPatterns(fullPath, this.includePatterns)) {

                    //     const message = structuredClone(baseMessage)
                    const message = SysUtils.copyMessage(baseMessage)
                    message.filename = entry.name
                    message.subdir = path.dirname(path.relative(targetPath, fullPath)).split(path.sep)[1]
                    //     message.subdir = path.dirname(path.relative(message.targetPath, fullPath)).split(path.sep)[1]
                    message.fullPath = fullPath
                    message.filepath = path.relative(baseMessage.targetPath || baseMessage.rootDir, fullPath)
                    message.done = false
                    message.count = this.count++

                    message.slug = message.filename.split('.')[0]

                    logger.trace(`DirWalker emitting :
                        message.targetPath: ${message.targetPath}
                        message.filename: ${message.filename}
                        message.fullPath: ${message.fullPath}
                        message.subdir: ${message.subdir}
                        message.filepath: ${message.filepath}
                        message.slugs: ${message.slugs}`)
                    //        process.exit()

                    logger.trace(` - DirWalker emit ${this.count++} : ${message.fullPath}`)
                    this.emit('message', message)
                }
            }
        }
    }
}

export default DirWalker

================
File: src/processors/fs/FileCopy.js
================
// src/processors/fs/FileCopy.js
/**
 * @class FileCopy
 * @extends Processor
 * @classdesc
 * **a Transmissions Processor**
 *
 * Copies files or entire directories on the local filesystem.
 *
 * #### __*Configuration*__
 * If a `settings` is provided in the transmission:
 * * **`ns.trn.source`** - The source path relative to `applicationRootDir`
 * * **`ns.trn.destination`** - The destination path relative to `applicationRootDir`
 *
 * #### __*Input*__
 * * **`message.rootDir`** (optional) - The root directory of the operation
 * * **`message.applicationRootDir`** (optional) - The root directory of the application, fallback `rootDir`
 * * **`message.source`** (if no `settings`) - The source path of the file or directory to copy
 * * **`message.destination`** (if no `settings`) - The destination path for the copied file or directory
 *
 * #### __*Output*__
 * * **`message`** - unmodified
 *
 * #### __*Behavior*__
 * * Copies the specified file or directory to the destination
 * * Checks and creates target directories if they don't exist
 * * Copies individual files directly
 * * Recursively copies directories and their contents
 * * Logs detailed information about the copying process for debugging
 *
 * #### __Tests__
 * * **`./run file-copy-remove-test`**
 * * **`npm test -- tests/integration/file-copy-remove-test.spec.js`**
 */

import { copyFile, mkdir, readdir, stat } from 'node:fs/promises'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'


class FileCopy extends Processor {
    constructor(config) {
        super(config)
    }

    /**
     * Executes the file copy operation
     * @param {Object} message - The input message
     */
    async process(message) {
        //  logger.setLogLevel("info")

        logger.debug("message.rootDir = " + message.rootDir)
        var source = super.getProperty(ns.trn.source)
        var destination = super.getProperty(ns.trn.destination)

const wd = super.getProperty(ns.trn.workingDir)
source = path.join(wd,source)
destination = path.join(wd,destination)
        logger.debug(`Source: ${source}`)
        logger.debug(`Destination: ${destination}`)

        try {
            await this.ensureDirectoryExists(path.dirname(destination))
            const sourceStat = await stat(source)

            if (sourceStat.isFile()) {
                logger.debug(`Copying file from ${source} to ${destination}`)
                await copyFile(source, destination)
            } else if (sourceStat.isDirectory()) {
                logger.debug(`Copying directory from ${source} to ${destination}`)
                await this.copyDirectory(source, destination)
            }
        } catch (err) {
            logger.error(`Error in FileCopy: ${err.message}`)
            logger.error(`Source: ${source}`)
            logger.error(`Destination: ${destination}`)
        }

        return this.emit('message', message)
    }

    /**
     * Ensures the specified directory exists, creating it if necessary
     * @param {string} dirPath - The directory path to ensure
     */
    async ensureDirectoryExists(dirPath) {
        logger.debug(`Ensuring directory exists: ${dirPath}`)
        try {
            await mkdir(dirPath, { recursive: true })
            logger.debug(`Directory created/ensured: ${dirPath}`)
        } catch (err) {
            logger.debug(`Error creating directory ${dirPath}: ${err.message}`)
            throw err
        }
    }

    /**
     * Recursively copies a directory and its contents
     * @param {string} source - The source directory path
     * @param {string} destination - The destination directory path
     */
    async copyDirectory(source, destination) {
        logger.debug(`Copying directory: ${source} to ${destination}`)
        try {
            await this.ensureDirectoryExists(destination)
            const entries = await readdir(source, { withFileTypes: true })

            for (const entry of entries) {
                const srcPath = path.join(source, entry.name)
                const destPath = path.join(destination, entry.name)

                logger.debug(`Processing: ${srcPath} to ${destPath}`)

                if (entry.isDirectory()) {
                    await this.copyDirectory(srcPath, destPath)
                } else {
                    await copyFile(srcPath, destPath)
                    logger.debug(`File copied: ${srcPath} to ${destPath}`)
                }
            }
        } catch (err) {
            logger.debug(`Error in copyDirectory: ${err.message}`)
            throw err
        }
    }
}

export default FileCopy

================
File: src/processors/fs/FilenameMapper.js
================
import Processor from '../../model/Processor.js'
import path from 'path'
import logger from '../../utils/Logger.js'

class FilenameMapper extends Processor {
    constructor(config) {
        super(config)
        this.extensions = {
            html: '.mm.html',
            svg: '.mm.svg'
        }
    }

    async process(message) {
        const format = message.format || 'html'
        const extension = this.extensions[format]

        if (!extension) {
            throw new Error(`Unknown format: ${format}`)
        }

        const parsedPath = path.parse(message.filepath)
        message.filepath = path.join(
            parsedPath.dir,
            parsedPath.name + extension
        )

        return this.emit('message', message)
    }
}

export default FilenameMapper

================
File: src/processors/fs/FileReader.js
================
import { readFile } from 'node:fs/promises'
import { access, constants, statSync } from 'node:fs'
import path from 'path'
import mime from 'node-mime-types'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'

class FileReader extends Processor {
    constructor(config) {
        super(config)
        this.defaultFilePath = 'input/input.md'
    }

    async process(message) {
        logger.trace(`FileReader.process, done=${message.done}`)

        if (message.done) return

        let filePath

        // TODO tidy up
        // First try deriving path from message properties
        if (message.fullPath) {
            filePath = message.fullPath
        } else if (message.filepath) {
            if (message.targetPath && !path.isAbsolute(message.filepath)) {
                filePath = path.join(message.targetPath, message.filepath)
            } else {
                filePath = message.filepath
            }
        } else {
            // Fall back to getting path from config
            filePath = await this.getProperty(ns.trn.sourceFile)
            if (!filePath) {
                logger.warn(`No source file path provided, defaulting to ${this.defaultFilePath}`)
                filePath = this.defaultFilePath
            }

            logger.trace(`filePath = ${filePath}`)
            // Resolve relative to targetPath or rootDir

            if (!path.isAbsolute(filePath)) {
                //     filePath = path.join(message.targetPath || message.rootDir, filePath)
                filePath = path.join(message.targetPath || message.workingDir, filePath)
            }
        }

        logger.trace(`FileReader.process(), reading file: ${filePath}`)
        logger.trace(`FileReader.process(), process.cwd() = ${process.cwd()}`)

        // Verify file is readable
        await new Promise((resolve, reject) => {
            access(filePath, constants.R_OK, (err) => {
                if (err) {
                    reject(new Error(`File ${filePath} is not readable: ${err.message}`))
                }
                resolve()
            })
        })

        // Handle metadata if requested
        const metaField = await super.getProperty(ns.trn.metaField)
        if (metaField) {
            const metadata = this.getFileMetadata(filePath)
            message[metaField] = metadata
        }


        // Read and return file content
        const content = await readFile(filePath, 'utf8')
        logger.debug(` - FileReader read: ${filePath}`)

        message.filePath = filePath

        const mediaType = super.getProperty(ns.trn.mediaType)
        logger.trace(`mediaType = ${mediaType}`)

        if (mediaType === 'application/json') {
            message.content = JSON.parse(content)
        } else {
            message.content = content
        }
        return this.emit('message', message)
    }

    getFileMetadata(filePath) {
        try {
            const stats = statSync(filePath)
            const filename = path.basename(filePath)
            return {
                filename: filename,
                mediaType: mime.getMIMEType(filename),
                filepath: filePath,
                size: stats.size,
                created: stats.birthtime,
                modified: stats.mtime,
                accessed: stats.atime,
                isDirectory: stats.isDirectory(),
                isFile: stats.isFile(),
                permissions: stats.mode,
                owner: stats.uid,
                group: stats.gid
            }
        } catch (error) {
            logger.error(`Error getting file metadata: ${error.message}`)
            return null
        }
    }
}

export default FileReader

================
File: src/processors/fs/FileRemove.js
================
// src/processors/fs/FileRemove.js
/**
 * FileRemove Processor
 *
 * Removes files or directory contents on the local filesystem.
 * @extends Processor
 *
 * #### __*Input*__
 * * message.applicationRootDir (optional) - The root directory of the application
 * * message.target (if no settings) - The path of the file or directory to remove
 *
 * #### __*Configuration*__
 * If a settings is provided in the transmission:
 * * ns.trn.target - The target path relative to applicationRootDir
 *
 * #### __*Output*__
 * * Removes the specified file or directory contents
 * * message (unmodified) - The input message is passed through
 *
 * #### __*Behavior*__
 * * Removes individual files directly
 * * Recursively removes directory contents
 * * Logs debug information about the removal process
 *
 * #### __Tests__
 * `./run file-copy-remove-test`
 * `npm test -- tests/integration/file-copy-remove-test.spec.js`
 *
 */

import { unlink, readdir, stat, rm } from 'node:fs/promises'
import path from 'path'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class FileRemove extends Processor {
    constructor(config) {
        super(config)
    }

    /**
     * Executes the file or directory removal operation
     * @param {Object} message - The input message
     */
    async process(message) {

        //  logger.setLogLevel('debug')

        this.ignoreDotfiles = true // default, simplify ".gitinclude"

        var target = await this.getProperty(ns.trn.target)
          const wd = super.getProperty(ns.trn.workingDir)
          target = path.join(wd,target)
        logger.debug('FileRemove, target = ' + target)
     
     //   return this.emit('message', message)
        try {
            const removeStat = await stat(target)

            if (removeStat.isFile()) {
                await this.removeFile(target)
            } else if (removeStat.isDirectory()) {
                await this.removeDirectoryContents(target)
            }
        } catch (err) {
            // probably already gone
            logger.debug('FileRemove, target stat caused err : ' + target)
        }

        return this.emit('message', message)
    }

    /**
     * Removes a file
     * @param {string} filePath - The path to the file to remove
     */
    async removeFile(filePath) {
        await unlink(filePath)
    }

    /**
     * Recursively removes the contents of a directory
     * @param {string} dirPath - The path to the directory
     */
    async removeDirectoryContents(dirPath) {
        logger.debug('FileRemove, dirPath = ' + dirPath)
        const entries = await readdir(dirPath, { withFileTypes: true })

        for (const entry of entries) {
            if (this.ignoreDotfiles && (entry.name.charAt(0) === ".")) {
                continue
            }
            const entryPath = path.join(dirPath, entry.name)

            if (entry.isDirectory()) {
                await this.removeDirectoryContents(entryPath)
            } else {
                await unlink(entryPath)
            }
        }
    }
}

export default FileRemove

================
File: src/processors/fs/FileWriter.js
================
import path from 'path'
import { access, constants, writeFileSync } from 'node:fs'
import ns from '../../utils/ns.js'
import { writeFile } from 'node:fs/promises'
import { dirname, join } from 'node:path'
import { mkdir, mkdirSync } from 'node:fs'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import JSONUtils from '../../utils/JSONUtils.js'
/**
 * FileWriter class that extends Processor
 * Write data to a file.
 *
 * First checks `message.targetFilepath` and if not set, uses the value from `processors.ttl` using `settings` for this processor instance.
 *
 * #### __*Input*__
 * * message.filepath
 * * message.content
 * #### __*Output*__
 * * as Input
 *
 * if message.loadContext is set, that is used as a name in the message for the file content
 */
class FileWriter extends Processor {

    /**
     * Constructs a new FileWriter object.
     * @param {Object} config - The configuration object for the FileWriter.
     */
    constructor(config) {
        super(config)
    }

    /**
     * Executes the write operation.
     * @param {Object} message - The execution message.
     */
    async process(message) {
        logger.trace(`\n\nFileWriter.process, message.done = ${message.done}`)
        logger.trace(`FileWriter.process, count = ${message.eachCount}`)
        if (message.done) { // TODO fix this bloody thing
            logger.trace(`\n\nFileWriter.process, message.done = ${message.done} SKIPPING!!`)
            return
        }

        if (message.dump) {
            // TODO make optional (on done?) - is a pain for multi
            //    const filename = `message_${new Date().toISOString()}.json`
            const filename = 'message.json'
            const f = path.join(message.workingDir, filename)
            const content = JSON.stringify(message)
            // Check if the file is readable.
            access(f, constants.W_OK, (err) => {
                if (err) {
                    logger.error(`FileWriter error : ${f} is not writable.`)
                    logger.reveal(message)
                }
            })
            return this.doWrite(f, content, message)
        }

        var filePath = await this.getProperty(ns.trn.destinationFile)
        if (!filePath) {
            filePath = await this.getProperty(ns.trn.workingDir)
        }

        // Resolve relative to targetPath or rootDir
        if (!path.isAbsolute(filePath)) {
            filePath = path.join(message.targetPath || message.workingDir, filePath)
        }

        logger.trace(`Filewriter, filepath = ${filePath}`)
        const dirName = dirname(filePath)
        logger.trace("Filewriter, dirName = " + dirName)

        const contentPath = super.getProperty(ns.trn.contentField, 'content')
        //  var content = message.content // generalise, see above
        logger.trace(`Filewriter, contentPath = ${contentPath}`)
        const content = JSONUtils.get(message, contentPath)
        logger.trace(`Filewriter, content = ${content}`)


        this.mkdirs(dirName) // sync - see below
        await this.doWrite(filePath, content, message)
        return this.emit('message', message)
    }

    async doWrite(f, content, message) {
        logger.trace(`FileWriter.doWrite, file = ${f}`)
        logger.trace(`typeof content = ${typeof content}`)
        if (typeof content != 'string') {
            content = JSON.stringify(content)
        }
        logger.log(' - FileWriter writing : ' + f)

        /*
                if (f.includes(`[object Object]`)) {
                    logger.reveal(message)

                }
        */

        // maybe stat first, check validity - the intended target dir was blocked by a of the same name
        await writeFile(f, content)
        //writeFileSync(f, content)
        logger.trace(' - FileWriter written : ' + f)
    }

    mkdirs(dir) {
        logger.trace(`FileWriter.mkdirs, dir = ${dir}`)
        try {
            mkdirSync(dir, { recursive: true })
        }
        catch (e) {
            logger.warn(`Warn: FileWriter.mkdirs, maybe dir exists : ${dir} ?`)
        }
    }
}

export default FileWriter

================
File: src/processors/fs/FsProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import DirWalker from './DirWalker.js'
import FileReader from './FileReader.js'
import FileWriter from './FileWriter.js'
import FileCopy from './FileCopy.js'
import FileRemove from './FileRemove.js'
import FilenameMapper from './FilenameMapper.js'

class FsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.DirWalker)) {
            return new DirWalker(config)
        }
        if (type.equals(ns.trn.FileReader)) {
            return new FileReader(config)
        }
        if (type.equals(ns.trn.FileWriter)) {
            return new FileWriter(config)
        }
        if (type.equals(ns.trn.FileCopy)) {
            return new FileCopy(config)
        }
        if (type.equals(ns.trn.FileRemove)) {
            return new FileRemove(config)
        }
        if (type.equals(ns.trn.FilenameMapper)) {
            return new FilenameMapper(config)
        }
        return false
    }
}

export default FsProcessorsFactory

================
File: src/processors/github/GitHubList_no-pag.js
================
import { Octokit } from '@octokit/rest'
import dotenv from 'dotenv'
import Processor from '../../model/Processor.js'
import logger from '../../utils/Logger.js'

dotenv.config({ path: './trans-apps/applications/git-apps/.env' })

class GitHubList extends Processor {
    constructor(config) {
        super(config)
        logger.debug('GitHubList constructor called')
        this.octokit = new Octokit({ auth: process.env.GITHUB_TOKEN })
        logger.debug('Octokit instance created')
    }

    async process(message) {
        logger.debug('GitHubList execute method called')
        logger.debug('Input message:', JSON.stringify(message, null, 2))

        if (!message.github || !message.github.name) {
            logger.error('GitHub username not provided in the message')
            throw new Error('GitHub username not provided in the message')
        }

        const username = message.github.name
        logger.debug(`Fetching repositories for username: ${username}`)

        try {
            logger.debug('Calling GitHub API')
            const { data } = await this.octokit.repos.listForUser({ username })
            logger.debug(`Fetched ${data.length} repositories`)

            const repositories = data.map(repo => repo.name)
            logger.debug('Extracted repository names:' + repositories)

            message.github.repositories = repositories
            logger.debug('Updated message:', JSON.stringify(message, null, 2))

            this.emit('message', message)
            logger.debug('Emitted updated message')
        } catch (error) {
            logger.error(`Error fetching repositories for ${username}:`, error)
            logger.debug('Error details:', JSON.stringify(error, null, 2))
            if (error.status === 403) {
                logger.warn('Possible rate limit exceeded. Check GitHub API rate limits.')
            }
            throw error
        }
    }
}

export default GitHubList

================
File: src/processors/github/GitHubList.js
================
import { Octokit } from '@octokit/rest'
import dotenv from 'dotenv'
import Processor from '../../model/Processor.js'
import logger from '../../../../transmissions/src/utils/Logger.js'

dotenv.config({ path: './trans-apps/applications/git-apps/.env' })

class GitHubList extends Processor {
    constructor(config) {
        super(config)
        logger.debug('GitHubList constructor called')
        this.octokit = new Octokit({ auth: process.env.GITHUB_TOKEN })
        logger.debug('Octokit instance created')
    }

    async process(message) {
        //    logger.setLogLevel('debug')
        logger.debug('GitHubList process method called')

        try {
            // Initialize payload.github if missing
            if (!message.payload) {
                message.payload = {}
            }
            if (!message.payload.github) {
                message.payload.github = {}
            }

            //     const username = message.payload.github.name
            const username = message.github.name
            logger.debug(`Processing for username: ${username}`)

            logger.debug('Calling GitHub API with pagination')
            logger.info(`Starting repository fetch for ${username}`)

            const repositories = await this.fetchAllRepositories(username)
            logger.debug(`Setting ${repositories.length} repositories in payload`)

            // Set in payload, not message.github
            message.payload.github.repositories = repositories
            message.payload.github.totalRepos = repositories.length

            return this.emit('message', message)
        } catch (error) {
            this.handleError(error, username)
        }
    }

    async fetchAllRepositories(username) {
        const repositories = []
        let page = 1
        const delay = ms => new Promise(resolve => setTimeout(resolve, ms))

        /*
        while (true) {
            try {
                const response = await this.octokit.repos.listForUser({
                    username,
                    per_page: 100,
                    page: page
                })

                repositories.push(...response.data.map(repo => repo.name))
                logger.debug(`Fetched page ${page} with ${response.data.length} repositories`)

                this.checkRateLimit(response.headers)

                if (response.data.length < 100) break
                page++

                await delay(1000) // 1 second delay between API calls
            } catch (error) {
                throw this.createDetailedError(error, 'Error fetching repositories page')
            }
        }
            */

        while (true) {
            const response = await this.octokit.repos.listForUser({
                username,
                per_page: 100,
                page
            })

            let data = response.data
            //  data = data.slice(0, 3) // Limit to first 3 repos

            logger.debug(`Page ${page}: Got ${data.length} repos`)

            repositories.push(...data.map(repo => repo.name))

            if (data.length < 100) break
            page++

            // Add delay between requests
            await new Promise(r => setTimeout(r, 5000))
        }

        logger.debug(`Total repositories found: ${repositories.length}`)

        return repositories
    }

    checkRateLimit(headers) {
        const remaining = headers['x-ratelimit-remaining']
        const resetTime = new Date(headers['x-ratelimit-reset'] * 1000)
        logger.info(`Rate limit remaining: ${remaining}, Reset time: ${resetTime}`)

        if (remaining < 10) {
            logger.warn(`Rate limit is low. Only ${remaining} requests left. Reset at ${resetTime}`)
        }
    }

    createDetailedError(error, message) {
        const detailedError = new Error(`${message}: ${error.message}`)
        detailedError.status = error.status
        detailedError.response = error.response
        return detailedError
    }

    handleError(error, username) {
        logger.error(`Error fetching repositories for ${username}:`, error.message)
        logger.debug('Error details:', JSON.stringify(error, null, 2))

        if (error.status === 403) {
            logger.warn('Rate limit exceeded. Check GitHub API rate limits.')
            throw new Error('GitHub API rate limit exceeded')
        } else if (error.status === 404) {
            logger.warn(`User ${username} not found on GitHub`)
            throw new Error(`GitHub user ${username} not found`)
        } else {
            throw new Error(`Failed to fetch GitHub repositories: ${error.message}`)
        }
    }
}

export default GitHubList

================
File: src/processors/github/GitHubProcessorsFactory.js
================
// GitHubProcessorsFactory.js
import logger from '../../../../transmissions/src/utils/Logger.js';
import ns from '../../../../transmissions/src/utils/ns.js';
import GitHubList from './GitHubList.js';

class GitHubProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.GitHubList)) {
            return new GitHubList(config);
        }
        return false;
    }
}

export default GitHubProcessorsFactory;

================
File: src/processors/http/services/MetricsService.js
================
import WebSocket from 'ws';
import os from 'os';

class MetricsService {
    constructor(server) {
        this.wss = new WebSocket.Server({ server });
        this.metrics = {
            startTime: Date.now(),
            requests: 0,
            connections: 0,
            memory: {},
            cpu: {}
        };
        this.setupWebSocket();
        this.startMetricsCollection();
    }

    setupWebSocket() {
        this.wss.on('connection', (ws) => {
            this.metrics.connections++;
            ws.on('close', () => this.metrics.connections--);
        });
    }

    startMetricsCollection() {
        setInterval(() => {
            this.updateMetrics();
            this.broadcastMetrics();
        }, 1000);
    }

    updateMetrics() {
        this.metrics.uptime = (Date.now() - this.metrics.startTime) / 1000;
        this.metrics.memory = {
            used: process.memoryUsage().heapUsed,
            total: os.totalmem(),
            free: os.freemem()
        };
        this.metrics.cpu = {
            load: os.loadavg(),
            cores: os.cpus().length
        };
    }

    broadcastMetrics() {
        const data = JSON.stringify(this.metrics);
        this.wss.clients.forEach(client => {
            if (client.readyState === WebSocket.OPEN) {
                client.send(data);
            }
        });
    }

    incrementRequests() {
        this.metrics.requests++;
    }
}

export default MetricsService;

================
File: src/processors/http/services/ShutdownService.js
================
import crypto from 'crypto';

class ShutdownService {
    constructor() {
        // Generate random credentials on startup
        this.username = crypto.randomBytes(16).toString('hex');
        this.password = crypto.randomBytes(32).toString('hex');
    }

    setupMiddleware(app) {
        app.use('/admin', (req, res, next) => {
            const authHeader = req.headers.authorization;
            if (!this.validateAuth(authHeader)) {
                res.setHeader('WWW-Authenticate', 'Basic');
                return res.status(401).send('Authentication required');
            }
            next();
        });
    }

    validateAuth(authHeader) {
        if (!authHeader || !authHeader.startsWith('Basic ')) {
            return false;
        }
        const base64Credentials = authHeader.split(' ')[1];
        const credentials = Buffer.from(base64Credentials, 'base64').toString('utf-8');
        const [username, password] = credentials.split(':');

        return username === this.username && password === this.password;
    }

    setupEndpoints(app, shutdownCallback) {
        app.get('/admin/credentials', (req, res) => {
            res.json({ username: this.username, password: this.password });
        });

        app.post('/admin/shutdown', (req, res) => {
            res.send('Shutdown initiated');
            shutdownCallback();
        });
    }
}

export default ShutdownService;

================
File: src/processors/http/HttpClient.js
================
import logger from '../../utils/Logger.js' // path will likely change
import Processor from '../../model/Processor.js' // maybe more specific

/**
 * FileReader class that extends xxxxxProcessor.
 * DESCRIPTION
 * #### __*Input*__
 * **message.INPUT**
 * #### __*Output*__
 * **message.OUTPUT**
 */
class HttpClient extends Processor {

    /**
     * Constructs a new ProcessorExample instance.
     * @param {Object} config - The configuration object.
     */
    constructor(config) {
        super(config)
    }

    /**
     * Does something with the message and emits a 'message' event with the processed message.
     * @param {Object} message - The message object.
     */
    async process(message) {
        //   logger.setLogLevel('debug')

        // processing goes here
        return this.emit('message', message)
    }
}

export default HttpClient

================
File: src/processors/http/HttpProcessorsFactory.js
================
//import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import HttpServer from './HttpServer.js'
import HttpClient from './HttpClient.js'
import HttpProxy from './HttpProxy.js'

class HttpProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.trn.HttpServer)) {
            return new HttpServer(config)
        }
        if (type.equals(ns.trn.HttpClient)) {
            return new HttpClient(config)
        }
        if (type.equals(ns.trn.HttpProxy)) {
            return new HttpProxy(config)
        }

        return false
    }
}
export default HttpProcessorsFactory

================
File: src/processors/http/HttpProxy.js
================
import logger from '../../utils/Logger.js' // path will likely change
import Processor from '../../model/Processor.js' // maybe more specific

/**
 * FileReader class that extends xxxxxProcessor.
 * DESCRIPTION
 * #### __*Input*__
 * **message.INPUT**
 * #### __*Output*__
 * **message.OUTPUT**
 */
class HttpProxy extends Processor {

    /**
     * Constructs a new ProcessorExample instance.
     * @param {Object} config - The configuration object.
     */
    constructor(config) {
        super(config)
    }

    /**
     * Does something with the message and emits a 'message' event with the processed message.
     * @param {Object} message - The message object.
     */
    async process(message) {
        //    logger.setLogLevel('debug')

        // processing goes here
        return this.emit('message', message)
    }
}

export default HttpProxy

================
File: src/processors/http/HttpServer.js
================
import path from 'path'
import { Worker } from 'worker_threads'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'

class HttpServer extends Processor {
    constructor(config) {
        super(config)
        this.worker = null
        this.serverConfig = {
            port: this.getPropertyFromMyConfig(ns.trn.port) || 4000,
            basePath: this.getPropertyFromMyConfig(ns.trn.basePath) || '/transmissions/test/',
            staticPath: this.getPropertyFromMyConfig(ns.trn.staticPath),
            cors: this.getPropertyFromMyConfig(ns.trn.cors) || false,
            timeout: this.getPropertyFromMyConfig(ns.trn.timeout) || 30000,
            maxRequestSize: this.getPropertyFromMyConfig(ns.trn.maxRequestSize) || '1mb',
            rateLimit: {
                windowMs: 15 * 60 * 1000,
                max: 100
            }
        }
    }

    async process(message) {
        try {
            this.worker = new Worker(
                path.join(process.cwd(), 'src/processors/http/HttpServerWorker.js')
            )

            this.worker.on('message', (msg) => {
                switch (msg.type) {
                    case 'status':
                        if (msg.status === 'running') {
                            logger.info(`Server running on port ${msg.port}`)
                        } else if (msg.status === 'stopped') {
                            this.emit('message', { ...message, serverStopped: true })
                        }
                        break
                    case 'error':
                        logger.error(`Server error: ${msg.error}`)
                        this.emit('error', new Error(msg.error))
                        break
                }
            })

            this.worker.on('error', (error) => {
                logger.error(`Worker error: ${error}`)
                this.emit('error', error)
            })

            this.worker.postMessage({
                type: 'start',
                config: this.serverConfig
            })

            return new Promise((resolve) => {
                this.worker.on('exit', () => {
                    resolve(message)
                })
            })

        } catch (error) {
            logger.error(`Failed to start server: ${error}`)
            throw error
        }
    }

    async shutdown() {
        if (this.worker) {
            this.worker.postMessage({ type: 'stop' })
        }
    }
}

export default HttpServer

================
File: src/processors/http/HttpServerWorker.js
================
import { parentPort } from 'worker_threads';
import express from 'express';
import path from 'path';
import logger from '../../utils/Logger.js';

class ServerWorker {
    constructor(config) {
        this.app = express();
        this.server = null;
        this.config = config;
        this.setupMessageHandling();
    }

    setupMessageHandling() {
        parentPort.on('message', (message) => {
            switch (message.type) {
                case 'start':
                    this.start(message.config);
                    break;
                case 'stop':
                    this.stop();
                    break;
                default:
                    logger.warn(`Unknown message type: ${message.type}`);
            }
        });
    }

    async start(config) {
        try {
            const { port = 4000, basePath = '/transmissions/test/', staticPath } = config;

            if (staticPath) {
                this.app.use(basePath, express.static(staticPath));
            }

            this.app.post('/shutdown', (req, res) => {
                res.send('Server shutting down...');
                this.stop();
            });

            this.server = this.app.listen(port, () => {
                parentPort.postMessage({
                    type: 'status',
                    status: 'running',
                    port: port
                });
            });

        } catch (error) {
            parentPort.postMessage({
                type: 'error',
                error: error.message
            });
        }
    }

    async stop() {
        if (this.server) {
            try {
                await new Promise((resolve, reject) => {
                    this.server.close((err) => {
                        if (err) reject(err);
                        resolve();
                    });
                });

                parentPort.postMessage({
                    type: 'status',
                    status: 'stopped'
                });

                process.exit(0);
            } catch (error) {
                parentPort.postMessage({
                    type: 'error',
                    error: error.message
                });
                process.exit(1);
            }
        }
    }
}

const worker = new ServerWorker();

================
File: src/processors/json/Blanker.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'

class Blanker extends Processor {
    constructor(config) {
        super(config)
        //    logger.setLogLevel('debug')
        logger.debug(config.blankValue)
        this.blankValue = config.blankValue || ''
    }

    async process(message) {
        const pointer = this.getPropertyFromMyConfig(ns.trn.pointer)
        const preserve = this.getPropertyFromMyConfig(ns.trn.preserve)

        var preservePath = preserve.value ? preserve.value : 'nonono'

        logger.debug(`Blanker.process,  typeof preservePath = ${typeof preservePath}, preservePath = ${preservePath}`)
        logger.reveal(preservePath)
        if (!pointer) {
            if (preservePath) {
                message = this.blankValues(message, '', preservePath)
            } else {
                message = this.blankAllValues(message)
            }
        } else {
            const parts = pointer.toString().split('.')
            let target = message

            for (let i = 0; i < parts.length - 1; i++) {
                target = target[parts[i]]
                if (!target) break
            }

            if (target && target[parts[parts.length - 1]]) {
                if (preservePath) {
                    target[parts[parts.length - 1]] =
                        this.blankValues(target[parts[parts.length - 1]], parts.join('.'), preservePath)
                } else {
                    target[parts[parts.length - 1]] =
                        this.blankAllValues(target[parts[parts.length - 1]])
                }
            }
        }

        return this.emit('message', message)
    }

    shouldPreserve(path, preservePath) {
        logger.debug(`Blanker.shouldPreserve path = ${path}, preservePath = ${preservePath}`)
        if (!preservePath) return false
        const pathParts = path.split('.')
        const preserveParts = preservePath.split('.')

        if (pathParts.length < preserveParts.length) return false

        for (let i = 0; i < preserveParts.length; i++) {
            if (pathParts[i] !== preserveParts[i]) return false
        }
        return true
    }

    // TODO refactor...can I be arsed?
    blankAllValues(obj) {
        if (Array.isArray(obj)) {
            return obj.map(item => this.blankAllValues(item))
        } else if (typeof obj === 'object' && obj !== null) {
            const result = {}
            for (const [key, value] of Object.entries(obj)) {
                result[key] = this.blankAllValues(value)
            }
            return result
        } else if (typeof obj === 'string') {
            return ''
        }
        return obj
    }

    blankValues(obj, currentPath = '', preservePath = '') {
        if (Array.isArray(obj)) {
            return obj.map((item, index) =>
                this.blankValues(item, `${currentPath}[${index}]`, preservePath)
            )
        } else if (typeof obj === 'object' && obj !== null) {
            const result = {}
            for (const [key, value] of Object.entries(obj)) {
                const newPath = currentPath ? `${currentPath}.${key}` : key
                logger.debug(`Blanker.blankValues 1 newPath = ${newPath}, preservePath = ${preservePath}`)
                if (this.shouldPreserve(newPath, preservePath)) {
                    result[key] = value
                } else {
                    result[key] = this.blankValues(value, newPath, preservePath)
                }
            }
            return result
        } else if (typeof obj === 'string') {
            logger.debug(`Blanker.blankValues 2 currentPath = ${currentPath}, preservePath = ${preservePath}`)
            return this.shouldPreserve(currentPath, preservePath) ? obj : ''
        }
        return obj
    }
}

export default Blanker

================
File: src/processors/json/JSONProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import JSONWalker from './JSONWalker.js'
import Restructure from './Restructure.js'
import StringOps from './StringOps.js'
import Blanker from './Blanker.js'

class JSONProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.Restructure)) {
            return new Restructure(config)
        }
        if (type.equals(ns.trn.JSONWalker)) {
            return new JSONWalker(config)
        }
        if (type.equals(ns.trn.StringOps)) {
            return new StringOps(config)
        }
        if (type.equals(ns.trn.Blanker)) {
            return new Blanker(config)
        }
        return false
    }

}
export default JSONProcessorsFactory

================
File: src/processors/json/JsonRestructurer.js
================
// JsonRestructurer.js
/// Helper for Restructure.js
// TODO move to utils/JSONUtils.js

import logger from '../../utils/Logger.js'

class JsonRestructurer {
    constructor(mappings) {
        if (!mappings?.mappings || !Array.isArray(mappings.mappings)) {
            throw new Error('JsonRestructurer : Invalid mapping structure')
        }
        this.mappings = mappings.mappings
        logger.debug('JsonRestructurer,  this.mappings = ' + this.mappings)
        //    logger.reveal(this.mappings)
    }

    getValueByPath(obj, path, caller) {
        logger.debug('JsonRestructurer, path = ' + path)

        try {
            const sp = path.split('.')
            logger.debug('JsonRestructurer, sp = ' + sp)
            const reduced = sp.reduce((acc, part) => acc[part], obj)
            logger.debug('JsonRestructurer, reduced = ' + reduced)
            return reduced
        } catch (e) {
            logger.reveal(obj)
            logger.warn(`${e},
                caused by : JsonRestructurer.getValueByPath, path ${path} not found,
                message above.
                `)
            logger.debug(`JsonRestructurer.setValueByPath caller : ${caller}`)
            //  process.exit(1)
            //const err = new Error().stack

            return undefined
        }
    }

    setValueByPath(obj, path, value) {
        logger.debug(`JsonRestructurer.setValueByPath, obj = ${obj}, path = ${path}, value = ${value}`)
        const parts = path.split('.')
        const last = parts.pop()
        const target = parts.reduce((acc, part) => {
            acc[part] = acc[part] || {}
            return acc[part]
        }, obj)
        logger.debug(`JsonRestructurer.setValueByPath, target = ${target}, last = ${last}, value = ${value}`)
        target[last] = value
    }

    restructure(inputData, caller) {
        if (typeof inputData === 'string') {
            try {
                inputData = JSON.parse(inputData)
            } catch (e) {
                throw new Error('Invalid JSON string provided')
            }
        }

        const result = {}
        this.mappings.forEach(({ pre, post }) => {
            logger.debug(`PRE = ${pre}, POST = ${post}`)
            const value = this.getValueByPath(inputData, pre, caller)
            // logger.log(`PRE = ${pre}, POST = ${post} value = ${value}`)
            if (value !== undefined) {
                this.setValueByPath(result, post, value)
            }
        })
        return result
    }
}
export default JsonRestructurer

================
File: src/processors/json/JSONWalker.js
================
// src/processors/json/JSONWalker.js
/**
* @class JSONWalker
* @extends Processor
* @classdesc
* **A Transmissions Processor**
*
* Walks through a JSON structure and emits messages for each item.
*
* ### Processor Signature
*
* #### ***Configuration***
* ***`ns.trn.targetDir`** - Target directory path relative to current working directory
*
* #### ***Input***
* ***`message.payload`** - JSON object to process
*
* #### ***Output***
* * Emits a message for each item in the input payload
* * Final message has `done: true` flag
* * Each emitted message contains:
*   * ***`message.item`** - Current item being processed
*   * ***`message.payload`** - Empty object (configurable)
*
* #### ***Behavior***
* * Validates input is a JSON object
* * Creates separate message for each value in payload
* * Clones messages to prevent cross-contamination
* * Signals completion with done flag
*/

import path from 'path'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'

class JSONWalker extends Processor {
    constructor(config) {
        super(config)
    }

    /**
     * Processes JSON payload by walking its structure and emitting messages
     * @param {Object} message - Contains payload to process
     * @throws {Error} If payload is invalid
     * @emits message - For each item and completion
     */
    async process(message) {

        message.done = false
        var pointer = this.getProperty(ns.trn.pointer)

        logger.debug(`JSONWalker pointer =  ${pointer}`)

        //   process.exit(1)
        var content = structuredClone(message.content)
        if (typeof content === 'string') {
            logger.debug(`content is a string, parsing to JSON`)
            content = JSON.parse(content)
        }
        message.content = {} // TODO option in config
        logger.debug(`content.length  = ${content.length}`)
        var die = this.getProperty(ns.trn.die)
        logger.debug(`die = ${die}`)
        if (die == "true") {
            //    logger.debug(`content.slice(0, 10) = ${content.slice(0, 10)}`)
            //  logger.reveal(content.slice(0, 10))
            process.exit(1)
        }
        //  for (const item of Object.values(content)) {
        for (var i = 0; i < content.length; i++) {
            const newMessage = structuredClone(message)
            newMessage.content = content[i]
            this.emit('message', newMessage)
        }

        var finalMessage = structuredClone(message)
        finalMessage.content = content[content.length - 1]

        /* this is for values - dict
        for (const item of Object.values(content)) {
            const newMessage = structuredClone(message)
            newMessage.content = {}
            newMessage.content.items = []
            newMessage.content.items.push[item]
            //   message.item = item  // TODO refactor, it's just to grab the last
            finalMessage = newMessage
            this.emit('message', newMessage)
        }
            */

        finalMessage.done = true
        this.emit('message', finalMessage)

    }
}

export default JSONWalker

================
File: src/processors/json/Restructure.js
================
// TODO extract reusable bits to 'src/utils'
import JsonRestructurer from './JsonRestructurer.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import JSONUtils from '../../utils/JSONUtils.js'

import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'

class Restructure extends Processor {
    constructor(config) {
        super(config)
    }

    async process(message) {
        try {
            if (!message.done) { // TODO refactor
                message = await this.doRenames(message)
                message = await this.doRemoves(message)
            }
            return this.emit('message', message)
        } catch (err) {
            logger.error("Restructure processor error: " + err.message)
            logger.error(err.stack)
            throw err
        }
    }

    async getRenames() {
        logger.debug(`\nRestructure.getRenames`)
        logger.debug(`this.settingsNode.value = ${this.settingsNode.value}`)

        // Get renamesRDF as an array of NamedNode terms
        const renamesRDF = super.getValues(ns.trn.rename)

        if (!renamesRDF || !Array.isArray(renamesRDF) || renamesRDF.length === 0) {
            logger.debug('No rename values found')
            return []
        }

        logger.debug(`Found ${renamesRDF.length} rename values`)
        logger.debug(JSON.stringify(renamesRDF))

        // Determine which dataset to use based on targetPath
        var dataset = this.config
        if (this.message && this.message.targetPath) {
            dataset = this.app.dataset || this.config
        }

        var renames = []
        for (let i = 0; i < renamesRDF.length; i++) {
            try {
                let rename = typeof renamesRDF[i] === 'string'
                    ? rdf.namedNode(renamesRDF[i])
                    : renamesRDF[i]

                let poi = rdf.grapoi({ dataset: dataset, term: rename })
                let pre = poi.out(ns.trn.pre).value
                let post = poi.out(ns.trn.post).value

                logger.debug(`Found mapping: PRE: ${pre}, POST: ${post}`)

                if (pre && post) {
                    renames.push({ "pre": pre, "post": post })
                }
            } catch (err) {
                logger.error(`Error processing rename value at index ${i}: ${err.message}`)
            }
        }

        return renames
    }



    async doRemoves(message) {
        logger.debug('\n\nRestructure.doRemoves')
        const removes = super.getValues(ns.trn.remove)

        if (!removes || removes.length === 0) {
            logger.debug('No remove directives found')
            return message
        }

        logger.debug(`Found ${removes.length} remove directives`)
        logger.reveal(removes)

        for (let i = 0; i < removes.length; i++) {
            const path = removes[i]
            logger.debug(`Processing remove path = ${path}`)
            message = JSONUtils.remove(message, path)
        }

        return message
    }

    async doRenames(message) {
        logger.debug('\n\nRestructure.doRenames')
        // Extract mappings array from config
        var renames
        if (this.config.simples) {
            renames = this.config.rename
        } else {
            renames = await this.getRenames()
        }

        if (!renames || renames.length === 0) {
            logger.debug('No rename mappings found')
            return message
        }

        logger.debug(`Found ${renames.length} rename mappings:`)
        logger.debug(JSON.stringify(renames))

        // Initialize JsonRestructurer with mappings
        this.restructurer = new JsonRestructurer({
            mappings: renames
        })

        // Get input data from message
        const input = structuredClone(message)

        // Perform restructuring
        const restructured = this.restructurer.restructure(input, this)

        const type = typeof restructured
        logger.debug(`Restructuring output type: ${type}`)
        logger.debug('Restructuring result:')
        logger.debug(JSON.stringify(restructured))

        for (const key of Object.keys(restructured)) {
            message[key] = restructured[key]
        }
        return message
    }
}

export default Restructure

================
File: src/processors/json/StringOps.js
================
import path from 'path'
import logger from '../../utils/Logger.js'
import rdf from 'rdf-ext'
import ns from '../../utils/ns.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import JSONUtils from '../../utils/JSONUtils.js'
import Processor from '../../model/Processor.js'

// TODO FOR FUCKS SAKE REFACTOR
class StringOps extends Processor {

    constructor(config) {
        super(config)
    }

    /*
      :asPath true ;
  :values (:a :b :c :d) .
  :a :string "/home/danny/sites/strandz.it/postcraft/public" .
  :b :field "currentItem.relPath.value" .
  :c :field "currentItem.slug.value" .
  :d :string ".html" .
  */
    async process(message) {
        logger.trace(`\n\nStringOps.process`)
        logger.trace(this)
        if (message.done) return

        // TODO refactor
        const dataset = this.config
        let poi = rdf.grapoi({ dataset: dataset, term: this.settingsNode })
        const segments = GrapoiHelpers.listToArray(dataset, this.settingsNode, ns.trn.values)
        // logger.reveal(segments)
        const asPath = super.getProperty(ns.trn.asPath) === 'true'

        var combined = ''
        var segment
        for (var i = 0; i < segments.length; i++) {
            segment = segments[i]
            //   logger.log(`property = ${segment}`)
            //     logger.reveal(segment)

            let stringSegment = rdf.grapoi({ dataset: dataset, term: segment })
            let stringProperty = stringSegment.out(ns.trn.string)
            //  logger.log(`stringProperty = ${stringProperty.value}`)
            if (stringProperty && stringProperty.value) {
                if (asPath) {
                    combined = path.join(combined, stringProperty.value)
                    continue
                }
                combined = combined + stringProperty.value
                continue
            }

            let fieldSegment = rdf.grapoi({ dataset: dataset, term: segment })
            let fieldProperty = fieldSegment.out(ns.trn.field)
            //     logger.log(`ààààààààààààààààààààààààààààààààààààààà`)

            logger.trace(`fieldProperty = ${fieldProperty.value}`)
            //  logger.reveal(message)

            if (fieldProperty && fieldProperty.value) {
                let fieldValue = JSONUtils.get(message, fieldProperty.value)

                if (asPath) {
                    try {
                        combined = path.join(combined, fieldValue)
                    } catch (e) {
                        logger.error(`fieldProperty = ${fieldProperty.value}`)
                        logger.error(`fieldValue = ${fieldValue}`)
                        logger.error(`combined = ${combined}`)
                        process.exit(1)
                    }
                    continue
                }
                if ('string' != typeof fieldValue) {
                    fieldValue = JSON.stringify(fieldValue)
                }
                combined = combined + fieldValue
                continue
            }
        }
        //logger.trace(`combined = ${combined}`)

        const targetField = await this.getProperty(ns.trn.targetField)
        //logger.trace(`targetField = ${targetField}`)
        JSONUtils.set(message, targetField, combined)

        return this.emit('message', message)
    }

}
export default StringOps

================
File: src/processors/markup/LinkFinder.js
================
import * as cheerio from 'cheerio'

import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class LinkFinder extends Processor {

    async process(message) {

        await this.extractLinks(message)

        if (data === '~~done~~') {
            logger.log('LF DONE*****************')
            return this.emitLocal('message', '~~done~~', message)
            return
        }
    }


    relocate(filename, extension) {
        const split = filename.split('.').slice(0, -1)
        return split.join('.') + extension
    }

    async extractLinks(htmlContent, message) {

        const $ = cheerio.load(htmlContent)
        let label = ''

        $('a, h1, h2, h3, h4, h5, h6').each((_, element) => {
            const tagName = element.tagName.toLowerCase()
            if (tagName.startsWith('h')) {
                const level = tagName.substring(1)
                const headerText = $(element).text()
                label = `\n\n${'#'.repeat(parseInt(level))} ${headerText}\n`
            } else if (tagName === 'a') {
                const linkText = $(element).text()
                //  logger.debug('linkText = ' + linkText)
                let href = $(element).attr('href')
                // logger.debug('href = ' + href)
                if (!href || href.startsWith('#')) return
                // Create an absolute URL if the href is relative
                if (href && !href.includes('://')) {
                    //  logger.debug('message.sourceURL = ' + message.sourceURL)
                    const baseURL = message.sourceURL
                    //  logger.debug('this.baseUrl = ' + baseURL)
                    href = new URL(href, baseURL).toString()
                }
                label = `\n[${linkText}](${href})`

            }
            message.label = label
            return this.emit('message', message)
        })
    }
}

export default LinkFinder

================
File: src/processors/markup/MarkdownToHTML.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import JSONUtils from '../../utils/JSONUtils.js'
import Processor from '../../model/Processor.js'
// import { parse } from 'marked'
import { marked } from 'marked'

// marked extensions
import markedFootnote from 'marked-footnote'
import markedCodeFormat from 'marked-code-format'
// import customHeadingId from "marked-custom-heading-id";

class MarkdownToHTML extends Processor {


    async process(message) {
        logger.debug(`\n\nMarkdownToHTML.process`)
        if (message.done) return

        // logger.reveal(message)
        // TODO use config to point to I/O fields, add sensible defaults
        var input
        if (message.contentBlocks) { // using templating
            input = message.contentBlocks.content
            logger.debug(`MarkdownToHTML.process, using contentBlocks: ${input}`)
        } else { // default
            input = message.content
        }

        // new Marked()
        const html = await
            marked
                //                .use(customHeadingId())
                .use(markedFootnote())
                .use(
                    markedCodeFormat({
                        /* Prettier options */
                    })
                )
                .parse(input.toString())

        const outputFieldPath = await this.getProperty(ns.trn.outputField, 'content')
        logger.debug(`\nMarkdownToHTML.process, outputField = ${outputFieldPath}`)
        message = JSONUtils.set(message, outputFieldPath, html)

        logger.debug(`message.content = ${message.content}`)
        return this.emit('message', message)
    }
}

export default MarkdownToHTML

================
File: src/processors/markup/MarkupProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import MetadataExtractor from './MetadataExtractor.js'
import LinkFinder from './LinkFinder.js'
import MarkdownToHTML from './MarkdownToHTML.js'

class MarkupProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.MetadataExtractor)) {
            return new MetadataExtractor(config)
        }
        if (type.equals(ns.trn.MarkdownToHTML)) {
            return new MarkdownToHTML(config)
        }
        if (type.equals(ns.trn.LinkFinder)) {
            return new LinkFinder(config)
        }
        return false
    }
}

export default MarkupProcessorsFactory

================
File: src/processors/markup/MetadataExtractor.js
================
import * as cheerio from 'cheerio'

import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class MetadataExtractor extends Processor {

    async process(message) {
        const filename = data.filename
        const content = data.content

        logger.debug("MetadataExtractor input file : " + filename)
        const targetFilename = this.relocate(filename)
        logger.debug("MetadataExtractor outputfile : " + targetFilename)

        const jsonData = this.convertEmailToJSON(content)

        const jsonString = JSON.stringify(jsonData)

        const output = { filename: targetFilename, content: jsonString }

        return this.emit('message', output, message)
    }

    relocate(filename) {
        //   var newFileName = filename.replace(/\. \w+$/, '.json')
        const split = filename.split('.').slice(0, -1)
        var newFileName = split.join('.') + '.json'
        return newFileName
    }

    convertEmailToJSON(htmlContent) {
        const $ = cheerio.load(htmlContent)
        var subjectLine = $('H1').text().trim()
        var fromName = $('B').first().text().trim()
        var nextMessageLink = $('LINK[REL="Next"]').attr('HREF')
        var previousMessageLink = $('LINK[REL="Previous"]').attr('HREF')
        var messageText = $('PRE').text().trim()
        messageText = this.pruneContent(messageText)
        const jsonResult = {
            subjectLine: subjectLine,
            fromName: fromName,
            nextMessageLink: nextMessageLink,
            previousMessageLink: previousMessageLink,
            messageText: messageText

        }

        /*
                const jsonResult = {
                    subject: $('H1').text().trim(),
                    from: $('B').first().text().trim(),
                    'next-message': $('LINK[REL="Next"]').attr('HREF'),
                    'previous-message': $('LINK[REL="Previous"]').attr('HREF'),
                    'message-text': $('PRE').text().trim()
                };
                */

        /*
        + ' ' + $('A').first().attr('href').match(/mailto:(.+\?)/)[1].replace('?Subject=', ' '),
            cc: '', // The sample does not contain a CC field to extract
        // Removing parameters from email address in 'from' field
        jsonResult.from = jsonResult.from.split('?')[0];

        //    'in-reply-to': $('LINK[REL="made"]').attr('HREF').match(/In-Reply-To=(.*)/)[1],
        */
        /*
        :\n\n>
        */

        return jsonResult
    }

    pruneContent(content) {
        // "keep this\nremove this\n\n>: keep this";
        const regex1 = /(^|\n).*?:\n>/s
        content = content.replace(regex1, '$1')

        const regex2 = /\n>.*?\n/g

        //   const inputString = "keep before\n>remove this\nkeep after";
        //   const cleanedString = inputString.replace(regex2, '\n');
        //   console.log(cleanedString);
        content = content.replace(regex2, '\n')
        //  content = (content + '\n').replace(regex2, '\n').trim();
        return content
    }
}



export default MetadataExtractor

================
File: src/processors/mcp/McpClient.js
================
import logger from "../../utils/Logger.js" // path will likely change
import Processor from "../../model/Processor.js" // maybe more specific

/**
 * FileReader class that extends xxxxxProcessor.
 * DESCRIPTION
 * #### __*Input*__
 * **message.INPUT**
 * #### __*Output*__
 * **message.OUTPUT**
 */
class McpClient extends Processor {
  /**
   * Constructs a new ProcessorExample instance.
   * @param {Object} config - The configuration object.
   */
  constructor(config) {
    super(config)
  }

  /**
   * Does something with the message and emits a 'message' event with the processed message.
   * @param {Object} message - The message object.
   */
  async process(message) {
    // logger.setLogLevel("debug")

    // processing goes here
    return this.emit("message", message)
  }
}

export default McpClient

================
File: src/processors/mcp/McpProcessorsFactory.js
================
import logger from "../../utils/Logger.js";
import ns from "../../utils/ns.js";

import ProcessorTemplate from "./McpClient.js";

// ref needed in transmissions/src/processors/base/AbstractProcessorFactory.js

class ProcessorsFactoryTemplate {
  static createProcessor(type, config) {
    if (type.equals(ns.trn.ProcessorTemplate)) {
      return new ProcessorTemplate(config);
    }

    return false;
  }
}
export default ProcessorsFactoryTemplate;

================
File: src/processors/mcp/McpServer.js
================
import logger from "../../utils/Logger.js" // path will likely change
import Processor from "../../model/Processor.js" // maybe more specific

/**
 * FileReader class that extends xxxxxProcessor.
 * DESCRIPTION
 * #### __*Input*__
 * **message.INPUT**
 * #### __*Output*__
 * **message.OUTPUT**
 */
class McpServer extends Processor {
  /**
   * Constructs a new ProcessorExample instance.
   * @param {Object} config - The configuration object.
   */
  constructor(config) {
    super(config)
  }

  /**
   * Does something with the message and emits a 'message' event with the processed message.
   * @param {Object} message - The message object.
   */
  async process(message) {
    //  logger.setLogLevel("debug")

    // processing goes here
    return this.emit("message", message)
  }
}

export default McpServer

================
File: src/processors/postcraft/MakeEntry.js
================
import path from 'path'
import crypto from 'crypto'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import Processor from '../../model/Processor.js'

// Hacky placeholder!!

class MakeEntry extends Processor {

  constructor(config) {
    super(config)
  }

  async process(message) {

    if (message.done) {
      return this.emit('message', message)
    }
    const dates = this.extractDates(message)

    // rootDir
    const basePath = message.targetPath
    // ? message.targetPath : message.sourceDir
    const { rel, slug } = this.extractRelSlug(basePath, dates, message.filePath)

    logger.debug(`message.meta.filepath = ${message.meta.filepath}`)
    logger.debug(`slug = ${slug}`)

    var uri = this.getEntryURI(rel, slug)
    const relMap = super.getProperty(ns.trn.relMap, '')
    uri = uri.replace(message.sourceDir, relMap)

    const relative = rel.replace(message.sourceDir, relMap)

    const title = this.extractTitle(message)

    message.contentBlocks = {
      uri: uri,
      sourcePath: message.meta.filepath,
      mediaType: message.meta.mediaType,
      relative: relative,
      relPath: rel,
      relMap: relMap,
      title: title,
      content: message.content,
      slug: slug,
      title: this.extractTitle(message),
      dates: dates,
      creator: this.getCreator()
    }
    logger.log(` - made entry : ${uri}`)
    return this.emit('message', message)
  }

  getEntryURI(rel, slug) {
    const baseURI = super.getProperty(ns.trn.baseURI)
    logger.debug(`baseURI = ${baseURI}`)
    //  process.exit()
    //  const id = crypto.randomUUID()
    //  return path.join(baseURI, rel, slug)
    return baseURI + '/' + rel + '/' + slug
  }

  getCreator() {
    return {
      name: super.getProperty(ns.trn.creatorName),
      uri: super.getProperty(ns.trn.creatorURI)
    }
  }

  // filePath - appPath
  // (message.sourceDir, dates, message.filePath)
  extractRelSlug(basePath, dates, filePath) {
    logger.debug(`\n\nMakeEntry.extractRelSlug`)
    logger.debug(`basePath = ${basePath}`)
    logger.debug(`filePath = ${filePath}`)

    const baseLength = basePath.split(path.sep).length
    const split = filePath.split(path.sep)
    const splitLength = split.length

    logger.debug(`split = ${split}`)

    var slug = split.slice(splitLength - 1).toString()
    if (slug.includes('.')) {
      slug = slug.substr(0, slug.lastIndexOf('.'))
    }

    logger.debug(`slug = ${slug}`)
    logger.debug(`baseLength = ${baseLength}, splitLength = ${splitLength}`)

    const dirs = split.slice(baseLength, splitLength - 1)

    logger.debug(`dirs = ${dirs}`)

    /* TODO make this an option
    const day = dates.created.split('T')[0]
    const rel = path.join(dirs.join(path.sep), day)
    */
    const rel = dirs.join(path.sep)
    logger.debug(`rel = ${rel}`)
    return { rel, slug }
  }

  extractDates(message) {
    const now = (new Date()).toISOString()
    // TODO make note - FileReader gives date object
    const created = message.meta.created.toISOString()
    const modified = message.meta.modified.toISOString()
    const dates = {
      read: now,
      created: created,
      modified: modified
    }

    //  eg. 2024-04-19_hello-postcraft.md
    const nonExt = message.filePath.split('.').slice(0, -1).join()
    const shreds = nonExt.split('_')
    if (Date.parse(shreds[0])) { // filename version is not NaN
      dates.created = shreds[0]
    }
    return dates
  }

  // first heading in the markdown
  // or formatted from filename
  // or raw filename
  extractTitle(message) {
    let title = 'Title'
    let match = message.content.toString().match(/^#(.*)$/m)
    let contentTitle = match ? match[1].trim() : null
    if (contentTitle) {
      title = contentTitle.replaceAll('#', '') // TODO make nicer
      return title
    }

    // derive from filename
    // eg. 2024-04-19_hello-postcraft.md
    try {
      const nonExt = message.filename.split('.').slice(0, -1).join()
      const shreds = nonExt.split('_')

      // let title = shreds[1] // fallback, get it from filename
      title = shreds[1].split('-') // split the string into words
        .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // capitalize the first letter of each word
        .join(' ') // join the words back together with spaces
    } catch (err) {
      title = message.filename
    }
    return title
  }

  // first heading in the markdown
  // or formatted from filename
  // or raw filename
  extractTitle(message) {
    let title = 'Title'
    let match = message.content.toString().match(/^#(.*)$/m)
    let contentTitle = match ? match[1].trim() : null
    if (contentTitle) {
      title = contentTitle.replaceAll('#', '') // TODO make nicer
      return title
    }

    // derive from filename
    // eg. 2024-04-19_hello-postcraft.md
    try {
      const nonExt = message.filename.split('.').slice(0, -1).join()
      const shreds = nonExt.split('_')

      // let title = shreds[1] // fallback, get it from filename
      title = shreds[1].split('-') // split the string into words
        .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // capitalize the first letter of each word
        .join(' ') // join the words back together with spaces
    } catch (err) {
      title = message.filename
    }
    return title
  }
}

export default MakeEntry

================
File: src/processors/postcraft/PostcraftProcessorsFactory.js
================
import ns from '../../utils/ns.js'

import MakeEntry from './MakeEntry.js'
import RenderArticle from './PrepareArticle.js'

class PostcraftProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.MakeEntry)) {
            return new MakeEntry(config)
        }
        if (type.equals(ns.trn.PrepareArticle)) {
            return new PrepareArticle(config)
        }
        return false
    }
}

export default PostcraftProcessorsFactory

================
File: src/processors/postcraft/PrepareArticle.js
================
// src/processors/example-group/PrepareArticle.js

// NOT USED
/**
 * @class PrepareArticle
 * @extends Processor
 * @classdesc
 * **a Basic Transmissions Processor**
 *
 * Provides a template for creating new processors, demonstrates use of config settings.
 *
 * #### __*Input*__
 * * **`message.common`** - addressed by all instances of this PrepareArticle (optional, default undefined)
 * * **`message.something1`** - Template string (used if templateFilename is not provided)
 * * **`message.something2`** - Object with properties for template rendering (e.g., title, body)
 * * **`message.notavalue`** - Object with properties for template rendering (e.g., title, body)
 *
 * #### __*Output*__
 * * **`message.content`** - The rendered template content
 *
 * #### __*Processing*__
 * * Uses Nunjucks to render templates
 * * Can render from a template file or a template string
 * * Applies content from message.contentBlocks to the template
 *
* #### __*Side Effects*__
 *
 * #### __Tests__
 * *
 *
  * #### __*ToDo*__
 * * Add test information here
 * * Cache templates - cache in utils?
 */

import { readFile } from 'node:fs/promises'
import { access, constants } from 'node:fs'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'


class PrepareArticle extends Processor {
    constructor(config) {
        super(config)
    }

    /**
      * Does something with the message and emits a 'message' event with the processed message.
      * @param {Object} message - The message object.
      */
    async process(message) {
        logger.debug(`\n\nExampleProcessor.process`)

        // TODO figure this out better
        // may be needed if preceded by a spawning processor, eg. fs/DirWalker
        if (message.done) {
            return this.emit('message', message)
            // or simply return
        }

        //        const me = await this.getProperty(ns.trn.me)

        // message forwarded
        return this.emit('message', message)
    }
}
export default PrepareArticle

================
File: src/processors/protocols/HttpGet.js
================
// src/processors/http/HttpGet.js
/**
 * @class HttpGet
 * @extends Processor
 * @classdesc
 * **A Transmissions Processor**
 *
 * Retrieves content from HTTP URLs using GET requests.
 *
 * ### Processor Signature
 *
 * #### ***Input***
 * ***`url`** - The HTTP URL to fetch content from
 * ***`message`** - A message object that will be enriched with response data
 *
 * #### ***Output***
 * ***`message`** - Original message with added sourceURL property
 * ***`content`** - The retrieved content from the URL
 *
 * #### ***Behavior***
 * * Fetches content from specified HTTP URL
 * * Adds source URL to message object
 * * Handles special '~~done~~' URL marker
 * * Emits retrieved content along with enriched message
 * * Logs detailed debug information during operation
 */

import axios from 'axios'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class HttpGet extends Processor {
    constructor(config) {
        super(config)
    }

    /**
     * Processes an HTTP GET request for the given URL
     * @param {string} url - Target URL to fetch
     * @param {Object} message - Message object to enrich with response data
     * @emits message - Emits retrieved content and enriched message
     */
    async process(url, message) {
        //   logger.setLogLevel('debug')
        logger.debug('HttpGet, url = ' + url)
        if (url === '~~done~~') {
            logger.log('HG DONE*****************')
            return this.emit('message', url, message)
            return
        }
        try {
            logger.log('HG GETTING*****************')
            const response = await axios.get(url)
            const content = response.data

            message.sourceURL = url
            return this.emit('message', content, message)
        } catch (error) {
            logger.error("HttpGet.execute error\n" + error)
        }
    }
}

export default HttpGet

================
File: src/processors/protocols/ProtocolsProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import HttpGet from './HttpGet.js'



class ProtocolsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.HttpGet)) {
            return new HttpGet(config)
        }

        return false
    }
}

export default ProtocolsProcessorsFactory

================
File: src/processors/rdf/ConfigMap.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import path from 'path'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class ConfigMap extends Processor {
  constructor(config) {
    super(config)
  }

  async process(message) {
    // logger.setLogLevel('debug')

    /*
     if (!message.dataset) {
       logger.warn('No dataset provided')
       return this.emit('message', message)
     }
 */
    logger.debug(`ConfigMap.process`)
    this.showMyConfig()

    const basePath = message.targetPath || message.rootDir
    logger.debug(`ConfigMap using base path: ${basePath}`)

    const dataset = message.dataset
    const poi = grapoi({ dataset })

    // Find ConfigSet instances
    for (const quad of poi.out(ns.rdf.type, ns.trn.ConfigSet).quads()) {
      const groupID = quad.subject

      let groupName = ns.getShortname(groupID.value)
      //   let type = ns.getShortname(processorType.value)
      logger.debug(`*** groupName = ${groupName} `)
      // *** groupID.value = http://hyperdata.it/transmissions/AtomFeed

      const groupPoi = grapoi({ dataset, term: groupID })

      if (!message.contentGroup) message.contentGroup = {}
      // Extract paths
      if (groupPoi.out(ns.trn.sourceDirectory).term) {
        let sourceDir = this.resolvePath(
          basePath,
          groupPoi.out(ns.trn.sourceDirectory).term.value)

        if (!message.contentGroup[groupName]) message.contentGroup[groupName] = {}
        message.contentGroup[groupName].sourceDir = sourceDir
      }

      if (groupPoi.out(ns.trn.targetDirectory).term) {
        let targetDir = this.resolvePath(
          basePath,
          groupPoi.out(ns.trn.targetDirectory).term.value
        )
        if (!message.contentGroup[groupName]) message.contentGroup[groupName] = {}
        message.contentGroup[groupName].targetDir = targetDir
      }

      if (groupPoi.out(ns.trn.template).term) {
        let templateFile = this.resolvePath(
          basePath,
          groupPoi.out(ns.trn.template).term.value
        )
        if (!message.contentGroup[groupName]) message.contentGroup[groupName] = {}
        message.contentGroup[groupName].templateFile = templateFile
      }

      /*
logger.debug(`Resolved :
  groupName: ${groupName}
  sourceDir: ${sourceDir}
  targetDir: ${targetDir}
  templateFile: ${templateFile}`)


message.contentGroup[groupName] =
{ "sourceDir": sourceDir, "targetDir": targetDir, "templateFile": templateFile }
*/
      //  logger.reveal(message)
    }
    // process.exit()

    return this.emit('message', message)
  }

  resolvePath(basePath, relativePath) {
    if (!basePath || !relativePath) {
      throw new Error('Base path and relative path required')
    }

    const resolved = path.isAbsolute(relativePath)
      ? relativePath
      : path.join(basePath, relativePath)

    return path.normalize(resolved)
  }
}

export default ConfigMap

================
File: src/processors/rdf/DatasetReader.js
================
import path from 'path'
import rdf from 'rdf-ext'
import { fromFile } from 'rdf-utils-fs'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class DatasetReader extends Processor {
    constructor(config) {
        super(config)
    }

    async process(message) {
        try {
            const datasetFile = this.getPropertyFromMyConfig(ns.trn.datasetFile)
            const datasetPath = path.join(message.rootDir, datasetFile)

            logger.debug(`Reading dataset from ${datasetPath}`)
            const stream = fromFile(datasetPath)
            message.dataset = await rdf.dataset().import(stream)

            if (message.dataset.size === 0) {
                logger.warn('Empty dataset loaded')
            } else {
                logger.debug(`Loaded dataset with ${message.dataset.size} quads`)
            }

            return this.emit('message', message)
        } catch (err) {
            logger.error('Failed to read dataset:', err)
            throw err
        }
    }
}

export default DatasetReader

================
File: src/processors/rdf/RDFConfig.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class RDFConfig extends Processor {
  constructor(config) {
    super(config)
    this.configMap = new Map()
  }

  async process(message) {
    if (!message.dataset) {
      throw new Error('No RDF dataset provided')
    }

    const dataset = message.dataset
    const poi = grapoi({ dataset })

    // Find all ConfigGroup instances
    for (const configGroup of poi.out(ns.rdf.type, ns.trn.ConfigGroup).terms) {
      const groupPoi = grapoi({ dataset, term: configGroup })

      // Extract property mappings
      const mappings = {}
      for (const quad of groupPoi.quads()) {
        if (!quad.predicate.equals(ns.rdf.type)) {
          mappings[quad.predicate.value] = this.resolveValue(quad.object)
        }
      }

      this.configMap.set(configGroup.value, mappings)
      message.configMap = this.configMap
    }

    // Apply configuration patterns
    if (message.configPatterns) {
      for (const pattern of message.configPatterns) {
        const config = this.configMap.get(pattern)
        if (config) {
          Object.assign(message, config)
        }
      }
    }

    return this.emit('message', message)
  }

  resolveValue(term) {
    // Handle different RDF term types
    if (term.termType === 'NamedNode') {
      return term.value
    } else if (term.termType === 'Literal') {
      const value = term.value
      // Convert numeric literals
      return isNaN(value) ? value : Number(value)
    }
    return term.value
  }

  getConfig(groupId) {
    return this.configMap.get(groupId)
  }
}

export default RDFConfig

================
File: src/processors/rdf/RDFProcessorsFactory.js
================
import ns from '../../utils/ns.js'
import DatasetReader from './DatasetReader.js'
import ConfigMap from './ConfigMap.js'
import RDFConfig from './RDFConfig.js'

class RDFProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.DatasetReader)) {
            return new DatasetReader(config)
        }
        if (type.equals(ns.trn.ConfigMap)) {
            return new ConfigMap(config)
        }
        if (type.equals(ns.trn.RDFConfig)) {
            return new RDFConfig(config)
        }
        return false
    }
}

export default RDFProcessorsFactory

================
File: src/processors/sparql/bad-sparql.sparql
================
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX : <http://purl.org/stuff/transmissions/>

# https://schema.org/BlogPosting
# https://schema.org/Article

INSERT DATA {
  <http://danny.ayers.name/me> a schema:Person ;
      schema:name "Danny" .
  <http:/danny.ayers.name/blog/----- ENTRIES ----/home/danny/sites/danny,ayers,name/postcraft/content-raw/markor-docs/----- EN/2025-01-03_elfquake> a schema:Article ;
    :sourcePath "/home/danny/sites/danny.ayers.name/postcraft/content-raw/markor-docs/----- ENTRIES ----/2025-01-03_elfquake.md" ;
    :relPath "----- ENTRIES ----/home/danny/sites/danny,ayers,name/postcraft/content-raw/markor-docs/----- EN" ;
    :slug "2025-01-03_elfquake" ;
    schema:headline "ELFQuake" ;
    schema:datePublished "/home/danny/sites/danny,ayers,name/postcraft/content-raw/markor-docs/----- ENTRIES ----/2025-01-03"^^xsd:dateTime ;
    schema:dateModified "2025-02-15T13:50:07"^^xsd:dateTime ;
    schema:creator <http://danny.ayers.name/me> ;
    schema:articleBody """# ELFQuake

This is the earthquake prediction project I&#39;ve been working on. Info on that below, but first -

## Radio 4

[Jim Al-Khalili](https://en.wikipedia.org/wiki/Jim_Al-Khalili)&#39;s (very listenable) scientist-biography series &quot;The Life Scientific&quot; has [this episode](https://www.bbc.co.uk/sounds/play/m001k7tt) featuring geophysicist [James Jackson](https://en.wikipedia.org/wiki/James_Jackson_%28geophysicist%29). It&#39;s a great backgrounder on the seismo field. I got many important takeaways.

One was around popular perceptions. He made the point that (at the time of the broadcast at least) it had to be stressed that short-term prediction isn&#39;t possible. He referred to the social fallout from the [2009 L&#39;Aquila earthquake](https://en.wikipedia.org/wiki/2009_L%27Aquila_earthquake) (See also, below). A group of scientists were convicted of involuntary manslaughter for not predicting the quake (I&#39;m paraphrasing, the Wikipedia article has more of the full story). Jackson&#39;s stance on this is well justified, this is another problem (like climate change) with an aura of **&quot;science will solve it&quot;** which can lead to a **lack of necessary, immediately doable**, action.

 In contrast, Jackson mentions the [April 2015 Nepal earthquake](https://en.wikipedia.org/wiki/April_2015_Nepal_earthquake) (7.8 [Mw](https://en.wikipedia.org/wiki/Moment_magnitude_scale) - he attended a conference on prep there **two weeks before the quake** . A Nepalese guy (name?) behind an NGO that had built 300 (?) schools according to best known practices. All still standing after the event and used as emergency centres, while 800 (?) other schools where destroyed.

At a slight tangent, coincidentally this morning I saw a post by Steven Pemberton on [Facebook](https://www.facebook.com/share/12Cke36ivdW/)

Y2K was 25 years ago. The Guardian has an article describing it as a &quot;damp squib&quot;, as if they&#39;d have rather something go disastrously wrong...

The message: if a lot of people work for a decade to prevent a problem from happening, others will conclude it wasn&#39;t really a problem.

In comments there

## ELFQuake

""" .

}

================
File: src/processors/sparql/config.js
================
// config.js
export const config = {
    baseUrl: 'http://example.com',
    requiredFields: ['slug', 'title'],
    dateFormat: 'YYYY-MM-DDThh:mm:ssZ',
    
    setBaseUrl(url) {
        if (!url.startsWith('http')) {
            throw new Error('Base URL must start with http(s)');
        }
        this.baseUrl = url;
    }
};

================
File: src/processors/sparql/custom-predicates.js
================
// customPredicates.js
export const customPredicates = {
    validate(predicate, value) {
        if (!this[predicate]) {
            throw new Error(`Unknown predicate: ${predicate}`);
        }
        return this[predicate].validate(value);
    },
    
    format(predicate, value) {
        return this[predicate].format(value);
    },
    
    category: {
        validate: value => typeof value === 'string' && value.length > 0,
        format: value => `schema:category "${value}"`
    },
    
    keywords: {
        validate: value => Array.isArray(value) && value.every(v => typeof v === 'string'),
        format: value => value.map(keyword => `schema:keywords "${keyword}"`).join(' ; ')
    },
    
    license: {
        validate: value => typeof value === 'string' && value.startsWith('http'),
        format: value => `schema:license <${value}>`
    }
}

================
File: src/processors/sparql/SessionEnvironment.js
================
import axios from 'axios'
import nunjucks from 'nunjucks'
import fs from 'fs/promises'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

class SessionEnvironment {
    constructor(processor) {
        this.processor = processor
        this.endpoints = null
        this.templateCache = new Map()
    }

    async loadEndpoints(dir) {
        logger.debug(`SessionEnvironment.loadEndpoints dir = ${dir}`)
        const settingsPath = this.processor.getProperty(ns.trn.endpointSettings)
        logger.debug(`SessionEnvironment.loadEndpoints dir = ${dir}`)
        logger.debug(`SessionEnvironment.loadEndpoints settingsPath = ${settingsPath}`)

        if (!settingsPath) {
            throw new Error(`
Endpoint settings path is undefined
Config :
${logger.shorter(this.processor.config)}`)
        }

        const filePath = path.join(dir, settingsPath)
        logger.debug(`SessionEnvironment.loadEndpoints filePath = ${filePath}`)
        const data = await fs.readFile(filePath, 'utf8')
        this.endpoints = JSON.parse(data)
    }

    getQueryEndpoint() {
        return this.endpoints.find(e => e.type === 'query')
    }

    getUpdateEndpoint() {
        // logger.debug(`this.endpoints = ${this.endpoints}`)
        const ep = this.endpoints.find(e => e.type === 'update')
        // logger.log(`update endpoint = ${ep}`)
        // logger.reveal(ep)
        return ep
    }

    async getTemplate(dir, templateFilename) {

        logger.debug(`SessionEnvironment.getTemplate dir = ${dir}`)
        logger.debug(`SessionEnvironment.getTemplate templateFilename = ${templateFilename}`)

        const cacheKey = path.join(dir, templateFilename)

        if (this.templateCache.has(cacheKey)) {
            return this.templateCache.get(cacheKey)
        }

        const template = await fs.readFile(cacheKey, 'utf8')
        this.templateCache.set(cacheKey, template)
        logger.debug(`SessionEnvironment.getTemplate cacheKey = ${cacheKey}`)
        logger.debug(`SessionEnvironment.getTemplate template = ${template}`)
        return template
    }

    clearTemplateCache() {
        this.templateCache.clear()
    }

    // getProperty(property) {
    //   return property;
    //}

    getBasicAuthHeader(endpoint) {
        return `Basic ${Buffer.from(
            `${endpoint.credentials.user}:${endpoint.credentials.password}`
        ).toString('base64')}`
    }
}

export default SessionEnvironment

================
File: src/processors/sparql/SPARQLProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import SPARQLSelect from './SPARQLSelect.js'
import SPARQLUpdate from './SPARQLUpdate.js'

/**
 * @class SPARQLProcessorsFactory
 * @classdesc
 * **Factory for SPARQL Processors**
 *
 * Creates instances of SPARQL query and update processors.
 *
 */
class SPARQLProcessorsFactory {

    static createProcessor(type, config) {

        if (type.equals(ns.trn.SPARQLSelect)) {
            return new SPARQLSelect(config)
        }
        if (type.equals(ns.trn.SPARQLUpdate)) {
            return new SPARQLUpdate(config)
        }

        return false
    }
}
export default SPARQLProcessorsFactory

================
File: src/processors/sparql/SPARQLSelect.js
================
import axios from 'axios'
import nunjucks from 'nunjucks'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'
import SessionEnvironment from './SessionEnvironment.js'

class SPARQLSelect extends Processor {
    constructor(config) {
        super(config)
        this.env = new SessionEnvironment(this)
    }

    async getQueryEndpoint(message) {
        if (!this.env.endpoints) {
            const dir = this.getProperty(ns.trn.targetPath, message.rootDir)
            await this.env.loadEndpoints(dir)
        }
        return this.env.getQueryEndpoint()
    }

    async process(message) {

        const endpoint = await this.getQueryEndpoint(message)
        const dir = this.getProperty(ns.trn.targetPath, message.rootDir)
        const template = await this.env.getTemplate(
            dir,
            await this.getProperty(ns.trn.templateFilename)
        )

        const queryData = {
            startDate: new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString(),
            ...message
        }

        const query = nunjucks.renderString(template, queryData)
        logger.debug(`query = ${query}`)
        logger.debug(`endpoint.url = ${endpoint.url}`)
        try {
            const response = await axios.post(endpoint.url, query, {
                headers: {
                    'Content-Type': 'application/sparql-query',
                    'Accept': 'application/json',
                    'Authorization': this.env.getBasicAuthHeader(endpoint)
                }
            })

            logger.debug(`response.data = ${response.data}`)
            //  logger.reveal(response.data)
            message.queryResults = response.data
            return this.emit('message', message)
        } catch (error) {
            logger.error('SPARQL query error:', error)
            throw error
        }
    }
}

export default SPARQLSelect

================
File: src/processors/sparql/SPARQLUpdate.js
================
import axios from 'axios'
import nunjucks from 'nunjucks'
import crypto from 'crypto'
import logger from '../../utils/Logger.js'
import SlowableProcessor from '../../model/SlowableProcessor.js'
import ns from '../../utils/ns.js'
import SessionEnvironment from './SessionEnvironment.js'

class SPARQLUpdate extends SlowableProcessor {
    constructor(config) {
        super(config)
        this.env = new SessionEnvironment(this)
    }

    async process(message) {
        logger.trace(`\nSPARQLUpdate.process`)

        const endpoint = await this.getUpdateEndpoint(message)
        //    const endpoint = this.env.getQueryEndpoint()


        logger.trace(`SPARQLUpdate.process endpoint = ${endpoint}`)

        const dir = this.getProperty(ns.trn.targetPath, message.rootDir)
        const template = await this.env.getTemplate(
            dir,
            await this.getProperty(ns.trn.templateFilename)
        )
        logger.trace(`\nSPARQLUpdate.process template = ${template}`)

        const now = new Date().toISOString()

        const updateID = crypto.randomUUID()

        const dataField = super.getProperty(ns.trn.dataBlock)
        const updateData = message[dataField]

        const update = nunjucks.renderString(template, updateData)

        logger.trace(`dataField = ${dataField}`)
        logger.trace(`updateData = `)
        //   logger.reveal(updateData)
        logger.trace(`update = ${update}`)
        //   process.exit()
        const response = await axios.post(endpoint.url, update, {
            headers: await this.makeHeaders(endpoint)
        })

        // https://axios-http.com/docs/res_schema
        if (response.status === 200 || response.status === 204) {
            message.updateStatus = 'success'
            return this.emit('message', message)
        }
        logger.trace(`SPARQLUpdate error, response : ${response.status} ${response.statusText}
            ${response.headers}`)
        //    logger.reveal(response)
    }


    async getUpdateEndpoint(message) {
        if (!this.env.endpoints) {
            const dir = this.getProperty(ns.trn.targetPath, message.rootDir)
            logger.debug(`SPARQLUpdate.getUpdateEndpoint, dir = ${dir}`)
            await this.env.loadEndpoints(dir)
        }
        return this.env.getUpdateEndpoint()
    }


    async makeHeaders(endpoint) {
        return {
            'Content-Type': 'application/sparql-update',
            'Authorization': this.env.getBasicAuthHeader(endpoint)
        }
    }
}

export default SPARQLUpdate

================
File: src/processors/sparql/validator.js
================
// validator.js
import { config } from './config.js';
import { TextUtils } from './TextUtils.js';
import { customPredicates } from './customPredicates.js';

export class MessageValidator {
    static validate(message) {
        const errors = [];
        
        // Check required fields
        for (const field of config.requiredFields) {
            if (!message[field]) {
                errors.push(`Missing required field: ${field}`);
            }
        }
        
        // Validate dates
        if (message.datePublished && !TextUtils.isValidDateTime(message.datePublished)) {
            errors.push('Invalid datePublished format');
        }
        
        if (message.dateModified && !TextUtils.isValidDateTime(message.dateModified)) {
            errors.push('Invalid dateModified format');
        }
        
        // Validate URLs
        if (message.author?.homepage && !TextUtils.isValidURL(message.author.homepage)) {
            errors.push('Invalid author homepage URL');
        }
        
        // Validate custom properties
        if (message.customProperties) {
            for (const [prop, value] of Object.entries(message.customProperties)) {
                try {
                    if (!customPredicates.validate(prop, value)) {
                        errors.push(`Invalid value for custom predicate: ${prop}`);
                    }
                } catch (e) {
                    errors.push(e.message);
                }
            }
        }
        
        return {
            isValid: errors.length === 0,
            errors
        };
    }
}

================
File: src/processors/staging/MarkdownFormatter.js
================
import path from 'path'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class MarkdownFormatter extends Processor {
    constructor(config) {
        super(config)
    }

    async process(message) {

        if (message.done) return

        // TODO move to config
        const dir = '/home/danny/github-danny/hyperdata/docs/postcraft/content-raw/chat-archives/md'

        const filename = `${message.content.created_at.substring(0, 10)}_${message.content.uuid.substring(0, 3)}.md`

        message.filepath = path.join(dir, message.meta.conv_uuid.substring(0, 4), filename)
        message.content = this.extractMarkdown(message)

        return this.emit('message', message)
    }

    // https://claude.ai/chat/be3ae4a9-51cd-42f6-a903-3193af00fed8

    extractMarkdown(message) {
        // TODO move to config
        const urlBase = 'https://claude.ai/chat/'

        const lines = []
        lines.push(`# [${message.meta.conv_name}](${urlBase}${message.meta.conv_uuid})\n`)
        //   lines.push(`${message.meta.conv_uuid}\n`)
        lines.push(`${message.content.uuid}\n`)
        // lines.push('')
        lines.push(message.content.text)
        lines.push('\n---\n')

        for (const [key, value] of Object.entries(message)) {
            if (key !== 'content' && value !== null) {
                if (value) {
                    const v = typeof value === 'object' ? JSON.stringify(value, null, 2) : value.toString()
                    lines.push(`* **${key}** : ${v}`)
                } else {
                    lines.push(`* **${key}** : [undefined]`)
                }
            }
        }

        return lines.join('\n')
    }

}

export default MarkdownFormatter

================
File: src/processors/staging/StagingProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import MarkdownFormatter from './MarkdownFormatter.js'
import TurtleFormatter from './TurtleFormatter.js'

// ref needed in transmissions/src/engine/AbstractProcessorFactory.js


class StagingProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.trn.MarkdownFormatter)) {
            return new MarkdownFormatter(config)
        }
        if (type.equals(ns.trn.TurtleFormatter)) {
            return new TurtleFormatter(config)
        }
        return false
    }
}
export default StagingProcessorsFactory

================
File: src/processors/staging/TurtleFormatter.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class TurtleFormatter extends Processor {
    constructor(config) {
        super(config)
        this.baseURI = config.baseURI || 'http://example.org/'
    }

    async process(message) {
        try {
            const item = message.currentItem
            if (!item) {
                return
            }

            // Convert item to Turtle
            const turtle = this.formatTurtle(item)
            message.content = turtle
            message.targetFile = `${item.id}.ttl`

            this.emit('message', message)
        } catch (err) {
            logger.error("TurtleFormatter.execute error: " + err.message)
            throw err
        }
    }

    formatTurtle(item) {
        const lines = []
        lines.push('@prefix : <http://example.org/ns#> .')
        lines.push('@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .')
        lines.push('')

        const subject = `<${this.baseURI}${item.id}>`
        lines.push(`${subject} a :Item ;`)

        const entries = Object.entries(item)
        entries.forEach(([key, value], index) => {
            if (value !== null) {
                const isLast = index === entries.length - 1
                const literal = typeof value === 'string' ?
                    `"${value.replace(/"/g, '\\"')}"` :
                    `"${JSON.stringify(value)}"`
                lines.push(`    :${key} ${literal}${isLast ? ' .' : ' ;'}`)
            }
        })

        return lines.join('\n')
    }
}

export default TurtleFormatter

================
File: src/processors/system/EnvLoader.js
================
import 'dotenv/config'

// import { readFile } from 'node:fs/promises' // whatever else

import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js' // maybe more specific


/**
 * FileReader class that extends xxxxxProcessor.
 * DESCRIPTION
 * #### __*Input*__
 * **message.INPUT**
 * #### __*Output*__
 * **message.OUTPUT**
 *
 * ### References
 * * https://dotenvx.com/
 * * https://github.com/motdotla/dotenv
*/
class EnvLoader extends Processor {

    /**
     * Constructs a new ProcessorExample instance.
     * @param {Object} config - The configuration object.
     */
    constructor(config) {
        super(config)
    }

    /**
     * Does something with the message and emits a 'message' event with the processed message.
     * @param {Object} message - The message object.
     */
    async process(message) {
        //   logger.setLogLevel('debug')
        // console.log(process.env)

        this.config.whiteboard.env = process.env

        return this.emit("message", message)
    }
}

export default EnvLoader

================
File: src/processors/system/SystemProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import EnvLoader from './EnvLoader.js'



class SystemsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.EnvLoader)) {
            return new EnvLoader(config)
        }
        return false
    }
}
export default SystemsProcessorsFactory

================
File: src/processors/terrapack/comment-stripper.js
================
import path from 'path';

const LANGUAGE_PATTERNS = {
    js: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    jsx: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    ts: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    tsx: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    py: {
        single: '#',
        multi: { start: '"""', end: '"""' }
    },
    java: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    cpp: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    c: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    h: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    }
};

export function commentStripper(content, filepath) {
    const ext = path.extname(filepath).toLowerCase().slice(1);
    const patterns = LANGUAGE_PATTERNS[ext];

    if (!patterns) {
        return content;
    }

    let lines = content.split('\n');
    let inMultiLineComment = false;
    let result = [];

    for (let i = 0; i < lines.length; i++) {
        let line = lines[i].trim();

        if (inMultiLineComment) {
            if (line.includes(patterns.multi.end)) {
                inMultiLineComment = false;
                line = line.split(patterns.multi.end)[1];
            } else {
                continue;
            }
        }

        if (patterns.multi && line.includes(patterns.multi.start)) {
            const parts = line.split(patterns.multi.start);
            if (!parts[1].includes(patterns.multi.end)) {
                inMultiLineComment = true;
                line = parts[0];
            } else {
                line = parts[0] + parts[1].split(patterns.multi.end)[1];
            }
        }

        if (patterns.single && line.startsWith(patterns.single)) {
            continue;
        }

        if (patterns.single) {
            const commentIndex = line.indexOf(patterns.single);
            if (commentIndex >= 0) {
                line = line.substring(0, commentIndex).trim();
            }
        }

        if (line.trim()) {
            result.push(line);
        }
    }

    return result.join('\n');
}

================
File: src/processors/terrapack/CommentStripper.js
================
import path from 'path';

const LANGUAGE_PATTERNS = {
    js: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    jsx: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    ts: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    tsx: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    py: {
        single: '#',
        multi: { start: '"""', end: '"""' }
    },
    java: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    cpp: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    c: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    h: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    }
};

export function commentStripper(content, filepath) {
    const ext = path.extname(filepath).toLowerCase().slice(1);
    const patterns = LANGUAGE_PATTERNS[ext];

    if (!patterns) {
        return content;
    }

    let lines = content.split('\n');
    let inMultiLineComment = false;
    let result = [];

    for (let i = 0; i < lines.length; i++) {
        let line = lines[i].trim();

        if (inMultiLineComment) {
            if (line.includes(patterns.multi.end)) {
                inMultiLineComment = false;
                line = line.split(patterns.multi.end)[1];
            } else {
                continue;
            }
        }

        if (patterns.multi && line.includes(patterns.multi.start)) {
            const parts = line.split(patterns.multi.start);
            if (!parts[1].includes(patterns.multi.end)) {
                inMultiLineComment = true;
                line = parts[0];
            } else {
                line = parts[0] + parts[1].split(patterns.multi.end)[1];
            }
        }

        if (patterns.single && line.startsWith(patterns.single)) {
            continue;
        }

        if (patterns.single) {
            const commentIndex = line.indexOf(patterns.single);
            if (commentIndex >= 0) {
                line = line.substring(0, commentIndex).trim();
            }
        }

        if (line.trim()) {
            result.push(line);
        }
    }

    return result.join('\n');
}

================
File: src/processors/terrapack/file-container.js
================
import Processor from '../../model/Processor.js'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import path from 'path'

class FileContainer extends Processor {
    constructor(config) {
        super(config)
        this.container = {
            files: {},
            summary: {
                totalFiles: 0,
                fileTypes: {},
                timestamp: new Date().toISOString()
            }
        }
    }

    async process(message) {
        if (message.done) {
            message.content = JSON.stringify(this.container, null, 2)
            message.filepath = this.getPropertyFromMyConfig(ns.trn.destination)
            return this.emit('message', message)
        }

        if (!message.filepath || !message.content) {
            logger.warn('FileContainer: Missing filepath or content')
            return
        }

        // Get relative path from target dir
        const targetDir = message.targetPath || message.rootDir
        const relativePath = path.relative(targetDir, message.filepath)

        // Store file content and metadata
        this.container.files[relativePath] = {
            content: message.content,
            type: path.extname(message.filepath),
            timestamp: new Date().toISOString()
        }

        // Update summary stats
        this.container.summary.totalFiles++
        const fileType = path.extname(message.filepath) || 'unknown'
        this.container.summary.fileTypes[fileType] = (this.container.summary.fileTypes[fileType] || 0) + 1

        return this.emit('message', message)
    }
}

export default FileContainer

================
File: src/processors/terrapack/FileContainer.js
================
import Processor from '../../model/Processor.js'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import path from 'path'

class FileContainer extends Processor {
    constructor(config) {
        super(config)
        this.container = {
            files: {},
            summary: {
                totalFiles: 0,
                fileTypes: {},
                timestamp: new Date().toISOString()
            }
        }
    }

    async process(message) {
        //    logger.setLogLevel('debug')
        message.filepath = await this.getProperty(ns.trn.destination)
        if (message.done) {

            // TODO FIX ME
            message.filepath = message.filepath + '_done.txt'

            message.content = JSON.stringify(this.container, null, 2)
            //   message.filepath = this.getPropertyFromMyConfig(ns.trn.destination);

            return this.emit('message', message)
        }

        if (!message.filepath || !message.content) {
            logger.warn('FileContainer: Missing filepath or content')
            // this.emit('message', message);
            return
        }

        // Store relative path from target directory
        const targetDir = message.targetPath || message.rootDir
        const relativePath = path.relative(targetDir, message.filepath)

        // Add file to container
        this.container.files[relativePath] = {
            content: message.content,
            type: path.extname(message.filepath),
            timestamp: new Date().toISOString()
        }

        // Update summary
        this.container.summary.totalFiles++
        const fileType = path.extname(message.filepath) || 'unknown'
        this.container.summary.fileTypes[fileType] = (this.container.summary.fileTypes[fileType] || 0) + 1

        return this.emit('message', message)
    }
}

export default FileContainer

================
File: src/processors/terrapack/terrapack-factory.js
================
import logger from '../../utils/Logger.js';
import ns from '../../utils/ns.js';
import FileContainer from './FileContainer.js';

class PackerProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.FileContainer)) {
            logger.debug('PackerProcessorsFactory: Creating FileContainer processor');
            return new FileContainer(config);
        }
        return false;
    }
}

export default PackerProcessorsFactory;

================
File: src/processors/terrapack/TerrapackProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import FileContainer from './FileContainer.js'

class TerrapackProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.trn.FileContainer)) {
            logger.debug('TerrapackProcessorsFactory: Creating FileContainer processor')
            return new FileContainer(config)
        }
        return false
    }
}

export default TerrapackProcessorsFactory

================
File: src/processors/test/TestProcessorsFactory.js
================
import ns from '../../utils/ns.js'

import TestSetting from './TestSetting.js'
import TestSettings from './TestSettings.js'


class TestProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.trn.TestSettings)) {
            return new TestSettings(config)
        }
        if (type.equals(ns.trn.TestSetting)) {
            return new TestSetting(config)
        }
        return false
    }
}

export default TestProcessorsFactory

================
File: src/processors/test/TestSetting.js
================
// src/processors/example-group/TestSettings.js
/**
 * @class TestSettings
 * @extends Processor
 * @classdesc
 * **a Basic Transmissions Processor**
 *
 * Provides a template for creating new processors, demonstrates use of config settings.
 *
 * #### __*Input*__
 * * **`message.common`** - addressed by all instances of this TestSettings (optional, default undefined)
 * * **`message.something1`** - Template string (used if templateFilename is not provided)
 * * **`message.something2`** - Object with properties for template rendering (e.g., title, body)
 * * **`message.notavalue`** - Object with properties for template rendering (e.g., title, body)
 *
 * #### __*Output*__
 * * **`message.content`** - The rendered template content
 *
 * #### __*Behavior*__
 * * Uses Nunjucks to render templates
 * * Can render from a template file or a template string
 * * Applies content from message.contentBlocks to the template
 *
 * #### __Tests__
 * *
 *
  * #### __*ToDo*__
 * * Add test information here
 * * Cache templates - cache in utils?
 */

import { readFile } from 'node:fs/promises'
import { access, constants } from 'node:fs'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'


class TestSettings extends Processor {
    constructor(config) {
        super(config)
    }

    /**
      * Does something with the message and emits a 'message' event with the processed message.
      * @param {Object} message - The message object.
      */
    async process(message) {

        logger.debug(`\n\nTestSetting.process`)

        // property values pulled from message | config settings | fallback

        message.settingValue = this.getProperty(ns.trn.theSettingProperty)

        logger.log(`\nmessage.settingValue ${message.settingValue}`)

        // message forwarded
        return this.emit('message', message)
    }
}
export default TestSettings

================
File: src/processors/test/TestSettings.js
================
// src/processors/example-group/TestSettings.js
/**
 * @class TestSettings
 * @extends Processor
 * @classdesc
 * **a Basic Transmissions Processor**
 *
 * Provides a template for creating new processors, demonstrates use of config settings.
 *
 * #### __*Input*__
 * * **`message.common`** - addressed by all instances of this TestSettings (optional, default undefined)
 * * **`message.something1`** - Template string (used if templateFilename is not provided)
 * * **`message.something2`** - Object with properties for template rendering (e.g., title, body)
 * * **`message.notavalue`** - Object with properties for template rendering (e.g., title, body)
 *
 * #### __*Output*__
 * * **`message.content`** - The rendered template content
 *
 * #### __*Behavior*__
 * * Uses Nunjucks to render templates
 * * Can render from a template file or a template string
 * * Applies content from message.contentBlocks to the template
 *
 * #### __Tests__
 * *
 *
  * #### __*ToDo*__
 * * Add test information here
 * * Cache templates - cache in utils?
 */

import { readFile } from 'node:fs/promises'
import { access, constants } from 'node:fs'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'


class TestSettings extends Processor {
    constructor(config) {
        super(config)
    }

    /**
      * Does something with the message and emits a 'message' event with the processed message.
      * @param {Object} message - The message object.
      */
    async process(message) {
        //   logger.setLogLevel('debug')
        logger.debug(`\n\nTestSettings.process`)

        // property values pulled from message | config settings | fallback

        const me = this.getProperty(ns.trn.me, 'default me')
        logger.log(`\nI am ${me}`)

        switch (me) {
            case ':settingsUseMessage':
                message.test = 'this came from message'
                logger.log(`${this.getProperty(ns.trn.test)}`)
                break

            case ':settingsSingle':
                logger.log(`${this.getProperty(ns.trn.name)}`)
                break

            case ':settingsURI':
                logger.log(`${this.getProperty(ns.trn.uri)}`)
                break

            case ':settingsPath':
                // need to check ontology for this
                // TODO needs something like :path a :Path .
                logger.log(`${this.getProperty(ns.trn.path)}`)
                break

            case ':settingsMulti':
                logger.log(`${this.getProperty(ns.trn.name)}`)
                logger.log(`${this.getProperty(ns.trn.uri)}`)
                break

            // TODO where did I use this?
            case ':settingsKeyValue':
                logger.log(`${this.getProperty(ns.trn.name)}`)
                break

            case ':settingsLists':
                logger.log(`aSetting : \n${this.getProperty(ns.trn.aSetting)}`)
                logger.log(`bSetting : \n${this.getProperty(ns.trn.bSetting)}`)
                break

            case ':settingsCollection':
                const values = this.getValues(ns.trn.items)
                logger.log(`values = `)
           //   logger.reveal(values)
                logger.sh(this.config)
                break

            default:
                logger.log(`This is fallback : ${this.getProperty(ns.trn.name, 'yes it is')}`)
                break
        }

        // message forwarded
        return this.emit('message', message)
    }
}
export default TestSettings

================
File: src/processors/text/LineReader.js
================
import { readFile } from 'node:fs/promises'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class LineReader extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {

        const text = data.toString()


        const lines = text.split('\n')
        for await (let line of lines) {
            if (line.trim() && !line.startsWith('#')) {
                logger.debug('Line = [[[' + line + ']]]')
                return this.emit('message', line, message)
            }
        }

        return this.emit('message', '~~done~~', message)
    }
}

export default LineReader

================
File: src/processors/text/StringFilter.js
================
import path from 'path'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'
import StringUtils from '../../utils/StringUtils.js'

class StringFilter extends Processor {
    constructor(config) {
        super(config)
    }

    // filepath maybe a good default, but..?
    async process(message) {
        if (message.done) {
            return this.emit('message', message)
        }

        if (!message.filepath) {
            logger.warn('StringFilter: No filepath provided')
            return
        }
        this.includePatterns = this.getValues(ns.trn.includePattern)
        this.excludePatterns = this.getValues(ns.trn.excludePattern)

        if (this.isAccepted(message.filepath)) {
            return this.emit('message', message)
        }
    }

    //matchPattern(filePath, pattern) {
    //  try {
    //    const regexPattern = pattern
    //      .replace(/\./g, '\\.')
    //    .replace(/\*/g, '.*')
    //  .replace(/\?/g, '.');
    //        const regex = new RegExp(`^${regexPattern}$`);
    //      const filename = path.basename(filePath);
    //    return regex.test(filename);
    //} catch (error) {
    //  logger.error('Pattern matching error:', { pattern, error });
    //return false;
    //  }
    // }

    isAccepted(filePath) {
        if (!filePath) return false

        if (this.excludePatterns.length === 0 && this.includePatterns.length === 0) {
            return true
        }

        /*
        for (const pattern of this.excludePatterns) {
            if (this.matchPattern(filePath, pattern)) {
                logger.debug(`File ${filePath} excluded by pattern ${pattern}`)
                return false
            }
        }
        */
        if (StringUtils.matchPatterns(filePath, this.excludePatterns)) {
            return false
        }

        if (StringUtils.matchPatterns(filePath, this.includePatterns)) {
            return true
        }

        /*
        if (this.includePatterns.length > 0) {
            for (const pattern of this.includePatterns) {
                if (this.matchPattern(filePath, pattern)) {
                    logger.debug(`File ${filePath} included by pattern ${pattern}`)
                    return true
                }
            }
            return false
        }
            */

        return true
    }
}

export default StringFilter

================
File: src/processors/text/StringMerger.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

// OLD : PRE-REFACTOR
class StringMerger extends Processor {
    constructor(config) {
        super(config)
        this.merged = ''
    }

    async process(message) {
        logger.log('SMDATA*********************************\n' + data)

        if (data === '~~done~~') {
            logger.log('SM  DONE**********************************\n' + this.merged)
            return this.emit('message', this.merged, message)
            return
        }
        this.merged = this.merged + data

    }
}

export default StringMerger

================
File: src/processors/text/StringReplace.js
================
// src/processors/text/StringReplace.js
/**
 * @class StringReplace
 * @extends Processor
 * @classdesc
 * **a Transmissions Processor**
 *
 * Replaces all occurrences of a specified substring in the content with a replacement string.
 *
 * ### Signature
 *
 * #### __*Input*__
 * * **`message.content`** - The original string content
 * * **`message.match`** - The substring to be replaced
 * * **`message.replace`** - The replacement string
 *
 * #### __*Output*__
 * * **`message.content`** - The modified string with replacements
 *
 * #### __*Behavior*__
 * * Replaces every exact occurrence of `message.match` in `message.content` with `message.replace`
 * * If `message.match` is not found, the content remains unchanged
 *
 * #### __Tests__
 * * TODO: Add test information
 */

import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class StringReplace extends Processor {
    constructor(config) {
        super(config)
    }

    /**
     * Executes the string replacement process
     * @param {Object} message - The message object containing content, match, and replace strings
     */
    async process(message) {
        // logger.setLogLevel('debug')
        const inputField = await this.getProperty(ns.trn.inputField)
        const outputField = await this.getProperty(ns.trn.outputField)

        var match = message.match ? message.match : await this.getProperty(ns.trn.match)
        var replace = message.replace ? message.replace : await this.getProperty(ns.trn.replace)

        var input = message.input ? message.input : message[inputField]
        if (!input) {
            input = message.content
        }

        logger.debug('StringReplace.process input = ' + input)

        // global s & r
        const output = input.split(match).join(replace)

        logger.debug('StringReplace output: ' + output)
        try {
            message[outputField] = output
        } catch {
            message.content = output
        }
        return this.emit('message', message)
    }
}

export default StringReplace

================
File: src/processors/text/Templater.js
================
// src/processors/text/Templater.js
/**
 * @class Templater
 * @extends Processor
 * @classdesc
 * **a Transmissions Processor**
 *
 * Provides templating functionality using Nunjucks.
 *
 * #### __*Input*__
 * * **`message.templateFilename`** - Path to the template file (optional)
 * * **`message.template`** - Template string (used if templateFilename is not provided)
 * * **`message.contentBlocks`** - Object with properties for template rendering (e.g., title, body)
 *
 * #### __*Output*__
 * * **`message.content`** - The rendered template content
 *
 * #### __*Behavior*__
 * * Uses Nunjucks to render templates
 * * Can render from a template file or a template string
 * * Applies content from message.contentBlocks to the template
 *
 * #### __Tests__
 * *
 *
  * #### __TODO__
 * * Add test information here
 * * Cache templates - cache in utils?
 */

import Processor from '../../model/Processor.js'
import nunjucks from 'nunjucks'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

class Templater extends Processor {
    constructor(config) {
        super(config)
    }

    /**
     * Executes the templating process
     * @param {Object} message - The message object containing template and content information
     */
    async process(message) {

        logger.debug(`\n\nTemplater.process`)
        logger.debug(`\nTemplater.process, message.contentBlocks = ${JSON.stringify(message.contentBlocks)}`)
        var templateFilename = await this.getProperty(ns.trn.templateFilename)

        if (templateFilename) {
            //       logger.debug(`\nTemplater.process, templateFilename = ${templateFilename}`)

            // TODO tidy this up (move out?)
            // Extract path and filename from templateFilename
            var templatePath = templateFilename.substr(0, templateFilename.lastIndexOf("/"))
            const filename = templateFilename.substr(templateFilename.lastIndexOf("/") + 1)

            if (!path.isAbsolute(templatePath)) { // needed?
                super.getProperty(ns.trn.targetPath)
                templatePath = path.join(super.getProperty(ns.trn.targetPath), templatePath) + path.sep
            }

            logger.debug('\nTemplater, templatePath = ' + templatePath)
            logger.debug('Templater, filename = ' + filename)
            logger.debug(`\nTemplater.process, templateFilename = ${templateFilename}`)

            // Configure Nunjucks with the template path
            nunjucks.configure(templatePath, { autoescape: false })

            //   logger.debug(`content PRE = ${message.content}`)
            // Render the template file
            try {
                // message.content = nunjucks.render(templateFilename, message.contentBlocks)
                message.content = nunjucks.render(filename, message.contentBlocks)
            } catch (err) {
                logger.error(`\nTemplater.process, error rendering template: ${err}`)
                logger.error(`templatePath = ${templatePath}`)
                logger.error(`templateFilename = ${templateFilename}`)
                return
            }
            //
            logger.debug(`content POST = ${message.content}`)


        } else {   // Configure Nunjucks for string templates
            // TODO priorities

            nunjucks.configure({ autoescape: false })

            //    logger.reveal(message)
            // Render the template string
            message.content = nunjucks.renderString(message.template, message.contentBlocks)
        }

        return this.emit('message', message)
    }
}
export default Templater

================
File: src/processors/text/TextProcessorsFactory.js
================
import ns from '../../utils/ns.js'

import LineReader from './LineReader.js'
import StringFilter from './StringFilter.js'
import StringMerger from './StringMerger.js'
import StringReplace from './StringReplace.js'
import Templater from './Templater.js'

class TextProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.trn.Templater)) {
            return new Templater(config)
        }
        if (type.equals(ns.trn.LineReader)) {
            return new LineReader(config)
        }

        if (type.equals(ns.trn.StringFilter)) {
            return new StringFilter(config)
        }

        if (type.equals(ns.trn.StringMerger)) {
            return new StringMerger(config)
        }

        if (type.equals(ns.trn.StringReplace)) {
            return new StringReplace(config)
        }

        //     if (type.equals(ns.trn.CommentStripper)) {
        //       return new CommentStripper(config)
        // }

        return false
    }
}

export default TextProcessorsFactory

================
File: src/processors/unsafe/_RunCommand.spec.js
================
import RunCommand from '../../src/processors/unsafe/RunCommand.js';
import { expect } from 'chai';
import fs from 'fs/promises';
import path from 'path';

describe('RunCommand', function () {
    let runCommand;
    const workingDir = 'src/applications/test_runcommand/data';

    beforeEach(function () {
        jasmine.DEFAULT_TIMEOUT_INTERVAL = 3000;
        runCommand = new RunCommand({
            simples: true,
            allowedCommands: ['echo', 'ls'],
            blockedPatterns: ['rm', '|']
        });
    });

    it('should validate command output against required file', async function () {
        const requiredPath = path.join(workingDir, 'output', 'required-01.txt');
        const required = await fs.readFile(requiredPath, 'utf8');
        const message = { command: 'echo "Hello from RunCommand!"' };

        const result = await runCommand.process(message);
        expect(result.content.trim()).to.equal(required.trim());
    });

    it('should handle timeouts', async function () {
        try {
            await runCommand.executeCommand('sleep 10');
            expect.fail('Should have timed out');
        } catch (error) {
            expect(error.message).to.include('timeout');
        }
    });
});

================
File: src/processors/unsafe/BashCommand.js
================
import { exec } from 'child_process'
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import ns from '../../utils/ns.js'

class BashCommand extends Processor {
    constructor(config) {
        super(config)
        this.allowedCommands = config.allowedCommands || []
        this.blockedPatterns = config.blockedPatterns || []
        this.timeout = config.timeout || 5000
        this.initializeSecurity()
    }

    async initializeSecurity() {
        if (this.settings) {
            const allowed = await this.getPropertyFromMyConfig(ns.trn.allowedCommands)
            this.allowedCommands = allowed ? allowed.split(',') : []

            const blocked = await this.getPropertyFromMyConfig(ns.trn.blockedPatterns)
            this.blockedPatterns = blocked ? blocked.split(',') : []
        }
    }

    validateCommand(command) {
        if (!command) {
            throw new Error('No command specified')
        }

        const commandName = command.split(' ')[0]
        const isAllowed = this.allowedCommands.length === 0 ||
            this.allowedCommands.includes(commandName)

        if (!isAllowed) {
            throw new Error(`Command '${commandName}' not in allowed list`)
        }

        const hasBlocked = this.blockedPatterns.some(pattern =>
            command.includes(pattern)
        )
        if (hasBlocked) {
            throw new Error('Command contains blocked pattern')
        }
    }

    async process(message) {
        let command = message.command
        if (!command) {
            command = this.getPropertyFromMyConfig(ns.trn.command)
        }

        try {
            this.validateCommand(command)
            const result = await this.executeCommand(command)
            message.content = result.stdout
            message.commandResult = result
            logger.debug(`Command executed successfully: ${command}`)
        } catch (error) {
            logger.error(`Command error: ${error.message}`)
            message.commandError = error.message
            message.content = error.message
            throw error
        }

        return this.emit('message', message)
    }

    executeCommand(command) {
        return new Promise((resolve, reject) => {
            const child = exec(command, {
                timeout: this.timeout
            }, (error, stdout, stderr) => {
                if (error) {
                    if (error.signal === 'SIGTERM') {
                        reject(new Error('Command timeout'))
                    } else {
                        reject(error)
                    }
                    return
                }
                resolve({
                    stdout: stdout.toString(),
                    stderr: stderr.toString(),
                    code: 0
                })
            })
        })
    }
}

export default BashCommand

================
File: src/processors/unsafe/ExampleProcessor.js
================
// src/processors/example-group/ExampleProcessor.js
/**
 * @class ExampleProcessor
 * @extends Processor
 * @classdesc
 * **a Basic Transmissions Processor**
 *
 * Provides a template for creating new processors, demonstrates use of config settings.
 *
 * #### __*Input*__
 * * **`message.common`** - addressed by all instances of this ExampleProcessor (optional, default undefined)
 * * **`message.something1`** - Template string (used if templateFilename is not provided)
 * * **`message.something2`** - Object with properties for template rendering (e.g., title, body)
 * * **`message.notavalue`** - Object with properties for template rendering (e.g., title, body)
 *
 * #### __*Output*__
 * * **`message.content`** - The rendered template content
 *
 * #### __*Processing*__
 * * Uses Nunjucks to render templates
 * * Can render from a template file or a template string
 * * Applies content from message.contentBlocks to the template
 *
* #### __*Side Effects*__
 *
 * #### __Tests__
 * *
 *
  * #### __*ToDo*__
 * * Add test information here
 * * Cache templates - cache in utils?
 */

import { readFile } from 'node:fs/promises'
import { access, constants } from 'node:fs'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'


class ExampleProcessor extends Processor {
    constructor(config) {
        super(config)
    }

    /**
      * Does something with the message and emits a 'message' event with the processed message.
      * @param {Object} message - The message object.
      */
    async process(message) {
        logger.debug(`\n\nExampleProcessor.process`)

        // TODO figure this out better
        // may be needed if preceded by a spawning processor, eg. fs/DirWalker
        if (message.done) {
            return this.emit('message', message)
            // or simply return
        }

        // message is processed here :

        // property values pulled from message | config settings | fallback
        const me = await this.getProperty(ns.trn.me)
        logger.log(`\nI am ${me}`)

        message.common = await this.getProperty(ns.trn.common)
        message.something1 = await this.getProperty(ns.trn.something1)

        message.something2 = await this.getProperty(ns.trn.something2)

        var added = await this.getProperty(ns.trn.added, '')
        message.something1 = message.something1 + added

        message.notavalue = await this.getProperty(ns.trn.notavalue, 'fallback value')

        // message forwarded
        return this.emit('message', message)
    }
}
export default ExampleProcessor

================
File: src/processors/unsafe/UnsafeProcessorsFactory.js
================
// import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import BashCommand from './BashCommand.js'


class UnsafeProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.trn.BashCommand)) {
            return new BashCommand(config)
        }

        return false
    }
}
export default UnsafeProcessorsFactory

================
File: src/processors/util/CaptureAll.js
================
/// `USE flow/Accumulate.js instead`
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class CaptureAll extends Processor {
    constructor(config) {
        // Ensure whiteboard is initialized as an array
        if (!config.whiteboard || !Array.isArray(config.whiteboard)) {
            config.whiteboard = []
        }
        super(config)

        if (CaptureAll.singleInstance) {
            return CaptureAll.singleInstance
        }
        CaptureAll.singleInstance = this
    }

    async process(message) {
        logger.debug(`CaptureAll at [${message.tags}] ${this.getTag()}, done=${message.done}`)
        throw Error(`USE flow/Accumulate.js instead`)
        this.config.whiteboard.push(message)
        return this.emit('message', message)
    }
}

export default CaptureAll

================
File: src/processors/util/SetMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'

class SetMessage extends Processor {

    constructor(config) {
        super(config)
        logger.log('SetMessage constructor')
    }

    async process(message) {
        //   logger.setLogLevel('debug')
        const setters = await this.getSetters(this.config, this.settingsNode, ns.trn.setValue)
        for (let i = 0; i < setters.length; i++) {
            message[setters[i].key] = setters[i].value
        }
        return this.emit('message', message)
    }

    async getSetters(config, settings, term) { // TODO refactor - is same in RestructureJSON
        //   logger.debug(`***** config = ${config}`)
        logger.debug(`***** settings.value = ${settings.value}`)
        logger.debug(`***** term = ${term}`)
        const settersRDF = GrapoiHelpers.listToArray(config, settings, term)
        const dataset = this.config
        var setters = []
        for (let i = 0; i < settersRDF.length; i++) {
            let setter = settersRDF[i]
            let poi = rdf.grapoi({ dataset: dataset, term: setter })
            let key = poi.out(ns.trn.key).value
            let value = poi.out(ns.trn.value).value
            setters.push({ "key": key, "value": value })
        }
        return setters
    }
}

export default SetMessage

================
File: src/processors/util/ShowConfig.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class ShowConfig extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {

        logger.log("***************************")
        logger.log("***   Config Triples   ***")
        logger.log(this.config)
        logger.log("***************************")
        return this.emit('message', message)
    }
}

export default ShowConfig

================
File: src/processors/util/ShowMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'
import chalk from 'chalk'

class ShowMessage extends Processor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async process(message) {
        //   if (this.verbose) logger.log("\n***  Show Message ***")

        logger.log(chalk.bgYellow.black('\nMessage vvvvvvvvvvvvvvvvvvvvvvvv'))
        logger.reveal(message)
        logger.log(chalk.bgYellow.black('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^'))
        //     logger.log("***  Trace")
        //   console.trace() // move to Logger, only when debugging
        // logger.log("***************************")

        return this.emit('message', message)
    }
}

export default ShowMessage

================
File: src/processors/util/ShowSettings.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../../model/Processor.js'

// probably not needed, see TestSetting
class ShowSettings extends Processor {

    constructor(config) {
        super(config)
        //   this.verbose = false
    }

    async process(message) {
        //  logger.setLogLevel('debug')
        logger.debug(`ShowSettings.process`)

        const property = ns.trn.name

        logger.debug(`ShowSettings.process, property = ${property}`)

        const value = await this.getProperty(property)

        logger.debug(`ShowSettings.process, value  = ${value}`)

        return this.emit('message', message)
    }
}

export default ShowSettings

================
File: src/processors/util/ShowTransmission.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class ShowTransmission extends Processor {

    async process(message) {
        logger.log(this.transmission.toString())
        return this.emit('message', message)
    }
}

export default ShowTransmission

================
File: src/processors/util/Stash.js
================
import rdf from 'rdf-ext'
import { fromFile, toFile } from 'rdf-utils-fs'
import Processor from '../../model/Processor.js'

/**
 * Takes the input and stashes it in the message as told by processors.ttl
 *
 * #### __*Input*__
 * **data** : any
 * **message** : any
 * #### __*Output*__
 * **data** : as Input
 * **message** : adds key:value determined by processors.ttl
 * @extends Processor
 */
class Stash extends Processor {

    /**
     * Create a DatasetReader.
     * @param {Object} config - The configuration object.
     */
    constructor(config) {
        super(config)
    }

    /**
     * Execute the DatasetReader processor.
     * @param {string} data -.
     * @param {Object} message - .
     */
    async process(message) {
        const manifestFilename = rootDir + '/manifest.ttl'
        const stream = fromFile(manifestFilename)

        // should append RDF to incoming
        message.rootDir = rootDir
        message.dataset = await rdf.dataset().import(stream)
        return this.emit('message', message)
    }
}
export default Stash

================
File: src/processors/util/UtilProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import ShowMessage from './ShowMessage.js'
import ShowTransmission from './ShowTransmission.js'
import CaptureAll from './CaptureAll.js'
import ShowConfig from './ShowConfig.js'
import WhiteboardToMessage from './WhiteboardToMessage.js'
import SetMessage from './SetMessage.js'
import ShowSettings from './ShowSettings.js'
ShowSettings

class UtilProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.trn.ShowMessage)) {
            return new ShowMessage(config)
        }
        if (type.equals(ns.trn.ShowTransmission)) {
            return new ShowTransmission(config)
        }
        if (type.equals(ns.trn.CaptureAll)) {
            return new CaptureAll(config)
        }
        if (type.equals(ns.trn.ShowConfig)) {
            return new ShowConfig(config)
        }
        if (type.equals(ns.trn.WhiteboardToMessage)) {
            return new WhiteboardToMessage(config)
        }
        if (type.equals(ns.trn.SetMessage)) {
            return new SetMessage(config)
        }
        if (type.equals(ns.trn.ShowSettings)) {
            return new ShowSettings(config)
        }
        return false
    }
}
export default UtilProcessorsFactory

================
File: src/processors/util/WhiteboardToMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../../model/Processor.js'

class WhiteboardToMessage extends Processor {

    constructor(config) {
        super(config)
    }
    async process(message) {

        logger.debug('WhiteboardToMessage at [' + message.tags + '] ' + this.getTag())

        const originalArray = this.config.whiteboard

        message.whiteboard = Object.keys(originalArray).reduce((acc, key) => {
            const value = originalArray[key]
            if (value !== undefined && value !== null) {
                Object.keys(value).forEach((prop) => {
                    if (!acc[prop]) {
                        acc[prop] = []
                    }
                    acc[prop].push(value[prop])
                })
            }
            return acc
        }, {})

        return this.emit('message', message)
    }
}

export default WhiteboardToMessage

================
File: src/processors/xmpp/XmppClient.js
================
import logger from "../../utils/Logger.js" // path will likely change
import Processor from "../../model/Processor.js" // maybe more specific

/**
 * FileReader class that extends xxxxxProcessor.
 * DESCRIPTION
 * #### __*Input*__
 * **message.INPUT**
 * #### __*Output*__
 * **message.OUTPUT**
 */
class XmppClient extends Processor {
  /**
   * Constructs a new ProcessorExample instance.
   * @param {Object} config - The configuration object.
   */
  constructor(config) {
    super(config)
  }

  /**
   * Does something with the message and emits a 'message' event with the processed message.
   * @param {Object} message - The message object.
   */
  async process(message) {
    logger.setLogLevel("debug")

    // processing goes here
    return this.emit("message", message)
  }
}

export default XmppClient

================
File: src/processors/xmpp/XmppProcessorsFactory.js
================
import logger from "../../utils/Logger.js";
import ns from "../../utils/ns.js";

import ProcessorTemplate from "./XmppClient.js";

// ref needed in transmissions/src/processors/base/AbstractProcessorFactory.js

class XmppProcessorsFactory {
  static createProcessor(type, config) {
    if (type.equals(ns.trn.XmppClient)) {
      return new XmppClient(config)
    }
    return false
  }
}
export default XmppProcessorsFactory

================
File: src/processors/about.md
================
# Creating a new Processor

- update repopacks for `transmissions` and `trans-apps`
- create a new chat session in existing Project
- upload repopacks to Claude, with anything else that might be relevant (handover from previous session?)
- follow the prompt model as in `/home/danny/workspaces_hkms-desktop/postcrafts-raw/transmissions/prompts/github-list.md`
- remember additions to `xProcessorsFactory.js` and `transmissions/src/engine/AbstractProcessorFactory.js`

#:todo add comment creation
#:todo check simples & application suitability
#:todo create document creation workflow
#:todo create manifest.ttl creation
#:todo make crossrefs.md, crossrefs.ttl
#:todo create manifest.ttl consumption
#:todo add test creation
#:todo wire to an API, include file creation ops
#:todo add support in #:hyperdata-desktop

#:todo dedicated transmissions model, fine-tuned on relevant docs

#:todo extract todos as something like :

```turtle
<http://hyperdata.it/transmissions/src/processors/about/nid123> a pv:ToDoItem ;
dc:source <http://hyperdata.it/transmissions/src/processors/about.md> ;
pv:semtag "#:todo" ;
dc:line "3" ;
dc:title "tbd" ;
dc:content "extract todos as something like :" .
```

================
File: src/simples/env-loader/about.md
================
node src/apps-simple/env-loader/env-loader.js

from:

:envy a trm:Pipeline ;

# trm:pipe (:SC :s10 :s20 :SM) .

trm:pipe (:p10 :p20 :SC) .
:p10 a :EnvLoader .
:p20 a :WhiteboardToMessage .

================
File: src/simples/env-loader/env-loader.js
================
import logger from '../../utils/Logger.js'
import EnvLoader from '../../processors/system/EnvLoader.js'
import WhiteboardToMessage from '../../processors/util/WhiteboardToMessage.js'

logger.log('EnvLoader simple')

const config = { whiteboard: [] }

const p10 = new EnvLoader(config)
p10.id = 'http://purls.org/stuff/#p10'

const p20 = new WhiteboardToMessage(config)
p10.id = 'http://purls.org/stuff/#p20'

var message = {
    "workingDir": "src/applications/env-loader-test/data",
    "rootDir": "[no key]",
    "tags": "SM"
}

const x = 3

message = await p10.process(message)

logger.log('p10 output ' + p10.getTag() + message)

message = await p20.process(message)

logger.log('p20 output ')

logger.reveal(message)

================
File: src/simples/nop/nop.js
================
// nop.js
// node src/simples/nop/nop.js

import NOP from '../../processors/flow/NOP.js'

const config = {
    "runmode": "functions",
    whiteboard: []
}

const nop = new NOP(config)

var message = { 'value': '42' }

message = await nop.process(message)

console.log('value = ' + message.value)

================
File: src/simples/nop/simple-runner.js
================
// simple-runner.js
import NOP from '../../processors/flow/NOP.js'
import Fork from '../../processors/flow/Fork.js'

/*
async function runProcessor(processor, message) {
    const outputs = await processor.process(message)
    return outputs
}
*/

async function main() {
    const config = {}
    const nop = new NOP(config)
    const fork = new Fork(config)

    var message = { 'value': '42' }

    // Run NOP
    // message = await nop.execut(message)
    var outputs = await nop.process(message)
    console.log('NOP outputs:', outputs)

    // Run Fork
    message.nForks = 3
    outputs = await fork.process(message)
    console.log('Fork outputs:', outputs)
}

main().catch(console.error)

================
File: src/simples/set-message/set-message.js
================
// set-message.js
// node src/simples/set-message/set-message.js 

import logger from '../../utils/Logger.js'
import SetMessage from '../../processors/util/SetMessage.js'

const config = {
    "runmode": "functions",
    whiteboard: []
}

const setm = new SetMessage(config)

var message = { 'value': '42' }

message = await setm.process(message)

logger.log('value = ' + message.value)

logger.reveal(message)

================
File: src/tools/node-flow/editor-html.html
================
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Transmission Pipeline Editor</title>
  <style>
    body, html {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
      overflow: hidden;
      font-family: Arial, sans-serif;
    }
    
    #editor-container {
      position: relative;
      width: 100%;
      height: 100%;
    }
    
    #canvas {
      width: 100%;
      height: 100%;
      display: block;
    }
    
    #toolbar {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      padding: 10px;
      background-color: #0c2b35;
      color: #afb9bb;
      display: flex;
      gap: 10px;
      z-index: 100;
    }
    
    button {
      padding: 5px 10px;
      background-color: #154050;
      color: #afb9bb;
      border: 1px solid #1c1c1c;
      border-radius: 4px;
      cursor: pointer;
      transition: background-color 0.2s;
    }
    
    button:hover {
      background-color: #195366;
    }
    
    #status {
      margin-left: auto;
      padding: 5px 10px;
      background-color: #07212a;
      border-radius: 4px;
    }
    
    .dialog {
      display: none;
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      background-color: #0c2b35;
      border: 1px solid #154050;
      border-radius: 8px;
      padding: 20px;
      z-index: 200;
      min-width: 300px;
    }
    
    .dialog h2 {
      margin-top: 0;
      color: #afb9bb;
    }
    
    .dialog label {
      display: block;
      margin-bottom: 5px;
      color: #afb9bb;
    }
    
    .dialog input {
      width: 100%;
      padding: 8px;
      background-color: #07212a;
      border: 1px solid #154050;
      border-radius: 4px;
      color: #afb9bb;
      margin-bottom: 15px;
    }
    
    .dialog-buttons {
      display: flex;
      justify-content: flex-end;
      gap: 10px;
    }
  </style>
</head>
<body>
  <div id="toolbar">
    <button id="load-btn">Load TTL</button>
    <button id="save-btn">Save TTL</button>
    <button id="new-btn">New Transmission</button>
    <button id="organize-btn">Organize</button>
    <span id="status">Ready</span>
  </div>
  
  <div id="editor-container">
    <canvas id="canvas"></canvas>
  </div>
  
  <div id="new-dialog" class="dialog">
    <h2>New Transmission</h2>
    <label for="transmission-name">Transmission Name:</label>
    <input type="text" id="transmission-name" value="New Transmission">
    <div class="dialog-buttons">
      <button id="cancel-new">Cancel</button>
      <button id="create-new">Create</button>
    </div>
  </div>
  
  <!-- Hidden file input -->
  <input type="file" id="file-input" accept=".ttl" style="display:none">
  
  <script type="module">
    import TransmissionEditor from './TransmissionEditor.js';
    
    document.addEventListener('DOMContentLoaded', () => {
      // Initialize the editor
      const canvas = document.getElementById('canvas');
      const editor = new TransmissionEditor(canvas);
      
      // Set up UI elements
      const fileInput = document.getElementById('file-input');
      const loadBtn = document.getElementById('load-btn');
      const saveBtn = document.getElementById('save-btn');
      const newBtn = document.getElementById('new-btn');
      const organizeBtn = document.getElementById('organize-btn');
      const status = document.getElementById('status');
      
      // New transmission dialog
      const newDialog = document.getElementById('new-dialog');
      const transmissionName = document.getElementById('transmission-name');
      const cancelNewBtn = document.getElementById('cancel-new');
      const createNewBtn = document.getElementById('create-new');
      
      // Load TTL file
      loadBtn.addEventListener('click', () => {
        fileInput.click();
      });
      
      fileInput.addEventListener('change', async (e) => {
        const file = e.target.files[0];
        if (file) {
          try {
            status.textContent = `Loading ${file.name}...`;
            await editor.loadFromFile(file.path || URL.createObjectURL(file));
            status.textContent = `Loaded ${file.name}`;
          } catch (error) {
            status.textContent = `Error: ${error.message}`;
            console.error(error);
          }
        }
      });
      
      // Save TTL file
      saveBtn.addEventListener('click', async () => {
        try {
          status.textContent = 'Saving...';
          
          // In a real application, we would show a file dialog here
          // For now, we'll just save to the loaded file
          await editor.saveToFile();
          
          status.textContent = 'Saved!';
        } catch (error) {
          status.textContent = `Error: ${error.message}`;
          console.error(error);
        }
      });
      
      // New transmission
      newBtn.addEventListener('click', () => {
        newDialog.style.display = 'block';
      });
      
      cancelNewBtn.addEventListener('click', () => {
        newDialog.style.display = 'none';
      });
      
      createNewBtn.addEventListener('click', () => {
        const name = transmissionName.value.trim() || 'New Transmission';
        editor.createNewTransmission(name);
        newDialog.style.display = 'none';
        status.textContent = `Created new transmission: ${name}`;
      });
      
      // Organize graph
      organizeBtn.addEventListener('click', () => {
        try {
          editor.getGraph().organize();
          status.textContent = 'Graph organized!';
        } catch (error) {
          status.textContent = `Error: ${error.message}`;
          console.error(error);
        }
      });
      
      // Initial status
      status.textContent = 'Ready - Click "Load TTL" to open a transmission file';
    });
  </script>
</body>
</html>

================
File: src/tools/node-flow/ProcessorNodePublisher.js
================
// ProcessorNodePublisher.js
// Defines node types for transmission processors in node-flow

import { Publisher } from '@elicdavis/node-flow'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import rdf from 'rdf-ext'

class ProcessorNodePublisher extends Publisher {
  /**
   * Creates a publisher that defines node types for transmission processors
   */
  constructor() {
    super({
      name: 'Transmissions',
      description: 'Processor nodes for Transmissions',
      version: '1.0.0',
      nodes: {}
    })
    
    // Register common processor types
    this.registerCommonProcessorTypes()
  }

  /**
   * Registers common processor types found in transmission files
   */
  registerCommonProcessorTypes() {
    // Core processors
    this.registerProcessor('DeadEnd', 'Ends the pipeline without error')
    this.registerProcessor('Halt', 'Stops pipeline execution with error')
    this.registerProcessor('ShowConfig', 'Displays configuration and continues')
    this.registerProcessor('ShowMessage', 'Displays message contents and continues')
    this.registerProcessor('NOP', 'No operation, just passes message through')
    this.registerProcessor('Unfork', 'Collapses all pipes but one')
    
    // File processors
    this.registerProcessor('FileReader', 'Reads a file into the message')
    this.registerProcessor('FileWriter', 'Writes message content to a file')
    
    // JSON processors
    this.registerProcessor('JSONWalker', 'Navigates JSON structure')
    
    // Text processors
    this.registerProcessor('Restructure', 'Restructures message content')
    this.registerProcessor('MarkdownFormatter', 'Formats content as Markdown')
  }

  /**
   * Registers a processor type as a node type
   * @param {string} type - Processor type name
   * @param {string} description - Description of the processor
   */
  registerProcessor(type, description = '') {
    const config = this.createProcessorNodeConfig(type, description)
    this.register(type, config)
    logger.debug(`ProcessorNodePublisher: Registered processor type: ${type}`)
  }

  /**
   * Creates a node configuration for a processor type
   * @param {string} type - Processor type name
   * @param {string} description - Description of the processor
   * @returns {Object} - Node configuration object
   */
  createProcessorNodeConfig(type, description) {
    return {
      title: type,
      subTitle: description,
      info: description,
      canEditInfo: true,
      locked: false,
      style: {
        title: {
          color: '#154050',
          textStyle: {
            color: '#afb9bb'
          }
        }
      },
      inputs: [{
        name: 'in',
        type: 'message'
      }],
      outputs: [{
        name: 'out',
        type: 'message'
      }],
      data: {
        processorType: type
      }
    }
  }

  /**
   * Registers processor types discovered in a transmission
   * @param {Array} transmissions - Array of transmission data objects
   */
  registerProcessorsFromTransmissions(transmissions) {
    const registeredTypes = new Set()
    
    for (const transmission of transmissions) {
      for (const processor of transmission.processors) {
        const shortType = processor.shortType
        
        if (shortType && !registeredTypes.has(shortType) && !this.nodes().has(shortType)) {
          this.registerProcessor(shortType)
          registeredTypes.add(shortType)
        }
      }
    }
  }
}

export default ProcessorNodePublisher

================
File: src/tools/node-flow/README.txt
================
# Transmission Pipeline Editor

This module provides a visual editor for transmission pipelines using the node-flow library. It allows you to:

- Load existing transmission TTL files
- Visualize processors and connections
- Edit the pipeline by adding, removing, and connecting nodes
- Save changes back to TTL format

## Architecture

The editor is built with a loosely-coupled architecture that allows the transmission system and node-flow to work together without tight dependencies:

1. **TransmissionsLoader** - Loads TTL files and extracts transmission data
2. **ProcessorNodePublisher** - Defines node types for transmission processors
3. **TransmissionsGraphBuilder** - Builds node-flow graphs from transmission data
4. **TransmissionsExporter** - Exports node-flow graphs back to TTL format
5. **TransmissionEditor** - Main component that provides the editing functionality

## Installation

1. Place these files in the `src/tools/nodeflow` directory
2. Make sure node-flow is installed (npm package `@elicdavis/node-flow`)

## Usage

### Basic Setup

```javascript
import TransmissionEditor from './tools/nodeflow/TransmissionEditor.js';

// Initialize the editor with a canvas element
const canvas = document.getElementById('editor-canvas');
const editor = new TransmissionEditor(canvas);

// Load a transmission TTL file
await editor.loadFromFile('path/to/transmission.ttl');

// Save changes
await editor.saveToFile('path/to/output.ttl');
```

### Running the Editor

The simplest way to use the editor is to open the included `index.html` file in a browser that supports ES modules.

## Key Features

- **Visualization** - See processors and their connections graphically
- **Editing** - Add, remove, and connect processors visually
- **Organization** - Automatically organize the graph layout
- **Persistence** - Load and save transmission files in TTL format

## Integration Notes

This editor is designed to work with both the transmission system and node-flow without modifying either. The editor acts as a bridge between the two systems, translating between RDF-based transmission definitions and node-flow's visual graph representation.

## Extension Points

- **Adding Processor Types** - Modify `ProcessorNodePublisher.js` to add new processor types
- **Custom Styling** - Adjust node appearance in `ProcessorNodePublisher.js`
- **Advanced Features** - Extend `TransmissionEditor.js` to add new editing capabilities

## Future Improvements

- Support for editing processor settings
- Better handling of comments and documentation
- Multi-transmission editing in a single graph
- Visual indication of processor types
- Proper file dialogs for loading/saving

================
File: src/tools/node-flow/TransmissionEditor.js
================
// TransmissionEditor.js
// Main component that provides transmission editing functionality

import { NodeFlowGraph } from '@elicdavis/node-flow'
import TransmissionsLoader from './TransmissionsLoader.js'
import TransmissionsGraphBuilder from './TransmissionsGraphBuilder.js'
import TransmissionsExporter from './TransmissionsExporter.js'
import ProcessorNodePublisher from './ProcessorNodePublisher.js'
import logger from '../../utils/Logger.js'

class TransmissionEditor {
  /**
   * Creates a transmission editor
   * @param {HTMLCanvasElement} canvas - Canvas element for the editor
   */
  constructor(canvas) {
    // Initialize the graph
    this.graph = new NodeFlowGraph(canvas, {
      backgroundColor: '#07212a'
    })
    
    // Create component instances
    this.loader = new TransmissionsLoader()
    this.builder = new TransmissionsGraphBuilder(this.graph)
    this.exporter = new TransmissionsExporter(this.graph)
    this.publisher = new ProcessorNodePublisher()
    
    // Register publisher
    this.graph.addPublisher('transmissions', this.publisher)
    
    // Track loaded file
    this.currentFile = null
    this.loadedTransmissions = []
    
    // Set up event handlers
    this.setupEvents()
    
    logger.info('TransmissionEditor: Initialized')
  }
  
  /**
   * Sets up event handlers for the editor
   */
  setupEvents() {
    // Set up node creation handler
    this.graph.addOnNodeCreatedListener((publisher, nodeType, node) => {
      logger.debug(`TransmissionEditor: Node created - ${nodeType}`)
      
      // Set default metadata
      if (this.loadedTransmissions.length > 0) {
        const transmission = this.loadedTransmissions[0]
        node.setMetadataProperty('transmissionId', transmission.id)
        node.setMetadataProperty('transmissionLabel', transmission.label)
        node.setMetadataProperty('processorType', `http://purl.org/stuff/transmissions/${nodeType}`)
      }
    })
  }
  
  /**
   * Loads transmissions from a TTL file
   * @param {string} filePath - Path to the TTL file
   * @returns {Promise<void>}
   */
  async loadFromFile(filePath) {
    try {
      logger.info(`TransmissionEditor: Loading from ${filePath}`)
      
      // Load transmissions from file
      const transmissions = await this.loader.loadFromFile(filePath)
      this.loadedTransmissions = transmissions
      
      // Register processor types discovered in the file
      this.publisher.registerProcessorsFromTransmissions(transmissions)
      
      // Build the graph
      this.builder.buildGraph(transmissions)
      
      // Store current file
      this.currentFile = filePath
      
      logger.info(`TransmissionEditor: Loaded ${transmissions.length} transmissions from ${filePath}`)
      return transmissions
    } catch (error) {
      logger.error(`TransmissionEditor: Error loading file: ${error.message}`)
      throw error
    }
  }
  
  /**
   * Saves the current graph to a TTL file
   * @param {string} filePath - Path to save to (defaults to the loaded file)
   * @param {string} transmissionId - Optional ID of specific transmission to save
   * @returns {Promise<void>}
   */
  async saveToFile(filePath = null, transmissionId = null) {
    try {
      const targetFile = filePath || this.currentFile
      if (!targetFile) {
        throw new Error('No file specified and no current file loaded')
      }
      
      await this.exporter.saveToFile(targetFile, transmissionId)
      logger.info(`TransmissionEditor: Saved to ${targetFile}`)
    } catch (error) {
      logger.error(`TransmissionEditor: Error saving file: ${error.message}`)
      throw error
    }
  }
  
  /**
   * Creates a new transmission with a default node
   * @param {string} label - Transmission label
   * @returns {Object} - The created transmission data
   */
  createNewTransmission(label = 'New Transmission') {
    const transmissionId = `http://purl.org/stuff/transmissions/${label.replace(/\s+/g, '_')}`.toLowerCase()
    
    const transmission = {
      id: transmissionId,
      shortId: label.replace(/\s+/g, '_').toLowerCase(),
      label: label,
      comment: 'Created with Node Flow Editor',
      processors: [],
      connections: []
    }
    
    this.loadedTransmissions = [transmission]
    
    // Add default node
    const nodeType = 'ShowMessage' // Default processor type
    const node = this.graph.addNode(new FlowNode({
      title: 'SM',
      position: { x: 200, y: 200 },
      data: {}
    }))
    
    node.setMetadataProperty('transmissionId', transmissionId)
    node.setMetadataProperty('transmissionLabel', label)
    node.setMetadataProperty('processorType', `http://purl.org/stuff/transmissions/${nodeType}`)
    
    return transmission
  }
  
  /**
   * Gets the current node-flow graph
   * @returns {NodeFlowGraph} - The graph
   */
  getGraph() {
    return this.graph
  }
}

export default TransmissionEditor

================
File: src/tools/node-flow/TransmissionsExporter.js
================
// TransmissionsExporter.js
// Exports node-flow graphs back to TTL format

import RDFUtils from '../../utils/RDFUtils.js'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import rdf from 'rdf-ext'

class TransmissionsExporter {
  /**
   * Creates an exporter for converting node-flow graphs to TTL
   * @param {NodeFlowGraph} graph - The node-flow graph
   */
  constructor(graph) {
    this.graph = graph
  }
  
  /**
   * Creates an RDF dataset from the graph
   * @param {string} transmissionId - ID of the transmission to export (or null for all)
   * @returns {Dataset} - RDF dataset
   */
  createDataset(transmissionId = null) {
    const dataset = rdf.dataset()
    const nodes = this.graph.getNodes()
    
    // Group nodes by transmission
    const transmissions = this.organizeNodesByTransmission(nodes)
    
    // Filter to a specific transmission if requested
    if (transmissionId) {
      const filtered = new Map()
      if (transmissions.has(transmissionId)) {
        filtered.set(transmissionId, transmissions.get(transmissionId))
      }
      return this.buildDatasetFromTransmissions(filtered, dataset)
    }
    
    return this.buildDatasetFromTransmissions(transmissions, dataset)
  }
  
  /**
   * Builds an RDF dataset from organized transmission data
   * @param {Map} transmissions - Map of transmission ID to processors
   * @param {Dataset} dataset - RDF dataset to add to
   * @returns {Dataset} - The updated dataset
   */
  buildDatasetFromTransmissions(transmissions, dataset) {
    // Create TTL for each transmission
    for (const [transmissionId, transmissionData] of transmissions.entries()) {
      const { label, comment, processors } = transmissionData
      this.addTransmissionToDataset(dataset, transmissionId, label, comment, processors)
    }
    
    return dataset
  }
  
  /**
   * Organizes nodes into transmission groups
   * @param {Array} nodes - Array of flow nodes
   * @returns {Map} - Map of transmission ID to transmission data
   */
  organizeNodesByTransmission(nodes) {
    const transmissions = new Map()
    
    // Default transmission if none specified
    const defaultId = 'http://purl.org/stuff/transmissions/main'
    let defaultLabel = 'main'
    
    for (const node of nodes) {
      // Get transmission data from node (if available)
      const transmissionId = node.getMetadataProperty('transmissionId') || defaultId
      const transmissionLabel = node.getMetadataProperty('transmissionLabel') || defaultLabel
      
      // Create transmission entry if it doesn't exist
      if (!transmissions.has(transmissionId)) {
        transmissions.set(transmissionId, {
          label: transmissionLabel,
          comment: node.getMetadataProperty('transmissionComment') || '',
          processors: []
        })
      }
      
      // Add node to transmission
      transmissions.get(transmissionId).processors.push(node)
    }
    
    // Sort processors in each transmission based on connections
    for (const [id, transmission] of transmissions.entries()) {
      transmission.processors = this.sortProcessorsByConnections(transmission.processors)
    }
    
    return transmissions
  }
  
  /**
   * Sorts processors based on connection order
   * @param {Array} processors - Array of processor nodes
   * @returns {Array} - Sorted array of processors
   */
  sortProcessorsByConnections(processors) {
    if (processors.length <= 1) {
      return processors
    }
    
    // Create a map of node output connections
    const outputMap = new Map()
    const inputMap = new Map()
    
    for (const node of processors) {
      for (const outputPort of node.outputs()) {
        for (const connection of outputPort.connections()) {
          const toNode = connection.inNode()
          if (toNode) {
            if (!outputMap.has(node.title())) {
              outputMap.set(node.title(), [])
            }
            outputMap.get(node.title()).push(toNode.title())
            
            if (!inputMap.has(toNode.title())) {
              inputMap.set(toNode.title(), [])
            }
            inputMap.get(toNode.title()).push(node.title())
          }
        }
      }
    }
    
    // Find start nodes (no inputs)
    const startNodes = processors.filter(node => 
      !inputMap.has(node.title()) || inputMap.get(node.title()).length === 0
    )
    
    if (startNodes.length === 0) {
      // No clear starting point, return as-is
      return processors
    }
    
    // Build ordered list
    const ordered = []
    const visited = new Set()
    
    const visit = (node) => {
      if (visited.has(node.title())) return
      visited.add(node.title())
      ordered.push(node)
      
      const outputs = outputMap.get(node.title()) || []
      for (const outputTitle of outputs) {
        const outputNode = processors.find(n => n.title() === outputTitle)
        if (outputNode) {
          visit(outputNode)
        }
      }
    }
    
    // Visit from each start node
    for (const startNode of startNodes) {
      visit(startNode)
    }
    
    // Add any remaining nodes
    for (const node of processors) {
      if (!visited.has(node.title())) {
        ordered.push(node)
      }
    }
    
    return ordered
  }
  
  /**
   * Adds a transmission to the dataset
   * @param {Dataset} dataset - RDF dataset
   * @param {string} transmissionId - Transmission ID
   * @param {string} label - Transmission label
   * @param {string} comment - Transmission comment
   * @param {Array} processors - Array of processor nodes
   */
  addTransmissionToDataset(dataset, transmissionId, label, comment, processors) {
    const transmissionNode = rdf.namedNode(transmissionId)
    
    // Add transmission type
    dataset.add(rdf.quad(
      transmissionNode,
      ns.rdf.type,
      ns.trn.Transmission
    ))
    
    // Add label
    dataset.add(rdf.quad(
      transmissionNode,
      ns.rdfs.label,
      rdf.literal(label)
    ))
    
    // Add comment if present
    if (comment) {
      dataset.add(rdf.quad(
        transmissionNode,
        ns.rdfs.comment,
        rdf.literal(comment)
      ))
    }
    
    // Create pipe list
    const pipeNodes = processors.map(node => {
      // Use the node's processorId if available, otherwise generate one
      const nodeId = node.getMetadataProperty('processorId') || 
                    `http://purl.org/stuff/transmissions/${node.title()}`
      return rdf.namedNode(nodeId)
    })
    
    // Add pipe list to dataset
    this.addListToDataset(dataset, transmissionNode, ns.trn.pipe, pipeNodes)
    
    // Add processor definitions
    for (let i = 0; i < processors.length; i++) {
      const node = processors[i]
      const nodeId = node.getMetadataProperty('processorId') || 
                    `http://purl.org/stuff/transmissions/${node.title()}`
      const processorId = rdf.namedNode(nodeId)
      
      // Get processor type
      const processorType = node.getMetadataProperty('processorType') || node.title()
      const processorTypeNode = rdf.namedNode(`http://purl.org/stuff/transmissions/${processorType}`)
      
      // Add processor type
      dataset.add(rdf.quad(
        processorId,
        ns.rdf.type,
        processorTypeNode
      ))
      
      // Add settings if present
      const settings = node.getMetadataProperty('settings')
      if (settings) {
        const settingsNode = rdf.namedNode(settings)
        dataset.add(rdf.quad(
          processorId,
          ns.trn.settings,
          settingsNode
        ))
      }
      
      // Add comment if present
      const comment = node.getMetadataProperty('comment')
      if (comment) {
        dataset.add(rdf.quad(
          processorId,
          ns.rdfs.comment,
          rdf.literal(comment)
        ))
      }
    }
  }
  
  /**
   * Adds an RDF list to the dataset
   * @param {Dataset} dataset - RDF dataset
   * @param {Term} subject - Subject term
   * @param {Term} predicate - Predicate term
   * @param {Array} items - Array of list items
   */
  addListToDataset(dataset, subject, predicate, items) {
    if (items.length === 0) {
      // Empty list
      dataset.add(rdf.quad(
        subject,
        predicate,
        ns.rdf.nil
      ))
      return
    }
    
    // Create list
    let listNode = rdf.blankNode()
    dataset.add(rdf.quad(
      subject,
      predicate,
      listNode
    ))
    
    // Add items
    for (let i = 0; i < items.length; i++) {
      const item = items[i]
      
      // Add first item
      dataset.add(rdf.quad(
        listNode,
        ns.rdf.first,
        item
      ))
      
      // Add rest link
      if (i < items.length - 1) {
        const nextNode = rdf.blankNode()
        dataset.add(rdf.quad(
          listNode,
          ns.rdf.rest,
          nextNode
        ))
        listNode = nextNode
      } else {
        // End of list
        dataset.add(rdf.quad(
          listNode,
          ns.rdf.rest,
          ns.rdf.nil
        ))
      }
    }
  }
  
  /**
   * Saves the RDF dataset to a TTL file
   * @param {string} filePath - Path to save to
   * @param {string} transmissionId - Optional ID of specific transmission to save
   * @returns {Promise<void>}
   */
  async saveToFile(filePath, transmissionId = null) {
    try {
      const dataset = this.createDataset(transmissionId)
      await RDFUtils.writeDataset(dataset, filePath)
      logger.info(`TransmissionsExporter: Saved to ${filePath}`)
    } catch (error) {
      logger.error(`TransmissionsExporter: Error saving file: ${error.message}`)
      throw error
    }
  }
}

export default TransmissionsExporter

================
File: src/tools/node-flow/TransmissionsGraphBuilder.js
================
// TransmissionsGraphBuilder.js
// Builds node-flow graphs from transmission data

import { FlowNode } from '@elicdavis/node-flow'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'

class TransmissionsGraphBuilder {
  /**
   * Creates a graph builder for transmission data
   * @param {NodeFlowGraph} graph - The node-flow graph
   */
  constructor(graph) {
    this.graph = graph
  }

  /**
   * Builds a graph from transmission data
   * @param {Array} transmissions - Array of transmission data objects
   */
  buildGraph(transmissions) {
    logger.debug(`TransmissionsGraphBuilder: Building graph from ${transmissions.length} transmissions`)
    
    // Process each transmission
    for (const transmission of transmissions) {
      this.buildTransmission(transmission)
    }
  }

  /**
   * Builds a single transmission in the graph
   * @param {Object} transmission - Transmission data object
   */
  buildTransmission(transmission) {
    logger.debug(`TransmissionsGraphBuilder: Building transmission: ${transmission.label}`)
    
    // Create nodes for each processor
    const nodes = new Map()
    for (let i = 0; i < transmission.processors.length; i++) {
      const processor = transmission.processors[i]
      
      // Calculate position (simple horizontal layout)
      const position = {
        x: 200 + (i * 250),
        y: 200
      }
      
      // Create node
      const node = this.createProcessorNode(processor, position, transmission)
      
      // Add to graph
      this.graph.addNode(node)
      nodes.set(processor.id, node)
    }
    
    // Create connections
    for (const connection of transmission.connections) {
      const fromNode = nodes.get(connection.from)
      const toNode = nodes.get(connection.to)
      
      if (fromNode && toNode) {
        this.graph.connectNodes(fromNode, 0, toNode, 0)
      }
    }
  }

  /**
   * Creates a node for a processor
   * @param {Object} processor - Processor data
   * @param {Object} position - Position coordinates
   * @param {Object} transmission - Parent transmission data
   * @returns {FlowNode} - The created flow node
   */
  createProcessorNode(processor, position, transmission) {
    // Create node configuration
    const config = {
      title: processor.shortId || processor.id,
      position: position,
      info: processor.comments.join("\n") || "",
      canEditInfo: true,
      locked: false,
      data: {} // Will be set via metadata
    }
    
    // Create node
    const node = new FlowNode(config)
    
    // Set metadata properties
    node.setMetadataProperty('processorId', processor.id)
    node.setMetadataProperty('processorType', processor.type)
    node.setMetadataProperty('shortType', processor.shortType)
    
    if (processor.settings) {
      node.setMetadataProperty('settings', processor.settings)
      node.setMetadataProperty('shortSettings', processor.shortSettings)
    }
    
    // Add comments as metadata
    if (processor.comments.length > 0) {
      node.setMetadataProperty('comment', processor.comments.join('\n'))
    }
    
    // Add transmission information
    node.setMetadataProperty('transmissionId', transmission.id)
    node.setMetadataProperty('transmissionLabel', transmission.label)
    
    if (transmission.comment) {
      node.setMetadataProperty('transmissionComment', transmission.comment)
    }
    
    return node
  }

  /**
   * Clears the current graph
   */
  clearGraph() {
    // Node-flow doesn't have a built-in clear method,
    // so we track and remove each node
    const nodes = this.graph.getNodes()
    for (const node of nodes) {
      // We would need to implement node removal if node-flow provides this API
      // For now, we can warn about this limitation
      logger.warn('TransmissionsGraphBuilder: Node-flow API does not provide a way to remove nodes')
    }
  }
}

export default TransmissionsGraphBuilder

================
File: src/tools/node-flow/TransmissionsLoader.js
================
// TransmissionsLoader.js
// Loads transmission TTL files into node-flow format

import RDFUtils from '../../utils/RDFUtils.js'
import grapoi from 'grapoi'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'

class TransmissionsLoader {
  /**
   * Loads a transmission TTL file and converts it to a format suitable for node-flow
   * @param {string} filePath - Path to the TTL file
   * @returns {Promise<Array>} - Array of transmission data objects
   */
  async loadFromFile(filePath) {
    try {
      logger.debug(`TransmissionsLoader: Loading from ${filePath}`)
      const dataset = await RDFUtils.readDataset(filePath)
      return this.parseDataset(dataset, filePath)
    } catch (error) {
      logger.error(`TransmissionsLoader: Error loading file: ${error.message}`)
      throw error
    }
  }

  /**
   * Parses an RDF dataset and extracts transmission information
   * @param {Dataset} dataset - RDF dataset containing transmission data
   * @param {string} filePath - Original file path (for reference)
   * @returns {Array} - Array of transmission data objects
   */
  parseDataset(dataset, filePath) {
    const transmissions = []
    const poi = grapoi({ dataset })
    
    // Find all transmissions
    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.trn.Transmission)) {
        const transmissionID = q.subject
        const transmission = this.extractTransmission(dataset, transmissionID)
        transmission.filePath = filePath
        transmissions.push(transmission)
      }
    }
    
    logger.debug(`TransmissionsLoader: Found ${transmissions.length} transmissions`)
    return transmissions
  }

  /**
   * Extracts information about a specific transmission
   * @param {Dataset} dataset - RDF dataset
   * @param {Term} transmissionID - RDF term for the transmission ID
   * @returns {Object} - Transmission data object
   */
  extractTransmission(dataset, transmissionID) {
    const transPoi = grapoi({ dataset, term: transmissionID })
    
    // Get transmission label
    let label = ''
    let comment = ''
    
    for (const quad of transPoi.out(ns.rdfs.label).quads()) {
      label = quad.object.value
    }
    
    for (const quad of transPoi.out(ns.rdfs.comment).quads()) {
      comment = quad.object.value
    }
    
    // Get pipe nodes
    const pipeNodes = GrapoiHelpers.listToArray(dataset, transmissionID, ns.trn.pipe)
    
    // Extract processor details
    const processors = []
    for (const node of pipeNodes) {
      const np = grapoi({ dataset, term: node })
      const processorType = np.out(ns.rdf.type).term
      const settingsNode = np.out(ns.trn.settings).term
      
      // Extract comments for this processor node
      const nodeComments = []
      for (const quad of np.quads()) {
        if (quad.predicate.equals(ns.rdfs.comment)) {
          nodeComments.push(quad.object.value)
        }
      }
      
      processors.push({
        id: node.value,
        shortId: ns.shortName(node.value),
        type: processorType ? processorType.value : null,
        shortType: processorType ? ns.shortName(processorType.value) : null,
        settings: settingsNode ? settingsNode.value : null,
        shortSettings: settingsNode ? ns.shortName(settingsNode.value) : null,
        comments: nodeComments
      })
    }
    
    // Create connections between processors
    const connections = []
    for (let i = 0; i < processors.length - 1; i++) {
      connections.push({
        from: processors[i].id,
        to: processors[i + 1].id
      })
    }
    
    return {
      id: transmissionID.value,
      shortId: ns.shortName(transmissionID.value),
      label,
      comment,
      processors,
      connections
    }
  }
}

export default TransmissionsLoader

================
File: src/utils/cache.js
================
/* TODO implement

filename/URI, meta, content
*/

================
File: src/utils/footpath.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import logger from './Logger.js'

/*
resolves paths so both runtime and tests work

must be a better way of doing this, but I can't be bothered looking today
*/

let footpath = {}

footpath.resolve = function footpath(here, relative, start) {

    const loggy = false
    if (loggy) {
        logger.debug("\n*** start footpath.resolve ***")
        logger.debug("process.cwd() = " + process.cwd())
        logger.debug("here = " + here)
        logger.debug("relative = " + relative)
        logger.debug("start = " + start)
    }

    const __filename = fileURLToPath(here)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, relative)
    const filePath = path.join(rootDir, start)

    if (loggy) {
        logger.debug("__filename = " + __filename)
        logger.debug("__dirname = " + __dirname)
        logger.debug("rootDir = " + rootDir)
        logger.debug("filePath = " + filePath)
        logger.debug("*** end footpath.resolve ***\n")
    }

    return filePath
}

footpath.urlLastPart = function footpath(url = 'http://example.org/not-a-url') {
    // logger.debug('url = ' + url)
    //   try {
    const urlObj = new URL(url);
    const hash = urlObj.hash;
    const path = urlObj.pathname;
    const lastPart = hash ? hash.replace(/^#/, '') : path.split('/').pop();
    // } catch {
    //  return 'not-a-url'
    //}
    return lastPart;
}

export default footpath

================
File: src/utils/GrapoiHelpers.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'
import ns from './ns.js'
import logger from './Logger.js'

// probably already exist - ask Bergi

class GrapoiHelpers {

    // file utils
    static async readDataset(filename) {
        const stream = fromFile(filename)
        const dataset = await rdf.dataset().import(stream)
        return dataset
    }

    static async writeDataset(dataset, filename) {
        await toFile(dataset.toStream(), filename)
    }

    // follows chain in rdf:List
    static listToArray(dataset, term, property) {
        const poi = rdf.grapoi({ dataset: dataset, term: term })
        const first = poi.out(property).term

        let p = rdf.grapoi({ dataset, term: first })
        let object = p.out(ns.rdf.first).term
        //  logger.log('object = ' + object.value)
        const result = [object]

        while (true) {
            let restHead = p.out(ns.rdf.rest).term
            if (!restHead) break
            let p2 = rdf.grapoi({ dataset, term: restHead })
            let object = p2.out(ns.rdf.first).term
            //   logger.log('restHead = ' + restHead.value)
            if (restHead.equals(ns.rdf.nil)) break
            result.push(object)
            p = rdf.grapoi({ dataset, term: restHead })
        }
        return result
    }



    // unused & untested
    // [subjects] predicate ->  [objects]
    static listObjects(dataset, subjectList, predicate) {
        const objects = []
        for (const subject of subjectList) {
            logger.log("subject = " + subject.value)
            let p = rdf.grapoi({ dataset, term: subject })
            let object = p.out(predicate).term
            logger.log("object = " + object.value)
            objects.push(object)
        }
        return objects
    }
}
export default GrapoiHelpers

================
File: src/utils/JSONUtils.js
================
import logger from './Logger.js'

class JSONUtils {
    /* these functions might already be built into JS, but I couldn't find it

    for docs :

    const a = {
    b: {
        c: 5
    }
};

console.log(find(a, "b.c")); // Output: 5
console.log(find(a, "b.d")); // Output: undefined (key doesn't exist)

const a = {
    b: {
        c: [10, 20, 30],
        d: {
            e: [40, 50]
        }
    }
};
    console.log(find(a, "b.c[1]")); // Output: 20
    console.log(find(a, "b.d.e[0]")); // Output: 40
    console.log(find(a, "b.c[3]")); // Output: undefined (index out of bounds)
    console.log(find(a, "b.d.f[0]")); // Output: undefined (key doesn't exist)
    */

    static get(obj, path) {
        return JSONUtils.find(obj, path, false, false)
    }

    static set(obj, path, value) {
        JSONUtils.find(obj, path, value, false)
        return obj
    }

    // TODO this isn't right.
    static remove(obj, path) {
        if (!path) {
            return
        }
        logger.trace(`>>>>>>>>>>>>>>>>>>>>>>>>>> PATH = ${path}`)
        if (!path.includes('.')) {
            obj[path] = null
            return obj
        }
        JSONUtils.find(obj, path, false, true)
        return obj
    }

    static find(obj, path, setValue = false, remove = false) {
        // TODO I don't think set() is working for nested.objects - make tests, fix
        // meanwhile, simplest case
        if (setValue && !path.includes('.')) {
            //    logger.debug(`setting ${path} = ${setValue}`)
            obj[path] = setValue
        }

        const keys = path.split('.')
        let result = obj

        // Traverse the object to the second-to-last key
        for (let i = 0; i < keys.length - 1; i++) {
            let key = keys[i]

            // Handle array indices (e.g., "c[0]")
            const arrayMatch = key.match(/^(.*)\[(\d+)\]$/)
            if (arrayMatch) {
                key = arrayMatch[1]
                const index = parseInt(arrayMatch[2], 10)
                if (result && typeof result === 'object' && key in result && Array.isArray(result[key])) {
                    result = result[key][index]
                } else {
                    return undefined // Invalid path
                }
            } else {
                if (result && typeof result === 'object' && key in result) {
                    result = result[key]
                } else {
                    return undefined // Invalid path
                }
            }
        }

        // Handle the last key
        const lastKey = keys[keys.length - 1]
        const lastKeyArrayMatch = lastKey.match(/^(.*)\[(\d+)\]$/)

        // TODO simplify
        if (lastKeyArrayMatch) {
            const arrayKey = lastKeyArrayMatch[1]
            const index = parseInt(lastKeyArrayMatch[2], 10)

            if (result && typeof result === 'object' && arrayKey in result && Array.isArray(result[arrayKey])) {
                if (remove) {
                    logger.debug(`JSONUtils.find, removing arrayKey ${arrayKey}`)
                    // Remove the element from the array
                    result[arrayKey].splice(index, 1)
                    return true // Indicate success
                } else {

                    if (setValue) {
                        result[arrayKey][index] = setValue
                        return true
                    }
                    return result[arrayKey][index] // Return the element
                }
            } else {
                return undefined // Invalid path
            }
        } else {
            // Handle regular keys for the last key
            if (result && typeof result === 'object' && lastKey in result) {
                if (remove) {
                    // Remove the key from the object
                    logger.debug(`JSONUtils.find, removing lastKey ${lastKey}`)
                    delete result[lastKey]
                    return true // Indicate success
                } else {
                    if (setValue) {
                        result[lastKey] = setValue
                        return true
                    }
                    return result[lastKey] // Return the value
                }
            } else {
                return undefined // Invalid path
            }
        }
    }

}
export default JSONUtils

================
File: src/utils/Logger.js
================
import log from 'loglevel'
import fs from 'fs'
import chalk from 'chalk'
import ns from './ns.js'

const logger = {}

//  logger.log(`\n\nconfig dataset: ${[...config].map(q => `${q.subject.value} ${q.predicate.value} ${q.object.value}`).join('\n')}`)


// Map log levels to chalk styles
const LOG_STYLES = {
    "trace": chalk.bgGray.greenBright,
    "debug": chalk.cyanBright,
    "info": chalk.white,
    "warn": chalk.red.italic,
    "error": chalk.red.bold
}
const LOG_LEVELS = ["trace", "debug", "info", "warn", "error"]

logger.logfile = 'latest.log'
logger.currentLogLevel = "warn"

log.setLevel(logger.currentLogLevel)

logger.getLevel = () => log.getLevel()
logger.enableAll = () => log.enableAll()
logger.disableAll = () => log.disableAll()
logger.setDefaultLevel = (level) => log.setDefaultLevel(level)
logger.getLogger = (name) => {
    const namedLogger = log.getLogger(name)
    return wrapLogger(namedLogger, name)
}

logger.methodFactory = log.methodFactory

logger.noConflict = () => log.noConflict()

function wrapLogger(baseLogger, name = 'root') {
    const wrapped = {}

    wrapped.log = function (msg, level = "info") {  // Changed default to info
        const timestamp = chalk.dim(`[${logger.timestampISO()}]`)
        const levelStyle = LOG_STYLES[level] || LOG_STYLES["info"]  // Fallback to info style
        const levelTag = levelStyle(`[${level.toUpperCase()}]`)
        const nameTag = chalk.green(`[${name}]`)
        const message = levelStyle(msg)

        //   const consoleMessage = `${timestamp} ${levelTag} ${nameTag} - ${message}`;
        const consoleMessage = `${message}`
        const fileMessage = `[${logger.timestampISO()}] [${level.toUpperCase()}] [${name}] - ${msg}`

        baseLogger[level](consoleMessage)
        logger.appendLogToFile(fileMessage)
    }

    LOG_LEVELS.forEach(level => {
        wrapped[level] = (msg) => wrapped.log(msg, level)
    })

    wrapped.getLevel = () => baseLogger.getLevel()
    wrapped.setLevel = (level, persist) => baseLogger.setLevel(level, persist)
    wrapped.setDefaultLevel = (level) => baseLogger.setDefaultLevel(level)
    wrapped.enableAll = () => baseLogger.enableAll()
    wrapped.disableAll = () => baseLogger.disableAll()
    wrapped.methodFactory = baseLogger.methodFactory
    wrapped.setMethodFactory = function (factory) {
        baseLogger.methodFactory = factory
        baseLogger.rebuild()
    }

    return wrapped
}

logger.appendLogToFile = function (message) {
    if (logger.logfile) {
        fs.appendFileSync(logger.logfile, message + '\n', 'utf8')
    }
}

logger.setLogLevel = function (logLevel = "warn", persist = true) {
    logger.currentLogLevel = logLevel
    log.setLevel(logLevel, persist)
}

logger.timestampISO = function () {
    return new Date().toISOString()
}

logger.log = function (msg, level = "info") {
    const levelStyle = LOG_STYLES[level] || LOG_STYLES["info"]
    const message = levelStyle(msg)
    const consoleMessage = `${message}`
    const fileMessage = `[${logger.timestampISO()}] [${level.toUpperCase()}] [root] - ${msg}`
    try {
        //   console.log(`level = ${level}`)
        // console.log(`consoleMessage = ${consoleMessage}`)

        log[level](consoleMessage)
        logger.appendLogToFile(fileMessage)
    } catch (err) {
        console.log(`wtf? ${err.message}`)
    }

}

// abbrev URLs in text - dirty version!
logger.sh = function (string) {
    // const loglevel = logger.getLevel()
    // logger.setLogLevel('trace')
    string = logger.shorter(string)
    logger.log(string)
}

logger.shorter = function (rdfString) {
    rdfString = rdfString.toString() // to be sure, to be sure
    Object.entries(ns.prefixMap).forEach(([key, value]) => {
        rdfString = rdfString.replaceAll(key, chalk.green(value))
    })
    return chalk.magentaBright(rdfString)
}

// TODO have this return a string
logger.reveal = function (instance, verbose = true) {

    if (!instance) return

    const serialized = {}

    const loglevel = logger.getLevel()
    logger.setLogLevel('trace')

    for (const key in instance) {
        if (key === 'app') {
            logger.log(chalk.yellow(chalk.bold('message.app :')), 'debug')
            logger.reveal(instance[key], false)
            continue
        }
        if (key === 'dataset') {
            logger.log(chalk.yellow.italic('[[dataset found, skipping]]'), 'debug')
            continue
        }

        if (key.startsWith('_')) {
            logger.log(`       ${key}`, 'debug')
            continue
        }

        if (instance.hasOwnProperty(key)) {
            var value = instance[key]

            if (value) {
                if (Buffer.isBuffer(value)) {
                    value = value.toString()
                }
                if (value.length > 100) {
                    try {
                        value = value.substring(0, 100) + '...'
                    } catch (e) {
                        value = value.slice(0, 99)
                    }
                }

                serialized[key] = value

            } else {
                serialized[key] = '[no key]'
            }
        }
    }

    const props = JSON.stringify(serialized, null, 2)
    if (verbose) {
        logger.log(`Instance of ${chalk.yellow(chalk.bold(instance.constructor.name))} with properties - `)
    }
    logger.log(`${chalk.yellow(props)}`)
    logger.setLogLevel(loglevel)
}

LOG_LEVELS.forEach(level => {
    logger[level] = (msg) => logger.log(msg, level)
})

logger.poi = function exploreGrapoi(grapoi, predicates, objects, subjects) {
    console.log(chalk.bold('Properties of the Grapoi object:'))
    for (const prop in grapoi) {
        console.log(chalk.cyan(`\t${prop}: ${grapoi[prop]}`))
    }

    console.log(chalk.bold('\nPath:'))
    const path = grapoi.out(predicates, objects).in(predicates, subjects)
    for (const quad of path.quads()) {
        console.log(chalk.cyan(`\t${quad.predicate.value}: ${quad.object.value}`))
    }
}

function handleExit(options, exitCode) {
    if (options.cleanup) {
        // TODO cleanup
    }
    if (exitCode || exitCode === 0) console.log(exitCode)
    if (options.exit) process.exit()
}

process.on('exit', handleExit.bind(null, { cleanup: true }))
process.on('SIGINT', handleExit.bind(null, { exit: true }))
process.on('SIGUSR1', handleExit.bind(null, { exit: true }))
process.on('SIGUSR2', handleExit.bind(null, { exit: true }))
process.on('uncaughtException', handleExit.bind(null, { exit: true }))

// TODO testing
// logger.setLogLevel('info')
// logger.log('a log() message on info - show yellow, concise')
// logger.debug('a debug() message on info -  dont show')
// logger.setLogLevel('debug')
// logger.log('a log() message on debug - show yellow, with prefix')
// logger.debug('a debug() message on debug - show red, with prefix')

export default logger

================
File: src/utils/MockApplicationManager.js
================
import logger from './Logger.js'

class MockApplicationManager {
    constructor() {
        this.appsDir = 'src/applications'
        logger.debug('MockApplicationManager: Created new instance')
    }

    async initialize(appName, appPath, subtask, target, flags) {
        logger.debug(`MockApplicationManager.initialize(${appName}, ${appPath}, ${subtask}, ${target})`)

        if (!appName) {
            throw new Error('Application name is required')
        }

        this.app = {
            appName,
            appPath: appPath || appName,
            subtask,
            targetPath: target,
            dataset: {},
            manifestFilename: target ? `${target}/manifest.ttl` : null
        }

        return Promise.resolve()
    }

    async start(message = {}) {
        logger.debug('MockApplicationManager.start()')
        logger.debug('Message:', message)

        if (!this.app) {
            throw new Error('Application not initialized')
        }

        // Simulate successful processing
        return {
            success: true,
            whiteboard: [
                { type: 'processingComplete', timestamp: new Date().toISOString() }
            ]
        }
    }

    async listApplications() {
        logger.debug('MockApplicationManager.listApplications()')

        // Return mock list of applications
        return [
            'test_app1',
            'test_app2',
            'example_app'
        ]
    }

    resolveApplicationPath(appName) {
        if (!appName) {
            throw new Error('Application name is required')
        }

        logger.debug(`MockApplicationManager.resolveApplicationPath(${appName})`)

        if (appName.startsWith('/')) {
            return appName
        }

        if (appName.startsWith('..')) {
            return path.resolve(process.cwd(), appName)
        }

        return path.join(process.cwd(), this.appsDir, appName)
    }
}

export default MockApplicationManager

================
File: src/utils/ns.js
================
import rdf from 'rdf-ext'



const ns = {
    rdf: rdf.namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#'),
    rdfs: rdf.namespace('http://www.w3.org/2000/01/rdf-schema#'),
    dc: rdf.namespace('http://purl.org/dc/terms/'),
    schema: rdf.namespace('http://schema.org/'),
    xsd: rdf.namespace('http://www.w3.org/2001/XMLSchema#'),
    trn: rdf.namespace('http://purl.org/stuff/transmissions/'),
    //  t: rdf.namespace('http://hyperdata.it/transmissions/'),
    //  fs: rdf.namespace('http://purl.org/stuff/filesystem/'),
    //pc: rdf.namespace('http://purl.org/stuff/postcraft/')
}

// TODO wrap this into ns and/or use whatever rdf-ext, rdfjs built-in is available
ns.prefixMap = {
    "http://www.w3.org/1999/02/22-rdf-syntax-ns#": "rdf:",
    "http://www.w3.org/2000/01/rdf-schema#": "rdfs:",
    "http://purl.org/dc/terms/": "dc:",
    "http://schema.org/": "schema:",
    "http://www.w3.org/2001/XMLSchema#": "xsd",
    "http://purl.org/stuff/transmissions/": ":",
    "<": "",
    ">": ""
}

//ns.getPrefix = function (nsObj) {
//  return Object.keys(nsObj)[0];
//}

ns.shortName = ns.getShortname = function (url) { // I keep mixing up the name

    if (!url) return
    url = url.toString()
    const lastSlashIndex = url.lastIndexOf('/')
    const lastHashIndex = url.lastIndexOf('#')
    const path = url.slice(lastSlashIndex + 1)
    return path.split('#')[0].split('?')[0]
}
export default ns

================
File: src/utils/RDFUtils.js
================
import rdf from 'rdf-ext'
import { fromFile } from 'rdf-utils-fs'
import { fileURLToPath } from 'url'
import path from 'path'
import logger from './Logger.js'

class RDFUtils {

    static async readDataset(filename) {
        const stream = fromFile(filename)
        const dataset = await rdf.dataset().import(stream)
        return dataset
    }

    static async writeDataset(dataset, filename) {
        await toFile(dataset.toStream(), filename)
    }

    // TODO tidy up woth the above
    static async loadDataset(relativePath) {
        try {
            const __filename = fileURLToPath(import.meta.url)
            const __dirname = path.dirname(__filename)
            const rootDir = path.resolve(__dirname, '../..')
            const filePath = path.join(rootDir, relativePath)

            logger.debug(`Loading RDF dataset from: ${filePath}`)
            const stream = fromFile(filePath)
            const dataset = await rdf.dataset().import(stream)
            logger.debug(`Loaded ${dataset.size} quads`)

            return dataset
        } catch (error) {
            logger.error(`Error loading dataset: ${error.message}`)
            logger.error(`Stack: ${error.stack}`)
            throw error
        }
    }
}

export default RDFUtils

================
File: src/utils/StringUtils.js
================
import logger from './Logger.js'

class StringUtils {

    // const markdownFiles = files.filter(file => matchesPattern(file, '*.md'));
    static matchPatterns(str, patterns) {
        logger.trace(`StringUtils.matchPatterns, patterns = ${patterns}`)
        const matches = patterns.filter(pattern => this.matchesPattern(str, pattern))
        if (matches.length > 0) {
            return matches
        }
        return false
    }

    // Convert glob pattern to regex
    static matchesPattern(str, pattern) {

        logger.trace(`StringUtils.matchesPattern, pattern = ${pattern}`)
        const regexPattern = pattern
            .replace(/\./g, '\\.')   // Escape dots
            .replace(/\*/g, '.*')   // Convert * to .*
        const regex = new RegExp(`^${regexPattern}$`)
        return regex.test(str)
    }
}
export default StringUtils

================
File: src/utils/SysUtils.js
================
// import logger from './Logger.js'

class SysUtils {

    /* Workaround for structuredClone limitation (bits get lost) */
    static copyMessage(message) {
        const dataset = message.app.dataset
        message = structuredClone(message)
        message.app.dataset = dataset
        return message
    }

    static sleep(ms = 100) {
        return new Promise((resolve) => {
            setTimeout(resolve, ms)
        })
    }
}
export default SysUtils

================
File: src/utils/t2j.js
================
/*
* Turtle to JSON-LD converter
*/

import { Readable } from 'readable-stream'
import rdf from '@rdfjs/data-model'
import SerializerJsonld from '@rdfjs/serializer-jsonld'
import Serializer from '@rdfjs/serializer-turtle'
import N3Parser from '@rdfjs/parser-n3'
import { fromFile } from 'rdf-utils-fs'
import { toFile } from 'rdf-utils-fs'

const testTurtle = `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <https://hyperdata.it/transmissions/> . # for custom terms & instances

:simplepipe a trm:TransmissionTransmission ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a trm:StringSource .
:s2 a trm:AppendProcess .
:s3 a trm:StringSink .
`
export class Turtle2JSONLD {
    static async convert(turtle) {
        // create N3 parser instance
        let parser = new N3Parser({ factory: rdf })
        //   const stream = Turtle2JSONLD.stringToStream(turtle)
        //  let quadStream = parser.import(stream)

        const input = Readable.from(turtle)

        const output = parser.import(input)

        const serializerJsonld = new SerializerJsonld()
        const jsonStream = serializerJsonld.import(output)

        //      let outputJson = ''

        //            outputJson = outputJson + JSON.stringify(jsonld, null, 2)
        //    })
        const outputJson = await Turtle2JSONLD.streamToString(jsonStream)
        return outputJson
    }

    static stringToStream(str) {
        const stream = new Readable();
        stream.push(str); // Add the string to the stream
        stream.push(null); // Indicates that the stream has ended
        return stream;
    }

    static streamToString(stream) {
        const chunks = [];
        return new Promise((resolve, reject) => {
            stream.on('data', (chunk) => {
                chunks.push(Buffer.from(chunk))
                console.log('chunk:', chunk)
            }
            );
            stream.on('error', (err) => reject(err));
            stream.on('end', () => {
                const result = Buffer.concat(chunks).toString('utf8')
                resolve(result)
                console.log('****************** result:', result)
            });
        })
    }
}

// Convert a string to a stream

const testJson = await Turtle2JSONLD.convert(testTurtle)
console.log('àààààààààààààààààààààà')
console.log(testJson)

================
File: src/utils/test_runner.js
================
import fs from 'fs';
import path from 'path';

const testFiles = fs.readdirSync(__dirname).filter(file => file.startsWith('test_'));

testFiles.forEach(testFile => {
    console.log(`Running ${testFile}`);
    require(path.join(__dirname, testFile));
});

================
File: src/utils/text-utils.js
================
/**
 * Utility functions for escaping text content in Turtle RDF
 * @module TextUtils
 */

/**
 * Escapes a string literal for use in Turtle RDF
 * Handles quotes, backslashes, line breaks per RDF 1.1 Turtle spec
 * @param {string} str The input string to escape
 * @returns {string} The escaped string
 */
/**
 * Regular expression for validating BCP47 language tags
 * Supports basic language tags like 'en', 'en-US', 'zh-Hans' etc.
 */
const LANGUAGE_TAG_REGEX = /^[a-zA-Z]{1,8}(-[a-zA-Z0-9]{1,8})*$/;

/**
 * Validates a language tag according to BCP47 rules
 * @param {string} langTag Language tag to validate
 * @returns {boolean} True if valid
 */
export function isValidLanguageTag(langTag) {
    return LANGUAGE_TAG_REGEX.test(langTag);
}

/**
 * Creates a Turtle string literal with optional language tag or datatype
 * @param {string} str The string value
 * @param {Object} options Configuration options
 * @param {string} [options.language] Language tag (BCP47)
 * @param {string} [options.datatype] Datatype IRI
 * @returns {string} Formatted Turtle literal
 */
export function escapeStringLiteral(str, options = {}) {
    if (!str) return '';
    
    const escaped = str.includes('\n') 
        ? `"""${str.replace(/"""/g, '\\"\\"\\"')
                 .replace(/\\/g, '\\\\')
                 .replace(/\r/g, '\\r')
                 .replace(/\t/g, '\\t')}"""`
        : `"${str.replace(/"/g, '\\"')
               .replace(/\\/g, '\\\\')
               .replace(/\r/g, '\\r')
               .replace(/\n/g, '\\n')
               .replace(/\t/g, '\\t')}"`;
               
    if (options.language && isValidLanguageTag(options.language)) {
        return `${escaped}@${options.language.toLowerCase()}`;
    }
    
    if (options.datatype) {
        return `${escaped}^^${options.datatype}`;
    }
    
    return escaped;
}

/**
 * Escapes an IRI for use in Turtle RDF
 * Handles characters not allowed in IRIs per RFC 3987
 * @param {string} iri The IRI to escape
 * @returns {string} The escaped IRI
 */
export function escapeIRI(iri) {
    if (!iri) return '';
    
    return iri.replace(/[\x00-\x20<>"{}|^`\\]/g, (char) => {
        return `\\u${char.charCodeAt(0).toString(16).padStart(4, '0')}`;
    });
}

/**
 * Escapes a local name for use in Turtle RDF prefixed names
 * Handles special characters in local names per RDF 1.1 Turtle spec
 * @param {string} localName The local name to escape
 * @returns {string} The escaped local name
 */
export function escapeLocalName(localName) {
    if (!localName) return '';
    
    return localName.replace(/[~.!$&'()*+,;=/?#@%_-]/g, '\\$&');
}

/**
 * Validates if a string is a valid xsd:dateTime
 * @param {string} dateStr The date string to validate
 * @returns {boolean} True if valid xsd:dateTime format
 */
export function isValidDateTime(dateStr) {
    const regex = /^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$/;
    return regex.test(dateStr);
}

/**
 * Clean and normalize a string for use as a URL slug
 * @param {string} str The input string
 * @returns {string} URL-safe slug
 */
export function createSlug(str) {
    return str.toLowerCase()
        .replace(/[^a-z0-9]+/g, '-')
        .replace(/^-+|-+$/g, '');
}

/**
 * Validates a URL string
 * @param {string} url The URL to validate
 * @returns {boolean} True if valid URL
 */
export function isValidURL(url) {
    try {
        new URL(url);
        return true;
    } catch {
        return false;
    }
}

================
File: staging/_transmissions.config.json
================
{
    "paths": {
        "applications": {
            "include":
        }
        [
            "applications"
        ]
    }
}

================
File: staging/schema-documentation.md
================
# Transmissions Templates Schema Documentation

## JSON Schema
The JSON schema provides a strict validation structure for application definitions:

### Core Components
1. `appName`: String identifier used in paths & configurations
2. `purpose`: Object describing application goals
   - `primaryGoal`: Single sentence description
   - `inputs`/`outputs`: Array of expected formats
   - `behavior`: Expected processing behavior 

3. `processingRequirements`: Object defining data flow
   - `input`: Message & file specifications
   - `steps`: Array of processing stages
   - `output`: Expected results format

4. `components`: Required implementation pieces
   - `newProcessors`: New code needed
   - `configFiles`: Configuration files
   - `existingProcessors`: Reused components

5. `testing`: Test specifications
   - `unitTests`: Component-level tests
   - `integrationTests`: Pipeline tests

## RDF Schema
The RDF schema models the application definition as linked data:

### Core Classes
1. `trm:ApplicationDefinition`
   - Links requirements, components, testing
   - Provides metadata about application

2. `trm:Requirements` 
   - Models input/output specifications
   - Defines processing steps
   - Links to configurations

3. `trm:ComponentList`
   - Catalogs needed processors
   - Specifies configurations
   - References existing code

4. `trm:TestingRequirements`
   - Defines test scenarios
   - Specifies test data
   - Documents expectations

### Additional Properties
```turtle
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix prj: <http://purl.org/stuff/project/> .

trm:ApplicationDefinition
    trm:hasVersion "1.0" ;
    trm:requiresTransmissionsVersion "2.0" ;
    trm:category "data-processing" ;
    prj:status "development" ;
    prj:priority "medium" ;
    prj:estimatedEffort "2d" ;
    prj:dependencies [
        a prj:DependencyList ;
        prj:requires "markmap-lib", "rdf-ext"
    ] ;
    prj:documentation [
        a prj:DocumentationRequirements ;
        prj:requiresAPIDoc true ;
        prj:requiresUserGuide true
    ] ;
    prj:deployment [
        a prj:DeploymentRequirements ;
        prj:environment "node16+" ;
        prj:memoryRequirements "512MB"
    ] .
```

================
File: staging/template-cli.js
================
#!/usr/bin/env node
import TemplateGenerator from './TemplateGenerator.js';

const generator = new TemplateGenerator();
generator.run();

================
File: staging/template-generator.js
================
import fs from 'fs/promises';
import path from 'path';
import { Command } from 'commander';
import inquirer from 'inquirer';
import { rdf, namespace } from '@rdfjs/data-model';
import { Writer } from 'n3';

const ns = {
    trm: namespace('http://purl.org/stuff/transmission/'),
    prj: namespace('http://purl.org/stuff/project/'),
    app: namespace('http://example.org/app/')
};

class TemplateGenerator {
    constructor() {
        this.program = new Command();
        this.setupCommands();
    }

    setupCommands() {
        this.program
            .name('trans-template')
            .description('Generate Transmissions application templates')
            .version('1.0.0');

        this.program
            .command('create')
            .description('Create new application templates')
            .argument('<name>', 'Application name')
            .option('-f, --format <format>', 'Output format (json|turtle|markdown)', 'json')
            .action(async (name, options) => {
                const answers = await this.promptForDetails(name);
                await this.generateTemplates(name, answers, options.format);
            });
    }

    async promptForDetails(name) {
        return inquirer.prompt([
            {
                type: 'input',
                name: 'primaryGoal',
                message: 'What is the primary goal of this application?'
            },
            {
                type: 'input',
                name: 'inputs',
                message: 'Input formats (comma-separated):',
                filter: input => input.split(',').map(i => i.trim())
            },
            {
                type: 'input',
                name: 'outputs',
                message: 'Output formats (comma-separated):',
                filter: input => input.split(',').map(i => i.trim())
            },
            {
                type: 'input',
                name: 'processors',
                message: 'Required processors (comma-separated):',
                filter: input => input.split(',').map(i => i.trim())
            },
            {
                type: 'confirm',
                name: 'needsTests',
                message: 'Generate test templates?',
                default: true
            }
        ]);
    }

    async generateTemplates(name, answers, format) {
        const outputDir = path.join(process.cwd(), name);
        await fs.mkdir(outputDir, { recursive: true });

        const templates = {
            json: () => this.generateJSON(name, answers),
            turtle: () => this.generateTurtle(name, answers),
            markdown: () => this.generateMarkdown(name, answers)
        };

        const content = templates[format]();
        const fileExt = format === 'turtle' ? 'ttl' : format;

        await fs.writeFile(
            path.join(outputDir, `app-definition.${fileExt}`),
            content
        );

        // Generate basic file structure
        await this.generateFileStructure(outputDir, answers);

        console.log(`Generated ${format} template in ${outputDir}`);
    }

    generateJSON(name, answers) {
        return JSON.stringify({
            appName: name,
            purpose: {
                primaryGoal: answers.primaryGoal,
                inputs: answers.inputs,
                outputs: answers.outputs
            },
            processingRequirements: {
                steps: answers.processors.map(p => ({
                    name: p,
                    processor: p,
                    config: {}
                }))
            },
            testing: {
                unitTests: answers.processors.map(p => ({
                    component: p,
                    cases: ['basic', 'error']
                }))
            }
        }, null, 2);
    }

    generateTurtle(name, answers) {
        const writer = new Writer();
        const app = ns.app(name);

        writer.addQuad(
            app,
            ns.trn('title'),
            rdf.literal(name)
        );

        writer.addQuad(
            app,
            ns.trn('primaryGoal'),
            rdf.literal(answers.primaryGoal)
        );

        answers.processors.forEach(p => {
            const proc = ns.app(p);
            writer.addQuad(
                app,
                ns.trn('hasProcessor'),
                proc
            );
        });

        return writer.toString();
    }

    generateMarkdown(name, answers) {
        return `# ${name}

## Purpose
${answers.primaryGoal}

## Inputs
${answers.inputs.map(i => `- ${i}`).join('\n')}

## Outputs
${answers.outputs.map(o => `- ${o}`).join('\n')}

## Processors
${answers.processors.map(p => `- ${p}`).join('\n')}

## Testing
${answers.needsTests ? '- Unit tests required\n- Integration tests required' : 'No tests specified'}
`;
    }

    async generateFileStructure(outputDir, answers) {
        const dirs = [
            'processors',
            'tests',
            'config'
        ];

        for (const dir of dirs) {
            await fs.mkdir(path.join(outputDir, dir), { recursive: true });
        }

        // Create basic files
        const files = {
            'transmissions.ttl': '',
            'config.ttl': '',
            'about.md': `# ${path.basename(outputDir)}\n\n${answers.primaryGoal}`
        };

        for (const [file, content] of Object.entries(files)) {
            await fs.writeFile(
                path.join(outputDir, file),
                content
            );
        }
    }

    run() {
        this.program.parse();
    }
}

export default TemplateGenerator;

================
File: staging/template-tool-docs.md
================
# Transmissions Template Generator

## Overview
Command-line tool to generate scaffold for new Transmissions applications.

## Installation
```bash
npm install -g trans-template
```

## Usage
```bash
# Generate new application template
trans-template create my-app

# Specify output format
trans-template create my-app --format turtle

# Help
trans-template --help
```

## Generated Structure
```
my-app/
├── processors/      # New processors
├── tests/          # Test files
├── config/         # Configuration files
├── transmissions.ttl  # Pipeline definition
├── config.ttl         # Service configuration
└── about.md          # Application documentation
```

## Template Formats

### JSON
- Full application definition
- Validates against JSON schema
- Used for tooling/automation

### Turtle
- RDF representation
- Linked data model
- Integration with semantic tools

### Markdown
- Human-readable format
- Documentation focus
- GitHub-friendly

## Environment Variables
- `TRANS_TEMPLATE_PATH`: Base path for templates
- `TRANS_CONFIG_PATH`: Path to configuration

## Error Handling
- Validates input parameters
- Creates missing directories
- Reports detailed errors

## Extension
Custom templates can be added in:
```bash
~/.config/trans-template/templates/
```

================
File: staging/transmissions-prompt-template.md
================
# Transmissions Application Definition Template

## Application Name

[Short name for the application, will be used in file paths]

## Purpose

- Primary goal in one sentence
- Key inputs and outputs
- Expected behavior

## Technical Context

- Base paths:
  - Transmissions core: ~/hyperdata/transmissions
  - Applications: ~/hyperdata/trans-apps

## Processing Requirements

1. Input Format

   - Message structure
   - File formats/paths
   - Required fields

2. Processing Steps

   - List processing stages in sequence
   - Note any existing processors to use
   - Identify new processors needed

3. Output Format
   - Expected message structure
   - File formats/paths
   - Required fields

## Required Components

- New processors to create [list]
- Configuration files needed [list]
- Existing processors to reuse [list]

## Example Usage

```bash
./trans [app-name] [example command line arguments]
```

## Success Criteria

- List specific conditions that indicate successful implementation
- Example outputs or results

## Technical Constraints

- Note any performance requirements
- Special error handling needs
- Specific processor features needed

## Reference Material

- Links to example code
- Related processors
- Documentation needed

================
File: staging/transmissions-template-schema.json
================
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Transmissions Application Definition",
  "type": "object",
  "required": ["appName", "purpose", "technicalContext", "processingRequirements", "components", "testing"],
  "properties": {
    "appName": {
      "type": "string",
      "description": "Short name for the application"
    },
    "purpose": {
      "type": "object",
      "required": ["primaryGoal", "inputs", "outputs", "behavior"],
      "properties": {
        "primaryGoal": { "type": "string" },
        "inputs": { "type": "array", "items": { "type": "string" }},
        "outputs": { "type": "array", "items": { "type": "string" }},
        "behavior": { "type": "string" }
      }
    },
    "technicalContext": {
      "type": "object",
      "required": ["transmissionsPath", "applicationsPath"],
      "properties": {
        "transmissionsPath": { "type": "string" },
        "applicationsPath": { "type": "string" }
      }
    },
    "processingRequirements": {
      "type": "object",
      "required": ["input", "steps", "output"],
      "properties": {
        "input": {
          "type": "object",
          "properties": {
            "messageStructure": { "type": "object" },
            "fileFormats": { "type": "array", "items": { "type": "string" }},
            "requiredFields": { "type": "array", "items": { "type": "string" }}
          }
        },
        "steps": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": { "type": "string" },
              "processor": { "type": "string" },
              "config": { "type": "object" }
            }
          }
        },
        "output": {
          "type": "object",
          "properties": {
            "messageStructure": { "type": "object" },
            "fileFormats": { "type": "array", "items": { "type": "string" }},
            "requiredFields": { "type": "array", "items": { "type": "string" }}
          }
        }
      }
    },
    "components": {
      "type": "object",
      "required": ["newProcessors", "configFiles", "existingProcessors"],
      "properties": {
        "newProcessors": { 
          "type": "array", 
          "items": { "type": "string" }
        },
        "configFiles": { 
          "type": "array", 
          "items": { "type": "string" }
        },
        "existingProcessors": { 
          "type": "array", 
          "items": { "type": "string" }
        }
      }
    },
    "testing": {
      "type": "object",
      "required": ["unitTests", "integrationTests"],
      "properties": {
        "unitTests": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "component": { "type": "string" },
              "cases": { 
                "type": "array",
                "items": { "type": "string" }
              }
            }
          }
        },
        "integrationTests": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "scenario": { "type": "string" },
              "steps": { 
                "type": "array",
                "items": { "type": "string" }
              }
            }
          }
        }
      }
    }
  }
}

================
File: staging/transmissions-template-turtle.txt
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix prov: <http://www.w3.org/ns/prov#> .
@prefix prj: <http://purl.org/stuff/project/> .
@prefix app: <http://example.org/app/> .

app:Application a trm:ApplicationDefinition ;
    dcterms:title "Application Name" ;
    dcterms:description "Primary goal description" ;
    trm:basePath "/path/to/application" ;
    trm:hasRequirement app:ProcessingRequirements ;
    trm:hasComponent app:Components ;
    trm:hasTesting app:Testing .

app:ProcessingRequirements a trm:Requirements ;
    trm:input [
        a trm:InputRequirement ;
        trm:messageStructure "JSON structure" ;
        trm:fileFormat "format specification" ;
        trm:requiredField "field1", "field2"
    ] ;
    trm:processing [
        a trm:ProcessingStep ;
        trm:order 1 ;
        trm:processor "ProcessorName" ;
        trm:configuration app:ProcessorConfig
    ] ;
    trm:output [
        a trm:OutputRequirement ;
        trm:messageStructure "JSON structure" ;
        trm:fileFormat "format specification" ;
        trm:requiredField "field1", "field2"
    ] .

app:Components a trm:ComponentList ;
    trm:newProcessor [
        a trm:Processor ;
        dcterms:title "Processor Name" ;
        trm:class "ProcessorClass" ;
        trm:sourcePath "/path/to/source"
    ] ;
    trm:configFile [
        a trm:Configuration ;
        dcterms:title "Config Name" ;
        trm:format "Turtle" ;
        trm:path "/path/to/config"
    ] ;
    trm:existingProcessor [
        a trm:Processor ;
        dcterms:title "Existing Processor" ;
        trm:class "ProcessorClass"
    ] .

app:Testing a trm:TestingRequirements ;
    trm:unitTest [
        a trm:UnitTest ;
        dcterms:title "Test Name" ;
        trm:component "ComponentName" ;
        trm:testCase "test description"
    ] ;
    trm:integrationTest [
        a trm:IntegrationTest ;
        dcterms:title "Test Scenario" ;
        trm:step "step description" ;
        trm:expectedResult "expected outcome"
    ] ;
    trm:testData [
        a trm:TestData ;
        trm:input "/path/to/test/input" ;
        trm:expected "/path/to/test/output"
    ] .

================
File: staging/transmissions-testing-template.md
================
# Transmissions Testing Requirements Template

## Unit Tests
1. Individual Processors
   - Input validation tests
   - Core processing tests 
   - Error handling tests
   - Edge case tests
   - Sample data needed

2. Configuration Tests
   - Config file loading
   - Config validation
   - Default values
   - Error conditions

## Integration Tests
1. Pipeline Tests
   - Full transmission flow
   - Inter-processor communication
   - Message transformations
   - File I/O operations

2. System Tests
   - CLI interface testing
   - File system interactions
   - Error recovery
   - Resource cleanup

## Test Data Requirements
1. Input Test Files
   - Sample files needed
   - File formats
   - Edge cases
   - Invalid data samples

2. Expected Outputs
   - Reference output files
   - Validation criteria
   - Format specifications
   - Error conditions

## Test Environment
1. Setup Requirements
   - Directory structure
   - Required permissions
   - External dependencies
   - Configuration files

2. Cleanup Procedures
   - File cleanup
   - Resource cleanup
   - State reset
   - Verification steps

## Documentation
1. Test Coverage
   - Required coverage metrics
   - Critical paths
   - Exception paths
   - Performance criteria

2. Test Reports
   - Required metrics
   - Format specifications
   - Success criteria
   - Failure analysis

================
File: tests/applications/config-setting-manifest/manifest.ttl
================
# used by `src/applications/test/config-setting-manifest`

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://purl.org/stuff/transmissions/> .
@base <http://purl.org/stuff/path/> .

:theSettingsNode a :ConfigSet ;
    :theSettingProperty "the setting value from MANIFEST - TEST_PASSED" .

================
File: tests/applications/about.md
================
# Applcations to run as integration tests

'TEST_PASSED'

To run :

```sh
cd ~/hyperdata/transmissions # my project path
npm test -- tests/applications/ApplicationRunner.spec.js
```

================
File: tests/applications/ApplicationRunner.spec.js
================
// tests/integration/fork.spec.js
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs'
import chalk from 'chalk'

describe('', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const logFile = path.join(__dirname, '../../latest.log')
    const commandsFile = path.join(__dirname, 'applications.json') // JSON file for commands

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 5000

    const testRegex = /TEST_PASSED/g

    // Read commands from the JSON file
    const commands = JSON.parse(fs.readFileSync(commandsFile, 'utf8'))

    commands.forEach((test, index) => {
        const { command, label, description, requiredMatchCount } = test

        it(`run ${label}`, (done) => {
            console.log(`${chalk.bold(description)}, command :\n   ${chalk.yellow(command)}`) // Print description to console
            exec(command, async (error, stdout, stderr) => {
                if (error) {
                    console.error(chalk.red('Exec error:'), error)
                    done(error)
                    return
                }
                try {
                    // Parse console log file
                    const logs = stdout.toString()
                    // console.log(`LOGS = \n${logs}`);
                    const matches = logs.match(testRegex)
                    const matchCount = matches ? matches.length : 0
                    expect(matchCount).to.equal(requiredMatchCount)
                    done()
                } catch (err) {
                    console.error(chalk.red('Test error:'), err)
                    //  console.log('Logs:\n', err)
                    done(err)
                }
            })
        })
    })
})

================
File: tests/applications/applications.json
================
[
    {
        "command": "./trans accumulate -m '{\"foreach\": [\"first TEST_PASSED\", \"second TEST_PASSED\", \"third TEST_PASSED\"]}'",
        "label": "stringops",
        "description": "test accumulate application",
        "requiredMatchCount": 6
    },
    {
        "command": "./trans config-setting  --message '{\"theSettingProperty\": \"the setting value from message TEST_PASSED\"}'",
        "label": "config-setting_message",
        "description": "test config-setting application, setting in message",
        "requiredMatchCount": 1
    },
    {
        "command": "./trans config-setting",
        "label": "config-setting_config",
        "description": "test config-setting application, setting in config.ttl",
        "requiredMatchCount": 3
    },
    {
        "command": "./trans config-settings",
        "label": "config-settings",
        "description": "test config-settings application",
        "requiredMatchCount": 8
    },
    {
        "command": "./trans config-setting-manifest tests/applications/config-setting-manifest",
        "label": "config-setting-manifest",
        "description": "test config-setting-manifest application",
        "requiredMatchCount": 1
    },
    {
        "command": "./trans dirwalker",
        "label": "dirwalker",
        "description": "test dirwalker application",
        "requiredMatchCount": 2
    },
    {
        "command": "./trans system/echo -m '{\"message\":\"Hello, World, TEST_PASSED!\"}'",
        "label": "echo",
        "description": "test echo application",
        "requiredMatchCount": 1
    },
    {
        "command": "./trans filereader",
        "label": "filereader",
        "description": "test filereader application",
        "requiredMatchCount": 1
    },
    {
        "command": "./trans foreach -m '{\"foreach\": [\"first TEST_PASSED\", \"second TEST_PASSED\", \"third TEST_PASSED\"]}'",
        "label": "foreach",
        "description": "test foreach application",
        "requiredMatchCount": 6
    },
    {
        "command": "./trans restructure",
        "label": "restructurer",
        "description": "test restructure application",
        "requiredMatchCount": 1
    },
    {
        "command": "./trans restructure src/applications/test/restructure/manifest",
        "label": "restructurer+manifest",
        "description": "test restructure application with manifest",
        "requiredMatchCount": 2
    },
    {
        "command": "./trans -v stringops -m '{\"fields\": {\"fieldB\" : \"TEST\",\"fieldC\":\"_PASSED\"}}'",
        "label": "stringops",
        "description": "test stringops application",
        "requiredMatchCount": 4
    },
    {
        "command": "./trans file-remove-copy",
        "label": "file-remove-copy",
        "description": "file-remove-copy VALUES NOT TESTED",
        "requiredMatchCount": 0
    }
]

================
File: tests/examples/test-data-usage.js
================
import TestDataGenerator from '../helpers/TestDataGenerator.js';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

async function generateTestData() {
    // Create generator instance
    const generator = new TestDataGenerator(
        path.join(__dirname, '../../src/applications/test_markmap/data')
    );

    try {
        // Initialize directories
        await generator.init();

        // Generate basic test files
        const files = await generator.generateMarkdownFiles(3);
        console.log('Generated basic test files:', files);

        // Generate nested structure
        await generator.generateNestedStructure();
        console.log('Generated nested structure');

        // Generate edge cases
        await generator.generateEdgeCases();
        console.log('Generated edge cases');

        // Generate required outputs
        await generator.generateRequiredOutputs(
            path.join(generator.baseDir, 'input')
        );
        console.log('Generated required outputs');

    } catch (error) {
        console.error('Error generating test data:', error);
    }
}

// Run generator
generateTestData().catch(console.error);

================
File: tests/tests-support/helpers/file-test-helper.js
================
import fs from 'fs/promises';
import path from 'path';
import logger from '../../src/utils/Logger.js';

class FileTestHelper {
    constructor(baseDir) {
        this.baseDir = baseDir;
    }

    async setup() {
        await fs.mkdir(this.baseDir, { recursive: true });
        await fs.mkdir(path.join(this.baseDir, 'input'), { recursive: true });
        await fs.mkdir(path.join(this.baseDir, 'output'), { recursive: true });
    }

    async cleanup() {
        try {
            await fs.rm(this.baseDir, { recursive: true, force: true });
        } catch (error) {
            logger.error('Cleanup error:', error);
        }
    }

    async createTestFile(subPath, content) {
        const filePath = path.join(this.baseDir, subPath);
        await fs.mkdir(path.dirname(filePath), { recursive: true });
        await fs.writeFile(filePath, content);
        return filePath;
    }

    async compareFiles(actualPath, expectedPath) {
        try {
            const actual = await fs.readFile(actualPath, 'utf8');
            const expected = await fs.readFile(expectedPath, 'utf8');
            return {
                match: actual.trim() === expected.trim(),
                actual: actual.trim(),
                expected: expected.trim()
            };
        } catch (error) {
            logger.error('File comparison error:', error);
            return {
                match: false,
                error: error.message
            };
        }
    }

    async clearOutputFiles(pattern = 'output-*') {
        const outputDir = path.join(this.baseDir, 'output');
        const files = await fs.readdir(outputDir);
        for (const file of files) {
            if (file.match(pattern)) {
                await fs.unlink(path.join(outputDir, file));
            }
        }
    }

    async fileExists(filePath) {
        try {
            await fs.access(path.join(this.baseDir, filePath));
            return true;
        } catch {
            return false;
        }
    }
}

export default FileTestHelper;

================
File: tests/tests-support/helpers/reporter.js
================
import { SpecReporter } from 'jasmine-spec-reporter';

class CustomReporter {
    constructor() {
        this.specReporter = new SpecReporter({
            spec: {
                displayPending: true // Display pending (not fully implemented) specs
            }
        });
    }

    jasmineStarted() {
        this.specReporter.jasmineStarted.apply(this.specReporter, arguments);
    }

    suiteStarted() {
        this.specReporter.suiteStarted.apply(this.specReporter, arguments);
    }

    specStarted() {
        this.specReporter.specStarted.apply(this.specReporter, arguments);
    }

    specDone() {
        this.specReporter.specDone.apply(this.specReporter, arguments);
    }

    suiteDone() {
        this.specReporter.suiteDone.apply(this.specReporter, arguments);
    }

    jasmineDone() {
        this.specReporter.jasmineDone.apply(this.specReporter, arguments);
    }
}

export default CustomReporter;

/*
import { SpecReporter } from 'jasmine-spec-reporter';

jasmine.getEnv().clearReporters(); // Clear default console reporter
jasmine.getEnv().addReporter(new SpecReporter({
    spec: {
        displayPending: true // Display pending (not fully implemented) specs
    }
}));
*/

================
File: tests/tests-support/helpers/test-data-generator.js
================
import path from 'path';
import fs from 'fs/promises';
import logger from '../../src/utils/Logger.js';

class TestDataGenerator {
    constructor(baseDir) {
        this.baseDir = baseDir;
    }

    async init() {
        await fs.mkdir(this.baseDir, { recursive: true });
        await fs.mkdir(path.join(this.baseDir, 'input'), { recursive: true });
        await fs.mkdir(path.join(this.baseDir, 'output'), { recursive: true });
    }

    async generateMarkdownFiles(count = 5) {
        const files = [];
        for (let i = 1; i <= count; i++) {
            const content = this.generateMarkdownContent(i);
            const filename = `test-${String(i).padStart(2, '0')}.md`;
            const filepath = path.join(this.baseDir, 'input', filename);
            
            await fs.writeFile(filepath, content);
            files.push(filepath);
        }
        return files;
    }

    generateMarkdownContent(depth = 3) {
        const content = [];
        content.push(`# Test Document ${depth}`);
        
        for (let i = 1; i <= depth; i++) {
            content.push(`\n${'#'.repeat(i + 1)} Section ${i}`);
            content.push(this.generateListItems(i));
            
            if (i < depth) {
                content.push(this.generateParagraph(i));
            }
        }
        
        return content.join('\n');
    }

    generateListItems(count) {
        const items = [];
        for (let i = 1; i <= count; i++) {
            items.push(`* List item ${i}`);
            if (Math.random() > 0.5) {
                for (let j = 1; j <= 2; j++) {
                    items.push(`  * Nested item ${i}.${j}`);
                }
            }
        }
        return items.join('\n');
    }

    generateParagraph(seed) {
        const sentences = [
            "Lorem ipsum dolor sit amet.",
            "Consectetur adipiscing elit.",
            "Sed do eiusmod tempor incididunt.",
            "Ut labore et dolore magna aliqua.",
            "Ut enim ad minim veniam."
        ];
        
        return sentences.slice(0, seed + 1).join(' ');
    }

    async generateNestedStructure(depth = 3) {
        for (let i = 1; i <= depth; i++) {
            const dirPath = path.join(this.baseDir, 'input', 'nested', 
                ...Array(i).fill(0).map((_, idx) => `level-${idx + 1}`));
            
            await fs.mkdir(dirPath, { recursive: true });
            
            const content = this.generateMarkdownContent(i);
            const filepath = path.join(dirPath, `nested-${i}.md`);
            await fs.writeFile(filepath, content);
        }
    }

    async generateEdgeCases() {
        const cases = {
            'empty.md': '',
            'only-title.md': '# Solo Title',
            'special-chars.md': '# Test & < > " \' Document',
            'very-deep.md': this.generateDeepStructure(10),
            'wide.md': this.generateWideStructure(10)
        };

        const edgeCaseDir = path.join(this.baseDir, 'input', 'edge-cases');
        await fs.mkdir(edgeCaseDir, { recursive: true });

        for (const [filename, content] of Object.entries(cases)) {
            await fs.writeFile(path.join(edgeCaseDir, filename), content);
        }
    }

    generateDeepStructure(depth) {
        return Array(depth)
            .fill(0)
            .map((_, i) => `${'#'.repeat(i + 1)} Level ${i + 1}`)
            .join('\n');
    }

    generateWideStructure(width) {
        const content = ['# Wide Document'];
        for (let i = 1; i <= width; i++) {
            content.push(`## Section ${i}`);
            for (let j = 1; j <= width; j++) {
                content.push(`* Item ${i}.${j}`);
            }
        }
        return content.join('\n');
    }

    async generateRequiredOutputs(sourceDir) {
        const files = await fs.readdir(sourceDir);
        for (const file of files) {
            if (file.endsWith('.md')) {
                const content = await fs.readFile(path.join(sourceDir, file));
                
                // Generate HTML required output
                await fs.writeFile(
                    path.join(this.baseDir, 'output', `required-${file.replace('.md', '.mm.html')}`),
                    this.wrapHTML(content.toString())
                );
                
                // Generate SVG required output
                await fs.writeFile(
                    path.join(this.baseDir, 'output', `required-${file.replace('.md', '.mm.svg')}`),
                    this.generateSVG(content.toString())
                );
            }
        }
    }

    wrapHTML(content) {
        return `<!DOCTYPE html>
<html>
<head>
    <title>Markmap</title>
</head>
<body>
    <div class="markmap">
        ${content}
    </div>
</body>
</html>`;
    }

    generateSVG(content) {
        // Simple SVG wrapper for test purposes
        return `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 600">
    <text x="10" y="20">Test SVG for: ${content.split('\n')[0]}</text>
</svg>`;
    }

    async cleanup() {
        try {
            await fs.rm(this.baseDir, { recursive: true, force: true });
        } catch (error) {
            logger.error('Cleanup error:', error);
        }
    }
}

export default TestDataGenerator;

================
File: tests/tests-support/jasmine-browser.json
================
{
  "srcDir": "src",
  "srcFiles": [
    "**/*.js"
  ],
  "specDir": "spec",
  "specFiles": [
    "**/*[sS]pec.js"
  ],
  "helpers": [
    "helpers/**/*.js"
  ],
  "env": {
    "stopSpecOnExpectationFailure": false,
    "stopOnSpecFailure": false,
    "random": true
  },
  "browser": {
    "name": "firefox"
  }
}

================
File: tests/about.md
================
```sh
cd ~/hyperdata/transmissions # my local dir

npm test -- tests/unit/ProcessorSettings.spec.js

npm test -- tests/integration/string-filter.spec.js


npm test -- tests/unit/Application.spec.js

npm test -- tests/integration/application-manager.spec.js


```

================
File: tests_pending/integration/app-context.spec.js
================
import { expect } from 'chai'
import path from 'path'
import { fileURLToPath } from 'url'
import fs from 'fs/promises'
import rdf from 'rdf-ext'
import ApplicationManager from '../../src/engine/ApplicationManager.js'
import ns from '../../src/utils/ns.js'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

describe('Application Context Integration', () => {
    let manager
    const testAppRoot = path.join(__dirname, '../../src/applications/test_app_context')
    const testTarget = path.join(testAppRoot, 'target')

    beforeEach(async () => {
        manager = new ApplicationManager()
        await setupTestFiles()
    })

    afterEach(async () => {
        await cleanupTestFiles()
    })

    async function setupTestFiles() {
        await fs.mkdir(testAppRoot, { recursive: true })
        await fs.mkdir(path.join(testAppRoot, 'data'), { recursive: true })
        await fs.mkdir(testTarget, { recursive: true })

        const transmissionsTtl = `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .
:test a :Transmission ;
    :pipe (:p10 :p20) .
:p10 a :ShowMessage .
:p20 a :ShowConfig .`

        const configTtl = `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .
:testConfig a :ConfigSet ;
    :testKey "testValue" .`

        await fs.writeFile(path.join(testAppRoot, 'transmissions.ttl'), transmissionsTtl)
        await fs.writeFile(path.join(testAppRoot, 'config.ttl'), configTtl)
    }

    async function cleanupTestFiles() {
        try {
            await fs.rm(testAppRoot, { recursive: true, force: true })
        } catch (err) {
            if (err.code !== 'ENOENT') throw err
        }
    }

    it('should create application graph during initialization', async () => {
        await manager.initialize('test_app_context', testAppRoot, null, testTarget)

        const appNode = rdf.namedNode('http://purl.org/stuff/transmissions/test_app_context')
        const hasType = manager.dataset.match(appNode, ns.rdf.type, ns.trn.Application).size > 0
        expect(hasType).to.be.true
    })

    it('should propagate app context through processors', async () => {
        await manager.initialize('test_app_context', testAppRoot, null, testTarget)

        const message = { test: 'value' }
        const result = await manager.start(message)

        expect(result).to.have.property('success', true)
        expect(message).to.have.property('app')
        expect(message.app).to.have.property('sessionNode')
        expect(message.app.dataset).to.be.instanceOf(rdf.dataset().constructor)
    })

    it('should resolve paths correctly using app context', async () => {
        await manager.initialize('test_app_context', testAppRoot, null, testTarget)

        const testMessage = {
            test: 'value',
            appPath: testAppRoot,
            processorPaths: []
        }

        const result = await manager.start(testMessage)
        expect(result).to.have.property('success', true)
        expect(path.join(testAppRoot, 'test/path')).to.be.a('string')
    })

    it('should preserve existing functionality', async () => {
        await manager.initialize('test_app_context', testAppRoot, null, testTarget)
        const result = await manager.start()

        expect(result).to.have.property('success', true)
        const appConfig = await manager.app.getConfigPath()
        expect(appConfig).to.include('config.ttl')
    })
})

================
File: tests_pending/integration/application-manager.spec.js
================
import { expect } from 'chai'
import path from 'path'
import fs from 'fs/promises'
import { fileURLToPath } from 'url'
import ApplicationManager from '../../src/engine/ApplicationManager.js'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

describe('ApplicationManager Integration', () => {
    let manager
    const testAppRoot = path.join(__dirname, '../../src/applications/test_app_manager')
    const testTarget = path.join(testAppRoot, 'target')

    beforeEach(async () => {
        manager = new ApplicationManager()
        await setupTestFiles()
    })

    afterEach(async () => {
        await cleanupTestFiles()
    })

    async function setupTestFiles() {
        await fs.mkdir(testAppRoot, { recursive: true })
        await fs.mkdir(path.join(testAppRoot, 'data'), { recursive: true })
        await fs.mkdir(testTarget, { recursive: true })

        await fs.writeFile(path.join(testAppRoot, 'transmissions.ttl'), `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .
:test a :Transmission ;
    :pipe (:p10) .
:p10 a :NOP .`)

        await fs.writeFile(path.join(testAppRoot, 'config.ttl'), `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://purl.org/stuff/transmissions/> .
:config a :ConfigSet .`)

        await fs.writeFile(path.join(testAppRoot, 'about.md'), '# Test App')
    }

    async function cleanupTestFiles() {
        try {
            await fs.rm(testAppRoot, { recursive: true, force: true })
        } catch (err) {
            if (err.code !== 'ENOENT') throw err
        }
    }

    describe('message processing', () => {
        it('should propagate application context in messages', async () => {
            await manager.initialize('test_app_manager', testAppRoot, null, testTarget)

            const testMessage = { test: 'value' }
            const result = await manager.start(testMessage)

            expect(result).to.have.property('success', true)
            expect(testMessage).to.have.property('appName', 'test_app_manager')
            expect(testMessage).to.have.property('appPath', testAppRoot)
            expect(testMessage).to.have.property('targetPath', testTarget)
            expect(testMessage).to.have.property('workingDir').that.includes('data')
        })
    })
})

================
File: tests_pending/integration/configmap.spec.js
================
import { expect } from 'chai';
import rdf from 'rdf-ext';
import ConfigMap from '../../src/processors/rdf/ConfigMap.js';
import ns from '../../src/utils/ns.js';

describe('ConfigMap Integration Tests', () => {
  let configMap;
  let message;
  const testBasePath = '/test/base';

  beforeEach(() => {
    configMap = new ConfigMap({});
    message = {
      rootDir: testBasePath,
      dataset: rdf.dataset()
    };
  });

  function addTestData(predicates) {
    const subject = rdf.namedNode('http://hyperdata.it/transmissions/Content');
    message.dataset.add(rdf.quad(
      subject,
      ns.rdf.type,
      ns.trn.ConfigSet
    ));

    for (const [pred, obj] of Object.entries(predicates)) {
      message.dataset.add(rdf.quad(
        subject,
        ns.trn[pred],
        rdf.literal(obj)
      ));
    }
  }

  it('should resolve paths from ContentGroup', async () => {
    addTestData({
      sourceDirectory: 'content/src',
      targetDirectory: 'content/out'
    });

    await configMap.process(message);
    expect(message.contentGroup?.Content?.sourceDir).to.equal('/test/base/content/src');
    expect(message.contentGroup?.Content?.targetDir).to.equal('/test/base/content/out');
  });

  it('should preserve absolute paths', async () => {
    addTestData({
      sourceDirectory: '/abs/path/src',
      targetDirectory: '/abs/path/out'
    });

    await configMap.process(message);
    expect(message.contentGroup?.Content?.sourceDir).to.equal('/abs/path/src');
    expect(message.contentGroup?.Content?.targetDir).to.equal('/abs/path/out');
  });

  it('should handle missing paths', async () => {
    addTestData({});
    await configMap.process(message);
    expect(message.contentGroup?.Content?.sourceDir).to.be.undefined;
    expect(message.contentGroup?.Content?.targetDir).to.be.undefined;
  });

  it('should normalize paths', async () => {
    addTestData({
      sourceDirectory: 'content/../src'
    });
    await configMap.process(message);
    expect(message.contentGroup?.Content?.sourceDir).to.equal('/test/base/src');
  });
});

================
File: tests_pending/integration/file-container-integration-test.js
================
import { expect } from 'chai';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

describe('FileContainer Integration', () => {
    const testDir = path.join(__dirname, '../../src/applications/test_file-container/data');
    const inputDir = path.join(testDir, 'input');
    const outputDir = path.join(testDir, 'output');

    beforeEach(async () => {
        await fs.mkdir(outputDir, { recursive: true });
        await fs.writeFile(
            path.join(inputDir, 'test1.js'),
            'console.log("test1");'
        );
        await fs.writeFile(
            path.join(inputDir, 'test2.js'),
            'console.log("test2");'
        );
    });

    afterEach(async () => {
        try {
            await fs.rm(outputDir, { recursive: true });
        } catch (err) {
            if (err.code !== 'ENOENT') throw err;
        }
    });

    it('should process files in pipeline', async () => {
        const { exec } = await import('child_process');
        const util = await import('util');
        const execAsync = util.promisify(exec);

        const result = await execAsync('node src/api/cli/run.js test_file-container', {
            cwd: path.resolve(__dirname, '../../')
        });

        const output = JSON.parse(await fs.readFile(
            path.join(outputDir, 'container-output.json'),
            'utf8'
        ));

        expect(output.files).to.have.property('test1.js');
        expect(output.files).to.have.property('test2.js');
        expect(output.summary.totalFiles).to.equal(2);
        expect(output.summary.fileTypes['.js']).to.equal(2);
    });
});

================
File: tests_pending/integration/filename-mapper.spec.js
================
import { expect } from 'chai';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

describe('filename-mapper test', () => {
    const workingDir = path.join(__dirname, '../../src/applications/test_filename-mapper/data');
    const inputDir = path.join(workingDir, 'input');
    const outputDir = path.join(workingDir, 'output');

    async function setupTestFiles() {
        await fs.mkdir(outputDir, { recursive: true });
        const inputContent = 'Test content for filename mapping';
        await fs.writeFile(path.join(inputDir, 'input-01.txt'), inputContent);
    }

    async function cleanup() {
        try {
            const files = await fs.readdir(outputDir);
            for (const file of files) {
                if (file.startsWith('output-')) {
                    await fs.unlink(path.join(outputDir, file));
                }
            }
        } catch (err) {
            if (err.code !== 'ENOENT') throw err;
        }
    }

    beforeEach(async () => {
        await cleanup();
        await setupTestFiles();
    });

    afterAll(async () => {
        await cleanup();
    });

    async function compareFiles() {
        const outputFile = path.join(outputDir, 'output-01.txt');
        const requiredFile = path.join(outputDir, 'required-01.txt');

        const [output, required] = await Promise.all([
            fs.readFile(outputFile, 'utf8'),
            fs.readFile(requiredFile, 'utf8')
        ]);

        return output.trim() === required.trim();
    }

    it('should process files correctly', async () => {
        const { exec } = await import('child_process');
        const util = await import('util');
        const execAsync = util.promisify(exec);

        const result = await execAsync('node src/api/cli/run.js test_filename-mapper', {
            cwd: path.resolve(__dirname, '../..')
        });

        const matched = await compareFiles();
        expect(matched).to.be.true;
    });
});

================
File: tests_pending/integration/fork.spec.js
================
// tests/integration/fork.spec.js
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'

describe('fork test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const logFile = path.join(__dirname, '../../latest.log')

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000

    it('should create correct number of message paths', (done) => {
        exec('node src/api/cli/run.js test_fork', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {
                // Parse log file to count NOP processor executions
                const logs = stdout.toString()
                const nopMatches = logs.match(/NOP at/g)
                const nopCount = nopMatches ? nopMatches.length : 0

                // nForks=2 (default) should result in 2 NOP executions + 1 for done message
                expect(nopCount).to.equal(3)
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })
})

================
File: tests_pending/integration/fs-rw_simple.spec.js
================
import footpath from '../../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import fs from 'fs/promises'

describe('fs-rw simple test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, '../../')

    const outputFile = path.join(rootDir, 'src/applications/test_fs-rw/data/output/output-01.md')
    const requiredFile = path.join(rootDir, 'src/applications/test_fs-rw/data/output/required-01.md')

    beforeEach(async () => {
        try {
            await fs.unlink(outputFile)
        } catch (error) {
            if (error.code !== 'ENOENT') throw error
        }
    })

    it('should process file correctly', async () => {
        // Run the simple script
        await import('../../../src/applications/_pending/test_fs-rw/simple.js')

        // Read and compare files
        const output = await fs.readFile(outputFile, 'utf8')
        const required = await fs.readFile(requiredFile, 'utf8')

        expect(output.trim()).to.equal(required.trim())
    })
})

================
File: tests_pending/integration/fs-rw.spec.js
================
import { expect } from 'chai';
import rdf from 'rdf-ext';
import ConfigMap from '../../src/processors/rdf/ConfigMap.js';
import ns from '../../src/utils/ns.js';

describe('ConfigMap Integration Tests', () => {
    let configMap;
    let message;
    const testBasePath = '/test/base';

    beforeEach(() => {
        configMap = new ConfigMap({});
        message = {
            rootDir: testBasePath,
            dataset: rdf.dataset()
        };
    });

    function addTestData(predicates) {
        const subject = rdf.namedNode('http://hyperdata.it/transmissions/Content');
        message.dataset.add(rdf.quad(
            subject,
            ns.rdf.type,
            ns.trn.ConfigSet
        ));

        for (const [pred, obj] of Object.entries(predicates)) {
            message.dataset.add(rdf.quad(
                subject,
                ns.trn[pred],
                rdf.literal(obj)
            ));
        }
    }

    it('should resolve paths from ContentGroup', async () => {
        addTestData({
            sourceDirectory: 'content/src',
            targetDirectory: 'content/out'
        });

        await configMap.process(message);
        expect(message.contentGroup?.Content?.sourceDir).to.equal('/test/base/content/src');
        expect(message.contentGroup?.Content?.targetDir).to.equal('/test/base/content/out');
    });

    it('should preserve absolute paths', async () => {
        addTestData({
            sourceDirectory: '/abs/path/src',
            targetDirectory: '/abs/path/out'
        });

        await configMap.process(message);
        expect(message.contentGroup?.Content?.sourceDir).to.equal('/abs/path/src');
        expect(message.contentGroup?.Content?.targetDir).to.equal('/abs/path/out');
    });

    it('should handle missing paths', async () => {
        addTestData({});
        await configMap.process(message);
        expect(message.contentGroup?.Content?.sourceDir).to.be.undefined;
        expect(message.contentGroup?.Content?.targetDir).to.be.undefined;
    });

    it('should normalize paths', async () => {
        addTestData({
            sourceDirectory: 'content/../src'
        });
        await configMap.process(message);
        expect(message.contentGroup?.Content?.sourceDir).to.equal('/test/base/src');
    });
});

================
File: tests_pending/integration/http-server.spec.js
================
import { expect } from 'chai';
import fetch from 'node-fetch';
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);

describe('HTTP Server Integration', () => {
    const SERVER_URL = 'http://localhost:4000';
    const TEST_VALUES = { testKey: 'testValue' };
    let serverProcess;

    before(async () => {
        serverProcess = exec('node src/api/cli/run.js test_http-server');
        await new Promise(resolve => setTimeout(resolve, 1000));
    });

    after(async () => {
        try {
            await fetch(`${SERVER_URL}/shutdown`, {
                method: 'POST'
            });
        } catch (e) {
            console.log('Server already stopped');
        }
    });

    it('should serve static files', async () => {
        const response = await fetch(`${SERVER_URL}/transmissions/test/`);
        expect(response.status).to.equal(200);
        const html = await response.text();
        expect(html).to.include('HTTP Server Test Interface');
    });

    it('should accept message values and shutdown', async () => {
        const response = await fetch(`${SERVER_URL}/shutdown`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(TEST_VALUES)
        });
        expect(response.status).to.equal(200);
    });
});

================
File: tests_pending/integration/markmap.spec.js
================
import path from 'path';
import { fileURLToPath } from 'url';
import { expect } from 'chai';
import fs from 'fs/promises';
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

describe('Markmap Integration', () => {
    const testDir = path.join(__dirname, '../../src/applications/markmap/data/test');
    const testFiles = ['test1.md', 'test2.md'];
    
    beforeAll(async () => {
        // Create test directory and files
        await fs.mkdir(testDir, { recursive: true });
        
        // Create test markdown files
        await fs.writeFile(
            path.join(testDir, 'test1.md'),
            '# Test 1\n## Section 1\n* Item 1\n* Item 2'
        );
        
        await fs.writeFile(
            path.join(testDir, 'test2.md'),
            '# Test 2\n## Section 2\n* Item A\n* Item B'
        );
    });

    afterAll(async () => {
        // Clean up test files
        await fs.rm(testDir, { recursive: true, force: true });
    });

    it('should process multiple markdown files through ForEach', async () => {
        const message = {
            paths: testFiles.map(f => path.join(testDir, f))
        };

        const result = await execAsync(
            `./trans markmap -m '${JSON.stringify(message)}'`
        );

        // Verify outputs exist
        for (const file of testFiles) {
            const basePath = path.join(testDir, path.parse(file).name);
            
            // Check HTML output
            const htmlPath = `${basePath}.mm.html`;
            const htmlExists = await fs.access(htmlPath)
                .then(() => true)
                .catch(() => false);
            expect(htmlExists).to.be.true;
            
            // Verify HTML content
            const html = await fs.readFile(htmlPath, 'utf8');
            expect(html).to.include('<html');
            expect(html).to.include(`Test ${file[4]}`); // Check title from original file
            
            // Check SVG output
            const svgPath = `${basePath}.mm.svg`;
            const svgExists = await fs.access(svgPath)
                .then(() => true)
                .catch(() => false);
            expect(svgExists).to.be.true;
            
            // Verify SVG content
            const svg = await fs.readFile(svgPath, 'utf8');
            expect(svg).to.include('<svg');
            expect(svg).to.include(`Test ${file[4]}`);
        }
    });

    it('should handle empty input paths array', async () => {
        const message = { paths: [] };
        
        const result = await execAsync(
            `./trans markmap -m '${JSON.stringify(message)}'`
        );
        
        // Verify no files were created
        const files = await fs.readdir(testDir);
        expect(files.filter(f => f.endsWith('.mm.html') || f.endsWith('.mm.svg')))
            .to.have.lengthOf(0);
    });

    it('should handle invalid markdown files gracefully', async () => {
        // Create invalid markdown file
        const invalidPath = path.join(testDir, 'invalid.md');
        await fs.writeFile(invalidPath, '# Title\n## [Invalid markdown');
        
        const message = {
            paths: [invalidPath]
        };

        try {
            await execAsync(`./trans markmap -m '${JSON.stringify(message)}'`);
        } catch (error) {
            expect(error.message).to.include('Error processing markdown');
        }
    });
});

================
File: tests_pending/integration/restructure_simple.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import fs from 'fs/promises'

describe('restructure simple test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, '../../')

    const outputFile = path.join(rootDir, 'src/applications/test_restructure/data/output/output-01.json')
    const requiredFile = path.join(rootDir, 'src/applications/test_restructure/data/output/required-01.json')

    beforeEach(async () => {
        try {
            await fs.unlink(outputFile)
        } catch (error) {
            if (error.code !== 'ENOENT') throw error
        }
    })

    it('should process JSON file correctly', async () => {
        console.log('Running restructure test')
        // Run the simple restructure script
        await import('../../src/applications/test_restructure/simple.js')

        // Read and parse both files
        const output = JSON.parse(await fs.readFile(outputFile, 'utf8'))
        const required = JSON.parse(await fs.readFile(requiredFile, 'utf8'))

        // Compare JSON structures
        expect(output).to.deep.equal(required)
    })
})

================
File: tests_pending/integration/restructure.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs/promises'

describe('test_restructure', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const workingDir = path.join(__dirname, '../../src/applications/test_restructure/data')

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000

    async function clearOutputFiles() {
        console.log('Clearing output files...')
        const outputDir = path.join(workingDir, 'output')
        const files = await fs.readdir(outputDir)
        for (const file of files) {
            if (file.startsWith('output-')) {
                await fs.unlink(path.join(outputDir, file))
                console.log(`Deleted ${file}`)
            }
        }
    }

    async function compareFiles(index) {
        const outputFile = path.join(workingDir, 'output', `output-${index}.json`)
        const requiredFile = path.join(workingDir, 'output', `required-${index}.json`)

        console.log(`Comparing files:`)
        console.log(`Output: ${outputFile}`)
        console.log(`Required: ${requiredFile}`)

        const output = JSON.parse(await fs.readFile(outputFile, 'utf8'))
        const required = JSON.parse(await fs.readFile(requiredFile, 'utf8'))

        // Deep compare objects instead of strings
        return JSON.stringify(output) === JSON.stringify(required)
    }

    beforeEach(async () => {
        await clearOutputFiles()
    })

    it('should process files correctly', (done) => {
        console.log('Running transmission...')
        exec('node src/api/cli/run.js test_restructure', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {
                console.log('Transmission output:', stdout)
                if (stderr) console.error('Stderr:', stderr)

                const matched = await compareFiles('01')
                expect(matched).to.be.true
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })
})

================
File: tests_pending/integration/run-command.spec.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs/promises'

describe('run-command test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const testDir = path.resolve(__dirname, '../../src/applications/test_run-command')

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000

    // Create test directory with config if it doesn't exist
    beforeAll(async function () {
        try {
            await fs.mkdir(testDir, { recursive: true })

            const configTtl = `@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:RunCommandConfig a trm:ConfigSet ;
    trm:settings t:runCommand ;
    trm:command "echo \\"test\\"" .`

            await fs.writeFile(path.join(testDir, 'config.ttl'), configTtl)

            const transmissionsTtl = `@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

:test_run_command a trm:Transmission ;
    trm:pipe (:p10 :p20) .

:p10 a :RunCommand ;
    trm:settings :runCommand .

:p20 a :ShowMessage .`

            await fs.writeFile(path.join(testDir, 'transmissions.ttl'), transmissionsTtl)
        } catch (err) {
            console.error('Setup error:', err)
            throw err
        }
    })

    it('should execute command successfully', (done) => {
        exec('node src/api/cli/run.js test_run-command', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {
                expect(stdout).to.include('test')
                expect(stderr).to.be.empty
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })

    // Clean up test directory after tests
    afterAll(async function () {
        try {
            await fs.rm(testDir, { recursive: true, force: true })
        } catch (err) {
            console.error('Cleanup error:', err)
        }
    })
})

================
File: tests_pending/integration/string-filter.spec.js
================
import { expect } from 'chai';
import path from 'path';
import fs from 'fs/promises';
import rdf from 'rdf-ext';
import StringFilter from '../../src/processors/text/StringFilter.js';
import ns from '../../src/utils/ns.js';

describe('StringFilter Integration', () => {
    let filter;
    let testDir;

    beforeEach(async () => {
        testDir = path.join(process.cwd(), 'test-data', 'string-filter');
        await fs.mkdir(testDir, { recursive: true });
    });

    afterEach(async () => {
        await fs.rm(testDir, { recursive: true, force: true });
    });

    async function createTestFiles() {
        await Promise.all([
            fs.writeFile(path.join(testDir, 'test.js'), ''),
            fs.writeFile(path.join(testDir, 'test.css'), ''),
            fs.writeFile(path.join(testDir, 'build/test.js'), ''),
            fs.writeFile(path.join(testDir, 'node_modules/test.js'), '')
        ]);
    }

    function createConfigDataset(patterns) {
        const dataset = rdf.dataset();
        const subject = rdf.namedNode('http://example.org/config');

        dataset.add(rdf.quad(
            subject,
            ns.rdf.type,
            ns.trn.ConfigSet
        ));

        if (patterns.commaPattern) {
            dataset.add(rdf.quad(
                subject,
                ns.trn.excludePatterns,
                rdf.literal(patterns.commaPattern)
            ));
        }

        if (patterns.singlePatterns) {
            patterns.singlePatterns.forEach(pattern => {
                dataset.add(rdf.quad(
                    subject,
                    ns.trn.excludePattern,
                    rdf.literal(pattern)
                ));
            });
        }

        return { dataset, subject };
    }

    it('should handle comma-separated patterns', async () => {
        const { dataset, subject } = createConfigDataset({
            commaPattern: 'node_modules/*,build/*'
        });

        filter = new StringFilter({ dataset });
        filter.settingsNode = subject;

        await createTestFiles();

        const messages = await Promise.all([
            filter.process({ filepath: path.join(testDir, 'test.js') }),
            filter.process({ filepath: path.join(testDir, 'build/test.js') }),
            filter.process({ filepath: path.join(testDir, 'node_modules/test.js') })
        ]);

        const passedFiles = messages.filter(Boolean);
        expect(passedFiles).to.have.lengthOf(1);
        expect(passedFiles[0].filepath).to.include('test.js');
        expect(passedFiles[0].filepath).to.not.include('build');
        expect(passedFiles[0].filepath).to.not.include('node_modules');
    });

    it('should handle multiple single patterns', async () => {
        const { dataset, subject } = createConfigDataset({
            singlePatterns: ['node_modules/*', 'build/*']
        });

        filter = new StringFilter({ dataset });
        filter.settingsNode = subject;

        await createTestFiles();

        const messages = await Promise.all([
            filter.process({ filepath: path.join(testDir, 'test.js') }),
            filter.process({ filepath: path.join(testDir, 'build/test.js') }),
            filter.process({ filepath: path.join(testDir, 'node_modules/test.js') })
        ]);

        const passedFiles = messages.filter(Boolean);
        expect(passedFiles).to.have.lengthOf(1);
        expect(passedFiles[0].filepath).to.include('test.js');
        expect(passedFiles[0].filepath).to.not.include('build');
        expect(passedFiles[0].filepath).to.not.include('node_modules');
    });

    it('should handle mixed pattern styles', async () => {
        const { dataset, subject } = createConfigDataset({
            commaPattern: 'node_modules/*',
            singlePatterns: ['build/*']
        });

        filter = new StringFilter({ dataset });
        filter.settingsNode = subject;

        await createTestFiles();

        const messages = await Promise.all([
            filter.process({ filepath: path.join(testDir, 'test.js') }),
            filter.process({ filepath: path.join(testDir, 'build/test.js') }),
            filter.process({ filepath: path.join(testDir, 'node_modules/test.js') })
        ]);

        const passedFiles = messages.filter(Boolean);
        expect(passedFiles).to.have.lengthOf(1);
        expect(passedFiles[0].filepath).to.include('test.js');
        expect(passedFiles[0].filepath).to.not.include('build');
        expect(passedFiles[0].filepath).to.not.include('node_modules');
    });
});

================
File: tests_pending/integration/test_apps.spec.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs/promises'
import { glob } from 'glob'
import { existsSync } from 'fs'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const rootDir = path.resolve(__dirname, '../../')

async function runCommand(command, options) {
    return new Promise((resolve) => {
        const startTime = process.hrtime()
        const proc = exec(`./trans ${command}`, { ...options, cwd: rootDir })
        let stdout = '', stderr = ''

        proc.stdout.on('data', (data) => {
            stdout += data
            process.stdout.write(data)
        })

        proc.stderr.on('data', (data) => {
            stderr += data
            process.stderr.write(data)
        })

        proc.on('exit', (code, signal) => {
            const endTime = process.hrtime(startTime)
            const duration = (endTime[0] + endTime[1] / 1e9).toFixed(3)

            // Check for error messages in output
            const hasError = stdout.includes('TypeError:') ||
                stdout.includes('Error:') ||
                stderr.includes('TypeError:') ||
                stderr.includes('Error:')

            const result = {
                stdout,
                stderr,
                code,
                signal,
                success: code === 0 && !hasError,
                duration
            }
            resolve(result)
        })
    })
}

describe('Application Integration Tests', function () {
    it('should run test applications', async function () {
        const testApps = await glob(path.join(rootDir, 'src/applications/test_*'))
        expect(testApps.length).to.be.greaterThan(0)

        for (const appDir of testApps) {
            const appName = path.basename(appDir)
            console.log(`\nTesting ${appName}`)

            const configPath = path.join(appDir, 'test-config.json')
            const config = existsSync(configPath) ?
                JSON.parse(await fs.readFile(configPath, 'utf8')) :
                { transmissions: [{ name: appName }] }

            for (const tx of config.transmissions) {
                let cmd = tx.name
                if (tx.message) cmd += ` -m '${JSON.stringify(tx.message)}'`

                const result = await runCommand(cmd)

                if (!result.success) {
                    console.error('\n' + '='.repeat(80))
                    console.error(`🔴 Test failed for ${cmd}`)
                    console.error('='.repeat(80))
                    console.error('\nExecution Details:')
                    console.error('-'.repeat(40))
                    console.error(`Duration: ${result.duration}s`)
                    console.error('Exit code:', result.code)
                    console.error('Signal:', result.signal)

                    if (result.error) {
                        console.error('\nError Details:')
                        console.error('-'.repeat(40))
                        console.error('Message:', result.error.message)
                        console.error('Stack:', result.error.stack)
                    }

                    if (result.stderr) {
                        console.error('\nStderr Output:')
                        console.error('-'.repeat(40))
                        console.error(result.stderr)
                    }

                    console.error('\nStdout Output:')
                    console.error('-'.repeat(40))
                    console.error(result.stdout || '(no stdout output)')

                    console.error('\nTest Configuration:')
                    console.error('-'.repeat(40))
                    console.error(JSON.stringify(tx, null, 2))
                    console.error('\n' + '='.repeat(80))

                    try {
                        const failuresDir = path.join(rootDir, 'test-failures', appName, new Date().toISOString().replace(/:/g, '-'))
                        await fs.mkdir(failuresDir, { recursive: true })
                        await fs.writeFile(
                            path.join(failuresDir, 'test-output.json'),
                            JSON.stringify({ result, config: tx }, null, 2)
                        )
                        console.error(`Failure details saved to: ${failuresDir}`)
                    } catch (err) {
                        console.error('Failed to save failure details:', err)
                    }
                } else {
                    console.log(`✅ ${cmd} completed successfully (${result.duration}s)`)
                }

                expect(result.success, `Command failed: ${cmd} with exit code ${result.code}`).to.be.true

                if (tx.requiredFiles) {
                    for (const pattern of tx.requiredFiles) {
                        const outputFiles = await glob(path.join(appDir, 'data/output', pattern))
                        for (const outputFile of outputFiles) {
                            const requiredFile = outputFile.replace('output-', 'required-')
                            const [output, required] = await Promise.all([
                                fs.readFile(outputFile, 'utf8'),
                                fs.readFile(requiredFile, 'utf8')
                            ])
                            expect(output.trim()).to.equal(required.trim())
                        }
                    }
                }
            }
        }
    })
})

================
File: tests_pending/integration/test-data-generator_string-filter.js
================
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

class TestDataGenerator {
    constructor(baseDir) {
        this.baseDir = baseDir;
        this.inputDir = path.join(baseDir, 'input');
        this.outputDir = path.join(baseDir, 'output');
        this.fileTypes = new Set(['.txt', '.js', '.log', '.md', '.json']);
    }

    async init() {
        await fs.mkdir(this.inputDir, { recursive: true });
        await fs.mkdir(this.outputDir, { recursive: true });
    }

    async cleanup() {
        await fs.rm(this.baseDir, { recursive: true, force: true });
    }

    async createDirectoryStructure() {
        const structure = {
            'docs': {
                'api': ['config.md', 'overview.md'],
                'examples': ['basic.js', 'advanced.js']
            },
            'src': {
                'lib': ['util.js', 'core.js'],
                'test': ['test.js', 'mock.js']
            },
            'temp': ['temp1.log', 'temp2.log'],
            'data': ['data.json', 'schema.json']
        };

        for (const [dir, contents] of Object.entries(structure)) {
            const dirPath = path.join(this.inputDir, dir);
            await fs.mkdir(dirPath, { recursive: true });

            if (Array.isArray(contents)) {
                for (const file of contents) {
                    await this.createTestFile(path.join(dir, file));
                }
            } else {
                for (const [subdir, files] of Object.entries(contents)) {
                    const subdirPath = path.join(dirPath, subdir);
                    await fs.mkdir(subdirPath, { recursive: true });
                    
                    for (const file of files) {
                        await this.createTestFile(path.join(dir, subdir, file));
                    }
                }
            }
        }
    }

    async createTestFile(relativePath, content = '') {
        const filePath = path.join(this.inputDir, relativePath);
        const ext = path.extname(filePath);
        
        if (!content) {
            content = this.generateContent(ext);
        }
        
        await fs.writeFile(filePath, content);
        return filePath;
    }

    generateContent(ext) {
        const timestamp = new Date().toISOString();
        
        switch (ext) {
            case '.js':
                return `// Test file generated ${timestamp}
export function test() {
    return 'test content';
}`;

            case '.json':
                return JSON.stringify({
                    generated: timestamp,
                    type: 'test',
                    version: '1.0.0'
                }, null, 2);

            case '.md':
                return `# Test File
Generated: ${timestamp}

## Content
Test content for markdown file.`;

            case '.log':
                return `[${timestamp}] INFO Test log content
[${timestamp}] DEBUG Additional details`;

            default:
                return `Test content generated at ${timestamp}`;
        }
    }

    async createPatternTestSet() {
        const testCases = [
            { file: 'include.js', shouldMatch: true },
            { file: 'exclude.txt', shouldMatch: false },
            { file: '.hidden.js', shouldMatch: false },
            { file: 'temp.dat', shouldMatch: false },
            { file: 'test.min.js', shouldMatch: true },
            { file: 'backup.js.bak', shouldMatch: false }
        ];

        const files = await Promise.all(testCases.map(async ({ file }) => {
            const filePath = await this.createTestFile(file);
            return { path: filePath, name: file };
        }));

        return {
            files,
            testCases: testCases.reduce((acc, { file, shouldMatch }) => {
                acc[file] = shouldMatch;
                return acc;
            }, {})
        };
    }

    async createNestedStructure(depth = 3, filesPerLevel = 2) {
        const files = [];
        
        async function createLevel(currentPath, currentDepth) {
            if (currentDepth > depth) return;

            const dirPath = path.join(this.inputDir, currentPath);
            await fs.mkdir(dirPath, { recursive: true });

            for (let i = 1; i <= filesPerLevel; i++) {
                const filename = `level${currentDepth}-file${i}.js`;
                const filePath = path.join(currentPath, filename);
                await this.createTestFile(filePath);
                files.push(filePath);
            }

            await createLevel.call(
                this,
                path.join(currentPath, `level${currentDepth + 1}`),
                currentDepth + 1
            );
        }

        await createLevel.call(this, 'nested', 1);
        return files;
    }

    async createLargeFileSet(count = 1000) {
        const files = [];
        const types = Array.from(this.fileTypes);
        
        for (let i = 1; i <= count; i++) {
            const type = types[i % types.size];
            const filename = `file${String(i).padStart(5, '0')}${type}`;
            const filePath = await this.createTestFile(filename);
            files.push(filePath);
        }

        return files;
    }

    getExpectedOutput(inputPath) {
        const filename = path.basename(inputPath);
        if (!filename.includes('.')) return null;
        
        const [name, ...extensions] = filename.split('.');
        const baseExt = extensions.pop();
        
        return {
            html: path.join(this.outputDir, `${name}.mm.html`),
            svg: path.join(this.outputDir, `${name}.mm.svg`)
        };
    }
}

export default TestDataGenerator;

================
File: tests_pending/integration/test-settings-integration.js
================
import { expect } from 'chai'
import path from 'path'
import { fileURLToPath } from 'url'
import { exec } from 'child_process'
import fs from 'fs/promises'

describe('TestSettings Integration', function() {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const testDir = path.join(__dirname, '../../src/applications/test_config-settings')

    // Verify test files exist before running
    beforeAll(async () => {
        const files = ['config.ttl', 'transmissions.ttl']
        for (const file of files) {
            const exists = await fs.access(path.join(testDir, file))
                .then(() => true)
                .catch(() => false)
            expect(exists, `${file} exists`).to.be.true
        }
    })

    // Test the full application pipeline
    it('should process settings through transmission pipeline', (done) => {
        exec('node src/api/cli/run.js test_config-settings', {
            cwd: path.resolve(__dirname, '../..')
        }, async (error, stdout, stderr) => {
            if (error) {
                done(error)
                return
            }

            try {
                // Verify expected output in logs
                expect(stdout).to.include('settingsSingle')
                expect(stdout).to.include('Alice')

                // Check multiple settings
                expect(stdout).to.include('settingsMulti')
                expect(stdout).to.include('Bob')
                expect(stdout).to.include('dirB')

                // Verify settings lists
                expect(stdout).to.include('settingsLists')
                expect(stdout).to.include('settingA1')
                expect(stdout).to.include('settingB1')

                done()
            } catch (err) {
                done(err)
            }
        })
    })

    // Test configuration error cases
    it('should handle missing configuration gracefully', (done) => {
        const badConfigPath = path.join(testDir, 'missing-config.ttl')
        
        exec(`node src/api/cli/run.js test_config-settings -c ${badConfigPath}`, {
            cwd: path.resolve(__dirname, '../..')
        }, (error, stdout, stderr) => {
            expect(stdout).to.include('fallback value')
            expect(stderr).to.not.include('UnhandledPromiseRejection')
            done()
        })
    })

    // Test settings inheritance
    it('should handle settings inheritance', (done) => {
        exec('node src/api/cli/run.js test_config-settings inherit', {
            cwd: path.resolve(__dirname, '../..')
        }, async (error, stdout, stderr) => {
            try {
                expect(stdout).to.include('base setting')
                expect(stdout).to.include('inherited setting')
                done()
            } catch (err) {
                done(err) 
            }
        })
    })

    // Test configuration reloading
    it('should reload changed configuration', async () => {
        const configPath = path.join(testDir, 'config.ttl')
        const backupPath = path.join(testDir, 'config.ttl.bak')

        // Backup original config
        await fs.copyFile(configPath, backupPath)

        try {
            // Modify config
            const config = await fs.readFile(configPath, 'utf8')
            const modified = config.replace('Alice', 'Modified')
            await fs.writeFile(configPath, modified)

            // Run test
            await new Promise((resolve, reject) => {
                exec('node src/api/cli/run.js test_config-settings', {
                    cwd: path.resolve(__dirname, '../..')
                }, (error, stdout, stderr) => {
                    try {
                        expect(stdout).to.include('Modified')
                        resolve()
                    } catch (err) {
                        reject(err)
                    }
                })
            })

        } finally {
            // Restore original config
            await fs.copyFile(backupPath, configPath)
            await fs.unlink(backupPath)
        }
    })

    // Test processor settings interaction
    it('should pass settings between processors', (done) => {
        exec('node src/api/cli/run.js test_config-settings chain', {
            cwd: path.resolve(__dirname, '../..')
        }, (error, stdout, stderr) => {
            try {
                // Verify settings flow through processor chain
                expect(stdout).to.include('first processor setting')
                expect(stdout).to.include('second processor setting')
                expect(stdout).to.include('combined settings')
                done()
            } catch (err) {
                done(err)
            }
        })
    })
})

================
File: tests_pending/tests-pending/test_env-loader/2024-11-28T17-44-11.419Z/test-output.json
================
{
  "result": {
    "stdout": "\nCommandUtils.run()\nCommandUtils.run, process.cwd() = /home/danny/github-danny/transmissions\nCommandUtils.run, application = test_env-loader\nCommandUtils.run, target = undefined\n\nCommandUtils.splitName, fullPath  = test_env-loader\n\nCommandUtils.splitName, parts  = test_env-loader\nCommandUtils.splitName, appName:test_env-loader, appPath:test_env-loader, task:false,\n\n\n    CommandUtils.run, \n    appName = test_env-loader\n    appPath = test_env-loader\n    subtask = undefined\n    target = undefined\n\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/envy>\n| Create processor :s10 of type :EnvLoader\n| Create processor :s20 of type :WhiteboardToMessage\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [s10] => [s20]\nTransmission.connect from http://hyperdata.it/transmissions/s10 to http://hyperdata.it/transmissions/s10\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s10 this.toName =  http://hyperdata.it/transmissions/s20\n  > Connect #1 [s20] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/s20 to http://hyperdata.it/transmissions/s20\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s20 this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/envy>\n| Running : http://hyperdata.it/transmissions/s10 a EnvLoader\nTypeError: (intermediate value).handle is not a function\n    at EnvLoader.process (file:///home/danny/github-danny/transmissions/src/processors/system/EnvLoader.js:43:22)\n    at EnvLoader.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at EnvLoader.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n    at EnvLoader.receive (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:145:20)\n    at Transmission.process (file:///home/danny/github-danny/transmissions/src/engine/Transmission.js:36:23)\n    at ApplicationManager.start (file:///home/danny/github-danny/transmissions/src/core/ApplicationManager.js:76:36)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async CommandUtils.begin (file:///home/danny/github-danny/transmissions/src/api/common/CommandUtils.js:40:16)\n    at async Object.handler (file:///home/danny/github-danny/transmissions/src/api/cli/run.js:69:13)\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.771"
  },
  "config": {
    "name": "test_env-loader"
  }
}

================
File: tests_pending/tests-pending/test_env-loader/2024-11-28T17-46-20.677Z/test-output.json
================
{
  "result": {
    "stdout": "\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/envy>\n| Create processor :s10 of type :EnvLoader\n| Create processor :s20 of type :WhiteboardToMessage\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [s10] => [s20]\nTransmission.connect from http://hyperdata.it/transmissions/s10 to http://hyperdata.it/transmissions/s10\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s10 this.toName =  http://hyperdata.it/transmissions/s20\n  > Connect #1 [s20] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/s20 to http://hyperdata.it/transmissions/s20\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s20 this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/envy>\n| Running : http://hyperdata.it/transmissions/s10 a EnvLoader\nTypeError: (intermediate value).handle is not a function\n    at EnvLoader.process (file:///home/danny/github-danny/transmissions/src/processors/system/EnvLoader.js:43:22)\n    at EnvLoader.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at EnvLoader.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n    at EnvLoader.receive (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:145:20)\n    at Transmission.process (file:///home/danny/github-danny/transmissions/src/engine/Transmission.js:36:23)\n    at ApplicationManager.start (file:///home/danny/github-danny/transmissions/src/core/ApplicationManager.js:76:36)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async CommandUtils.begin (file:///home/danny/github-danny/transmissions/src/api/common/CommandUtils.js:40:16)\n    at async Object.handler (file:///home/danny/github-danny/transmissions/src/api/cli/run.js:69:13)\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.709"
  },
  "config": {
    "name": "test_env-loader"
  }
}

================
File: tests_pending/tests-pending/test_env-loader/2024-11-28T18-31-38.300Z/test-output.json
================
{
  "result": {
    "stdout": "\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/envy>\n| Create processor :s10 of type :EnvLoader\n| Create processor :s20 of type :WhiteboardToMessage\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [s10] => [s20]\nTransmission.connect from http://hyperdata.it/transmissions/s10 to http://hyperdata.it/transmissions/s10\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s10 this.toName =  http://hyperdata.it/transmissions/s20\n  > Connect #1 [s20] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/s20 to http://hyperdata.it/transmissions/s20\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s20 this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/envy>\n| Running : http://hyperdata.it/transmissions/s10 a EnvLoader\nTypeError: (intermediate value).handle is not a function\n    at EnvLoader.process (file:///home/danny/github-danny/transmissions/src/processors/system/EnvLoader.js:43:22)\n    at EnvLoader.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at EnvLoader.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n    at EnvLoader.receive (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:145:20)\n    at Transmission.process (file:///home/danny/github-danny/transmissions/src/engine/Transmission.js:36:23)\n    at ApplicationManager.start (file:///home/danny/github-danny/transmissions/src/core/ApplicationManager.js:76:36)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async CommandUtils.begin (file:///home/danny/github-danny/transmissions/src/api/common/CommandUtils.js:40:16)\n    at async Object.handler (file:///home/danny/github-danny/transmissions/src/api/cli/run.js:69:13)\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.723"
  },
  "config": {
    "name": "test_env-loader"
  }
}

================
File: tests_pending/tests-pending/test_env-loader/2024-11-28T18-34-16.177Z/test-output.json
================
{
  "result": {
    "stdout": "\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/envy>\n| Create processor :s10 of type :EnvLoader\n| Create processor :s20 of type :WhiteboardToMessage\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [s10] => [s20]\nTransmission.connect from http://hyperdata.it/transmissions/s10 to http://hyperdata.it/transmissions/s10\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s10 this.toName =  http://hyperdata.it/transmissions/s20\n  > Connect #1 [s20] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/s20 to http://hyperdata.it/transmissions/s20\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s20 this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/envy>\n| Running : http://hyperdata.it/transmissions/s10 a EnvLoader\n| Running >>> :  (s10) s20 a WhiteboardToMessage\nWhiteboardToMessage at (s10.s20) s20\nTypeError: (intermediate value).handle is not a function\n    at WhiteboardToMessage.process (file:///home/danny/github-danny/transmissions/src/processors/util/WhiteboardToMessage.js:28:22)\n    at WhiteboardToMessage.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at WhiteboardToMessage.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n    at WhiteboardToMessage.receive (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:145:20)\n    at EnvLoader.<anonymous> (file:///home/danny/github-danny/transmissions/src/engine/Connector.js:32:25)\n    at EnvLoader.emit (node:events:518:28)\n    at EnvLoader.emit (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:246:15)\n    at EnvLoader.process (file:///home/danny/github-danny/transmissions/src/processors/system/EnvLoader.js:41:21)\n    at EnvLoader.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at EnvLoader.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.708"
  },
  "config": {
    "name": "test_env-loader"
  }
}

================
File: tests_pending/tests-pending/test_http-server/2024-11-30T12-30-16.673Z/test-output.json
================
{
  "result": {
    "stdout": "\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/mini>\n| Create processor :server of type :HttpServer\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [server] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/server to http://hyperdata.it/transmissions/server\nConnector.connect this.fromName = http://hyperdata.it/transmissions/server this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/mini>\n| Running : http://hyperdata.it/transmissions/server a HttpServer\nError: listen EADDRINUSE: address already in use :::4000\n    at Server.setupListenHandle [as _listen2] (node:net:1872:16)\n    at listenInCluster (node:net:1920:12)\n    at Server.listen (node:net:2008:7)\n    at Function.listen (/home/danny/github-danny/transmissions/node_modules/express/lib/application.js:635:24)\n    at ServerWorker.start (file:///home/danny/github-danny/transmissions/src/processors/http/HttpServerWorker.js:42:36)\n    at MessagePort.<anonymous> (file:///home/danny/github-danny/transmissions/src/processors/http/HttpServerWorker.js:18:26)\n    at [nodejs.internal.kHybridDispatch] (node:internal/event_target:826:20)\n    at exports.emitMessage (node:internal/per_context/messageport:23:28) {\n  code: 'EADDRINUSE',\n  errno: -98,\n  syscall: 'listen',\n  address: '::',\n  port: 4000\n}\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.898"
  },
  "config": {
    "name": "test_http-server"
  }
}

================
File: tests_pending/unit/Application.spec.js
================
import { expect } from 'chai'
import path from 'path'
import Application from '../../src/model/Application.js'

describe('Application', () => {
    let app

    beforeEach(() => {
        app = new Application()
    })

    describe('constructor', () => {
        it('should initialize with default values', () => {
            expect(app.appsDir).to.equal('src/applications')
            expect(app.transmissionFilename).to.equal('transmissions.ttl')
            expect(app.configFilename).to.equal('config.ttl')
            expect(app.appName).to.be.null
            expect(app.dataset).to.be.null
        })

        it('should accept custom options', () => {
            const customApp = new Application({
                appName: 'test-app',
                appPath: '/custom/path',
                subtask: 'custom-task'
            })
            expect(customApp.appName).to.equal('test-app')
            expect(customApp.appPath).to.equal('/custom/path')
            expect(customApp.subtask).to.equal('custom-task')
        })
    })

    describe('resolveApplicationPath', () => {
        it('should handle absolute paths', () => {
            const absolutePath = '/absolute/path/to/app'
            expect(app.resolveApplicationPath(absolutePath)).to.equal(absolutePath)
        })

        it('should handle relative paths', () => {
            const relativePath = '../relative/path'
            const expected = path.resolve(process.cwd(), relativePath)
            expect(app.resolveApplicationPath(relativePath)).to.equal(expected)
        })

        it('should resolve paths under appsDir', () => {
            const appName = 'test-app'
            const expected = path.join(process.cwd(), app.appsDir, appName)
            expect(app.resolveApplicationPath(appName)).to.equal(expected)
        })

        it('should throw error for empty app name', () => {
            expect(() => app.resolveApplicationPath()).to.throw('Application name is required')
        })
    })

    describe('initialize', () => {
        it('should set up application with valid parameters', async () => {
            const appName = 'test-app'
            const appPath = '/test/path'
            const subtask = 'test-task'
            const target = '/test/target'

            await app.initialize(appName, appPath, subtask, target)

            expect(app.appName).to.equal(appName)
            expect(app.appPath).to.equal(appPath)
            expect(app.subtask).to.equal(subtask)
            expect(app.targetPath).to.equal(target)
            expect(app.manifestFilename).to.equal(path.join(target, 'manifest.ttl'))
        })
    })

    describe('toMessage', () => {
        it('should generate correct message object', () => {
            const testApp = new Application({
                appName: 'test-app',
                appPath: '/test/path',
                subtask: 'test-task',
                targetPath: '/test/target'
            })

            const message = testApp.toMessage()

            expect(message).to.have.property('appName', 'test-app')
            expect(message).to.have.property('appPath', '/test/path')
            expect(message).to.have.property('subtask', 'test-task')
            expect(message).to.have.property('targetPath', '/test/target')
            expect(message).to.have.property('workingDir').that.includes('data')
            expect(message).to.have.property('dataset')
        })
    })
})

================
File: tests_pending/unit/file-container-unit-test.js
================
import { expect } from 'chai';
import FileContainer from '../../../src/processors/packer/FileContainer.js';

describe('FileContainer', () => {
    let container;
    const config = { destination: 'test-output.json' };

    beforeEach(() => {
        container = new FileContainer(config);
    });

    it('should store file content and metadata', async () => {
        const message = {
            filepath: '/test/file.js',
            content: 'console.log("test")',
            rootDir: '/test'
        };

        let outputMessage;
        container.on('message', (msg) => {
            outputMessage = msg;
        });

        await container.process(message);
        
        expect(container.container.files['file.js']).to.exist;
        expect(container.container.files['file.js'].content).to.equal(message.content);
        expect(container.container.files['file.js'].type).to.equal('.js');
    });

    it('should update summary statistics', async () => {
        await container.process({
            filepath: '/test/file1.js',
            content: 'test',
            rootDir: '/test'
        });

        expect(container.container.summary.totalFiles).to.equal(1);
        expect(container.container.summary.fileTypes['.js']).to.equal(1);
    });

    it('should handle done message correctly', async () => {
        await container.process({
            filepath: '/test/file.js',
            content: 'test',
            rootDir: '/test'
        });

        let finalMessage;
        container.on('message', (msg) => {
            finalMessage = msg;
        });

        await container.process({ done: true });
        
        expect(finalMessage.content).to.be.a('string');
        expect(finalMessage.filepath).to.equal(config.destination);
    });
});

================
File: tests_pending/unit/filename-mapper.spec.js
================
import FilenameMapper from '../../src/processors/fs/FilenameMapper.js';
import { expect } from 'chai';

describe('FilenameMapper', () => {
    let filenameMapper;

    beforeEach(() => {
        filenameMapper = new FilenameMapper({
            extensions: {
                html: '.mm.html',
                svg: '.mm.svg'
            }
        });
    });

    it('should map HTML extension correctly', async () => {
        const message = {
            filepath: '/test/example.md',
            format: 'html'
        };

        let outputMessage;
        filenameMapper.on('message', (msg) => {
            outputMessage = msg;
        });

        await filenameMapper.process(message);
        expect(outputMessage.filepath).to.equal('/test/example.mm.html');
    });

    it('should map SVG extension correctly', async () => {
        const message = {
            filepath: '/test/example.md',
            format: 'svg'
        };

        let outputMessage;
        filenameMapper.on('message', (msg) => {
            outputMessage = msg;
        });

        await filenameMapper.process(message);
        expect(outputMessage.filepath).to.equal('/test/example.mm.svg');
    });

    it('should throw error for missing filepath', async () => {
        const message = {
            format: 'html'
        };

        try {
            await filenameMapper.process(message);
            expect.fail('Should have thrown error');
        } catch (error) {
            expect(error.message).to.equal('No filepath provided in message');
        }
    });

    it('should throw error for unknown format', async () => {
        const message = {
            filepath: '/test/example.md',
            format: 'unknown'
        };

        try {
            await filenameMapper.process(message);
            expect.fail('Should have thrown error');
        } catch (error) {
            expect(error.message).to.equal('Unknown format: unknown');
        }
    });

    it('should preserve directory structure', async () => {
        const message = {
            filepath: '/deep/nested/path/example.md',
            format: 'html'
        };

        let outputMessage;
        filenameMapper.on('message', (msg) => {
            outputMessage = msg;
        });

        await filenameMapper.process(message);
        expect(outputMessage.filepath).to.equal('/deep/nested/path/example.mm.html');
    });
});

================
File: tests_pending/unit/http-server_MetricsService.spec.js
================
import { expect } from 'chai';
import WebSocket from 'ws';
import MetricsService from '../../src/processors/http/services/MetricsService.js';
import http from 'http';

describe('MetricsService', () => {
    let metricsService;
    let server;
    let wsClient;

    beforeEach((done) => {
        server = http.createServer();
        server.listen(0, () => {
            metricsService = new MetricsService(server);
            const port = server.address().port;
            wsClient = new WebSocket(`ws://localhost:${port}`);
            wsClient.on('open', done);
        });
    });

    afterEach((done) => {
        wsClient.close();
        server.close(done);
    });

    it('should send metrics updates', (done) => {
        wsClient.on('message', (data) => {
            const metrics = JSON.parse(data.toString());
            expect(metrics).to.have.property('uptime');
            expect(metrics).to.have.property('requests');
            expect(metrics).to.have.property('connections');
            expect(metrics).to.have.property('memory');
            expect(metrics).to.have.property('cpu');
            done();
        });
    });

    it('should increment requests counter', () => {
        const initialRequests = metricsService.metrics.requests;
        metricsService.incrementRequests();
        expect(metricsService.metrics.requests).to.equal(initialRequests + 1);
    });

    it('should track connections', (done) => {
        const newClient = new WebSocket(`ws://localhost:${server.address().port}`);
        newClient.on('open', () => {
            expect(metricsService.metrics.connections).to.equal(2);
            newClient.close();
            setTimeout(() => {
                expect(metricsService.metrics.connections).to.equal(1);
                done();
            }, 100);
        });
    });
});

================
File: tests_pending/unit/http-server_ShutdownService.spec.js
================
import { expect } from 'chai';
import express from 'express';
import ShutdownService from '../../src/processors/http/services/ShutdownService.js';

describe('ShutdownService', () => {
    let app;
    let shutdownService;
    let shutdownCalled = false;

    beforeEach(() => {
        app = express();
        shutdownService = new ShutdownService();
        shutdownService.setupMiddleware(app);
        shutdownService.setupEndpoints(app, () => { shutdownCalled = true; });
    });

    it('should reject requests without auth', (done) => {
        const mockReq = { headers: {} };
        const mockRes = {
            setHeader: jasmine.createSpy('setHeader'),
            status: function (code) {
                expect(code).toBe(401);
                return { send: function () { } };
            }
        };

        app._router.handle(mockReq, mockRes, () => { });
        expect(mockRes.setHeader).toHaveBeenCalledWith('WWW-Authenticate', 'Basic');
        done();
    });

    it('should accept valid credentials', (done) => {
        const credentials = Buffer.from(`${shutdownService.username}:${shutdownService.password}`).toString('base64');
        const mockReq = {
            headers: {
                authorization: `Basic ${credentials}`
            }
        };
        const mockRes = {
            status: jasmine.createSpy('status'),
            send: jasmine.createSpy('send')
        };
        const nextSpy = jasmine.createSpy('next');

        app._router.handle(mockReq, mockRes, nextSpy);
        expect(nextSpy).toHaveBeenCalled();
        done();
    });
});

================
File: tests_pending/unit/markmap.spec..js
================
import MarkMap from '../../../src/applications/markmap/processors/MarkMap.js';
import { expect } from 'chai';

describe('MarkMap', () => {
    let markMap;
    
    beforeEach(() => {
        markMap = new MarkMap({});
    });

    it('should transform markdown to HTML and SVG', async () => {
        const message = {
            filepath: '/test/example.md',
            content: '# Test Heading\n## Subheading\n* Item 1\n* Item 2'
        };

        let htmlMessage, svgMessage;

        markMap.on('message', (msg) => {
            if (msg.filepath.endsWith('.mm.html')) {
                htmlMessage = msg;
            } else if (msg.filepath.endsWith('.mm.svg')) {
                svgMessage = msg;
            }
        });

        await markMap.process(message);

        expect(htmlMessage).to.exist;
        expect(htmlMessage.content).to.include('<html');
        expect(htmlMessage.content).to.include('Test Heading');
        expect(htmlMessage.filepath).to.equal('/test/example.mm.html');

        expect(svgMessage).to.exist;
        expect(svgMessage.content).to.include('<svg');
        expect(svgMessage.content).to.include('Test Heading');
        expect(svgMessage.filepath).to.equal('/test/example.mm.svg');
    });

    it('should handle empty content', async () => {
        const message = {
            filepath: '/test/empty.md',
            content: ''
        };

        try {
            await markMap.process(message);
            expect.fail('Should have thrown error');
        } catch (error) {
            expect(error.message).to.equal('No content provided in message');
        }
    });
});

================
File: tests_pending/unit/NOP.spec.js
================
import NOP from '../../src/processors/util/NOP.js'
import { expect } from 'chai'

describe('NOP', function () {
    it('double() should return the input string concatenated with itself', function () {
        const nop = new NOP()
        const input = 'test'
        const expectedOutput = 'testtest'
        const output = nop.double(input)
        expect(output).to.equal(expectedOutput)
    })
})

================
File: tests_pending/unit/PostcraftPrep.spec.js
================
import PostcraftPrep from '../../src/processors/postcraft/PostcraftPrep.js'
import { expect } from 'chai'

describe('PostcraftPrep', function () {
    beforeEach(function () {
        this.context = {
            content: 'only text',
            filename: 'minimal-filename.md'
        }
    })

    it('extractTitle(context) should lift the title from the filename', function () {
        this.context.filename = '2024-05-10_this-thing.md'
        const input = this.context
        const expectedOutput = 'This Thing'
        const pp = new PostcraftPrep()
        const output = pp.extractTitle(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractSlug(context) should return filename without path and extension', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        const input = this.context
        const expectedOutput = '2024-05-10_hello-postcraft'
        const pp = new PostcraftPrep()
        const output = pp.extractSlug(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractTargetFilename(context) should return the correct target filename', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        this.context.rootDir = '/root'
        this.context.entryContentMeta = {
            targetDir: 'target'  // Remove the leading slash
        }
        const input = this.context
        const expectedOutput = '/root/target/2024-05-10_hello-postcraft.html'
        const pp = new PostcraftPrep()
        const output = pp.extractTargetFilename(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractDates(context) should return the correct dates', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        const input = this.context
        const expectedOutput = { created: '2024-05-10', updated: (new Date()).toISOString().split('T')[0] }
        const pp = new PostcraftPrep()
        const output = pp.extractDates(input)
        expect(output).to.deep.equal(expectedOutput)
    })
})

================
File: tests_pending/unit/ProcessorSettings.spec.js
================
import { expect } from 'chai'
import rdf from 'rdf-ext'
import ProcessorSettings from '../../src/processors/base/ProcessorSettings.js'
import ns from '../../src/utils/ns.js'

describe('ProcessorSettings', () => {
    let settings
    let config

    beforeEach(() => {
        const dataset = rdf.dataset()
        config = { dataset }
        settings = new ProcessorSettings(config)
    })

    function addTestData(subject, predicates) {
        const subjectTerm = rdf.namedNode(`http://example.org/${subject}`)
        config.dataset.add(rdf.quad(
            subjectTerm,
            ns.rdf.type,
            ns.trn.ConfigSet
        ))

        for (const [pred, values] of Object.entries(predicates)) {
            if (Array.isArray(values)) {
                values.forEach(value => {
                    config.dataset.add(rdf.quad(
                        subjectTerm,
                        ns.trn[pred],
                        rdf.literal(value)
                    ))
                })
            } else {
                config.dataset.add(rdf.quad(
                    subjectTerm,
                    ns.trn[pred],
                    rdf.literal(values)
                ))
            }
        }
        return subjectTerm
    }

    describe('getValues()', () => {
        it('should return array with single value when one exists', () => {
            const subject = addTestData('config', {
                testProp: 'value1'
            })
            settings.settingsNode = subject

            const values = settings.getValues(ns.trn.testProp)
            expect(values).to.be.an('array').with.lengthOf(1)
            expect(values[0]).to.equal('value1')
        })

        it('should return array with multiple individual values', () => {
            const subject = addTestData('config', {
                excludePattern: ['value1', 'value2', 'value3']
            })
            settings.settingsNode = subject

            const values = settings.getValues(ns.trn.excludePattern)
            expect(values).to.be.an('array').with.lengthOf(3)
            expect(values).to.include('value1')
            expect(values).to.include('value2')
            expect(values).to.include('value3')
        })

        it('should handle comma-separated values', () => {
            const subject = addTestData('config', {
                excludePatterns: 'value1,value2, value3'
            })
            settings.settingsNode = subject

            const values = settings.getValues(ns.trn.excludePattern)
            expect(values).to.be.an('array').with.lengthOf(3)
            expect(values).to.include('value1')
            expect(values).to.include('value2')
            expect(values).to.include('value3')
        })

        it('should handle empty or undefined settingsNode', () => {
            settings.settingsNode = null
            const values = settings.getValues(ns.trn.testProp)
            expect(values).to.be.an('array').with.lengthOf(0)
        })

        it('should handle values from referenced settings', () => {
            const refSubject = addTestData('ref', {
                testProp: ['refValue1', 'refValue2']
            })

            const mainSubject = addTestData('config', {})
            config.dataset.add(rdf.quad(
                mainSubject,
                ns.trn.settings,
                refSubject
            ))

            settings.settingsNode = mainSubject
            const values = settings.getValues(ns.trn.testProp)
            expect(values).to.be.an('array').with.lengthOf(2)
            expect(values).to.include('refValue1')
            expect(values).to.include('refValue2')
        })

        it('should return fallback in array when no values exist', () => {
            const subject = addTestData('config', {})
            settings.settingsNode = subject

            const values = settings.getValues(ns.trn.testProp, 'fallback')
            expect(values).to.be.an('array').with.lengthOf(1)
            expect(values[0]).to.equal('fallback')
        })
    })

    describe('getValue()', () => {
        it('should return first value when multiple exist', () => {
            const subject = addTestData('config', {
                testProp: ['value1', 'value2']
            })
            settings.settingsNode = subject

            const value = settings.getValue(ns.trn.testProp)
            expect(value).to.equal('value1')
        })

        it('should return fallback when no values exist', () => {
            const subject = addTestData('config', {})
            settings.settingsNode = subject

            const value = settings.getValue(ns.trn.testProp, 'fallback')
            expect(value).to.equal('fallback')
        })

        it('should handle undefined settingsNode', () => {
            settings.settingsNode = null
            const value = settings.getValue(ns.trn.testProp, 'fallback')
            expect(value).to.equal('fallback')
        })
    })

    describe('Integration', () => {
        it('should handle mixed configurations', () => {
            const subject = addTestData('config', {
                directValue: 'direct',
                commaPattern: 'one,two,three',
                multiValue: ['a', 'b', 'c']
            })
            settings.settingsNode = subject

            expect(settings.getValues(ns.trn.directValue)).to.have.lengthOf(1)
            expect(settings.getValues(ns.trn.multiValue)).to.have.lengthOf(3)
            expect(settings.getValue(ns.trn.commaPattern).split(','))
                .to.have.lengthOf(3)
        })
    })
})

================
File: tests_pending/unit/RunCommand.spec.js
================
import RunCommand from '../../../src/processors/unsafe/RunCommand.js'
import { expect } from 'chai'
import fs from 'fs/promises'
import path from 'path'

describe('RunCommand', function () {
    let runCommand
    const workingDir = 'src/applications/test_runcommand/data'

    beforeEach(function () {
        jasmine.DEFAULT_TIMEOUT_INTERVAL = 3000
        runCommand = new RunCommand({
            simples: true,
            allowedCommands: ['echo', 'ls'],
            blockedPatterns: ['rm', '|', ';'],
            timeout: 50  // 50ms timeout
        })
    })

    it('should validate command output against required file', async function () {
        const requiredPath = path.join(workingDir, 'output', 'required-01.txt')
        const required = await fs.readFile(requiredPath, 'utf8')
        const message = { command: 'echo "Hello from RunCommand!"' }

        const result = await runCommand.process(message)
        expect(result.content.trim()).to.equal(required.trim())
    })

    it('should handle timeouts', async function () {
        // Create an infinitely running command
        const neverEndingCommand = `echo "test" && while true; do :; done`
        try {
            await runCommand.executeCommand(neverEndingCommand)
            expect.fail('Should have timed out')
        } catch (error) {
            expect(error.message).to.equal('Command timeout')
        }
    })

    it('should block disallowed commands', async function () {
        const message = { command: 'rm -rf /' }
        try {
            await runCommand.process(message)
            expect.fail('Should have blocked dangerous command')
        } catch (error) {
            expect(error.message).to.include('not in allowed list')
        }
    })

    it('should block commands with dangerous patterns', async function () {
        const message = { command: 'echo "test" | grep test' }
        try {
            await runCommand.process(message)
            expect.fail('Should have blocked command with pipe')
        } catch (error) {
            expect(error.message).to.include('blocked pattern')
        }
    })
})

================
File: tests_pending/unit/StringFilter.spec.js
================
// tests/unit/StringFilter.spec.js

/**
 * This test file covers various scenarios for the StringFilter processor, including:
 *
 * empty include and exclude patterns
 * undefined content
 * include patterns separately
 * include patterns separately
 * combinations of include and exclude patterns
 *
 * The tests use a variety of filesystem paths and glob-like patterns to ensure comprehensive coverage of the StringFilter's functionality.
 * 
 * Run in isolation with :
 * `npm test -- tests/unit/StringFilter.spec.js`
 */

import StringFilter from '../../src/processors/text/StringFilter.js';
import { expect } from 'chai';

describe('StringFilter', function () {
    // Helper function to create test messages
    function compose(content, include, exclude) {
        return { content, include, exclude };
    }

    // Sample data
    const contentSamples = [
        '/home/user/documents/',
        '/home/user/documents/file.txt',
        '/var/log/',
        '/etc/config.conf',
        '/usr/local/bin/app',
        '/home/user/pictures/vacation/',
        '/home/user/pictures/vacation/photo.jpg',
        '/opt/',
        '/tmp/temp.file',
        '/home/user/.config/',
        '',
        undefined
    ];

    const patternSamples = [
        '*.txt',
        '*.jpg',
        '/home/user/*',
        '/var/*',
        '*/bin/*',
        ['*.txt', '*.jpg'],
        ['/home/user/*', '/var/*'],
        ['*/bin/*', '*.conf'],
        ['*.file', '/tmp/*'],
        ['/opt/*', '/etc/*'],
        '',
        [],
        undefined
    ];

    describe('isAccepted()', function () {
        it('should accept all content when include and exclude are empty', function () {
            const filter = new StringFilter();
            contentSamples.forEach(content => {
                if (content !== undefined) {
                    const message = compose(content, '', '');
                    expect(filter.isAccepted(message.content, message.exclude, message.include)).to.be.true;
                }
            });
        });

        it('should reject undefined content', function () {
            const filter = new StringFilter();
            const message = compose(undefined, '', '');
            expect(filter.isAccepted(message.content, message.exclude, message.include)).to.be.false;
        });

        it('should correctly apply include patterns', function () {
            const filter = new StringFilter();
            const includeTests = [
                { content: '/home/user/documents/file.txt', include: '*.txt', expected: true },
                { content: '/home/user/pictures/vacation/photo.jpg', include: '*.jpg', expected: true },
                { content: '/var/log/', include: '/var/*', expected: true },
                { content: '/home/user/documents/', include: '/home/user/*', expected: true },
                { content: '/usr/local/bin/app', include: '*/bin/*', expected: true },
                { content: '/etc/config.conf', include: ['*.conf', '*.txt'], expected: true },
                { content: '/opt/', include: ['/var/*', '/opt/*'], expected: true },
                { content: '/tmp/temp.file', include: '*.doc', expected: false }
            ];

            includeTests.forEach(test => {
                const message = compose(test.content, test.include, '');
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });

        it('should correctly apply exclude patterns', function () {
            const filter = new StringFilter();
            const excludeTests = [
                { content: '/home/user/documents/file.txt', exclude: '*.txt', expected: false },
                { content: '/home/user/pictures/vacation/photo.jpg', exclude: '*.jpg', expected: false },
                { content: '/var/log/', exclude: '/var/*', expected: false },
                { content: '/home/user/documents/', exclude: '/home/user/*', expected: false },
                { content: '/usr/local/bin/app', exclude: '*/bin/*', expected: false },
                { content: '/etc/config.conf', exclude: ['*.conf', '*.txt'], expected: false },
                { content: '/opt/', exclude: ['/var/*', '/tmp/*'], expected: true },
                { content: '/tmp/temp.file', exclude: '*.doc', expected: true }
            ];

            excludeTests.forEach(test => {
                const message = compose(test.content, '', test.exclude);
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });

        it('should correctly apply both include and exclude patterns', function () {
            const filter = new StringFilter();
            const combinedTests = [
                { content: '/home/user/documents/file.txt', include: '*.txt', exclude: '/var/*', expected: true },
                { content: '/var/log/system.log', include: '*.log', exclude: '/var/*', expected: false },
                { content: '/home/user/pictures/vacation/photo.jpg', include: ['/home/user/*', '*.jpg'], exclude: '*.png', expected: true },
                { content: '/etc/config.conf', include: ['*.conf', '*.txt'], exclude: ['/home/*', '/var/*'], expected: true },
                { content: '/usr/local/bin/app', include: '*/bin/*', exclude: '*/local/*', expected: false }
            ];

            combinedTests.forEach(test => {
                const message = compose(test.content, test.include, test.exclude);
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });
    });
});

================
File: tests_pending/unit/StringReplace.spec.js
================
// tests/unit/StringReplace.spec.js

import StringReplace from '../../src/processors/text/StringReplace.js'
import { expect } from 'chai'

/**
 * Unit tests for the StringReplace processor
 */
describe('StringReplace', function () {
    /**
     * Test case for the execute method of StringReplace
     */
    it('execute() should replace all occurrences of the match string with the replace string', function () {
        // Arrange
        const stringReplace = new StringReplace()
        const message = {
            content: 'Hello world! Hello universe!',
            match: 'Hello',
            replace: 'Hi'
        }

        // Act
        stringReplace.process(message)

        // Assert
        const expectedOutput = 'Hi world! Hi universe!'
        expect(message.content).to.equal(expectedOutput)
    })

    /**
     * Test case for when the match string is not found in the content
     */
    it('execute() should not modify the content if the match string is not found', function () {
        // Arrange
        const stringReplace = new StringReplace()
        const message = {
            content: 'Hello world!',
            match: 'Goodbye',
            replace: 'Hi'
        }

        // Act
        stringReplace.process(message)

        // Assert
        const expectedOutput = 'Hello world!'
        expect(message.content).to.equal(expectedOutput)
    })

    /**
     * Test case for when the content is an empty string
     */
    it('execute() should handle empty content string', function () {
        // Arrange
        const stringReplace = new StringReplace()
        const message = {
            content: '',
            match: 'Hello',
            replace: 'Hi'
        }

        // Act
        stringReplace.process(message)

        // Assert
        const expectedOutput = ''
        expect(message.content).to.equal(expectedOutput)
    })
})

================
File: tests_pending/unit/test.settings.spec.js
================
import { expect } from 'chai'
import rdf from 'rdf-ext'
import TestSettings from '../../src/processors/test/TestSettings.js'
import ns from '../../src/utils/ns.js'

describe('TestSettings', () => {
    let settings
    let config
    let dataset

    beforeEach(() => {
        dataset = rdf.dataset()
        config = { dataset }
        settings = new TestSettings(config)
    })

    function addTestData(subject, predicates) {
        const subjectTerm = rdf.namedNode(`http://example.org/${subject}`)
        config.dataset.add(rdf.quad(
            subjectTerm,
            ns.rdf.type,
            ns.trn.ConfigSet
        ))

        for (const [pred, values] of Object.entries(predicates)) {
            if (Array.isArray(values)) {
                values.forEach(value => {
                    config.dataset.add(rdf.quad(
                        subjectTerm,
                        ns.trn[pred],
                        rdf.literal(value)
                    ))
                })
            } else {
                config.dataset.add(rdf.quad(
                    subjectTerm,
                    ns.trn[pred],
                    rdf.literal(values)
                ))
            }
        }
        return subjectTerm
    }

    describe('process()', () => {
        it('should process message with direct settings', async () => {
            const subject = addTestData('test1', {
                name: 'Test Name',
                value: '42'
            })
            settings.settingsNode = subject

            const message = {}
            const result = await settings.process(message)
            
            expect(result).to.exist
            const name = settings.getProperty(ns.trn.name)
            expect(name).to.equal('Test Name')
        })

        it('should handle settings with multiple values', async () => {
            const subject = addTestData('test2', {
                setting: ['value1', 'value2', 'value3']
            })
            settings.settingsNode = subject

            const message = {}
            const result = await settings.process(message)
            
            const values = settings.getValues(ns.trn.setting)
            expect(values).to.have.length(3)
            expect(values).to.include('value1')
        })

        it('should handle message without settings', async () => {
            const message = {}
            const result = await settings.process(message)
            expect(result).to.exist
        })

        it('should preserve message properties', async () => {
            const subject = addTestData('test3', {
                name: 'Test'
            })
            settings.settingsNode = subject

            const message = {
                existingProp: 'value'
            }
            const result = await settings.process(message)
            
            expect(result.existingProp).to.equal('value')
        })
    })
})

================
File: tests_pending/unit/updated-shutdown-test.js
================
import { expect } from 'chai';
import express from 'express';
import jwt from 'jsonwebtoken';
import ShutdownService from '../../src/processors/http/services/ShutdownService.js';

describe('ShutdownService', () => {
    let app;
    let shutdownService;
    let shutdownCalled = false;

    beforeEach(() => {
        app = express();
        app.use(express.json());
        shutdownService = new ShutdownService();
        shutdownService.setupMiddleware(app);
        shutdownService.setupEndpoints(app, () => { shutdownCalled = true; });
    });

    it('should generate valid JWT tokens', (done) => {
        const token = shutdownService.generateToken();
        const decoded = jwt.verify(token, shutdownService.secret);
        expect(decoded).to.have.property('action', 'shutdown');
        done();
    });

    it('should require valid token for shutdown', (done) => {
        const validToken = shutdownService.generateToken();
        
        // Mock request/response objects
        const mockReq = {
            headers: { authorization: `Bearer ${validToken}` }
        };
        const mockRes = {
            status: function(code) {
                return { send: function(msg) {} };
            }
        };
        const nextSpy = jasmine.createSpy('next');

        app._router.handle(mockReq, mockRes, nextSpy);
        expect(nextSpy).toHaveBeenCalled();
        done();
    });

    it('should reject expired tokens', (done) => {
        const expiredToken = jwt.sign(
            { action: 'shutdown' },
            shutdownService.secret,
            { expiresIn: '0s' }
        );

        setTimeout(() => {
            const mockReq = {
                headers: { authorization: `Bearer ${expiredToken}` }
            };
            const mockRes = {
                status: function(code) {
                    expect(code).toBe(403);
                    return { 
                        send: function(msg) {
                            expect(msg).toBe('Invalid token');
                        }
                    };
                }
            };

            app._router.handle(mockReq, mockRes, () => {});
            done();
        }, 100);
    });
});

================
File: types/grapoi.d.ts
================
import { DatasetCore, Quad, Term } from "@rdfjs/types";

// Grapoi interface
interface Grapoi extends PathList {
    addList(predicates: Term | Term[], items: Term | Term[]): Grapoi;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): Grapoi;
    base(base: Term | Term[]): Grapoi;
}

// Edge interface
interface Edge {
    dataset: DatasetCore;
    end: string;
    quad: Quad;
    start: string;
    term: Term;
    graph: Term;
    startTerm: Term;
}

// Instruction interface
interface Instruction {
    operation?: string;
    quantifier?: string;
    start?: string;
    end?: string;
    subjects?: Term[];
    predicates?: Term[];
    objects?: Term[];
    graphs?: DatasetCore[];
    items?: Term[];
    callback?: (edge: Edge, ptr: Path | PathList) => Path | PathList;
}

// Path interface
interface Path {
    addList(predicates: Term | Term[], items: Term | Term[]): Path;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): Path;
    deleteIn(predicates: Term | Term[], subjects: Term | Term[]): Path;
    deleteList(predicates: Term | Term[]): Path;
    deleteOut(predicates: Term | Term[], objects: Term | Term[]): Path;
    extend(edge: Edge): Path;
    execute(instruction: Instruction): Path;
}

// PathList interface
interface PathList {
    addList(predicates: Term | Term[], items: Term | Term[]): PathList;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): PathList;
    deleteIn(predicates: Term | Term[], subjects: Term | Term[]): PathList;
    deleteList(predicates: Term | Term[]): PathList;
    deleteOut(predicates: Term | Term[], objects: Term | Term[]): PathList;
    distinct(): PathList;
    in(predicates: Term | Term[], subjects: Term | Term[]): PathList;
    isAny(): boolean;
    isList(): boolean;
    list(): Iterator<Term> | undefined;
    map(callback: Function): PathList[];
    out(predicates: Term | Term[], objects: Term | Term[]): PathList;
    quads(): Iterator<Quad>;
    execute(instruction: Instruction): PathList[];
}

================
File: types/processor.d.ts
================
import { Term, Dataset, NamedNode } from '@rdfjs/types';
import { EventEmitter } from 'events';

export interface ProcessorConfig {
    dataset?: Dataset;
    [key: string]: any;
}

export interface ProcessorMessage {
    content?: any;
    filepath?: string;
    done?: boolean;
    tags?: string;
    [key: string]: any;
}

export interface ProcessorSettings {
    config: ProcessorConfig;
    settingsNode: Term | null;
    getValues(property: Term, fallback?: any): string[];
    getValue(property: Term, fallback?: any): string | undefined;
}

export interface IProcessor {
    config: ProcessorConfig;
    settings: ProcessorSettings;
    messageQueue: { message: ProcessorMessage }[];
    processing: boolean;
    outputs: any[];
    settingsNode?: Term;
    message?: ProcessorMessage;

    getValues(property: Term, fallback?: any): string[];
    getProperty(property: Term, fallback?: any): string | undefined;
    preProcess(message: ProcessorMessage): Promise<void>;
    postProcess(message: ProcessorMessage): Promise<void>;
    process(message: ProcessorMessage): Promise<void>;
    receive(message: ProcessorMessage): Promise<void>;
    enqueue(message: ProcessorMessage): Promise<void>;
    executeQueue(): Promise<void>;
    emit(event: string, message: ProcessorMessage): Promise<ProcessorMessage>;
    getOutputs(): any[];
}

export interface StringFilterConfig extends ProcessorConfig {
    includePatterns?: string[];
    excludePatterns?: string[];
}

export interface IStringFilter extends IProcessor {
    initialized: boolean;
    includePatterns: string[];
    excludePatterns: string[];
    initialize(): Promise<void>;
    matchPattern(filePath: string, pattern: string): boolean;
    isAccepted(filePath: string): boolean;
}

================
File: _README.md
================
# transmissions

After _No Code_ and _Lo Code_ comes _Marginally Less Code_

**Transmissions** is a micro-framework intended to simplify construction of small pipeliney data processing applications in JavaScript (assuming you are already familiar with JavaScript and RDF).

The code is in active development, ie. **not stable**, subject to arbitrary changes.

A bit like `make` or a `package.json` builder. But much harder work (and fun).

Applications are defined in several places, the bits of interest are eg. Postcraft's [transmissions.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/transmissions.ttl) and [services.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/services.ttl).
The former defines the flow, the latter config of the services (under [src/services](https://github.com/danja/transmissions/tree/main/src/services)). The runtime instance of the application is given in the target [manifest.ttl](https://github.com/danja/postcraft/blob/main/danny.ayers.name/manifest.ttl).

### Installation etc.

This is not ready yet. But if you really must...

Make a fresh dir. Clone this repo and [Postcraft](https://github.com/danja/postcraft) into it.

```
cd transmissions
npm i
```

This may or may not work :

```
npm run test
```

Then if you do :

```
./trans postcraft /home/danny/github-danny/postcraft/danny.ayers.name
```

it may build a site (my blog - this is dogfooding to the max) under `public/home`

```
./trans
```

on its own should list the applications available. Most of these won't work, the code has been shapeshifting a lot.

### Status

**2024-09-02** Getting used as a serrrrriously over-engineered, feature-lacking static site builder, proof of concept is [Postcraft](https://github.com/danja/postcraft), as evinced by my [blog](https://danny.ayers.name/) (where, for now at least you will find update on this). But it mostly works as intended. Docs lagging. But now I have a documentation engine...

Documentation will be lagging behind code, be incomplete and out of date.

**2024-03-24** : a couple of simple data processing pipelines working and wired up as Jasmine e2e tests in place; started to develop actually useful pipelines for [foaf-archive](https://github.com/danja/foaf-archive) project

## Motivation

This was prompted by something of a ~~perfect storm~~ serendipitous convergence of ideas, requirements and environment.

- I had the realization that much of the software I want to write in the near future has the same basic high level **architecture** : pipeline-like, ie. a series of steps each of which transform data from one form into another ([Wikipedia](<https://en.wikipedia.org/wiki/Pipeline_(software)>)).
- **Pipelines** are approachable using concepts from Design Patterns like [Dependency Injection](https://en.wikipedia.org/wiki/Dependency_injection), [Inversion of control](https://en.wikipedia.org/wiki/Inversion_of_control) etc. A key benefit of such patterns is that, because the components are loosely coupled, they can largely be developed in isolation as small standalone units before integration into a larger application.
- **Large Language Models** currently struggle with large codebases and complex architectures (mostly due to [context window](https://en.wikipedia.org/wiki/Large_language_model#Prompt_engineering,_attention_mechanism,_and_context_window) limitations). But they are very good at building small, simple, self-contained units of code.

reused in different contexts.

_I'm writing this with GitHub Copilot enabled for markdown, which can be helpful, but sometimes it goes off on one... After the first point above it offered :_

```
This is true of a blog engine, a personal knowledge management system, a chatbot, a mail archive miner, a web scraper, a data visualizer, a data importer/exporter, a data transformer, a data validator, a data filter, a data aggregator, a data splitter, a data joiner, a data normalizer, a data summarizer, a data classifier, a data indexer, a data search engine, a data storage system, a data retrieval system, a data synchronization system, a data backup system, a data migration system, a data versioning system, a data sharing system, a data access control system, a data privacy system, a data security system, a data integrity system, a data quality system, a data governance system, a data provenance system, a data lineage system, a data lifecycle system, a data archiving system, a data retention system, a data deletion system, a data recovery system, a data discovery system, a data exploration system, a data analysis system, a data visualization system, a data reporting system, a data monitoring system, a data alerting system, a data forecasting system, a data prediction system, a data recommendation system, a data personalization system, a data optimization system, a data simulation system, a data modeling system, a data inference system, a data learning system, a data reasoning system, a data decision system, a data planning system, a data execution system, a data monitoring system, a data feedback system, a data control system, a data automation system, a data orchestration system, a data integration system, a data federation system, a data interoperability system, a data exchange system, a data transformation system, a data migration system, a data synchronization system, a data replication system, a data federation system, a data virtualization system, a data streaming system, a data batch system, a data real-time system, a data near-real-time system, a data historical system, a data temporal system, a data spatial system, a data geospatial system, a data network system, a data graph system, a data tree system, a data list system, a data set system, a data table system, a data document system, a data record system, a data entity system, a data attribute system, a data value system, a data type system, a data format system, a data encoding system, a data serialization
```

_... which is a little over the top, but it's a fair list of things that might be helped by a micro-framework like this. Remarkably the first 5 are pretty much my immediate intended uses for this._

**Docs below need updating**

for later :

The [Community Solid Server](https://github.com/CommunitySolidServer/CommunitySolidServer) uses [Components.js](https://componentsjs.readthedocs.io/en/latest/) to specify how modules and components need to be wired together at runtime. Components.js is a dependency injection framework for JavaScript applications.

## What it is

A low-level system for data processing pipelines/networks. Wherever convenient functionality will be defined declaratively with JSON-LD configuration files.

Dependency injection is used internally to allow loose coupling of components.

## What it isn't

There are several sophisticated frameworks for building interfaces between software applications and creating data processing networks. NodeRed, NoFlo etc. This is not one of them. This is much more basic and bare bones, down in the details.

See also [David Booth](https://github.com/dbooth-boston)'s [RDF Pipeline Framework](https://github.com/rdf-pipeline)

_I do eventually want to use this with NodeRed or whatever, but the entities created by transmissions will be at the level of nodes in such networks, not the network itself._

## Motivation

I'm in the process of writing yet another blog engine (Postcraft). I've also started working on a playground for interconnecting intelligent agents in an XMPP multiuser chat environment (Kia). I'm also revising a system for managing a personal knowledge base in the world of LLMs (HKMS). These all share functionality around connectivity to external data/messaging systems and internal data transformation. Might as well write this bit once only, and avoid thinking about software architecture more than I have to.

### Goals

To facilate :

- rapid development of small applications
- reuse of components in a loosely-couple environment
- versatility

### Soft Goals

- performance - low on the list
- scalability - ditto
- security - ditto

================
File: .babelrc
================
{
  "plugins": ["@babel/syntax-dynamic-import"],
  "presets": [
    [
      "@babel/preset-env",
      {
        "modules": false
      }
    ]
  ]
}

================
File: .gitignore
================
**/src-old
**/*\ copy.js
**/*repomix*.md
**/*terrapack*.md

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

================
File: .npmignore
================
**/src-old
**/_*
**/*\ copy.js
**/*repomix*.md
**/*terrapack*.md

================
File: delete-triples.sh
================
echo -e "\nDeleting data from ..."
curl -X POST http://localhost:4030/test-mem/update \
  -H "Authorization: Basic $(echo -n 'admin:admin123' | base64)" \
  -H "Content-Type: application/sparql-update" \
  -H "Accept: application/sparql-results+json" \
  --data "DELETE { ?s ?p ?o} WHERE { ?s ?p ?o }"

================
File: jsconfig.json
================
{
  "compilerOptions": {
    "target": "ES6",
    "module": "commonjs",
    "allowSyntheticDefaultImports": true,
    "baseUrl": "./",
    "paths": {
      "*": ["node_modules/*", "types/*"]
    }
  },
  "include": [
    "src/**/*",
    "src/api/cli/run.js",
    "../trans-apps/applications/git-apps/github_",
    "src-old/CommandUtilsNO.js",
    "../trans-apps/applications/markmap",
    "_old/GrapoiHelpers copy.js",
    "_old/claudeGrapoiHelpers copy.js",
    "_old/__ConfigMap.js"
  ],
  "exclude": ["node_modules", "**/node_modules/*"],
  "typeAcquisition": {
    "include": ["@rdfjs/types", "grapoi"]
  }
}

================
File: LICENSE
================
MIT License

Copyright (c) 2024 Danny Ayers

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: package.json
================
{
  "type": "module",
  "main": "src/cli/run.js",
  "version": "1.0.0",
  "description": "Transmissions Dataflow Driver",
  "name": "transmissions",
  "author": "Danny Ayers <danny.ayers@gmail.com> (https://danny.ayers.name)",
  "license": "MIT",
  "scripts": {
    "test": "jasmine --config=config/jasmine.json --reporter=tests/tests-support/helpers/reporter.js",
    "cov": "nyc -a --include=src --reporter=lcov npm run test",
    "docs": "jsdoc -c config/jsdoc.json",
    "build": "webpack --mode=production --node-env=production",
    "build:dev": "webpack --mode=development",
    "build:prod": "webpack --mode=production --node-env=production",
    "rp-no": "node --no-warnings $(which repomix) -c config/repomix.config-small.json . && node --no-warnings $(which repomix) -c config/repomix.config-large.json . && node --no-warnings $(which repomix) -c config/repomix.config-docs.json .",
    "rp": "repomix -c config/repomix.config-large.json .",
    "watch": "webpack --watch",
    "serve": "webpack serve"
  },
  "nyc": {
    "report-dir": "spec/coverage",
    "exclude": [
      "spec/**/*"
    ]
  },
  "devDependencies": {
    "@babel/core": "^7.23.7",
    "@babel/preset-env": "^7.23.8",
    "autoprefixer": "^10.4.17",
    "babel-loader": "^9.1.3",
    "chai": "^5.0.3",
    "css-loader": "^6.9.1",
    "html-webpack-plugin": "^5.6.0",
    "jasmine": "^5.1.0",
    "jasmine-browser-runner": "^2.3.0",
    "jasmine-core": "^5.1.1",
    "jasmine-spec-reporter": "^7.0.0",
    "jsdoc": "^4.0.2",
    "mini-css-extract-plugin": "^2.7.7",
    "nyc": "^17.1.0",
    "postcss": "^8.4.33",
    "postcss-loader": "^8.0.0",
    "prettier": "^3.2.4",
    "style-loader": "^3.3.4",
    "webpack": "^5.90.0",
    "webpack-cli": "^5.1.4",
    "webpack-dev-server": "^4.15.1",
    "workbox-webpack-plugin": "^7.0.0"
  },
  "dependencies": {
    "@dotenvx/dotenvx": "^1.14.2",
    "@elicdavis/node-flow": "^0.1.1",
    "@rdfjs/formats": "^4.0.0",
    "@rdfjs/parser-n3": "^2.0.2",
    "axios": "^1.6.8",
    "cheerio": "^1.0.0-rc.12",
    "cors": "^2.8.5",
    "d3": "^7.9.0",
    "ignore": "^7.0.0",
    "jsdom": "^25.0.0",
    "lodash": "^4.17.21",
    "loglevel": "^1.9.2",
    "marked": "^12.0.1",
    "marked-code-format": "^1.1.6",
    "marked-custom-heading-id": "^2.0.10",
    "marked-footnote": "^1.2.4",
    "markmap-lib": "^0.18.9",
    "markmap-render": "^0.17.0",
    "markmap-toolbar": "^0.17.0",
    "markmap-view": "^0.17.0",
    "node-mime-types": "^1.1.2",
    "nunjucks": "^3.2.4",
    "queue": "^7.0.0",
    "rdf-ext": "^2.5.2",
    "rdf-utils-fs": "^3.0.0",
    "repomix": "^0.2.12",
    "string-to-stream": "^3.0.1",
    "yargs": "^17.7.2"
  }
}

================
File: postcraft.sh
================
cd ~/hyperdata/transmissions # my local path

#./trans md-to-sparqlstore ~/sites/strandz.it/postcraft
#./trans postcraft-statics ~/sites/strandz.it/postcraft
#./trans sparqlstore-to-html ~/sites/strandz.it/postcraft
#./trans sparqlstore-to-site-indexes ~/sites/strandz.it/postcraft

./trans md-to-sparqlstore ~/sites/danny.ayers.name/postcraft
./trans postcraft-statics ~/sites/danny.ayers.name/postcraft
./trans sparqlstore-to-html ~/sites/danny.ayers.name/postcraft
./trans sparqlstore-to-site-indexes ~/sites/danny.ayers.name/postcraft

================
File: README.md
================
# Transmissions

_Wait for the docs_

**2025-03-29** mostly where I want it for v1, but needs a **lot** of cleaning up. Publishing now to npm registry so I don't have to rename later.

_2025-02-09_ fixing up, aiming for a release (that works away from my own desktop) soon

================
File: tbox.sh
================
sudo systemctl stop tbox
cd ~/hyperdata/tbox # my local dir
docker-compose down
docker-compose up -d

================
File: trans
================
#!/bin/bash

# use 'chmod +x run' to make this executable

# Execute the Node.js script with Node
node src/api/cli/run.js "$@"



================================================================
End of Codebase
================================================================
