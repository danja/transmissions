This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-20T19:15:53.664Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
- Code comments have been removed.

Additional Info:
----------------
User Provided Header:
-----------------------
Transmissions source code

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
output/
  output-01.md
raw-src/
  markmap/
    01.js
    02.js
  old/
    old-test/
      file-copy-remove-test.spec.js
      file-pipeline.spec.js
      string-pipeline.speeec.js
    restructure/
      input/
        input_01.json
        mapping_01.json
      output/
        output_01.json
        required_01.json
      temp/
        input-01.json
        required-01.json
      compare.js
      fileHandler.js
      JsonRestructurer.js
      restructure.js
      run.js
  viz/
    test-data/
      foaf-template.ttl
    jsonld-turtle.js
    jsonld-vis.css
    jsonld-vis.js
    playground-examples.js
    repopack.config.json
    viz-turtle.js
  containsAny.js
  crawl-fs.js
  grapoi-bits.js
  postcraft-transmission.ttl
  README.md
  regex-play.js
  structure-play.js
spec/
  support/
    jasmine-browser.json
src/
  api/
    about.md
    ApplicationManager.js
    CommandUtils.js
    ModuleLoader.js
    ModuleLoaderFactory.js
    WebRunner.js
  applications/
    app-template/
      about.md
      processors-config.ttl
      transmissions.ttl
    claude-json-converter/
      data/
        input/
          input-01.json
          users.json
      about.md
      processors-config.ttl
      transmissions.ttl
    dirwalker-test/
      about.md
      processors-config.ttl
      transmissions.ttl
    env-loader-test/
      about.md
      processors-config.ttl
      transmissions.ttl
    file-copy-remove-test/
      data/
        start/
          one.txt
          two.txt
      about.md
      init.sh
      processors-config.ttl
      transmissions.ttl
    file-pipeline/
      processors-config.ttl
      transmissions.ttl
    globbo/
      about.md
      processors-config.ttl
      transmissions.ttl
    html-to-md/
      about.md
      processors-config.ttl
      transmissions.ttl
    link-lister/
      about.md
      processors-config.ttl
      transmissions.ttl
    nop-test/
      about.md
      processors-config.ttl
      transmissions.ttl
    postcraft/
      about.md
      processors-config.ttl
      transmissions.ttl
    postcraft-new/
      processors-config.ttl
      transmissions.ttl
    postcraft-previous/
      processors.ttl
      transmission.ttl
    string-pipeline/
      processors-config.ttl
      transmissions.ttl
    test_blanker/
      data/
        input/
          input-01.json
        output/
          output-01.json
          required-01.json
      about.md
      processors-config.ttl
      transmissions.ttl
    test_foreach/
      about.md
      transmissions.ttl
    test_fork/
      about.md
      processors-config.ttl
      transmissions.ttl
    test_fs-rw/
      data/
        input/
          input-01.md
        output/
          output-01.md
          required-01.md
      about.md
      processors-config.ttl
      simple.js
      transmissions.ttl
    test_multi-pipe/
      processors-config.ttl
      transmissions.ttl
    test_restructure/
      data/
        input/
          input-01.json
        output/
          output-01.json
          required-01.json
      about.md
      processors-config.ttl
      simple.js
      transmissions.ttl
    two-transmissions/
      processors-config.ttl
      transmissions.ttl
  core/
    Application.js
    Director.js
    Procurer.js
  engine/
    AbstractProcessorFactory.js
    Connector.js
    Transmission.js
    TransmissionBuilder.js
    TransmissionRunner.js
    WorkerPool.js
  processors/
    base/
      Processor.js
    flow/
      FlowProcessorsFactory.js
      ForEach.js
    fs/
      DirWalker.js
      FileCopy.js
      FileReader.js
      FileRemove.js
      FileWriter.js
      FsProcessorsFactory.js
    github/
      GitHubList_no-pag.js
      GitHubList.js
      GitHubProcessorsFactory.js
    json/
      Blanker.js
      JSONProcessorsFactory.js
      JsonRestructurer.js
      JSONWalker.js
      Restructure.js
      ValueConcat.js
    markup/
      LinkFinder.js
      MarkdownToHTML.js
      MarkupProcessorsFactory.js
      MetadataExtractor.js
    postcraft/
      EntryContentToPagePrep.js
      FrontPagePrep.js
      PostcraftDispatcher.js
      PostcraftPrep.js
      PostcraftProcessorsFactory.js
    protocols/
      HttpGet.js
      ProtocolsProcessorsFactory.js
    rdf/
      ConfigMap.js
      DatasetReader.js
      RDFProcessorsFactory.js
    staging/
      MarkdownFormatter.js
      StagingProcessorsFactory.js
      TurtleFormatter.js
    system/
      EnvLoader.js
      SystemProcessorsFactory.js
    templates/
      ProcessorsFactoryTemplate.js
      ProcessorTemplate.js
    test/
      AppendProcess.js
      FileSink.js
      FileSource.js
      StringSink.js
      StringSource.js
      TestProcessorsFactory.js
    text/
      LineReader.js
      StringFilter.js
      StringMerger.js
      StringReplace.js
      Templater.js
      TextProcessorsFactory.js
    unsafe/
      chatgpt.md
    util/
      CaptureAll.js
      DeadEnd.js
      Fork.js
      Halt.js
      NOP.js
      SetMessage.js
      ShowConfig.js
      ShowMessage.js
      ShowTransmission.js
      Stash.js
      Unfork.js
      UtilProcessorsFactory.js
      WhiteboardToMessage.js
    about.md
  simples/
    env-loader/
      about.md
      env-loader.js
    nop/
      nop.js
      simple-runner.js
    set-message/
      set-message.js
  utils/
    footpath.js
    GrapoiHelpers.js
    Logger.js
    ns.js
    t2j.js
    test_runner.js
tests/
  helpers/
    reporter.js
  integration/
    fork.spec.js
    fs-rw_simple.spec.js
    fs-rw.spec.js
    restructure_simple.spec.js
    restructure.spec.js
  unit/
    NOP.spec.js
    PostcraftPrep.spec.js
    StringFilter.spec.js
    StringReplace.spec.js
  grapoi-raw-tests.js
types/
  grapoi.d.ts
.babelrc
.gitignore
CC_2024-10-16T16:31:52.md
jasmine.json
jc.txt
jsconfig.json
jsdoc.json
LICENSE
package.json
postcss.config.js
README.md
repopack.config-large.json
repopack.config-small.json
run.js
trans
users.json
webpack.config.js

================================================================
Repository Files
================================================================

================
File: output/output-01.md
================
Hello!

================
File: raw-src/markmap/01.js
================
import { Transformer } from 'markmap-lib';
import { fillTemplate } from 'markmap-render';

const input = `# A markdown string
## another

* bullet1
* bullet2`

function expandNode(node, depth = 0) {
    const indent = '  '.repeat(depth);
    console.log(`${indent}content: ${JSON.stringify(node.content)}`);
    if (node.payload) {
        console.log(`${indent}payload: ${JSON.stringify(node.payload)}`);
    }
    if (node.children && node.children.length > 0) {
        console.log(`${indent}children:`);
        node.children.forEach(child => expandNode(child, depth + 1));
    }
}


const transformer = new Transformer();
const A = transformer.transform(input);


const B = transformer.getAssets();


const C = fillTemplate(A.root, B);







expandNode(A.root);

================
File: raw-src/markmap/02.js
================
import { Transformer } from 'markmap-lib';
import { fillTemplate } from 'markmap-render';
import fs from 'fs/promises';

const input = '# A markdown string\n## Subheading\n- List item 1\n- List item 2';

async function generateMarkmap() {

    const transformer = new Transformer();
    const { root, features } = transformer.transform(input);


    const assets = transformer.getUsedAssets(features);


    const html = fillTemplate(root, assets);


    const svgMatch = html.match(/<svg[^>]*>[\s\S]*?<\/svg>/i);
    const svg = svgMatch ? svgMatch[0] : '';

    // Save outputs
    await fs.writeFile('output.html', html);
    await fs.writeFile('output.svg', svg);

    console.log('HTML and SVG files have been generated.');
}

generateMarkmap().catch(console.error);

================
File: raw-src/old/old-test/file-copy-remove-test.spec.js
================
import { expect } from 'chai';
import { exec } from 'child_process';
import * as fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const testDir = path.resolve(__dirname, '../../src/applications/file-copy-remove-test/data');

async function setupInitialStructure() {
    const startDir = path.join(testDir, 'start');
    await fs.mkdir(startDir, { recursive: true });
    await fs.writeFile(path.join(startDir, 'one.txt'), 'Hello from One');
    await fs.writeFile(path.join(startDir, 'two.txt'), 'Hello from Two');
}

async function checkFileExists(filePath) {
    try {
        await fs.access(filePath);
        return true;
    } catch {
        return false;
    }
}

describe('file-copy-remove-test', function () {

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 30000;

    beforeAll(async function () {
        console.log('Starting beforeAll...');
        const sourceFile = path.join(testDir, 'start', 'one.txt');
        const sourceExists = await checkFileExists(sourceFile);
        console.log(`Source file exists: ${sourceExists}`);
        await setupInitialStructure();
        await Promise.all([
            clearDir('single-empty'),
            clearDir('single-full'),
            clearDir('several-empty'),
            clearDir('several-full')
        ]);
        console.log('Finished beforeAll');
    });

    async function getDirContents(dir) {
        try {
            return await fs.readdir(path.join(testDir, dir));
        } catch (error) {
            if (error.code === 'ENOENT') return [];
            throw error;
        }
    }

    async function clearDir(dir) {
        try {
            await fs.rm(path.join(testDir, dir), { recursive: true, force: true });
        } catch (error) {
            if (error.code !== 'ENOENT') throw error;
        }
    }



    it('performs file operations correctly', function (done) {
        console.log('Starting test...');
        exec('node run.js file-copy-remove-test', async (error, stdout, stderr) => {
            console.log('Exec completed');
            if (error) {
                console.error(`exec error: ${error}`);
                return done(error);
            }
            if (stderr) {
                console.error(`stderr: ${stderr}`);
            }
            console.log(`stdout: ${stdout}`);
            try {
                const singleEmpty = await getDirContents('single-empty');
                console.log('single-empty contents:', singleEmpty);
                expect(singleEmpty).to.be.empty;

                const singleFull = await getDirContents('single-full');
                console.log('single-full contents:', singleFull);
                expect(singleFull).to.deep.equal(['one.txt']);

                const severalEmpty = await getDirContents('several-empty');
                console.log('several-empty contents:', severalEmpty);
                expect(severalEmpty).to.be.empty;

                const severalFull = await getDirContents('several-full');
                console.log('several-full contents:', severalFull);
                expect(severalFull).to.have.members(['one.txt', 'two.txt']);

                console.log('All assertions passed');
                done();
            } catch (err) {
                console.error('Error in assertions:', err);
                done(err);
            }
        });
    });
});

================
File: raw-src/old/old-test/file-pipeline.spec.js
================
import footpath from '../../../src/utils/footpath.js'

import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs'


describe('file-pipeline', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, '../../')

    const dataFile = footpath.resolve(import.meta.url, '../../src/applications/file-pipeline/data/', 'output.txt')
    console.log('in file-pipeline.spec.js dataFile = ' + dataFile)
    it('produces correct output', function (done) {

        if (fs.existsSync(dataFile)) {
            fs.unlinkSync(dataFile)
        }

        exec('node run.js file-pipeline', (error, stdout, stderr) => {
            if (error) {
                console.error(`exec error: ${error}`)
                return
            }


            fs.readFile(dataFile, 'utf8', (err, data) => {
                if (err) {
                    console.error(`readFile error: ${err}`)
                    return
                }


                expect(data.trim()).to.equal('Hello world world')
                done()
            })
        })
    })
})

================
File: raw-src/old/old-test/string-pipeline.speeec.js
================
import { expect } from 'chai'
import { exec } from 'child_process'






describe('string-pipeline', function () {
    it('produces correct output', function (done) {
        exec('node run.js string-pipeline Hello', (error, stdout, stderr) => {
            if (error) {
                console.error(`exec error: ${error}`);
                return;
            }

            const match = stdout.match(/"([^"]+)"/)
            const output = match ? match[1] : ''

            expect(output).to.equal('Hello world world')

            done();
        });
    });
});

================
File: raw-src/old/restructure/input/input_01.json
================
{
    "A": "zero",
    "B": {
        "a1": "one",
        "a2": "two",
        "a3": [
            {
                "a31": "three",
                "a32": "four"
            },
            {
                "a33": "five",
                "a34": "six"
            }
        ]
    },
    "C": {
        "c1": "seven"
    }
}

================
File: raw-src/old/restructure/input/mapping_01.json
================
{
    "mappings": [
        {
            "pre": "A",
            "post": "U"
        },
        {
            "pre": "B.a1",
            "post": "V"
        },
        {
            "pre": "B.a3",
            "post": "W"
        },
        {
            "pre": "C.c1",
            "post": "X.d"
        }
    ]
}

================
File: raw-src/old/restructure/output/output_01.json
================
{
  "U": "zero",
  "V": "one",
  "W": [
    {
      "a31": "three",
      "a32": "four"
    },
    {
      "a33": "five",
      "a34": "six"
    }
  ],
  "X": {
    "d": "seven"
  }
}

================
File: raw-src/old/restructure/output/required_01.json
================
{
    "U": "zero",
    "V": "one",
    "W": [
        {
            "a31": "three",
            "a32": "four"
        },
        {
            "a33": "five",
            "a34": "six"
        }
    ],
    "X": {
        "d": "sevenn"
    }
}

================
File: raw-src/old/restructure/temp/input-01.json
================
{
    "item": {
        "uuid": "convo1",
        "name": "Name of this convo",
        "created_at": "2024-10-29T17:57:50.229169Z",
        "chat_messages": [
            {
                "uuid": "id1",
                "text": "Text one"
            },
            {
                "uuid": "id2",
                "text": "Text two"
            }
        ]
    }
}

================
File: raw-src/old/restructure/temp/required-01.json
================
{
    "channel": [
        {
            "uuid": "id1",
            "text": "Text one"
        },
        {
            "uuid": "id2",
            "text": "Text two"
        }
    ],
    "filename": "convo",
    "title": "Name of this convo"
}

================
File: raw-src/old/restructure/compare.js
================
import fs from 'fs/promises';
import path from 'path';

const findDifferences = (obj1, obj2, path = '') => {
    const differences = [];

    // Handle different types or null/undefined
    if (typeof obj1 !== typeof obj2) {
        return [`${path}: Type mismatch - ${typeof obj1} vs ${typeof obj2}`];
    }
    if (obj1 === null || obj2 === null) {
        if (obj1 !== obj2) {
            return [`${path}: Null mismatch`];
        }
        return [];
    }

    // Handle arrays
    if (Array.isArray(obj1) && Array.isArray(obj2)) {
        if (obj1.length !== obj2.length) {
            return [`${path}: Array length mismatch - ${obj1.length} vs ${obj2.length}`];
        }
        obj1.forEach((item, idx) => {
            differences.push(...findDifferences(item, obj2[idx], `${path}[${idx}]`));
        });
        return differences;
    }

    // Handle objects
    if (typeof obj1 === 'object') {
        const keys1 = Object.keys(obj1);
        const keys2 = Object.keys(obj2);


        keys1.forEach(key => {
            if (!keys2.includes(key)) {
                differences.push(`${path ? path + '.' : ''}${key}: Missing in required`);
            }
        });

        keys2.forEach(key => {
            if (!keys1.includes(key)) {
                differences.push(`${path ? path + '.' : ''}${key}: Missing in output`);
            }
        });


        keys1.filter(key => keys2.includes(key)).forEach(key => {
            differences.push(...findDifferences(
                obj1[key],
                obj2[key],
                path ? `${path}.${key}` : key
            ));
        });

        return differences;
    }


    if (obj1 !== obj2) {
        return [`${path}: Value mismatch - ${obj1} vs ${obj2}`];
    }

    return [];
};

export async function verifyOutputs(dir = './output') {
    const results = {};
    try {
        const files = await fs.readdir(dir);
        const outputFiles = files.filter(f => f.startsWith('output_'));

        for (const outputFile of outputFiles) {
            const index = outputFile.match(/\d+/)[0];
            const requiredFile = `required_${index}.json`;

            if (!files.includes(requiredFile)) {
                console.warn(`No required file found for ${outputFile}`);
                continue;
            }

            const output = JSON.parse(
                await fs.readFile(path.join(dir, outputFile), 'utf8')
            );
            const required = JSON.parse(
                await fs.readFile(path.join(dir, requiredFile), 'utf8')
            );

            const differences = findDifferences(output, required);
            results[index] = {
                matches: differences.length === 0,
                differences
            };
        }

        return results;
    } catch (error) {
        console.error('Error verifying outputs:', error);
        throw error;
    }
}


const results = await verifyOutputs();
console.log(JSON.stringify(results, null, 2));

================
File: raw-src/old/restructure/fileHandler.js
================
import fs from 'fs/promises';
import path from 'path';
import { JsonRestructurer } from './JsonRestructurer.js';

export async function processFiles(inputDir = './input', outputDir = './output') {
    try {
        await fs.mkdir(outputDir, { recursive: true });
        const files = await fs.readdir(inputDir);

        const inputFiles = files.filter(f => f.startsWith('input_'));
        const mappingFiles = files.filter(f => f.startsWith('mapping_'));

        for (const inputFile of inputFiles) {
            const index = inputFile.match(/\d+/)[0];
            const mappingFile = mappingFiles.find(f => f.includes(index));

            if (!mappingFile) {
                console.warn(`No mapping file found for ${inputFile}`);
                continue;
            }

            const inputData = JSON.parse(
                await fs.readFile(path.join(inputDir, inputFile), 'utf8')
            );
            const mappingData = JSON.parse(
                await fs.readFile(path.join(inputDir, mappingFile), 'utf8')
            );

            const restructurer = new JsonRestructurer(mappingData);
            const result = restructurer.restructure(inputData);

            await fs.writeFile(
                path.join(outputDir, `output_${index}.json`),
                JSON.stringify(result, null, 2)
            );
        }
    } catch (error) {
        console.error('Error processing files:', error);
        throw error;
    }
}

================
File: raw-src/old/restructure/JsonRestructurer.js
================
export class JsonRestructurer {
    constructor(mappings) {
        if (!mappings?.mappings || !Array.isArray(mappings.mappings)) {
            throw new Error('Invalid mapping structure');
        }
        this.mappings = mappings.mappings;
    }

    getValueByPath(obj, path) {
        try {
            return path.split('.').reduce((acc, part) => acc[part], obj);
        } catch (e) {
            console.warn(`Warning: Path ${path} not found`);
            return undefined;
        }
    }

    setValueByPath(obj, path, value) {
        const parts = path.split('.');
        const last = parts.pop();
        const target = parts.reduce((acc, part) => {
            acc[part] = acc[part] || {};
            return acc[part];
        }, obj);
        target[last] = value;
    }

    restructure(inputData) {
        if (typeof inputData === 'string') {
            try {
                inputData = JSON.parse(inputData);
            } catch (e) {
                throw new Error('Invalid JSON string provided');
            }
        }

        const result = {};
        this.mappings.forEach(({ pre, post }) => {
            const value = this.getValueByPath(inputData, pre);
            if (value !== undefined) {
                this.setValueByPath(result, post, value);
            }
        });

        return result;
    }
}

================
File: raw-src/old/restructure/restructure.js
================
import fs from 'fs/promises';
import path from 'path';

const getValueByPath = (obj, path) => {
    try {
        return path.split('.').reduce((acc, part) => acc[part], obj);
    } catch (e) {
        console.warn(`Warning: Path ${path} not found`);
        return undefined;
    }
};

const setValueByPath = (obj, path, value) => {
    const parts = path.split('.');
    const last = parts.pop();
    const target = parts.reduce((acc, part) => {
        acc[part] = acc[part] || {};
        return acc[part];
    }, obj);
    target[last] = value;
};

export async function restructureJson(inputDir = './input', outputDir = './output') {
    try {
        await fs.mkdir(outputDir, { recursive: true });
        const files = await fs.readdir(inputDir);

        const inputFiles = files.filter(f => f.startsWith('input_'));
        const mappingFiles = files.filter(f => f.startsWith('mapping_'));

        for (const inputFile of inputFiles) {
            const index = inputFile.match(/\d+/)[0];
            const mappingFile = mappingFiles.find(f => f.includes(index));

            if (!mappingFile) {
                console.warn(`No mapping file found for ${inputFile}`);
                continue;
            }

            const inputData = JSON.parse(
                await fs.readFile(path.join(inputDir, inputFile), 'utf8')
            );
            const mappingData = JSON.parse(
                await fs.readFile(path.join(inputDir, mappingFile), 'utf8')
            );

            if (!mappingData.mappings || !Array.isArray(mappingData.mappings)) {
                throw new Error(`Invalid mapping structure in ${mappingFile}`);
            }

            const result = {};
            mappingData.mappings.forEach(({ pre, post }) => {
                const value = getValueByPath(inputData, pre);
                if (value !== undefined) {
                    setValueByPath(result, post, value);
                }
            });

            await fs.writeFile(
                path.join(outputDir, `output_${index}.json`),
                JSON.stringify(result, null, 2)
            );
        }
    } catch (error) {
        console.error('Error processing files:', error);
        throw error;
    }
}

================
File: raw-src/old/restructure/run.js
================
import { processFiles } from './fileHandler.js';
await processFiles();


import { JsonRestructurer } from './JsonRestructurer.js';

================
File: raw-src/viz/test-data/foaf-template.ttl
================
# foaf-template.ttl sample for form-a-matic

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .

<#ROOT>
    a foaf:Person ;
    foaf:name "LITERAL" ;
    foaf:mbox <#URI> ;
    foaf:homepage <#URI> ;
    foaf:nick "LITERAL" ;
    foaf:depiction <#URI> ;
    foaf:interest <#URI> ;
    foaf:knows [
        a foaf:Person ;
        foaf:name "LITERAL"
    ] .

================
File: raw-src/viz/jsonld-turtle.js
================
(function() {









function getProperties(obj) {

  var rval = [];
  for(var key in obj) {
    if(key !== '@id') {
      rval.push(key);
    }
  }
  return rval;
};








function isBnode(iri) {
  return iri.indexOf('_:') === 0;
};










function iriToTurtle(iri) {

  return isBnode(iri) ? iri : ('<' + iri + '>');
};









function objectToString(obj) {
  var rval = '';

  if(obj instanceof Array) {
    // if the object is an array, convert each object in the list
    var firstItem = true;
    for(i in obj) {
      if(firstItem) {
        firstItem = false;
        rval = '\n      ';
      }
      else {
        rval += ',\n      ';
      }
      rval += objectToString(obj[i]);
    }
  }
  else if(obj instanceof Object) {
    if('@value' in obj) {
      rval = '"' + obj['@value'] + '"';

      if('@type' in obj) {

        rval += '^^<' + obj['@type'] + '>';
      }
      else if('@language' in obj) {

        rval += '@' + obj['@language'];
      }
    }
    else if('@id' in obj) {
      rval = iriToTurtle(obj['@id']);
    }
  }
  else {

    rval = '"' + obj + '"';
  }

  return rval;
};








jsonld.turtle = function(input, options, callback) {
  jsonld.normalize(input, options, function(err, normalized) {
    var output = '';
    for(s in normalized) {
      // print out each key in the normalized array (the subjects)
      var subject = normalized[s];
      var iri = subject['@id'];


      if(Object.keys(subject).length === 1) {
        continue;
      }

      output += iriToTurtle(iri) + '\n';


      var properties = getProperties(subject);
      var numProperties = properties.length;


      var count = numProperties;
      for(p in properties) {

        property = properties[p];
        if(property === '@type') {
          output += '   <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ';
        }
        else {
          output += '   <' + property + '> ';
        }
        output += objectToString(subject[property]);

        if(count === 1) {

          output += '.\n';
        }
        else {

          output += ';\n';
        }
        count -= 1;
      }
    }
    callback(null, output);
  });
};

})();

================
File: raw-src/viz/jsonld-vis.css
================
@import url("https://fonts.googleapis.com/css?family=Open+Sans:300,400,600");

svg {
  border: none;
}

.node {
  cursor: pointer;
}

.node text {
  font-size: 12px;
  font-family: 'Open Sans', 'Helvetica Neue', Helvetica, sans-serif;
  fill: #333333;
}

.d3-tip {
  font-size: 14px;
  font-family: 'Open Sans', 'Helvetica Neue', Helvetica, sans-serif;
  color: #333333;
  border: 1px solid #CCCCCC;
  border-radius: 5px;
  padding: 10px 20px;
  max-width: 250px;
  word-wrap: break-word;
  background-color: rgba(255, 255, 255, 0.9);
  text-align: left;
}

.link {
  fill: none;
  stroke: #DADFE1;
  stroke-width: 1px;
}

================
File: raw-src/viz/jsonld-vis.js
================
(function() {
  'use strict';

  function jsonldVis(jsonld, selector, config) {
    if (!arguments.length) return jsonldVis;
    config = config || {};

    var h = config.h || 600
      , w = config.w || 800
      , maxLabelWidth = config.maxLabelWidth || 250
      , transitionDuration = config.transitionDuration || 750
      , transitionEase = config.transitionEase || 'cubic-in-out'
      , minRadius = config.minRadius || 5
      , scalingFactor = config.scalingFactor || 2;

    var i = 0;

    var tree = d3.layout.tree()
      .size([h, w]);

    var diagonal = d3.svg.diagonal()
      .projection(function(d) { return [d.y, d.x]; });

    var svg = d3.select(selector).append('svg')
      .attr('width', w)
      .attr('height', h)
      .append('g')
      .attr('transform', 'translate(' + maxLabelWidth + ',0)');

    var tip = d3.tip()
      .direction(function(d) {
        return d.children || d._children ? 'w' : 'e';
      })
      .offset(function(d) {
        return d.children || d._children ? [0, -3] : [0, 3];
      })
      .attr('class', 'd3-tip')
      .html(function(d) {
        return '<span>' + d.valueExtended + '</span>';
      });

    svg.call(tip);

    var root = jsonldTree(jsonld);
    root.x0 = h / 2;
    root.y0 = 0;
    root.children.forEach(collapse);

    function changeSVGWidth(newWidth) {
      if (w !== newWidth) {
        d3.select(selector + ' > svg').attr('width', newWidth);
      }
    }

    function jsonldTree(source) {
      var tree = {};

      if ('@id' in source) {
        tree.isIdNode = true;
        tree.name = source['@id'];
        if (tree.name.length > maxLabelWidth / 9) {
          tree.valueExtended = tree.name;
          tree.name = '...' + tree.valueExtended.slice(-Math.floor(maxLabelWidth / 9));
        }
      } else {
        tree.isIdNode = true;
        tree.isBlankNode = true;

        tree.name = '_' + Math.random().toString(10).slice(-7);
      }

      var children = [];
      Object.keys(source).forEach(function(key) {
        if (key === '@id' || key === '@context' || source[key] === null) return;

        var valueExtended, value;
        if (typeof source[key] === 'object' && !Array.isArray(source[key])) {
          children.push({
            name: key,
            children: [jsonldTree(source[key])]
          });
        } else if (Array.isArray(source[key])) {
          children.push({
            name: key,
            children: source[key].map(function(item) {
              if (typeof item === 'object') {
                return jsonldTree(item);
              } else {
                return { name: item };
              }
            })
          });
        } else {
          valueExtended = source[key];
          value = valueExtended;
          if (value.length > maxLabelWidth / 9) {
            value = value.slice(0, Math.floor(maxLabelWidth / 9)) + '...';
            children.push({
              name: key,
              value: value,
              valueExtended: valueExtended
            });
          } else {
            children.push({
              name: key,
              value: value
            });
          }
        }
      });

      if (children.length) {
        tree.children = children;
      }

      return tree;
    }

    function update(source) {
      var nodes = tree.nodes(root).reverse();
      var links = tree.links(nodes);

      nodes.forEach(function(d) { d.y = d.depth * maxLabelWidth; });

      var node = svg.selectAll('g.node')
        .data(nodes, function(d) { return d.id || (d.id = ++i); });

      var nodeEnter = node.enter()
        .append('g')
        .attr('class', 'node')
        .attr('transform', function(d) { return 'translate(' + source.y0 + ',' + source.x0 + ')'; })
        .on('click', click);

      nodeEnter.append('circle')
        .attr('r', 0)
        .style('stroke-width', function(d) {
          return d.isIdNode ? '2px' : '1px';
        })
        .style('stroke', function(d) {
          return d.isIdNode ? '#F7CA18' : '#4ECDC4';
        })
        .style('fill', function(d) {
          if (d.isIdNode) {
            return d._children ? '#F5D76E' : 'white';
          } else {
            return d._children ? '#86E2D5' : 'white';
          }
        })
        .on('mouseover', function(d) { if (d.valueExtended) tip.show(d); })
        .on('mouseout', tip.hide);

      nodeEnter.append('text')
        .attr('x', function(d) {
          var spacing = computeRadius(d) + 5;
          return d.children || d._children ? -spacing : spacing;
        })
        .attr('dy', '4')
        .attr('text-anchor', function(d) { return d.children || d._children ? 'end' : 'start'; })
        .text(function(d) { return d.name + (d.value ? ': ' + d.value : ''); })
        .style('fill-opacity', 0);

      var maxSpan = Math.max.apply(Math, nodes.map(function(d) { return d.y + maxLabelWidth; }));
      if (maxSpan + maxLabelWidth + 20 > w) {
        changeSVGWidth(maxSpan + maxLabelWidth);
        d3.select(selector).node().scrollLeft = source.y0;
      }

      var nodeUpdate = node.transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('transform', function(d) { return 'translate(' + d.y + ',' + d.x + ')'; });

      nodeUpdate.select('circle')
        .attr('r', function(d) { return computeRadius(d); })
        .style('stroke-width', function(d) {
          return d.isIdNode ? '2px' : '1px';
        })
        .style('stroke', function(d) {
          return d.isIdNode ? '#F7CA18' : '#4ECDC4';
        })
        .style('fill', function(d) {
          if (d.isIdNode) {
            return d._children ? '#F5D76E' : 'white';
          } else {
            return d._children ? '#86E2D5' : 'white';
          }
        });

      nodeUpdate.select('text').style('fill-opacity', 1);

      var nodeExit = node.exit().transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('transform', function(d) { return 'translate(' + source.y + ',' + source.x + ')'; })
        .remove();

      nodeExit.select('circle').attr('r', 0);
      nodeExit.select('text').style('fill-opacity', 0);

      var link = svg.selectAll('path.link')
        .data(links, function(d) { return d.target.id; });

      link.enter().insert('path', 'g')
        .attr('class', 'link')
        .attr('d', function(d) {
          var o = { x: source.x0, y: source.y0 };
          return diagonal({ source: o, target: o });
        });

      link.transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('d', diagonal);

      link.exit().transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('d', function(d) {
          var o = { x: source.x, y: source.y };
          return diagonal({ source: o, target: o });
        })
        .remove();

      nodes.forEach(function(d) {
        d.x0 = d.x;
        d.y0 = d.y;
      });
    }

    function computeRadius(d) {
      if (d.children || d._children) {
        return minRadius + (numEndNodes(d) / scalingFactor);
      } else {
        return minRadius;
      }
    }

    function numEndNodes(n) {
      var num = 0;
      if (n.children) {
        n.children.forEach(function(c) {
          num += numEndNodes(c);
        });
      } else if (n._children) {
        n._children.forEach(function(c) {
          num += numEndNodes(c);
        });
      } else {
        num++;
      }
      return num;
    }

    function click(d) {
      if (d.children) {
        d._children = d.children;
        d.children = null;
      } else {
        d.children = d._children;
        d._children = null;
      }

      update(d);


      if (d.children) {
        d.children.forEach(function(child) {
          if (child.isBlankNode && child._children) {
            click(child);
          }
        });
      }
    }

    function collapse(d) {
      if (d.children) {
        d._children = d.children;
        d._children.forEach(collapse);
        d.children = null;
      }
    }

    update(root);
  }

  if (typeof module !== 'undefined' && module.exports) {
    module.exports = jsonldVis;
  } else {
    d3.jsonldVis = jsonldVis;
  }
})();

================
File: raw-src/viz/playground-examples.js
================
(function($) {
  window.playground = window.playground || {};
  var playground = window.playground;


  playground.examples = {};
  playground.frames = {};
  playground.contexts = {};


  playground.examples["Person"] = {
    "@context": "http://schema.org/",
    "@type": "Person",
    "name": "Jane Doe",
    "jobTitle": "Professor",
    "telephone": "(425) 123-4567",
    "url": "http://www.janedoe.com"
  };


  playground.examples["Place"] = {
    "@context": {
      "name": "http://schema.org/name",
      "description": "http://schema.org/description",
      "image": {
        "@id": "http://schema.org/image",
        "@type": "@id"
      },
      "geo": "http://schema.org/geo",
      "latitude": {
        "@id": "http://schema.org/latitude",
        "@type": "xsd:float"
      },
      "longitude": {
        "@id": "http://schema.org/longitude",
        "@type": "xsd:float"
      },
      "xsd": "http://www.w3.org/2001/XMLSchema#"
    },
    "name": "The Empire State Building",
    "description": "The Empire State Building is a 102-story landmark in New York City.",
    "image": "http://www.civil.usherbrooke.ca/cours/gci215a/empire-state-building.jpg",
    "geo": {
       "latitude": "40.75",
       "longitude": "73.98"
    }
  };


  playground.examples["Event"] = {
    "@context": {
      "ical": "http://www.w3.org/2002/12/cal/ical#",
      "xsd": "http://www.w3.org/2001/XMLSchema#",
      "ical:dtstart": {
        "@type": "xsd:dateTime"
      }
    },
    "ical:summary": "Lady Gaga Concert",
    "ical:location": "New Orleans Arena, New Orleans, Louisiana, USA",
    "ical:dtstart": "2011-04-09T20:00:00Z"
  };


  playground.examples["Product"] = {
    "@context": {
      "gr": "http://purl.org/goodrelations/v1#",
      "pto": "http://www.productontology.org/id/",
      "foaf": "http://xmlns.com/foaf/0.1/",
      "xsd": "http://www.w3.org/2001/XMLSchema#",
      "foaf:page": {"@type": "@id"},
      "gr:acceptedPaymentMethods": {"@type": "@id"},
      "gr:hasBusinessFunction": {"@type": "@id"},
       "gr:hasCurrencyValue": {"@type": "xsd:float"}
    },
    "@id": "http://example.org/cars/for-sale#tesla",
    "@type": "gr:Offering",
    "gr:name": "Used Tesla Roadster",
    "gr:description": "Need to sell fast and furiously",
    "gr:hasBusinessFunction": "gr:Sell",
    "gr:acceptedPaymentMethods": "gr:Cash",
    "gr:hasPriceSpecification": {
      "gr:hasCurrencyValue": "85000",
      "gr:hasCurrency": "USD"
    },
    "gr:includes": {
      "@type": ["gr:Individual", "pto:Vehicle"],
      "gr:name": "Tesla Roadster",
      "foaf:page": "http://www.teslamotors.com/roadster"
    }
  };


  playground.examples["Recipe"] = {
    "@context": {
      "name": "http://rdf.data-vocabulary.org/#name",
      "ingredient": "http://rdf.data-vocabulary.org/#ingredients",
      "yield": "http://rdf.data-vocabulary.org/#yield",
      "instructions": "http://rdf.data-vocabulary.org/#instructions",
      "step": {
        "@id": "http://rdf.data-vocabulary.org/#step",
        "@type": "xsd:integer"
      },
      "description": "http://rdf.data-vocabulary.org/#description",
      "xsd": "http://www.w3.org/2001/XMLSchema#"
    },
    "name": "Mojito",
    "ingredient": ["12 fresh mint leaves", "1/2 lime, juiced with pulp",
      "1 tablespoons white sugar", "1 cup ice cubes",
      "2 fluid ounces white rum", "1/2 cup club soda"],
    "yield": "1 cocktail",
    "instructions" : [{
      "step": 1,
      "description": "Crush lime juice, mint and sugar together in glass."
    }, {
      "step": 2,
      "description": "Fill glass to top with ice cubes."
    }, {
      "step": 3,
      "description": "Pour white rum over ice."
    }, {
      "step": 4,
      "description": "Fill the rest of glass with club soda, stir."
    }, {
      "step": 5,
      "description": "Garnish with a lime wedge."
    }]
  };


  playground.examples["Library"] = {
    "@context": {
      "dc": "http://purl.org/dc/elements/1.1/",
      "ex": "http://example.org/vocab#",
      "xsd": "http://www.w3.org/2001/XMLSchema#",
      "ex:contains": {"@type": "@id"}
    },
    "@graph": [{
      "@id": "http://example.org/library",
      "@type": "ex:Library",
      "ex:contains": "http://example.org/library/the-republic"
    }, {
      "@id": "http://example.org/library/the-republic",
      "@type": "ex:Book",
      "dc:creator": "Plato",
      "dc:title": "The Republic",
      "ex:contains": "http://example.org/library/the-republic#introduction"
    }, {
      "@id": "http://example.org/library/the-republic#introduction",
      "@type": "ex:Chapter",
      "dc:description": "An introductory chapter on The Republic.",
      "dc:title": "The Introduction"
    }]
  };


  playground.frames["Library"] = {
    "@context": {
      "dc": "http://purl.org/dc/elements/1.1/",
      "ex": "http://example.org/vocab#"
    },
    "@type": "ex:Library",
    "ex:contains": {
      "@type": "ex:Book",
      "ex:contains": {"@type": "ex:Chapter"}
    }
  };



  playground.examples["Activity"] = {
    "@context": "https://www.w3.org/ns/activitystreams",
    "@type": "Create",
    "actor": {
      "@type": "Person",
      "@id": "acct:sally@example.org",
      "name": "Sally"
    },
    "object": {
      "@type": "Note",
      "content": "This is a simple note"
    },
    "published": "2015-01-25T12:34:56Z"
  }

})(jQuery);

================
File: raw-src/viz/repopack.config.json
================
{
    "output": {
        "filePath": "./viz-turtle-repopack.txt",
        "headerText": "viz-turtle current",
        "removeComments": true
    },
    "ignore": {
        "useDefaultPatterns": true,
        "customPatterns": [
            "*.log",
            "docs",
            "**/*repopack*"
        ]
    }
}

================
File: raw-src/viz/viz-turtle.js
================
import rdf from 'rdf-ext';
import { fromFile } from 'rdf-utils-fs';
import * as d3 from 'd3';
import { JSDOM } from 'jsdom';
import fs from 'fs/promises';


const dom = new JSDOM('<!DOCTYPE html><body></body>');
global.document = dom.window.document;
global.window = dom.window;


function datasetToJsonld(dataset) {
    const jsonld = {};
    for (const quad of dataset) {
        const subject = quad.subject.value;
        const predicate = quad.predicate.value;
        const object = quad.object.termType === 'Literal' ? quad.object.value : { '@id': quad.object.value };

        if (!jsonld[subject]) {
            jsonld[subject] = { '@id': subject };
        }
        if (!jsonld[subject][predicate]) {
            jsonld[subject][predicate] = [];
        }
        jsonld[subject][predicate].push(object);
    }
    return Object.values(jsonld);
}

function jsonldVis(jsonldData, config) {
    config = config || {};
    const h = config.h || 600;
    const w = config.w || 800;
    const maxLabelWidth = config.maxLabelWidth || 250;
    const transitionDuration = config.transitionDuration || 750;
    const minRadius = config.minRadius || 5;
    const scalingFactor = config.scalingFactor || 2;

    const body = d3.select(document.body);
    const svg = body.append('svg')
        .attr('width', w)
        .attr('height', h)
        .append('g')
        .attr('transform', `translate(${maxLabelWidth},0)`);

    const tree = d3.tree().size([h, w]);

    const diagonal = d3.linkHorizontal()
        .x(d => d.y)
        .y(d => d.x);


    const root = d3.hierarchy({
        name: 'RDF Graph',
        children: jsonldData.map(item => ({
            name: item['@id'],
            children: Object.entries(item)
                .filter(([key]) => key !== '@id')
                .map(([key, value]) => ({
                    name: key,
                    children: Array.isArray(value) ? value.map(v => ({ name: v['@id'] || v })) : [{ name: value['@id'] || value }]
                }))
        }))
    });

    root.x0 = h / 2;
    root.y0 = 0;

    update(root);

    function update(source) {
        const treeData = tree(root);
        const nodes = treeData.descendants();
        const links = treeData.links();

        nodes.forEach(d => { d.y = d.depth * maxLabelWidth; });

        const node = svg.selectAll('g.node')
            .data(nodes, d => d.id || (d.id = ++i));

        const nodeEnter = node.enter().append('g')
            .attr('class', 'node')
            .attr('transform', d => `translate(${source.y0},${source.x0})`)
            .on('click', (event, d) => {
                d.children = d.children ? null : d._children;
                update(d);
            });

        nodeEnter.append('circle')
            .attr('r', 1e-6)
            .style('fill', d => d._children ? 'lightsteelblue' : '#fff');

        nodeEnter.append('text')
            .attr('x', d => d.children || d._children ? -13 : 13)
            .attr('dy', '.35em')
            .attr('text-anchor', d => d.children || d._children ? 'end' : 'start')
            .text(d => d.data.name)
            .style('fill-opacity', 1e-6);

        const nodeUpdate = node.merge(nodeEnter).transition()
            .duration(transitionDuration)
            .attr('transform', d => `translate(${d.y},${d.x})`);

        nodeUpdate.select('circle')
            .attr('r', 10)
            .style('fill', d => d._children ? 'lightsteelblue' : '#fff');

        nodeUpdate.select('text')
            .style('fill-opacity', 1);

        const nodeExit = node.exit().transition()
            .duration(transitionDuration)
            .attr('transform', d => `translate(${source.y},${source.x})`)
            .remove();

        nodeExit.select('circle')
            .attr('r', 1e-6);

        nodeExit.select('text')
            .style('fill-opacity', 1e-6);

        const link = svg.selectAll('path.link')
            .data(links, d => d.target.id);

        link.enter().insert('path', 'g')
            .attr('class', 'link')
            .attr('d', d => {
                const o = { x: source.x0, y: source.y0 };
                return diagonal({ source: o, target: o });
            });

        link.merge(link).transition()
            .duration(transitionDuration)
            .attr('d', diagonal);

        link.exit().transition()
            .duration(transitionDuration)
            .attr('d', d => {
                const o = { x: source.x, y: source.y };
                return diagonal({ source: o, target: o });
            })
            .remove();

        nodes.forEach(d => {
            d.x0 = d.x;
            d.y0 = d.y;
        });
    }
    update(root);
    return body.html();
}

function changeSVGWidth(newWidth) {
    if (w !== newWidth) {
        d3.select(selector + ' > svg').attr('width', newWidth);
    }
}

function jsonldTree(source) {
    var tree = {};

    if ('@id' in source) {
        tree.isIdNode = true;
        tree.name = source['@id'];
        if (tree.name.length > maxLabelWidth / 9) {
            tree.valueExtended = tree.name;
            tree.name = '...' + tree.valueExtended.slice(-Math.floor(maxLabelWidth / 9));
        }
    } else {
        tree.isIdNode = true;
        tree.isBlankNode = true;

        tree.name = '_' + Math.random().toString(10).slice(-7);
    }

    var children = [];
    Object.keys(source).forEach(function (key) {
        if (key === '@id' || key === '@context' || source[key] === null) return;

        var valueExtended, value;
        if (typeof source[key] === 'object' && !Array.isArray(source[key])) {
            children.push({
                name: key,
                children: [jsonldTree(source[key])]
            });
        } else if (Array.isArray(source[key])) {
            children.push({
                name: key,
                children: source[key].map(function (item) {
                    if (typeof item === 'object') {
                        return jsonldTree(item);
                    } else {
                        return { name: item };
                    }
                })
            });
        } else {
            valueExtended = source[key];
            value = valueExtended;
            if (value.length > maxLabelWidth / 9) {
                value = value.slice(0, Math.floor(maxLabelWidth / 9)) + '...';
                children.push({
                    name: key,
                    value: value,
                    valueExtended: valueExtended
                });
            } else {
                children.push({
                    name: key,
                    value: value
                });
            }
        }
    });

    if (children.length) {
        tree.children = children;
    }

    return tree;
}

function update(source) {
    var nodes = tree.nodes(root).reverse();
    var links = tree.links(nodes);

    nodes.forEach(function (d) { d.y = d.depth * maxLabelWidth; });

    var node = svg.selectAll('g.node')
        .data(nodes, function (d) { return d.id || (d.id = ++i); });

    var nodeEnter = node.enter()
        .append('g')
        .attr('class', 'node')
        .attr('transform', function (d) { return 'translate(' + source.y0 + ',' + source.x0 + ')'; })
        .on('click', click);

    nodeEnter.append('circle')
        .attr('r', 0)
        .style('stroke-width', function (d) {
            return d.isIdNode ? '2px' : '1px';
        })
        .style('stroke', function (d) {
            return d.isIdNode ? '#F7CA18' : '#4ECDC4';
        })
        .style('fill', function (d) {
            if (d.isIdNode) {
                return d._children ? '#F5D76E' : 'white';
            } else {
                return d._children ? '#86E2D5' : 'white';
            }
        })
        .on('mouseover', function (d) { if (d.valueExtended) tip.show(d); })
        .on('mouseout', tip.hide);

    nodeEnter.append('text')
        .attr('x', function (d) {
            var spacing = computeRadius(d) + 5;
            return d.children || d._children ? -spacing : spacing;
        })
        .attr('dy', '4')
        .attr('text-anchor', function (d) { return d.children || d._children ? 'end' : 'start'; })
        .text(function (d) { return d.name + (d.value ? ': ' + d.value : ''); })
        .style('fill-opacity', 0);

    var maxSpan = Math.max.apply(Math, nodes.map(function (d) { return d.y + maxLabelWidth; }));
    if (maxSpan + maxLabelWidth + 20 > w) {
        changeSVGWidth(maxSpan + maxLabelWidth);
        d3.select(selector).node().scrollLeft = source.y0;
    }

    var nodeUpdate = node.transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('transform', function (d) { return 'translate(' + d.y + ',' + d.x + ')'; });

    nodeUpdate.select('circle')
        .attr('r', function (d) { return computeRadius(d); })
        .style('stroke-width', function (d) {
            return d.isIdNode ? '2px' : '1px';
        })
        .style('stroke', function (d) {
            return d.isIdNode ? '#F7CA18' : '#4ECDC4';
        })
        .style('fill', function (d) {
            if (d.isIdNode) {
                return d._children ? '#F5D76E' : 'white';
            } else {
                return d._children ? '#86E2D5' : 'white';
            }
        });

    nodeUpdate.select('text').style('fill-opacity', 1);

    var nodeExit = node.exit().transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('transform', function (d) { return 'translate(' + source.y + ',' + source.x + ')'; })
        .remove();

    nodeExit.select('circle').attr('r', 0);
    nodeExit.select('text').style('fill-opacity', 0);

    var link = svg.selectAll('path.link')
        .data(links, function (d) { return d.target.id; });

    link.enter().insert('path', 'g')
        .attr('class', 'link')
        .attr('d', function (d) {
            var o = { x: source.x0, y: source.y0 };
            return diagonal({ source: o, target: o });
        });

    link.transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('d', diagonal);

    link.exit().transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('d', function (d) {
            var o = { x: source.x, y: source.y };
            return diagonal({ source: o, target: o });
        })
        .remove();

    nodes.forEach(function (d) {
        d.x0 = d.x;
        d.y0 = d.y;
    });
}

function computeRadius(d) {
    if (d.children || d._children) {
        return minRadius + (numEndNodes(d) / scalingFactor);
    } else {
        return minRadius;
    }
}

function numEndNodes(n) {
    var num = 0;
    if (n.children) {
        n.children.forEach(function (c) {
            num += numEndNodes(c);
        });
    } else if (n._children) {
        n._children.forEach(function (c) {
            num += numEndNodes(c);
        });
    } else {
        num++;
    }
    return num;
}

function click(d) {
    if (d.children) {
        d._children = d.children;
        d.children = null;
    } else {
        d.children = d._children;
        d._children = null;
    }

    update(d);


    if (d.children) {
        d.children.forEach(function (child) {
            if (child.isBlankNode && child._children) {
                click(child);
            }
        });
    }
}

function collapse(d) {
    if (d.children) {
        d._children = d.children;
        d._children.forEach(collapse);
        d.children = null;
    }
}



export async function visualizeTurtle(turtleFile, outputFile) {
    try {
        const stream = fromFile(turtleFile);
        const dataset = await rdf.dataset().import(stream);
        const jsonldData = datasetToJsonld(dataset);
        const svgContent = jsonldVis(jsonldData);

        await fs.writeFile(outputFile, svgContent);
        console.log(`Visualization saved to ${outputFile}`);
    } catch (error) {
        console.error('Error generating visualization:', error);
    }
}


visualizeTurtle('test-data/foaf-template.ttl', 'output.svg');

================
File: raw-src/containsAny.js
================
function containsAny(contentString, matchers) {
    return matchers.some(matcher => contentString.includes(matcher));
}


const contentString = "Hello, world! Welcome to coding.";
const matchers = ["coding", "javascript"];

console.log(containsAny(contentString, matchers));

const matchers2 = ["python", "java"];
console.log(containsAny(contentString, matchers2));

================
File: raw-src/crawl-fs.js
================
import { readdir } from 'fs/promises'
import { join, extname } from 'path'


const desiredExtensions = ['.html', '.txt']

async function crawlDirectory(dirPath) {

    try {
        const entries = await readdir(dirPath, { withFileTypes: true })
        for (const entry of entries) {
            const fullPath = join(dirPath, entry.name)
            if (entry.isDirectory()) {
                await crawlDirectory(fullPath)
            } else {

                if (desiredExtensions.includes(extname(entry.name))) {
                    console.log(fullPath)
                }
            }
        }
    } catch (error) {
        console.error('An error occurred:', error)
    }
}


crawlDirectory('../data/mail-archive-sample')

================
File: raw-src/grapoi-bits.js
================
const poi = this.getMyPoi()
logger.log('POIPOI')
for (const quad of poi.out().quads()) {
    logger.log(`\t${quad.predicate.value}: ${quad.object.value}`)
}

================
File: raw-src/postcraft-transmission.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

:postcraft a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4 :s5 :s6) . #  :s6  :s10 :s11 :s12 :s13

:s1 a :DatasetReader . # the manifest
:s2 a :ConfigMap . 

### use services.ttl? - defer to Restructure as possible

# :s3 a :FileReader . # the template for main page
# :s4 a :Restructure ;
#    trm:configKey :mainTemplateDirMap .

:s3 a :FileReader . # the template for posts

:s4 a :Restructure ;
    trm:configKey :postTemplateMap .
# context.content -> context.template ## Remaps should go here????
#
:s5 a :DirWalker ;
    trm:configKey :files .

 #:s6 a :ShowMessage .

# process forks here 

:s6 a :FileReader . # the markdown content

:s7 a :PostcraftPrep . # set up title, filenames etc

:s8 a :MarkdownToHTML .
:s9 a :Templater .

:s10 a :Restructure ;
   trm:configKey :postSaver .

# :s11 a :NOP .
:s11 a :FileWriter .

:s12 a :NOP .
#:s12 a :Unfork ;
 #   trm:configKey :combinePosts .

:s13 a :ShowMessage .

================
File: raw-src/README.md
================
The files here are minimal versions of services etc, to try in isolation before integrating into the main codebase.

================
File: raw-src/regex-play.js
================
const mini = 'keep this\n>remove this\n\n keep this'

const replied = 'for this usage.\n> My real intent is to use karma as a type of point system or game/task\n>\n>\n> I am not sure. \n>and another\nfollowed by normal, then \n>one final quoted line'

const first = '> a quote first line\nthen normal'

const regexMini = /\n>.*\n/gi

const regexFirst = /(^|\n)>.*\n/gi

const regexReplied = /((^|\n)>)+.*\n/gi







console.log('[[[\n' + replied.replace(regexReplied, '\n') + '\n]]]\n\n')

================
File: raw-src/structure-play.js
================
let b = 'B'
let a = 'twat'


let c = a => b

================
File: spec/support/jasmine-browser.json
================
{
  "srcDir": "src",
  "srcFiles": [
    "**/*.js"
  ],
  "specDir": "spec",
  "specFiles": [
    "**/*[sS]pec.js"
  ],
  "helpers": [
    "helpers/**/*.js"
  ],
  "env": {
    "stopSpecOnExpectationFailure": false,
    "stopOnSpecFailure": false,
    "random": true
  },
  "browser": {
    "name": "firefox"
  }
}

================
File: src/api/about.md
================
# transmissions/src/api/

Interfaces for running transmissions.

================
File: src/api/ApplicationManager.js
================
import path from 'path'
import fs from 'fs/promises'
import logger from '../utils/Logger.js'

class ApplicationManager {
    constructor(appsDir) {
        this.appsDir = appsDir
    }

    async listApplications() {
        try {
            const entries = await fs.readdir(this.appsDir, { withFileTypes: true })
            const subdirChecks = entries
                .filter(dirent => dirent.isDirectory())
                .map(async (dirent) => {
                    const subdirPath = path.join(this.appsDir, dirent.name)
                    const files = await fs.readdir(subdirPath)
                    return files.includes('about.md') ? dirent.name : null
                })

            const validApps = (await Promise.all(subdirChecks)).filter(Boolean)
            return validApps
        } catch (err) {
            logger.error('Error listing applications:', err)
            return []
        }
    }


    resolveApplicationPath(appName) {
        logger.debug(`\nApplicationManager.resolveApplicationPath, appName = ${appName}`)

        if (appName.startsWith('/')) {
            return appName
        }






        if (appName.startsWith('..')) {

            const resolved = path.resolve(process.cwd(), appName)
            logger.debug(`ApplicationManager.resolveApplicationPath, resolved = ${resolved}`)
            return resolved
        }
        logger.debug(`ApplicationManager.resolveApplicationPath, this.appsDir = ${this.appsDir}`)


        return path.join(process.cwd(), this.appsDir, appName)
    }

    async getApplicationConfig(appPath) {
        logger.debug('appPath = ' + appPath)

        return {
            transmissionsFile: path.join(appPath, 'transmissions.ttl'),
            processorsConfigFile: path.join(appPath, 'processors-config.ttl'),
            modulePath: path.join(appPath, 'processors')
        }
    }
}

export default ApplicationManager

================
File: src/api/CommandUtils.js
================
import path from 'path'
import fs from 'fs/promises'
import logger from '../utils/Logger.js'

import TransmissionRunner from '../engine/TransmissionRunner.js'
import ApplicationManager from './ApplicationManager.js'

class CommandUtils {
    constructor(appsDir) {
        this.appManager = new ApplicationManager(appsDir)
        this.runner = new TransmissionRunner()
    }

    async run(application, target, message = {}) {
        logger.setLogLevel('debug')
        logger.debug('\nCommandUtils.run()')
        logger.debug('CommandUtils.run, process.cwd() = ' + process.cwd())
        logger.debug('CommandUtils.run, application = ' + application)
        logger.debug('CommandUtils.run, target = ' + target)


        if (target && !target.startsWith('/')) {
            target = path.join(process.cwd(), target)
        }

        var { appName, appPath, subtask } = CommandUtils.splitName(application)


        logger.debug('\nCommandUtils.run, appName = ' + appName)
        logger.debug(`CommandUtils.run, appPath = ${appPath}`)

        appPath = this.appManager.resolveApplicationPath(appPath)

        logger.debug('CommandUtils.run, appPath = ' + appPath)


        const config = await this.appManager.getApplicationConfig(appPath)

        logger.debug('config.modulePath = ' + config.modulePath)

        await this.runner.initialize(config.modulePath)

        const defaultDataDir = path.join(appPath, '/data')
        logger.debug('CommandUtils.run, defaultDataDir = ' + defaultDataDir)

        logger.debug('CommandUtils.run,  target = ' + target)
        logger.debug('CommandUtils.run,  application = ' + appPath)

        message = {
            ...message,
            dataDir: defaultDataDir,
            rootDir: target || appPath,
            applicationRootDir: appPath
        }

        return await this.runner.run({
            ...config,
            message,
            subtask
        })
    }

    static splitName(fullPath) {
        logger.debug(`\nCommandUtils.splitName, fullPath  = ${fullPath}`)
        const parts = fullPath.split(path.sep)
        logger.debug(`\nCommandUtils.splitName, parts  = ${parts}`)
        var lastPart = parts[parts.length - 1]

        var task = false
        if (lastPart.includes('.')) {
            const split = lastPart.split('.')
            task = split[1]
            lastPart = split[0]
        }
        var appPath = parts.slice(0, parts.length - 1).join(path.sep)
        appPath = path.join(appPath, lastPart)



        logger.debug(`CommandUtils.splitName, appName:${lastPart}, appPath:${appPath}, task:${task},`)

        return { appName: lastPart, appPath: appPath, task: task }
    }

    async listApplications() {
        return await this.appManager.listApplications()
    }

    static async parseOrLoadContext(contextArg) {
        logger.debug(`CommandUtils.parseOrLoadContext(), contextArg = ${contextArg}`)
        let message = {}
        try {
            message.payload = JSON.parse(contextArg)
        } catch (err) {
            logger.debug('*** Loading JSON from file...')
            const filePath = path.resolve(contextArg)
            const fileContent = await fs.readFile(filePath, 'utf8')
            message.payload = JSON.parse(fileContent)
        }
        return message
    }
}

export default CommandUtils

================
File: src/api/ModuleLoader.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import logger from '../utils/Logger.js'

class ModuleLoader {
    constructor(classpath) {
        if (!Array.isArray(classpath)) {
            throw new TypeError('Classpath must be an array')
        }


        this.classpath = classpath.map(p => {
            if (typeof p !== 'string') {
                throw new TypeError('All classpath entries must be strings')
            }
            return path.normalize(p)
        })

        this.moduleCache = new Map()
        logger.debug('ModuleLoader initialized with paths:', this.classpath)
    }

    async loadModule(moduleName) {
        logger.debug(`ModuleLoader.loadModule looking for module in classpath ${this.classpath} `)
        if (typeof moduleName !== 'string') {
            throw new TypeError('Module name must be a string')
        }

        logger.debug(`Attempting to load module: ${moduleName}`)


        if (this.moduleCache.has(moduleName)) {
            logger.debug(`Retrieved ${moduleName} from cache`)
            return this.moduleCache.get(moduleName)
        }


        for (const basePath of this.classpath) {
            try {
                const fullPath = path.join(basePath, `${moduleName}.js`)
                logger.debug(`Trying path: ${fullPath}`)

                const module = await import(fullPath)
                this.moduleCache.set(moduleName, module)
                logger.debug(`Successfully loaded ${moduleName} from ${fullPath}`)
                return module
            } catch (error) {
                logger.debug(`Failed to load from ${basePath}: ${error.message}`)
                continue
            }
        }

        throw new Error(`Module ${moduleName} not found in paths: ${this.classpath.join(', ')}`)
    }

    clearCache() {
        this.moduleCache.clear()
    }

    addPath(newPath) {
        if (typeof newPath !== 'string') {
            throw new TypeError('Path must be a string')
        }
        this.classpath.push(path.normalize(newPath))
    }
}
export default ModuleLoader

================
File: src/api/ModuleLoaderFactory.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import logger from '../utils/Logger.js'
import ModuleLoader from './ModuleLoader.js'

class ModuleLoaderFactory {
    static instance = null;

    static createModuleLoader(additionalPaths = []) {
        const __filename = fileURLToPath(import.meta.url)
        const __dirname = path.dirname(__filename)


        const corePath = path.resolve(__dirname, '../processors')


        const classpath = [corePath, ...additionalPaths].map(p => path.normalize(p.toString()))

        logger.debug('Creating ModuleLoader with paths:', classpath)


        if (!ModuleLoaderFactory.instance) {
            ModuleLoaderFactory.instance = new ModuleLoader(classpath)
        }

        return ModuleLoaderFactory.instance
    }


    static createApplicationLoader(appPath) {
        logger.setLogLevel('debug')
        logger.debug('')
        if (!appPath) {
            throw new Error('Application path is required')
        }


        const basePath = typeof appPath === 'string' ? appPath : appPath.toString()


        const normalizedPath = path.resolve(process.cwd(), basePath)
        logger.debug(`Creating application loader for normalized path: ${normalizedPath}`)

        const appProcessorsPath = path.join(normalizedPath, 'processors')
        const __filename = fileURLToPath(import.meta.url)
        const __dirname = path.dirname(__filename)
        const corePath = path.resolve(__dirname, '../processors')

        logger.debug(`App processors path: ${appProcessorsPath}`)
        logger.debug(`Core processors path: ${corePath}`)

        return this.createModuleLoader([appProcessorsPath, corePath])
    }

    static clearInstance() {
        ModuleLoaderFactory.instance = null
    }
}
export default ModuleLoaderFactory

================
File: src/api/WebRunner.js
================
import express from 'express'
import TransmissionRunner from '../engine/TransmissionRunner.js'
import ApplicationManager from './ApplicationManager.js'
import logger from '../utils/Logger.js'

class WebRunner {
    constructor(appsDir, port = 7247) {
        this.appManager = new ApplicationManager(appsDir)
        this.runner = new TransmissionRunner()
        this.app = express()
        this.port = port

        this.setupRoutes()
    }

    setupRoutes() {
        this.app.use(express.json())

        this.app.get('/applications', async (req, res) => {
            const apps = await this.appManager.listApplications()
            res.json(apps)
        })

        this.app.post('/run/:application', async (req, res) => {
            const { application } = req.params
            const { target, message } = req.body

            try {
                const config = await this.appManager.getApplicationConfig(application)
                await this.runner.initialize(config.modulePath)

                const result = await this.runner.run({
                    ...config,
                    message,
                    target
                })

                res.json(result)
            } catch (error) {
                logger.error('Error running application:', error)
                res.status(500).json({
                    success: false,
                    error: error.message
                })
            }
        })
    }

    start() {
        this.app.listen(this.port, () => {
            logger.log(`Web interface running on port ${this.port}`)
        })
    }
}

export default WebRunner

================
File: src/applications/app-template/about.md
================
# App Template

## Runner

```sh
cd ~/github-danny/transmissions # my local path
./trans app-template
```

## Description

---

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/app-template/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:setDemo a trm:ServiceConfig ;
    trm:setValue (t:sv0)  . # consider using blank nodes
    t:sv0   trm:key    "demo" ;
            trm:value    "a test value"  .

================
File: src/applications/app-template/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:mini a trm:Pipeline ;
    trm:pipe (:p10 :p20) .

:p10 a :SetMessage ;
     trm:configKey :setDemo .

:p20 a :ShowMessage .

================
File: src/applications/claude-json-converter/data/input/input-01.json
================
[
    {
        "uuid": "conversation 1 uuid",
        "name": "conversation 1 name",
        "created_at": "",
        "updated_at": "",
        "account": {
            "uuid": "account.uuid"
        },
        "chat_messages": [
            {
                "uuid": "c1 message 1 uuid",
                "text": "",
                "content": [
                    {
                        "type": "c1 message 1 type",
                        "text": "c1 message 1 text"
                    }
                ],
                "sender": "",
                "created_at": "",
                "updated_at": "",
                "attachments": [],
                "files": [
                    {
                        "file_name": ""
                    }
                ]
            },
            {
                "uuid": "c1 message 2 uuid",
                "text": "",
                "content": [
                    {
                        "type": "c1 message 2 type",
                        "text": "c1 message 2 text"
                    }
                ],
                "sender": "",
                "created_at": "",
                "updated_at": "",
                "attachments": [],
                "files": [
                    {
                        "file_name": ""
                    }
                ]
            }
        ]
    },
    {
        "uuid": "conversation 2 uuid",
        "name": "conversation 2 name",
        "created_at": "",
        "updated_at": "",
        "account": {
            "uuid": "account.uuid"
        },
        "chat_messages": [
            {
                "uuid": "c2 message 1 uuid",
                "text": "",
                "content": [
                    {
                        "type": "c2 message 1 type",
                        "text": "c2 message 1 text"
                    }
                ],
                "sender": "",
                "created_at": "",
                "updated_at": "",
                "attachments": [],
                "files": [
                    {
                        "file_name": ""
                    }
                ]
            },
            {
                "uuid": "c2 message 2 uuid",
                "text": "",
                "content": [
                    {
                        "type": "c2 message 2 type",
                        "text": "c2 message 2 text"
                    }
                ],
                "sender": "",
                "created_at": "",
                "updated_at": "",
                "attachments": [],
                "files": [
                    {
                        "file_name": ""
                    }
                ]
            }
        ]
    }
]

================
File: src/applications/claude-json-converter/data/input/users.json
================
[{"uuid": "dc67aa7d-f71f-4232-afb3-7f2688ac68f7", "full_name": "Danny Ayers", "email_address": "danny.ayers@gmail.com", "verified_phone_number": null}]

================
File: src/applications/claude-json-converter/about.md
================
```sh
cd ~/github-danny/transmissions/
./trans claude-json-converter

# -P src/applications/claude-json-converter/data/input/conversations.json
```

After `FileReader` (and `Blanker`):

```
{
    // system message bits,

    "content": [
        {
            "uuid": "",
            "name": "",
            "created_at": "",
            "updated_at": "",
            "account": {
                "uuid": ""
            },
            "chat_messages": [
                {
                    "uuid": "",
                    "text": "",
                    "content": [
                        {
                            "type": "",
                            "text": ""
                        }
                    ],
                    "sender": "",
                    "created_at": "",
                    "updated_at": "",
                    "attachments": [],
                    "files": [
                        {
                            "file_name": ""
                        }
                    ]
                },
                {
                    ...
                }
            ]
        }
}
```

`JSONWalker` fires off a message per-conversation.

These need `Restructure` to split off the common metadata as `message.content`, and move `chat_messages` to `message.content`, ready for -

`JSONWalker` fires off a message per-conversation.

================
File: src/applications/claude-json-converter/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

##################### only for testing
t:bContent a trm:ServiceConfig ;
    rdfs:label "Root node in JSON object for blanker" ;
    trm:configKey t:blankContent  ;
    trm:pointer "content"  .

t:setDump a trm:ServiceConfig ;
    trm:setValue (t:sv0)  . # consider using blank nodes
    t:sv0   trm:key    "dump" ;
            trm:value  "true"  .
#########################################################################

# should really be in a manifest.ttl
t:ReadFile a trm:ServiceConfig ;
    rdfs:label "Read file" ;
    trm:configKey t:readFile ;
    trm:sourceFile "input/conversations.json" ;
  #  trm:sourceFile "input/input-01.json" ;
    trm:mediaType "application/json" .


t:ConversationsWalker a trm:ServiceConfig ;
    trm:key t:conversationsConfig ;
    trm:pointer "content" .


t:retreeConvs a trm:ServiceConfig ;
    trm:rename (t:pp100 t:pp101 t:pp102  t:pp103) . 
    t:pp100     trm:pre     "content.uuid" ;
                trm:post    "meta.conv_uuid"  .
    t:pp101     trm:pre     "content.name" ;
                trm:post    "meta.conv_name"  .
    t:pp102     trm:pre     "content.updated_at" ;
                trm:post    "meta.updated_at"  .
    t:pp103     trm:pre     "content.chat_messages" ;
                trm:post    "content"  .

t:MessagesWalker a trm:ServiceConfig ;
    trm:key t:messagesConfig ;
    trm:pointer "content" .

# unused
t:retreeMsgs a trm:ServiceConfig ;
    trm:rename (t:pp200 t:pp201 t:pp202) . 

    t:pp200     trm:pre     "content.item.chat_messages" ;
                trm:post    "channel"  .

    t:pp201     trm:pre     "content.item.uuid" ;
                trm:post    "filename"  .
            
    t:pp202     trm:pre     "content.item.name" ;
                trm:post    "title"  .

#      filepath = this.getPropertyFromMyConfig(ns.trm.destinationFile)

# unused
t:Writer a trm:ServiceConfig ;
    trm:key t:writer ;
    trm:destinationFile "DESTINATION" .

================
File: src/applications/claude-json-converter/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 

# testing only - FileWriter will save message
:SD a :SetMessage ;
     trm:configKey :setDump .
:FW a :FileWriter .

:BLK a :Blanker ; # clear values
     trm:configKey :blankContent .

# :UF :SD :FW :DE 
####################################

:cjc a trm:Pipeline ;
trm:pipe (:p10 :p20 :p30 :p40 :p50  :p60) .

# Start 

:p10 a :FileReader ; # Claude conversations.json
       trm:configKey :readFile .

# Separates into conversations
:p20 a :JSONWalker ;
     trm:configKey :conversationsConfig . 


:p30 a :Restructure ;
     trm:configKey :retreeConvs .

# Separates into messages
:p40 a :JSONWalker ;
     trm:configKey :messagesConfig . 

#:p50 a :Restructure ;
 #    trm:configKey :retreeMsgs .

:p50 a :MarkdownFormatter .

:p60 a :FileWriter .

================
File: src/applications/dirwalker-test/about.md
================
it originally ran from

transmission.process('../../data/mail-archive-sample')

================
File: src/applications/dirwalker-test/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:FilePipelineMap a trm:DataMap ;
    trm:sourceFile "input.txt" ;
    trm:destinationFile "output.txt" .

================
File: src/applications/dirwalker-test/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:dirwalkpipe a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a :DirWalker .
:s2 a :AppendProcess .
:s3 a :StringSink .

================
File: src/applications/env-loader-test/about.md
================
```sh
cd ~/github-danny/transmissions
./trans env-loader-test
```

================
File: src/applications/env-loader-test/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/env-loader-test/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:envy a trm:Pipeline ;
    trm:pipe (:s10 :s20 :SM) .
# trm:pipe (:SC) .
:s10 a :EnvLoader .
:s20 a :WhiteboardToMessage .

================
File: src/applications/file-copy-remove-test/data/start/one.txt
================
Hello from One

================
File: src/applications/file-copy-remove-test/data/start/two.txt
================
Hello from Two

================
File: src/applications/file-copy-remove-test/about.md
================
# file-copy-remove-test

run with :

```
# in transmissions dir

./run file-copy-remove-test
```

or

```
npm test -- tests/integration/file-copy-remove-test.spec.js
```

this should :

- copy `start/one.txt` into `single-empty/`
- copy `single-empty/one.txt` into `single-full/`
- remove `single-empty/one.txt`

- copy everything in `start/` into `several-empty/`
- copy everything in `several-empty/` into `several-full/`
- remove everything in `several-empty/`

Hmm, test services would be helpful to check before and after - or maybe just use regular test runner script from npm?

================
File: src/applications/file-copy-remove-test/init.sh
================
rm -rf data/start
rm -rf data/single-empty
rm -rf data/single-full
rm -rf data/several-empty
rm -rf data/several-full



mkdir -p data/start






echo 'Hello from One' > data/start/one.txt


echo 'Hello from Two' > data/start/two.txt

tree data

================
File: src/applications/file-copy-remove-test/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

#t:copyOneToSingleEmpty a trm:ServiceConfig ;
 #   trm:key t:copyOneToSingleEmpty ;
  #  trm:source "data/start/one.txt" ;
   # trm:destination "data/single-empty/one.txt" .

t:copyOneToSingleEmpty a trm:ServiceConfig ;
    trm:key t:copyOneToSingleEmpty ;
    trm:source "data/start/one.txt" ;
    trm:destination "data/single-empty/one.txt" .

t:copySingleEmptyToSingleFull a trm:ServiceConfig ;
    trm:key t:copySingleEmptyToSingleFull ;
    trm:source "data/single-empty/one.txt" ;
    trm:destination "data/single-full/one.txt" .

t:removeSingleEmpty a trm:ServiceConfig ;
    trm:key t:removeSingleEmpty ;
    trm:target "data/single-empty/one.txt" .

t:copyStartToSeveralEmpty a trm:ServiceConfig ;
    trm:key t:copyStartToSeveralEmpty ;
    trm:source "data/start" ;
    trm:destination "data/several-empty" .

t:copySeveralEmptyToSeveralFull a trm:ServiceConfig ;
    trm:key t:copySeveralEmptyToSeveralFull ;
    trm:source "data/several-empty" ;
    trm:destination "data/several-full" .

t:removeSeveralEmpty a trm:ServiceConfig ;
    trm:key t:removeSeveralEmpty ;
    trm:target "data/several-empty" .

================
File: src/applications/file-copy-remove-test/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:file_copy_remove_test a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4 :s5 :s6) .

:s1 a :FileCopy ;
    trm:configKey :copyOneToSingleEmpty .

:s2 a :FileCopy ;
    trm:configKey :copySingleEmptyToSingleFull .

:s3 a :FileRemove ;
    trm:configKey :removeSingleEmpty .

:s4 a :FileCopy ;
    trm:configKey :copyStartToSeveralEmpty .

:s5 a :FileCopy ;
    trm:configKey :copySeveralEmptyToSeveralFull .

:s6 a :FileRemove ;
    trm:configKey :removeSeveralEmpty .

================
File: src/applications/file-pipeline/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:FilePipelineMap a trm:DataMap ;
    trm:sourceFile "input.txt" ;
    trm:destinationFile "output.txt" .

================
File: src/applications/file-pipeline/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:file_pipeline a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4) .

:s1 a :FileSource .
:s2 a :AppendProcess .
:s3 a :AppendProcess .
:s4 a :FileSink .

================
File: src/applications/globbo/about.md
================
# Globbo

```
./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}'
```

there are more notes under

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-application-walkthrough.md

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-service-walkthrough.md

## Description

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/globbo/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/globbo/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:globbo a trm:Pipeline ;
    trm:pipe (:s10 :s20 :s30 :s40 :SM) .

#:s40 a :Restructure ;
 #   trm:configKey :walkPrep .

:s10 a :DirWalker .
:s20 a :CaptureAll . # pushes all messages into config.whiteboard
:s30 a :Unfork .
:s40 a :WhiteboardToMessage .

================
File: src/applications/html-to-md/about.md
================
# HTML to Markdown

*a minimal application (that I need) which can also serve as an example in documentation*

```
./run html-to-md -c '{"rootDir": "./test-data/html-to-md", "filename":"webidl.html"}'
```

## Description

---

there are more notes under

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-application-walkthrough.md

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-service-walkthrough.md

## Description

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/html-to-md/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/html-to-md/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:h2m a trm:Pipeline ;
    trm:pipe (:s10 :s20 :s30 :s40 :SM) .

#:s40 a :Restructure ;
 #   trm:configKey :walkPrep .

:s10 a :FileReader .
:s20 a :CaptureAll . # pushes all messages into config.whiteboard
:s30 a :Unfork .
:s40 a :WhiteboardToMessage .

================
File: src/applications/link-lister/about.md
================
run.js had

const here = import.meta.url
const message = { runScript: here }

transmission.process('', message)

================
File: src/applications/link-lister/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:linklister trm:hasDataMap t:llSourceMap .
t:linklister trm:hasDataMap t:llGotMap .
t:linklister trm:hasDataMap t:llLinkMap .

t:llSourceMap a trm:DataMap ;
    trm:key t:sourceFile ;
    trm:value "starter-links.md" .

t:llGotMap a trm:DataMap ;
    trm:key t:gotFile ;
    trm:value "got.html" .

t:llLinkMap a trm:DataMap ;
    trm:key t:linkFile ;
    trm:value "links.md" .

t:htmlMap a trm:DataMap ;
    trm:key t:htmlFile ;
    trm:value "links.html" .

================
File: src/applications/link-lister/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:linklister a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4 :s5 :s6 :s7 :s8) .

:s1 a :FileReader ;
    trm:configKey :sourceFile .
    
:s2 a :LineReader .
:s3 a :HttpGet .

:s4 a :LinkFinder .

:s5 a :StringMerger .

:s6 a :FileWriter ;
    trm:configKey :linkFile .
# :s6 a :NOP .
:s7 a :MarkdownToHTML .

:s8 a :FileWriter ;
    trm:configKey :htmlFile .

#:s8 a :StringFilter .
#:s9 a :StringMerger .
#:s10 a :FileWriter 
#        trm:configKey :linkFile .
# :s4 a :NOP .
# :s4 a :FileWriter ;
#     trm:configKey :gotFile .

# :s5 a :NOP .

================
File: src/applications/nop-test/about.md
================
# nop

## Description

minimal for comparing with simple runner

---

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/nop-test/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

================
File: src/applications/nop-test/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:nope a trm:Pipeline ;
    trm:pipe (:N :SC :SM) .

================
File: src/applications/postcraft/about.md
================
```sh
cd ~/github-danny/transmissions

./trans postcraft ../postcraft/danny.ayers.name
```

================
File: src/applications/postcraft/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances
@prefix pc: <http://purl.org/stuff/postcraft/> .

### ConfigMap
t:PostcraftMap a trm:ServiceConfig ;
    trm:key t:postcraftMap ;
    trm:group pc:ContentGroup .

### clean 

t:cacheRemove a trm:ServiceConfig ;
    trm:key t:removeCache ;
    trm:target "cache/" .

t:articlesRemove a trm:ServiceConfig ;
    trm:key t:removeArticles ;
    trm:target "public/home/articles" .

t:entriesRemove a trm:ServiceConfig ;
    trm:key t:removeEntries ;
    trm:target "public/home/entries" .

t:journalRemove a trm:ServiceConfig ;
    trm:key t:removeJournal ;
    trm:target "public/home/journal" .

t:todoRemove a trm:ServiceConfig ;
    trm:key t:removeTodo ;
    trm:target "public/home/todo" .

t:indexRemove a trm:ServiceConfig ;
    trm:key t:removeIndex ;
    trm:target "public/home/index.html" .

### copy #####################################

t:copyStatic a trm:ServiceConfig ;
    trm:key t:staticCopy ;
    trm:source "content-static" ;
    trm:destination "public/home/static" .

t:copyMedia a trm:ServiceConfig ;
    trm:key t:mediaCopy ;
    trm:source "media" ;
    trm:destination "public/home/media" .
    
t:copyCSS a trm:ServiceConfig ;
    trm:key t:cssCopy ;
    trm:source "layouts/mediocre/css" ;
    trm:destination "public/home/css" .

t:copyFonts a trm:ServiceConfig ;
    trm:key t:fontsCopy ;
    trm:source "layouts/mediocre/fonts" ;
    trm:destination "public/home/fonts" .

t:copyJS a trm:ServiceConfig ;
    trm:key t:jsCopy ;
    trm:source "layouts/mediocre/js" ;
    trm:destination "public/home/js" .

### render ##################################

trm:Describe  a trm:ServiceConfig ; 
 trm:key trm:describe .

t:phaseOne a trm:ServiceConfig ; 
    trm:key t:markdownToRawPosts ;
    trm:marker "Phase One" .

# TODO IS COPY, not rename!!

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
  #  t:pp2   trm:pre     "filename" ;
   #         trm:post    "filename"  .
 
t:entryRawPrep a trm:ReMap ;
    trm:rename (t:er1 t:er3) . 
   t:er1   trm:pre     "targetFilename" ;
            trm:post    "filepath" . 
    t:er3   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

t:entryPagePrep a trm:ReMap ;
    trm:rename (t:ppp1) . 
    t:ppp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

================
File: src/applications/postcraft/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 
@prefix pc: <http://purl.org/stuff/postcraft/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe) 
:UF a :Unfork . # collapses all pipes but one 
#############################################################


## POSTCRAFT.CLEAN ##########################################

:clean a trm:Pipeline ;
    rdfs:label "clean" ;
    rdfs:comment "directory cleaner" ;
    trm:pipe (:r10 :r20 :r30 :r40 :r50 :r60) .

:r10 a :FileRemove ;
    trm:configKey :removeCache .

:r20 a :FileRemove ;
    trm:configKey  :removeArticles .

:r30 a :FileRemove ;
    trm:configKey  :removeJournal .

:r40 a :FileRemove ;
    trm:configKey  :removeEntries .

:r50 a :FileRemove ;
    trm:configKey  :removeTodo .

:r60 a :FileRemove ;
    trm:configKey  :removeIndex .

## POSTCRAFT.COPY ##################################################################

:copy a trm:Pipeline ;
    rdfs:label "copy" ;
    rdfs:comment "dir/file copier" ;
    trm:pipe (:cp10 :cp20 :cp30 :cp40 :cp50) .

### static dirs
:cp10 a :FileCopy ;
    trm:configKey :staticCopy .

### media dirs
:cp20 a :FileCopy ;
    trm:configKey :mediaCopy .
    
### layout dirs
:cp30 a :FileCopy ;
    trm:configKey :cssCopy .

:cp40 a :FileCopy ;
    trm:configKey :jsCopy .

:cp50 a :FileCopy ;
    trm:configKey :fontsCopy .

#####################


## POSTCRAFT.RENDER ###############################################################

:render a trm:Pipeline ;
    rdfs:label "render" ;
    rdfs:comment "render the pages" ;
   trm:pipe (:s10 :s20 :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100 
              :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .
 #  trm:pipe (:s10 :SM :s20 :SM2 :DE  :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100 
  #               :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .

:s10 a :DatasetReader . # read the manifest
# trm:configKey trm:describe .

:s20 a :ConfigMap ;
    trm:configKey :postcraftMap .

:s30 a :FileReader ; # the template for raw entry content
    trm:describe trm:all .

:s40 a :Restructure ;
    trm:configKey :walkPrep .

:s50 a :DirWalker . # automatically forks

:s60 a :FileReader . # the markdown content

:s70 a :PostcraftPrep . # set up title, filenames etc

:s80 a :MarkdownToHTML . 

:s90 a :Restructure ;
   trm:configKey :entryRawPrep .

 :s100 a :Templater .

:s110 a :FileWriter .

############### entryContentToEntryPage
:s120 a :EntryContentToPagePrep .
#:s12 a :Restructure ;
 #  trm:configKey :entryPagePrep .

:s130 a :Templater .

:s140 a :FileWriter .

#######################
:s150  a :Unfork .

:s160 a :FrontPagePrep . 

:s170 a :Templater .

:s180 a :FileWriter .

================
File: src/applications/postcraft-new/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances
@prefix pc: <http://purl.org/stuff/postcraft/> .

### ConfigMap
t:PostcraftMap a trm:ServiceConfig ;
    trm:key t:postcraftMap ;
    trm:group pc:ContentGroup .

### clean 

t:cacheRemove a trm:ServiceConfig ;
    trm:key t:removeCache ;
    trm:target "cache/" .

t:articlesRemove a trm:ServiceConfig ;
    trm:key t:removeArticles ;
    trm:target "public/home/articles" .

t:entriesRemove a trm:ServiceConfig ;
    trm:key t:removeEntries ;
    trm:target "public/home/entries" .

t:journalRemove a trm:ServiceConfig ;
    trm:key t:removeJournal ;
    trm:target "public/home/journal" .

t:todoRemove a trm:ServiceConfig ;
    trm:key t:removeTodo ;
    trm:target "public/home/todo" .

t:indexRemove a trm:ServiceConfig ;
    trm:key t:removeIndex ;
    trm:target "public/home/index.html" .

### copy #####################################

t:copyStatic a trm:ServiceConfig ;
    trm:key t:staticCopy ;
    trm:source "content-static" ;
    trm:destination "public/home/static" .

t:copyMedia a trm:ServiceConfig ;
    trm:key t:mediaCopy ;
    trm:source "media" ;
    trm:destination "public/home/media" .
    
t:copyCSS a trm:ServiceConfig ;
    trm:key t:cssCopy ;
    trm:source "layouts/mediocre/css" ;
    trm:destination "public/home/css" .

t:copyFonts a trm:ServiceConfig ;
    trm:key t:fontsCopy ;
    trm:source "layouts/mediocre/fonts" ;
    trm:destination "public/home/fonts" .

t:copyJS a trm:ServiceConfig ;
    trm:key t:jsCopy ;
    trm:source "layouts/mediocre/js" ;
    trm:destination "public/home/js" .

### render ##################################

trm:Describe  a trm:ServiceConfig ; 
 trm:key trm:describe .

t:phaseOne a trm:ServiceConfig ; 
    trm:key t:markdownToRawPosts ;
    trm:marker "Phase One" .

# TODO IS COPY, not rename!!

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
  #  t:pp2   trm:pre     "filename" ;
   #         trm:post    "filename"  .
 
t:entryRawPrep a trm:ReMap ;
    trm:rename (t:er1 t:er3) . 
   t:er1   trm:pre     "targetFilename" ;
            trm:post    "filepath" . 
    t:er3   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

t:entryPagePrep a trm:ReMap ;
    trm:rename (t:ppp1) . 
    t:ppp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

================
File: src/applications/postcraft-new/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 
@prefix pc: <http://purl.org/stuff/postcraft/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe) 
:UF a :Unfork . # collapses all pipes but one 
#############################################################


## POSTCRAFT.CLEAN ##########################################

:clean a trm:Pipeline ;
    rdfs:label "clean" ;
    rdfs:comment "directory cleaner" ;
    trm:pipe (:r10 :r20 :r30 :r40 :r50 :r60) .

:r10 a :FileRemove ;
    trm:configKey :removeCache .

:r20 a :FileRemove ;
    trm:configKey  :removeArticles .

:r30 a :FileRemove ;
    trm:configKey  :removeJournal .

:r40 a :FileRemove ;
    trm:configKey  :removeEntries .

:r50 a :FileRemove ;
    trm:configKey  :removeTodo .

:r60 a :FileRemove ;
    trm:configKey  :removeIndex .

## POSTCRAFT.COPY ##################################################################

:copy a trm:Pipeline ;
    rdfs:label "copy" ;
    rdfs:comment "dir/file copier" ;
    trm:pipe (:cp10 :cp20 :cp30 :cp40 :cp50) .

### static dirs
:cp10 a :FileCopy ;
    trm:configKey :staticCopy .

### media dirs
:cp20 a :FileCopy ;
    trm:configKey :mediaCopy .
    
### layout dirs
:cp30 a :FileCopy ;
    trm:configKey :cssCopy .

:cp40 a :FileCopy ;
    trm:configKey :jsCopy .

:cp50 a :FileCopy ;
    trm:configKey :fontsCopy .

#####################


## POSTCRAFT.RENDER ###############################################################

:render a trm:Pipeline ;
    rdfs:label "render" ;
    rdfs:comment "render the pages" ;
   trm:pipe (:s10 :s20 :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100 
              :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .
 #  trm:pipe (:s10 :SM :s20 :SM2 :DE  :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100 
  #               :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .

:s10 a :DatasetReader . # read the manifest
# trm:configKey trm:describe .

:s20 a :ConfigMap ;
    trm:configKey :postcraftMap .

:s30 a :FileReader ; # the template for raw entry content
    trm:describe trm:all .

:s40 a :Restructure ;
    trm:configKey :walkPrep .

:s50 a :DirWalker . # automatically forks

:s60 a :FileReader . # the markdown content

:s70 a :PostcraftPrep . # set up title, filenames etc

:s80 a :MarkdownToHTML . 

:s90 a :Restructure ;
   trm:configKey :entryRawPrep .

 :s100 a :Templater .

:s110 a :FileWriter .

############### entryContentToEntryPage
:s120 a :EntryContentToPagePrep .
#:s12 a :Restructure ;
 #  trm:configKey :entryPagePrep .

:s130 a :Templater .

:s140 a :FileWriter .

#######################
:s150  a :Unfork .

:s160 a :FrontPagePrep . 

:s170 a :Templater .

:s180 a :FileWriter .

================
File: src/applications/postcraft-previous/processors.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

trm:Describe  a trm:ServiceConfig ; 
 trm:key trm:describe .

t:phaseOne a trm:ServiceConfig ; 
    trm:key t:markdownToRawPosts ;
    trm:marker "Phase One" .

t:copyMedia a trm:ServiceConfig ;
    trm:key t:mediaCopy ;
    trm:source "media" ;
    trm:destination "public/home/media" .
    
t:copyCSS a trm:ServiceConfig ;
    trm:key t:cssCopy ;
    trm:source "layouts/mediocre/css" ;
    trm:destination "public/home/css" .

t:copyFonts a trm:ServiceConfig ;
    trm:key t:fontsCopy ;
    trm:source "layouts/mediocre/fonts" ;
    trm:destination "public/home/fonts" .

t:copyJS a trm:ServiceConfig ;
    trm:key t:jsCopy ;
    trm:source "layouts/mediocre/fonts" ;
    trm:destination "public/home/fonts" .

# TODO IS COPY, not rename!!

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
  #  t:pp2   trm:pre     "filename" ;
   #         trm:post    "filename"  .
 
t:entryRawPrep a trm:ReMap ;
    trm:rename (t:er1 t:er3) . 
   t:er1   trm:pre     "targetFilename" ;
            trm:post    "filepath" . 
    t:er3   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

t:entryPagePrep a trm:ReMap ;
    trm:rename (t:ppp1) . 
    t:ppp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

================
File: src/applications/postcraft-previous/transmission.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:postTemplating a trm:Pipeline ;
    trm:pipe (:s00 :s05 :s10 :s11 :s12 :s20  :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100 
                :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .

:s00 a :DatasetReader . # read the manifest
# trm:configKey trm:describe .

### Copy media dirs
:s05 a :FileCopy ;
    trm:configKey :mediaCopy .
    
### Copy layout dirs
:s10 a :FileCopy ;
    trm:configKey :cssCopy .

:s11 a :FileCopy ;
    trm:configKey :jsCopy .

:s12 a :FileCopy ;
    trm:configKey :fontsCopy .
#####################

:s20 a :ConfigMap ; ### use services.ttl? - defer to Restructure as possible
    trm:configKey :markdownToRawPosts .

:s30 a :FileReader ; # the template for raw entry content
    trm:describe trm:all .

:s40 a :Restructure ;
    trm:configKey :walkPrep .

:s50 a :DirWalker . # automatically forks

:s60 a :FileReader . # the markdown content

:s70 a :PostcraftPrep . # set up title, filenames etc

:s80 a :MarkdownToHTML . 

:s90 a :Restructure ;
   trm:configKey :entryRawPrep .

 :s100 a :Templater .

:s110 a :FileWriter .

############### entryContentToEntryPage
:s120 a :EntryContentToPagePrep .
#:s12 a :Restructure ;
 #  trm:configKey :entryPagePrep .

:s130 a :Templater .

:s140 a :FileWriter .

#######################
:s150  a :Unfork .

:s160 a :FrontPagePrep . 

:s170 a :Templater .

:s180 a :FileWriter .

================
File: src/applications/string-pipeline/processors-config.ttl
================
### NOT USED

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dc: <http://purl.org/dc/terms/> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:StringPipeline dc:title "Hello" .

================
File: src/applications/string-pipeline/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:stringpipe a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4) .

:s1 a :StringSource .
:s2 a :AppendProcess .
:s3 a :AppendProcess .
:s4 a :StringSink .

================
File: src/applications/test_blanker/data/input/input-01.json
================
{
    "notpayload": "keep",
    "payload": {
        "nottest": "also keep",
        "test": {
            "first": "firststuff",
            "second": {
                "two": "twotwo"
            },
            "third": [
                "31",
                "32",
                "33"
            ]
        }
    }
}

================
File: src/applications/test_blanker/data/output/output-01.json
================
{"notpayload":"keep","payload":{"nottest":"also keep","test":{"first":"","second":{"two":""},"third":["","",""]}}}

================
File: src/applications/test_blanker/data/output/required-01.json
================
{
    "notpayload": "keep",
    "payload": {
        "nottest": "also keep",
        "test": {
            "first": "",
            "second": {
                "two": ""
            },
            "third": [
                "",
                "",
                ""
            ]
        }
    }
}

================
File: src/applications/test_blanker/about.md
================
# App Template

## Runner

```sh
cd ~/github-danny/transmissions # my local path
./trans test_blanker
```

## Description

================
File: src/applications/test_blanker/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances


t:r a trm:ServiceConfig ;
    rdfs:label "Read file" ;
    trm:configKey t:readFile ;
    trm:sourceFile "input/input-01.json" ;
    trm:mediaType "application/json" .

t:pointy a trm:ServiceConfig ;
    rdfs:label "Root node in JSON object" ;
    trm:configKey t:blankin ;
    trm:pointer "content.payload.test"  .

t:w a trm:ServiceConfig ;
    rdfs:label "Write file" ;
    trm:configKey t:writeFile ;
    trm:destinationFile "output/output-01.json"  .

================
File: src/applications/test_blanker/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:testBlanker a trm:Pipeline ;
    trm:pipe (:p10 :p20 :p30 ) .

:p10 a :FileReader ; # JSON test file
       trm:configKey :readFile .

:p20 a :Blanker ; # clear values
     trm:configKey :blankin .

:p30 a :FileWriter ; # save result
       trm:configKey :writeFile .

================
File: src/applications/test_foreach/about.md
================
# ForEach processor module for Transmissions

```sh
./trans test_fork
```

Your Goal is to write a processor module for Transmissions that will initiate multiple processing pipelines based on a list provided in the incoming message. First review these instructions as a whole, and then identify the subgoals. Then, taking each subgoal in turn, break it down into a concrete series of tasks. Carry out the sequence of tasks.  
You have plenty of time, so don't rush, try to be as careful in understanding and operation as possible.
Existing source code may be found in the Project Knowledge files.

Two modules are required -

1. `ForEach` located in :

```sh
./transmissions/src/processors/flow/ForEach.js
```

modeled on :

```sh
./transmissions/src/processors/templates/ProcessorTemplate.js
```

2. `FlowProcessorsFactory` located in

```sh
./transmissions/src/processors/flow/FlowProcessorsFactory.js
```

modeled on :

```sh
/transmissions/src/processors/templates/TemplateProcessorsFactory.js
```

The input message will contain the list to be processed in the form of this example :

```json
{
  "foreach": ["item1", "item2", "item3"]
}
```

The behavior will be to emit the message to a subsequent processor using the existing engine infrastructure, like a simpler version of :

```sh
transmissions/src/processors/fs/DirWalker.js
```

Each message emitted will be a structuredClone of the input message.

Once this code is completed, create application definitions in the form of these examples :

```sh
transmissions/src/applications/test_fork/transmissions.ttl
transmissions/src/applications/test_fork/processors-config.ttl
```

After you have finished all these, re-read the high level Goal and taking each of your derived subgoals in turn, review your code to ensure that it fulfils the requirements.
Show me the full source of the implementations.

---

/home/danny/github-danny/postcraft/danny.ayers.name/content-raw/entries/2024-09-27_lively-distractions.md

https://github.com/github/rest-api-description

================
File: src/applications/test_foreach/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

:foreach_test a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a :ForEach .
:s2 a :ShowMessage .
:s3 a :DeadEnd .

================
File: src/applications/test_fork/about.md
================
# Test Fork/Unfork

```
./run test_fork | grep 's2 a NOP'
```

should show the number of forks + 1 (for `message.done`)

```
./run test_fork | grep s1.s2.s10.s11.s12.s13
```

should show just one

================
File: src/applications/test_fork/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

================
File: src/applications/test_fork/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:test_fork a :Transmission ;
   trm:contains :pipeA .

:pipeA a trm:Pipeline ;
trm:pipe (:p10 :p20 :SM ) .

:p10 a :Fork .

# :s10 a :Unfork .
:p20 a :NOP .

================
File: src/applications/test_fs-rw/data/input/input-01.md
================
Hello!

================
File: src/applications/test_fs-rw/data/output/output-01.md
================
Hello!

================
File: src/applications/test_fs-rw/data/output/required-01.md
================
Hello!

================
File: src/applications/test_fs-rw/about.md
================
# Application : test_fs-rw

Copies

```sh
src/applications/test_fs-rw/data/output/input-01.md
```

to

```sh
src/applications/test_fs-rw/data/output/output-01.md
```

the tests compare the new file with :

```sh
src/applications/test_fs-rw/data/output/required-01.md
```

```sh
cd ~/github-danny/transmissions/ # my local path

# run as application
./trans test_fs-rw

# run as simples
node src/applications/test_fs-rw/simple.js

## Tests in tests/integration

# test as application
npm test -- --filter="fs-rw test"

# test as simples
npm test -- --filter="fs-rw simple test"
```

---

```sh
cd ~/github-danny/transmissions/
./trans test_restructure -P ./src/applications/test_restructure/input/input-01.json
```

---

./trans claude-json-converter -P ./conversations.json

```turtle
:s40 a :Restructure ;
    trm:configKey :walkPrep .

...

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
```

================
File: src/applications/test_fs-rw/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .


t:inputFile a trm:Config ;
    trm:key t:input ; 
    trm:sourceFile  "input/input-01.md" .

t:outputFile a trm:Config ;
    trm:key t:output ;
    trm:destinationFile "output/output-01.md" .

 # http://purl.org/stuff/transmission/sourceFile

================
File: src/applications/test_fs-rw/simple.js
================
import FileReader from '../../processors/fs/FileReader.js'
import FileWriter from '../../processors/fs/FileWriter.js'

const config = {
    "simples": "true",
    "sourceFile": "input/input-01.md",
    "destinationFile": "output/output-01.md"
}

var message = { "dataDir": "src/applications/test_fs-rw/data" }

const read = new FileReader(config)

message = await read.process(message)

const write = new FileWriter(config)

message = await write.process(message)

================
File: src/applications/test_fs-rw/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:fsrw a trm:Pipeline ;
trm:pipe (:read :write ) .

:read a :FileReader ;
     trm:configKey :input .

:write a :FileWriter ;
     trm:configKey :output .

================
File: src/applications/test_multi-pipe/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:dirWalkerPosts a trm:ServiceConfig ;
    trm:key t:files .

t:postTemplateMap a trm:ReMap ;
   trm:rename (t:rn1) . # consider using blank nodes
     t:rn1    trm:pre     "content" ;
            trm:post    "template"  .

t:postSaver a trm:ReMap ;
    trm:rename (t:rn2) . 
    t:rn2   trm:pre     "targetFilename" ;
            trm:post    "filename" .

================
File: src/applications/test_multi-pipe/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

:test_multi-pipes a :Transmission ;
   trm:contains :pipeA, :pipeB, :pipeC .

:pipeA a trm:Pipeline ;
trm:pipe (:s1 :s2 :s3 ) .

:pipeB  a trm:Pipeline ;
 trm:pipe (:s3 :s104 :s105) .

:pipeC a trm:Pipeline ;
trm:pipe (:s3 :s204 :s205) .

# :postcraft a trm:Pipeline ;

:s1 a :NOP .
:s2 a :NOP .
:s3 a :NOP .

:s104 a :NOP .
:s105 a :NOP .

:s204 a :NOP .
:s205 a :ShowTransmission .

================
File: src/applications/test_restructure/data/input/input-01.json
================
{
    "item": {
        "uuid": "convo1",
        "name": "Name of this convo",
        "created_at": "2024-10-29T17:57:50.229169Z",
        "chat_messages": [
            {
                "uuid": "id1",
                "text": "Text one"
            },
            {
                "uuid": "id2",
                "text": "Text two"
            }
        ]
    }
}

================
File: src/applications/test_restructure/data/output/output-01.json
================
{"channel":[{"uuid":"id1","text":"Text one"},{"uuid":"id2","text":"Text two"}],"filename":"convo1","title":"Name of this convo"}

================
File: src/applications/test_restructure/data/output/required-01.json
================
{
    "channel": [
        {
            "uuid": "id1",
            "text": "Text one"
        },
        {
            "uuid": "id2",
            "text": "Text two"
        }
    ],
    "filename": "convo1",
    "title": "Name of this convo"
}

================
File: src/applications/test_restructure/about.md
================
# Application : test_restructure

Run with :

```sh
cd ~/github-danny/transmissions/ # local path of repo
./trans test_restructure
```

#:todo make this into something like processor signature
#:todo make Turtle version

## Description

Reads :

```sh
src/applications/test_restructure/data/output/input-01.json
```

as a message, restructures it according to config, then writes the result to :

```sh
src/applications/test_restructure/data/output/output-01.json
```

the tests compare the new file with :

```sh
src/applications/test_restructure/data/output/required-01.json
```

```sh
cd ~/github-danny/transmissions/ # my local path

# run as application
./trans test_restructure

# run as simples
node src/applications/test_restructure/simple.js

## Tests in tests/integration

# test as application
npm test -- --filter="restructure test"

# test as simples
npm test -- --filter="restructure_simple test"
```

---

```sh
cd ~/github-danny/transmissions/
./trans test_restructure -P ./src/applications/test_restructure/input/input-01.json
```

---

./trans claude-json-converter -P ./conversations.json

```turtle
:s40 a :Restructure ;
    trm:configKey :walkPrep .

...

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
```

================
File: src/applications/test_restructure/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:jsonFileIn a trm:Config ;
    trm:key t:reader ;
    trm:sourceFile "input/input-01.json" ;
    trm:mediaType "application/json" .

t:retree a trm:ServiceConfig ;
    trm:rename (t:pp1 t:pp2 t:pp3) . # consider using blank nodes
    t:pp1   trm:pre     "content.item.chat_messages" ;
            trm:post    "content.channel"  .
    t:pp2   trm:pre     "content.item.uuid" ;
            trm:post    "content.filename"  . 
    t:pp3   trm:pre     "content.item.name" ;
            trm:post    "content.title"  .


t:jsonFileOut a trm:Config ;
    trm:key t:writer ;
    trm:destinationFile "output/output-01.json" .

================
File: src/applications/test_restructure/simple.js
================
import FileReader from '../../processors/fs/FileReader.js'
import Restructure from '../../processors/json/Restructure.js'
import FileWriter from '../../processors/fs/FileWriter.js'

const config = {
    "simples": "true",
    "sourceFile": "input/input-01.json",
    "destinationFile": "output/output-01.json",
    "mediaType": "application/json",
    "rename": [{
        "pre": "content.item.chat_messages",
        "post": "content.channel"
    }, {
        "pre": "content.item.uuid",
        "post": "content.filename"
    }, {
        "pre": "content.item.name",
        "post": "content.title"
    }]
}

var message = { "dataDir": "src/applications/test_restructure/data" }

const read = new FileReader(config)
message = await read.process(message)

const restructure = new Restructure(config)
message = await restructure.process(message)

const write = new FileWriter(config)
await write.process(message)

================
File: src/applications/test_restructure/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:cjc a trm:Pipeline ;


trm:pipe (:read :retree1 :SM :writer) .

:read a :FileReader ;
     trm:configKey :reader .

:retree1 a :Restructure ;
     trm:configKey :retree .

:writer a :FileWriter ;
     trm:configKey :writer .

================
File: src/applications/two-transmissions/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

#t:copyOneToSingleEmpty a trm:ServiceConfig ;
 #   trm:key t:copyOneToSingleEmpty ;
  #  trm:source "data/start/one.txt" ;
   # trm:destination "data/single-empty/one.txt" .

t:copyOneToSingleEmpty a trm:ServiceConfig ;
    trm:key t:copyOneToSingleEmpty ;
    trm:source "data/start/one.txt" ;
    trm:destination "data/single-empty/one.txt" .

t:copySingleEmptyToSingleFull a trm:ServiceConfig ;
    trm:key t:copySingleEmptyToSingleFull ;
    trm:source "data/single-empty/one.txt" ;
    trm:destination "data/single-full/one.txt" .

t:removeSingleEmpty a trm:ServiceConfig ;
    trm:key t:removeSingleEmpty ;
    trm:target "data/single-empty/one.txt" .

t:copyStartToSeveralEmpty a trm:ServiceConfig ;
    trm:key t:copyStartToSeveralEmpty ;
    trm:source "data/start" ;
    trm:destination "data/several-empty" .

t:copySeveralEmptyToSeveralFull a trm:ServiceConfig ;
    trm:key t:copySeveralEmptyToSeveralFull ;
    trm:source "data/several-empty" ;
    trm:destination "data/several-full" .

t:removeSeveralEmpty a trm:ServiceConfig ;
    trm:key t:removeSeveralEmpty ;
    trm:target "data/several-empty" .

================
File: src/applications/two-transmissions/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:transmission_one a trm:Pipeline ;
    trm:pipe (:a1 :a2) .

:a1 a :NOP .

:a2 a :NOP .

:transmission_two a trm:Pipeline ;
    trm:pipe (:b1 :b2 :b3) .

:b1 a :NOP .

:b2 a :NOP .

:b3 a :NOP .

================
File: src/core/Application.js
================
class Application {
    constructor() {
        this.transmissions = new Map()
        this.config = null
        this.manifest = null
    }

    addTransmission(id, transmission) {
        this.transmissions.set(id, transmission)
    }
}

================
File: src/core/Director.js
================
class Director {
    constructor() {
        this.builder = new TransmissionBuilder()
        this.runner = new TransmissionRunner()
        this.procurer = new Procurer()
        this.proctor = new Proctor()
    }

    async initializeApplication(args) {
        const application = new Application()
        await this.procurer.loadResources(application, args)
        await this.builder.buildTransmissions(application)
        return application
    }

    async applyToTarget(application, target) {
        await this.runner.execute(application, target)
    }
}

export default Director

================
File: src/core/Procurer.js
================
class Procurer {
    constructor() {
        this.moduleLoader = ModuleLoaderFactory.createModuleLoader()
    }

    async loadResources(application, args) {
        const config = await this.loadConfig(args.configPath)
        const manifest = await this.loadManifest(args.target)
        application.config = config
        application.manifest = manifest
    }
}

export default Procurer

================
File: src/engine/AbstractProcessorFactory.js
================
import SystemProcessorsFactory from '../processors/system/SystemProcessorsFactory.js'
import TestProcessorsFactory from '../processors/test/TestProcessorsFactory.js'
import FsProcessorsFactory from '../processors/fs/FsProcessorsFactory.js'
import MarkupProcessorsFactory from '../processors/markup/MarkupProcessorsFactory.js'
import UtilProcessorsFactory from '../processors/util/UtilProcessorsFactory.js'
import TextProcessorsFactory from '../processors/text/TextProcessorsFactory.js'
import ProtocolsProcessorsFactory from '../processors/protocols/ProtocolsProcessorsFactory.js'
import RDFProcessorsFactory from '../processors/rdf/RDFProcessorsFactory.js'
import PostcraftProcessorsFactory from '../processors/postcraft/PostcraftProcessorsFactory.js'
import FlowProcessorsFactory from '../processors/flow/FlowProcessorsFactory.js'
import StagingProcessorsFactory from '../processors/staging/StagingProcessorsFactory.js'
import GitHubProcessorsFactory from '../processors/github/GitHubProcessorsFactory.js'
import JSONProcessorsFactory from '../processors/json/JSONProcessorsFactory.js'

class AbstractProcessorFactory {




    static createProcessor(type, config) {


        var processor = TestProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = UtilProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = MarkupProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = TextProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = ProtocolsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = RDFProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = PostcraftProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = SystemProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FlowProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = GitHubProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = StagingProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = JSONProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        throw new Error("Unknown processor type: " + type.value)
    }
}

export default AbstractProcessorFactory

================
File: src/engine/Connector.js
================
import { EventEmitter } from 'events'
import logger from '../utils/Logger.js'
import footpath from '../utils/footpath.js'

class Connector extends EventEmitter {


    constructor(fromName, toName) {
        super()
        this.fromName = fromName
        this.toName = toName
    }

    connect(processors) {
        logger.log(`Connector.connect this.fromName = ${this.fromName} this.toName =  ${this.toName}`)
        let fromProcessor = processors[this.fromName]
        let toProcessor = processors[this.toName]

        if (!fromProcessor) {
            throw new Error(`\nMissing processor : ${this.fromName}, going to ${this.toName} \n(check for typos in transmissions.ttl)\n`)
        }

        fromProcessor.on('message', (message) => {
            var tags = ''
            //     if (toProcessor.message) {
            tags = ' (' + fromProcessor.message.tags + ') '
            toProcessor.tags = tags

            const thisTag = footpath.urlLastPart(this.toName)
            logger.log("| Running >>> : " + tags + thisTag + " a " + toProcessor.constructor.name)

            toProcessor.receive(message)
        })

    }


}

export default Connector

================
File: src/engine/Transmission.js
================
import logger from '../utils/Logger.js'
import Connector from './Connector.js'

class Transmission {
  constructor() {
    this.processors = {}
    this.connectors = []

  }

  register(processorName, instance) {
    this.processors[processorName] = instance

  }

  get(processorName) {
    return this.processors[processorName]
  }

  connect(fromProcessorName, toProcessorName) {
    logger.log(`Transmission.connect from ${fromProcessorName} to ${fromProcessorName}`)
    let connector = new Connector(fromProcessorName, toProcessorName)
    this.connectors.push(connector)
    connector.connect(this.processors)
  }




  async process(message) {
    logger.log('\n+ ***** Execute Transmission : ' + this.label + ' <' + this.id + '>')
    const processorName = this.connectors[0]?.fromName || Object.keys(this.processors)[0]
    let processor = this.get(processorName)
    if (processor) {
      logger.log("| Running : " + processorName + " a " + processor.constructor.name)
      await processor.receive(message)
    } else {
      logger.error("No valid processor found to execute")
    }
  }






  toString() {
    let description = 'Transmission Structure:\n'


    description += 'Processors:\n'
    Object.keys(this.processors).forEach(processorName => {
      description += `  - ${processorName}\n`
    })


    description += 'Connectors:\n'
    this.connectors.forEach((connector, index) => {
      description += `  - Connector ${index + 1}: ${connector.fromName} -> ${connector.toName}\n`
    })

    return description
  }
}

export default Transmission

================
File: src/engine/TransmissionBuilder.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'

import ns from '../utils/ns.js'
import GrapoiHelpers from '../utils/GrapoiHelpers.js'
import logger from '../utils/Logger.js'

import ModuleLoader from '../api/ModuleLoader.js'
import AbstractProcessorFactory from "./AbstractProcessorFactory.js"
import Transmission from './Transmission.js'
import ModuleLoaderFactory from '../api/ModuleLoaderFactory.js'



class TransmissionBuilder {

  constructor(moduleLoader) {
    this.moduleLoader = moduleLoader
  }

  static async build(transmissionConfigFile, processorsConfigFile, appPath) {
    const transmissionConfig = await TransmissionBuilder.readDataset(transmissionConfigFile)
    const processorsConfig = await TransmissionBuilder.readDataset(processorsConfigFile)

    const builder = new TransmissionBuilder(this.moduleLoader)
    return builder.buildTransmissions(transmissionConfig, processorsConfig)
  }

  async buildTransmissions(transmissionConfig, processorsConfig) {
    const poi = grapoi({ dataset: transmissionConfig })
    const transmissions = []

    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.trm.Pipeline)) {
        const pipelineID = q.subject

        transmissions.push(await this.constructTransmission(transmissionConfig, pipelineID, processorsConfig))
      }
    }
    return transmissions
  }

  async constructTransmission(transmissionConfig, pipelineID, processorsConfig) {
    processorsConfig.whiteboard = {}

    const transmission = new Transmission()
    transmission.id = pipelineID.value
    transmission.label = ''

    const transPoi = grapoi({ dataset: transmissionConfig, term: pipelineID })

    // TODO has grapoi got a first/single property method?
    for (const quad of transPoi.out(ns.rdfs.label).quads()) {
      transmission.label = quad.object.value
    }
    logger.log('\n+ ***** Construct Transmission : ' + transmission.label + ' <' + transmission.id + '>')

    let previousName = "nothing"

    // grapoi probably has a built-in for all this
    const pipenodes = GrapoiHelpers.listToArray(transmissionConfig, pipelineID, ns.trm.pipe)
    await this.createNodes(transmission, pipenodes, transmissionConfig, processorsConfig) // was await, bad Claude
    //    this.createNodes(transmission, pipenodes, transmissionConfig, processorsConfig); // was await, bad Claude
    this.connectNodes(transmission, pipenodes)
    return transmission
  }

  async createNodes(transmission, pipenodes, transmissionConfig, processorsConfig) {
    for (let i = 0; i < pipenodes.length; i++) {
      let node = pipenodes[i]
      let processorName = node.value

      if (!transmission.get(processorName)) {
        let np = rdf.grapoi({ dataset: transmissionConfig, term: node })
        let processorType = np.out(ns.rdf.type).term
        let processorConfig = np.out(ns.trm.configKey).term

        try {
          let name = ns.getShortname(processorName)
          let type = ns.getShortname(processorType.value)

          logger.log("| Create processor :" + name + " of type :" + type)
          let processor = await this.createProcessor(processorType, processorsConfig) // was await
          processor.id = processorName
          processor.type = processorType
          processor.transmission = transmission

          //    logger.log("| processorConfig :" + processorConfig)
          //  logger.reveal(processorConfig)
          if (processorConfig) {
            processor.configKey = processorConfig
          }
          transmission.register(processorName, processor)
        } catch (err) {
          logger.error('-> Can\'t resolve ' + processorName + ' (check transmission.ttl for typos!)\n')
          logger.error(err)
        }
      }
    }
  }

  async connectNodes(transmission, pipenodes) {
    for (let i = 0; i < pipenodes.length - 1; i++) {
      let leftNode = pipenodes[i]
      let leftProcessorName = leftNode.value
      let rightNode = pipenodes[i + 1]
      let rightProcessorName = rightNode.value
      logger.log("  > Connect #" + i + " [" + ns.getShortname(leftProcessorName) + "] => [" + ns.getShortname(rightProcessorName) + "]")
      transmission.connect(leftProcessorName, rightProcessorName)
    }
  }

  async createProcessor(type, config) {

    const coreProcessor = AbstractProcessorFactory.createProcessor(type, config)
    if (coreProcessor) {
      return coreProcessor
    }

    logger.debug(`TransmissionBuilder, core processor not found for ${type.value}. Trying remote module loader...`)


    try {
      const shortName = type.value.split('/').pop()
      logger.debug(`Loading module: ${shortName}`)

      const ProcessorClass = await this.moduleLoader.loadModule(shortName)

      logger.debug(`Module loaded successfully: ${shortName}`)
      return new ProcessorClass.default(config)
    } catch (error) {

      logger.error(`TransmissionBuilder, failed to load processor ${type.value}`)
      process.exit(1)
    }
  }







  static async readDataset(filename) {
    const stream = fromFile(filename)
    const dataset = await rdf.dataset().import(stream)
    return dataset
  }

  static async writeDataset(dataset, filename) {
    await toFile(dataset.toStream(), filename)
  }


}

export default TransmissionBuilder

================
File: src/engine/TransmissionRunner.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import ModuleLoaderFactory from '../api/ModuleLoaderFactory.js'
import TransmissionBuilder from './TransmissionBuilder.js'
import logger from '../utils/Logger.js'

class TransmissionRunner {
    constructor() {
        this.moduleLoader = null
    }

    async initialize(modulePath) {
        logger.debug(`\nTransmissionRunner.initialize, modulePath = ${modulePath}`)
        if (typeof modulePath !== 'string') {
            throw new TypeError('Module path must be a string')
        }
        this.moduleLoader = ModuleLoaderFactory.createModuleLoader([modulePath])
    }

    async run(options) {
        const {
            transmissionsFile,
            processorsConfigFile,
            message = {},
            rootDir = '',
            applicationRootDir
        } = options

        logger.debug('\nTransmissionRunner.run()')
        logger.reveal(options)
        logger.debug('transmissionsFile =' + transmissionsFile)
        logger.debug('processorsConfigFile =' + processorsConfigFile)

        //  message.applicationRootDir
        try {
            if (!this.moduleLoader) {
                throw new Error('ModuleLoader not initialized. Call initialize() first.')
            }

            const transmissions = await TransmissionBuilder.build(
                transmissionsFile,
                processorsConfigFile,
                this.moduleLoader
            )

            if (!message.rootDir) {
                message.rootDir = rootDir
            }
            if (!message.applicationRootDir) {
                // logger.log('PWDDDDDDDDDDDDD ' + process.cwd())
                message.applicationRootDir = applicationRootDir
            }

            for (const transmission of transmissions) {
                if (!options.subtask || options.subtask === transmission.label) {
                    await transmission.process(message)
                }
            }

            return { success: true }
        } catch (error) {
            logger.error('Error in TransmissionRunner:', error)
            throw error
        }
    }
}

export default TransmissionRunner

================
File: src/engine/WorkerPool.js
================
import { Worker } from 'worker_threads'

class WorkerPool {
    constructor(module, size) {
        this.workers = [];
        this.queue = [];
        for (let i = 0; i < size; i++) {
            const worker = new Worker(module);
            worker.on('message', () => {

                this.markWorkerIdle(worker);
            });
            this.workers.push({ worker, busy: false });
        }
    }

    enqueueMessage(message) {
        this.queue.push(message);
        this.dispatch();
    }

    dispatch() {
        const idleWorkerWrapper = this.workers.find(wrapper => !wrapper.busy);
        if (idleWorkerWrapper && this.queue.length) {
            const message = this.queue.shift();
            idleWorkerWrapper.busy = true;
            idleWorkerWrapper.worker.postMessage(message);
        }
    }

    markWorkerIdle(workerWrapper) {
        workerWrapper.busy = false;
        this.dispatch();
    }
}

================
File: src/processors/base/Processor.js
================
import logger from '../../utils/Logger.js'
import { EventEmitter } from 'events'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'





class Processor extends EventEmitter {





    constructor(config) {
        super()
        this.config = config
        this.messageQueue = []
        this.processing = false
        this.done = false
        this.outputs = []
    }

    preProcess(message) {
        return







        const processorPoi = rdf.grapoi({ dataset: this.config, term: this.configKey })




        if (this.configKey.value === ns.trm.describe.value) {
            this.describe()
        }



    }

    describe() {
        logger.log('describe')
        const inputs = this.getInputKeys()
        const outputs = this.getOutputKeys()
        for (var input of inputs) {
            logger.log('input = ' + input)
            logger.log(this.message[input] + ' = ' + this.message[input])
        }
        for (var output of outputs) {
            logger.log('output = ' + output)
            logger.log(this.message[output] + ' = ' + this.message[output])
        }
    }





















    getMyConfigNode() {
        const dataset = this.config
        const configNode = grapoi({ dataset, term: this.configKey }).in()
        return configNode.term
    }

    getMyPoi() {
        const dataset = this.config
        const myConfigNode = this.getMyConfigNode()
        const poi = grapoi({ dataset: dataset, term: myConfigNode })
        return poi
    }

    async addPropertyToMyConfig(predicate, value) {
        logger.log('addPropertyToMyConfig predicate = ' + predicate)
        logger.log('addPropertyToMyConfig value = ' + value)
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('addPropertyToMyConfig  myConfigNode.value = ' + myConfigNode.value)
        const dataset = this.config
        dataset.add(myConfigNode, predicate, value)
        this.config = dataset
    }

    showMyConfig() {
        const poi = this.getMyPoi()
        logger.log('POI = ')
        logger.poi(poi)
    }

    getPropertyFromMyConfig(property) {
        if (this.config.simples) {
            const shortName = property.value.split('/').pop()
            logger.debug(`Processor (simples), property = ${shortName}`)
            const value = this.config[shortName]
            logger.debug(`Processor (simples), value = ${value}`)
            return value
        }
        const poi = this.getMyPoi()
        try {
            return poi.out(property).term.value
        } catch (err) {
            logger.debug('* Warn : Processor.getPropertyFromMyConfig(), property not defined : ' + property)
            return rdf.literal('undefined')
        }
    }

    async deletePropertyFromMyConfig(predicate, value) {
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('DELETING FROM ' + s)
        const dataset = this.config
        dataset.delete(myConfigNode, predicate, value)
        this.config = dataset
    }






    async receive(message) {
        await this.enqueue(message)
    }






    async enqueue(message) {
        this.messageQueue.push({ message })
        if (!this.processing) {
            this.executeQueue()
        }
    }

    cloneContext(baseContext) {
        const message = structuredClone(baseContext)
        if (baseContext.dataset) {


            message.dataset = baseContext.dataset
        }
        return message
    }





    async executeQueue() {
        this.processing = true
        while (this.messageQueue.length > 0) {
            let { message } = this.messageQueue.shift()

            message = this.cloneContext(message)
            this.message = message




            this.addTag(message)

            await this.process(message)
        }
        this.processing = false
    }

    addTag(message) {
        const tag = this.getTag()
        if (!message.tags) {
            message.tags = tag
            return
        }
        message.tags = message.tags + '.' + tag

    }

    getTag() {
        return footpath.urlLastPart(this.id)
    }







    async process(message) {
        throw new Error('execute method not implemented')
    }







    async doEmit(message) {
        this.emit(message)
    }

















    emit(event, message) {



        super.emit(event, message)
        return message


    }

    getOutputs() {
        const results = this.outputs
        this.outputs = []
        return results
    }
}

export default Processor

================
File: src/processors/flow/FlowProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import ForEach from './ForEach.js'

class FlowProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.ForEach)) {
            logger.debug('FlowProcessorsFactory: Creating ForEach processor')
            return new ForEach(config)
        }

        return false
    }
}

export default FlowProcessorsFactory

================
File: src/processors/flow/ForEach.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ForEach extends Processor {
    constructor(config) {
        super(config)
    }

    async process(message) {
        logger.setLogLevel('debug')
        logger.debug('ForEach execute method called')

        if (!message.foreach || !Array.isArray(message.foreach)) {
            logger.error('ForEach: Invalid or missing foreach array in message')
            message.foreach = ["testing-testing", "one", "two", "three"]

        }

        for (const item of message.foreach) {
            const clonedMessage = structuredClone(message)
            clonedMessage.currentItem = item
            delete clonedMessage.foreach

            logger.debug(`ForEach: Emitting message for item: ${item}`)
            this.emit('message', clonedMessage)
        }

        logger.debug('ForEach: Finished processing all items')
    }
}
export default ForEach

================
File: src/processors/fs/DirWalker.js
================
import { readdir } from 'fs/promises'
import { join, extname } from 'path'
import grapoi from 'grapoi'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
















class DirWalker extends Processor {





    constructor(config) {
        super(config)
        const dataset = this.config
        const poi = grapoi({ dataset })
        this.includeExtensions = ['.md']
        this.excludePrefixes = ['_']
    }






    async process(message) {
        logger.setLogLevel('info')
        logger.debug('DirWalker.process')
        await this.emitThem(message)



        message.done = true

        this.emit('message', message)
    }

    async emitThem(message) {
        message.counter = 0
        message.slugs = []
        message.done = false

        const dirPath = join(message.rootDir, message.sourceDir)
        logger.debug('DirWalker, dirPath = ' + dirPath)


        const entries = await readdir(dirPath, { withFileTypes: true })
        for (const entry of entries) {
            const fullPath = join(dirPath, entry.name)
            if (entry.isDirectory()) {
                await this.process(entry.name, message)
            } else {
                logger.debug('DirWalker, entry.name = ' + entry.name)

                const includeExtension = this.includeExtensions.includes(extname(entry.name))
                const prefix = entry.name.charAt(0)
                const excludePrefix = this.excludePrefixes.includes(prefix)


                if (!excludePrefix && includeExtension) {

                    message.filename = entry.name
                    message.filepath = join(message.sourceDir, '/', entry.name)
                    const slug = this.extractSlug(message.filename)
                    message.slugs.push(slug)





                    message.done = false
                    message.counter = message.counter + 1
                    const messageClone = structuredClone(message)
                    this.emit('message', messageClone)
                }
            }
        }




    }

    extractSlug(filepath) {
        var slug = filepath
        if (slug.includes('.')) {
            slug = slug.substr(0, slug.lastIndexOf("."))
        }
        return slug
    }
}
export default DirWalker

================
File: src/processors/fs/FileCopy.js
================
import { copyFile, mkdir, readdir, stat } from 'node:fs/promises'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../base/Processor.js'


class FileCopy extends Processor {
    constructor(config) {
        super(config)
    }





    async process(message) {



        if (message.rootDir == "") {
            message.rootDir = message.applicationRootDir
        }
        logger.debug("message.rootDir = " + message.rootDir)
        var source, destination


        if (this.configKey === 'undefined') {
            logger.debug('FileCopy: using message.source and message.destination')
            source = message.source
            destination = message.destination
        } else {
            logger.debug(`FileCopy: using configKey ${this.configKey.value}`)
            source = this.getPropertyFromMyConfig(ns.trm.source)
            destination = this.getPropertyFromMyConfig(ns.trm.destination)
            source = path.join(message.rootDir, source)
            destination = path.join(message.rootDir, destination)
        }

        logger.debug(`Source: ${source}`)
        logger.debug(`Destination: ${destination}`)

        try {
            await this.ensureDirectoryExists(path.dirname(destination))
            const sourceStat = await stat(source)

            if (sourceStat.isFile()) {
                logger.debug(`Copying file from ${source} to ${destination}`)
                await copyFile(source, destination)
            } else if (sourceStat.isDirectory()) {
                logger.debug(`Copying directory from ${source} to ${destination}`)
                await this.copyDirectory(source, destination)
            }
        } catch (err) {
            logger.error(`Error in FileCopy: ${err.message}`)
            logger.error(`Source: ${source}`)
            logger.error(`Destination: ${destination}`)
        }

        return this.emit('message', message)
    }





    async ensureDirectoryExists(dirPath) {
        logger.debug(`Ensuring directory exists: ${dirPath}`)
        try {
            await mkdir(dirPath, { recursive: true })
            logger.debug(`Directory created/ensured: ${dirPath}`)
        } catch (err) {
            logger.debug(`Error creating directory ${dirPath}: ${err.message}`)
            throw err
        }
    }






    async copyDirectory(source, destination) {
        logger.debug(`Copying directory: ${source} to ${destination}`)
        try {
            await this.ensureDirectoryExists(destination)
            const entries = await readdir(source, { withFileTypes: true })

            for (const entry of entries) {
                const srcPath = path.join(source, entry.name)
                const destPath = path.join(destination, entry.name)

                logger.debug(`Processing: ${srcPath} to ${destPath}`)

                if (entry.isDirectory()) {
                    await this.copyDirectory(srcPath, destPath)
                } else {
                    await copyFile(srcPath, destPath)
                    logger.debug(`File copied: ${srcPath} to ${destPath}`)
                }
            }
        } catch (err) {
            logger.debug(`Error in copyDirectory: ${err.message}`)
            throw err
        }
    }
}

export default FileCopy

================
File: src/processors/fs/FileReader.js
================
import { readFile } from 'node:fs/promises'
import { access, constants } from 'node:fs'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../base/Processor.js'











class FileReader extends Processor {





    constructor(config) {
        super(config)
    }






    async process(message) {
        logger.setLogLevel('info')
        var filepath = message.filepath

        if (!filepath) {
            filepath = this.getPropertyFromMyConfig(ns.trm.sourceFile)
            logger.debug(' - filepath from config : ' + filepath)
        }
        logger.debug(' - FileReader, process.cwd() : ' + process.cwd())

        var f = path.join(message.dataDir, filepath)



        logger.log(' - FileReader reading filepath : ' + f)

        const mediaType = this.getPropertyFromMyConfig(ns.trm.mediaType)


        logger.debug('FileReader, mediaType = ' + mediaType)


        access(f, constants.R_OK, (err) => {
            if (err) {
                logger.error(`FileReader error : ${f} is not readable.`)
                logger.reveal(message)
            }
        })

        const content = (await readFile(f)).toString()

        logger.debug('FileReader, content = ' + content)


        if (mediaType === 'application/json') {
            message.content = JSON.parse(content)
        } else {
            message.content = content
        }
        return this.emit('message', message)
    }
}

export default FileReader

================
File: src/processors/fs/FileRemove.js
================
import { unlink, readdir, stat, rm } from 'node:fs/promises'
import path from 'path'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class FileRemove extends Processor {
    constructor(config) {
        super(config)
    }





    async process(message) {

        logger.setLogLevel('debug')

        this.ignoreDotfiles = true

        var target




        if (this.configKey === 'undefined') {
            logger.debug('FileRemove no configKey from transmission, using message.target')
            target = message.target
        } else {
            logger.debug('FileRemove this.configKey = ' + this.configKey.value)
            target = this.getPropertyFromMyConfig(ns.trm.target)
            target = path.join(message.rootDir, target)
        }

        logger.debug('FileRemove, target = ' + target)
        try {
            const removeStat = await stat(target)

            if (removeStat.isFile()) {
                await this.removeFile(target)
            } else if (removeStat.isDirectory()) {
                await this.removeDirectoryContents(target)
            }
        } catch (err) {

            logger.debug('FileRemove, target stat caused err : ' + target)
        }

        return this.emit('message', message)
    }





    async removeFile(filePath) {
        await unlink(filePath)
    }





    async removeDirectoryContents(dirPath) {
        logger.debug('FileRemove, dirPath = ' + dirPath)
        const entries = await readdir(dirPath, { withFileTypes: true })

        for (const entry of entries) {
            if (this.ignoreDotfiles && (entry.name.charAt(0) === ".")) {
                continue
            }
            const entryPath = path.join(dirPath, entry.name)

            if (entry.isDirectory()) {
                await this.removeDirectoryContents(entryPath)
            } else {
                await unlink(entryPath)
            }
        }
    }
}

export default FileRemove

================
File: src/processors/fs/FileWriter.js
================
import path from 'path'
import { access, constants } from 'node:fs'
import ns from '../../utils/ns.js'
import { writeFile } from 'node:fs/promises'
import { dirname, join } from 'node:path'
import { mkdir, mkdirSync } from 'node:fs'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'














class FileWriter extends Processor {





    constructor(config) {
        super(config)
    }

    getInputKeys() {
        return ['filepath, content']
    }




    async process(message) {
        logger.setLogLevel('info')
        this.preProcess()

        if (message.dump) {



            const filename = 'message.json'
            const f = path.join(message.dataDir, filename)
            const content = JSON.stringify(message)


            access(f, constants.W_OK, (err) => {
                if (err) {
                    logger.error(`FileWriter error : ${f} is not writable.`)
                    logger.reveal(message)
                }
            })

            return this.doWrite(f, content, message)
        }

        logger.debug("Filewriter, message.filepath = " + message.filepath)

        var filepath = message.filepath


        if (!filepath) {
            filepath = this.getPropertyFromMyConfig(ns.trm.destinationFile)
            logger.log(' - filepath from config : ' + filepath)
        }

        var f
        if (filepath.startsWith('/')) {
            f = filepath
        } else {
            f = path.join(message.dataDir, filepath)
        }
        const dirName = dirname(f)
        logger.debug("Filewriter, dirName = " + dirName)

        var contentPath = this.getPropertyFromMyConfig(ns.trm.contentPath)

        if (typeof contentPath === 'undefined' || contentPath === 'undefined' || contentPath.value === 'undefined') {
            contentPath = 'content'
        }


        var content = message[contentPath.toString()]
        if (typeof content === 'object') {
            content = JSON.stringify(content)
        }

        logger.debug("Filewriter, content = " + content)
        logger.debug("Filewriter, typeof content = " + typeof content)


        this.mkdirs(dirName)

        return await this.doWrite(f, content, message)
    }

    async doWrite(f, content, message) {
        logger.log(' - FileWriter writing : ' + f)
        await writeFile(f, content)
        return this.emit('message', message)
    }

    mkdirs(dir) {
        mkdirSync(dir, { recursive: true })





    }
}

export default FileWriter

================
File: src/processors/fs/FsProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import DirWalker from './DirWalker.js'
import FileReader from './FileReader.js'
import FileWriter from './FileWriter.js'
import FileCopy from './FileCopy.js'
import FileRemove from './FileRemove.js'

class FsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.DirWalker)) {
            return new DirWalker(config)
        }
        if (type.equals(ns.t.FileReader)) {
            return new FileReader(config)
        }
        if (type.equals(ns.t.FileWriter)) {
            return new FileWriter(config)
        }
        if (type.equals(ns.t.FileCopy)) {
            return new FileCopy(config)
        }
        if (type.equals(ns.t.FileRemove)) {
            return new FileRemove(config)
        }
        return false
    }
}

export default FsProcessorsFactory

================
File: src/processors/github/GitHubList_no-pag.js
================
import { Octokit } from '@octokit/rest'
import dotenv from 'dotenv'
import Processor from '../base/Processor.js'
import logger from '../../utils/Logger.js'

dotenv.config({ path: './trans-apps/applications/git-apps/.env' })

class GitHubList extends Processor {
    constructor(config) {
        super(config)
        logger.debug('GitHubList constructor called')
        this.octokit = new Octokit({ auth: process.env.GITHUB_TOKEN })
        logger.debug('Octokit instance created')
    }

    async process(message) {
        logger.debug('GitHubList execute method called')
        logger.debug('Input message:', JSON.stringify(message, null, 2))

        if (!message.github || !message.github.name) {
            logger.error('GitHub username not provided in the message')
            throw new Error('GitHub username not provided in the message')
        }

        const username = message.github.name
        logger.debug(`Fetching repositories for username: ${username}`)

        try {
            logger.debug('Calling GitHub API')
            const { data } = await this.octokit.repos.listForUser({ username })
            logger.debug(`Fetched ${data.length} repositories`)

            const repositories = data.map(repo => repo.name)
            logger.debug('Extracted repository names:' + repositories)

            message.github.repositories = repositories
            logger.debug('Updated message:', JSON.stringify(message, null, 2))

            this.emit('message', message)
            logger.debug('Emitted updated message')
        } catch (error) {
            logger.error(`Error fetching repositories for ${username}:`, error)
            logger.debug('Error details:', JSON.stringify(error, null, 2))
            if (error.status === 403) {
                logger.warn('Possible rate limit exceeded. Check GitHub API rate limits.')
            }
            throw error
        }
    }
}

export default GitHubList

================
File: src/processors/github/GitHubList.js
================
import { Octokit } from '@octokit/rest'
import dotenv from 'dotenv'
import Processor from '../../../../transmissions/src/processors/base/Processor.js'
import logger from '../../../../transmissions/src/utils/Logger.js'

dotenv.config({ path: './trans-apps/applications/git-apps/.env' })

class GitHubList extends Processor {
    constructor(config) {
        super(config)
        logger.debug('GitHubList constructor called')
        this.octokit = new Octokit({ auth: process.env.GITHUB_TOKEN })
        logger.debug('Octokit instance created')
    }

    async process(message) {
        logger.debug('GitHubList process method called')

        try {

            if (!message.payload) {
                message.payload = {}
            }
            if (!message.payload.github) {
                message.payload.github = {}
            }

            const username = message.payload.github.name
            logger.debug(`Processing for username: ${username}`)

            logger.debug('Calling GitHub API with pagination')
            logger.info(`Starting repository fetch for ${username}`)

            const repositories = await this.fetchAllRepositories(username)
            logger.debug(`Setting ${repositories.length} repositories in payload`)


            message.payload.github.repositories = repositories
            message.payload.github.totalRepos = repositories.length

            return this.emit('message', message)
        } catch (error) {
            this.handleError(error, username)
        }
    }

    async fetchAllRepositories(username) {
        const repositories = []
        let page = 1
        const delay = ms => new Promise(resolve => setTimeout(resolve, ms))

























        while (true) {
            const response = await this.octokit.repos.listForUser({
                username,
                per_page: 100,
                page
            })

            let data = response.data


            logger.debug(`Page ${page}: Got ${data.length} repos`)

            repositories.push(...data.map(repo => repo.name))

            if (data.length < 100) break
            page++


            await new Promise(r => setTimeout(r, 5000))
        }

        logger.debug(`Total repositories found: ${repositories.length}`)

        return repositories
    }

    checkRateLimit(headers) {
        const remaining = headers['x-ratelimit-remaining']
        const resetTime = new Date(headers['x-ratelimit-reset'] * 1000)
        logger.info(`Rate limit remaining: ${remaining}, Reset time: ${resetTime}`)

        if (remaining < 10) {
            logger.warn(`Rate limit is low. Only ${remaining} requests left. Reset at ${resetTime}`)
        }
    }

    createDetailedError(error, message) {
        const detailedError = new Error(`${message}: ${error.message}`)
        detailedError.status = error.status
        detailedError.response = error.response
        return detailedError
    }

    handleError(error, username) {
        logger.error(`Error fetching repositories for ${username}:`, error.message)
        logger.debug('Error details:', JSON.stringify(error, null, 2))

        if (error.status === 403) {
            logger.warn('Rate limit exceeded. Check GitHub API rate limits.')
            throw new Error('GitHub API rate limit exceeded')
        } else if (error.status === 404) {
            logger.warn(`User ${username} not found on GitHub`)
            throw new Error(`GitHub user ${username} not found`)
        } else {
            throw new Error(`Failed to fetch GitHub repositories: ${error.message}`)
        }
    }
}

export default GitHubList

================
File: src/processors/github/GitHubProcessorsFactory.js
================
import logger from '../../../../transmissions/src/utils/Logger.js';
import ns from '../../../../transmissions/src/utils/ns.js';
import GitHubList from './GitHubList.js';

class GitHubProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.GitHubList)) {
            return new GitHubList(config);
        }
        return false;
    }
}

export default GitHubProcessorsFactory;

================
File: src/processors/json/Blanker.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'

class Blanker extends Processor {
    constructor(config) {
        super(config)
        this.blankValue = config.blankValue || ''
    }

    blankValues(obj) {
        if (Array.isArray(obj)) {
            return obj.map(item => this.blankValues(item))
        } else if (typeof obj === 'object' && obj !== null) {
            const result = {}
            for (const [key, value] of Object.entries(obj)) {
                result[key] = this.blankValues(value)
            }
            return result
        } else if (typeof obj === 'string') {
            return ''
        }
        return obj
    }

    async process(message) {
        try {
            const pointer = this.getPropertyFromMyConfig(ns.trm.pointer)

            if (!pointer) {
                // Blank entire message if no pointer specified
                message = this.blankValues(message)
            } else {
                // Get the nested object using the pointer
                const parts = pointer.toString().split('.')
                let target = message

                // Navigate to the target object
                for (let i = 0; i < parts.length - 1; i++) {
                    target = target[parts[i]]
                    if (!target) break
                }

                // Blank values in the target object
                if (target && target[parts[parts.length - 1]]) {
                    target[parts[parts.length - 1]] =
                        this.blankValues(target[parts[parts.length - 1]])
                }
            }

            return this.emit('message', message)
        } catch (err) {
            logger.error("Blanker processor error: " + err.message)
            throw err
        }
    }
}

export default Blanker

================
File: src/processors/json/JSONProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import JSONWalker from './JSONWalker.js'
import Restructure from './Restructure.js'
import ValueConcat from './ValueConcat.js'
import Blanker from './Blanker.js'

class JSONProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.Restructure)) {
            return new Restructure(config)
        }
        if (type.equals(ns.t.JSONWalker)) {
            return new JSONWalker(config)
        }
        if (type.equals(ns.t.ValueConcat)) {
            return new ValueConcat(config)
        }
        if (type.equals(ns.t.Blanker)) {
            return new Blanker(config)
        }
        return false
    }

}
export default JSONProcessorsFactory

================
File: src/processors/json/JsonRestructurer.js
================
import logger from '../../utils/Logger.js'

class JsonRestructurer {
    constructor(mappings) {
        logger.setLogLevel('info')
        if (!mappings?.mappings || !Array.isArray(mappings.mappings)) {
            throw new Error('Invalid mapping structure')
        }
        this.mappings = mappings.mappings
        logger.debug('JsonRestructurer,  this.mappings = ' + this.mappings)
        logger.reveal(this.mappings)
    }

    getValueByPath(obj, path) {







        try {
            const sp = path.split('.')
            logger.debug('JsonRestructurer, sp = ' + sp)
            const reduced = sp.reduce((acc, part) => acc[part], obj)
            logger.debug('JsonRestructurer, reduced = ' + reduced)
            return reduced
        } catch (e) {
            logger.warn(`Warning: Path ${path} not found`)
            return undefined
        }
    }

    setValueByPath(obj, path, value) {
        logger.debug(`JsonRestructurer.setValueByPath, obj = ${obj}, path = ${path}, value = ${value}`)
        const parts = path.split('.')
        const last = parts.pop()
        const target = parts.reduce((acc, part) => {
            acc[part] = acc[part] || {}
            return acc[part]
        }, obj)
        logger.debug(`JsonRestructurer.setValueByPath, target = ${target}, last = ${last}, value = ${value}`)
        target[last] = value
    }

    restructure(inputData) {

        if (typeof inputData === 'string') {
            try {
                inputData = JSON.parse(inputData)
            } catch (e) {
                throw new Error('Invalid JSON string provided')
            }
        }


        const result = {}
        this.mappings.forEach(({ pre, post }) => {

            const value = this.getValueByPath(inputData, pre)

            if (value !== undefined) {
                this.setValueByPath(result, post, value)
            }
        })



        return result
    }
}
export default JsonRestructurer

================
File: src/processors/json/JSONWalker.js
================
import path from 'path'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'

class JSONWalker extends Processor {
    constructor(config) {
        super(config)
    }







    async process(message) {
        try {
            message.done = false
            var pointer = this.getPropertyFromMyConfig(ns.trm.pointer)

            logger.debug(`JSONWalker pointer =  ${pointer}`)

            const content = structuredClone(message.content)
            message.content = {}


            for (var i = 0; i < content.length; i++) {
                const newMessage = structuredClone(message)
                newMessage.content = content[i]
                this.emit('message', newMessage)
            }

            var finalMessage = structuredClone(message)
            finalMessage.content = content[content.length - 1]













            finalMessage.done = true
            this.emit('message', finalMessage)
        } catch (err) {
            logger.error("JSONWalker.process error: " + err.message)
            throw err
        }
    }
}

export default JSONWalker

================
File: src/processors/json/Restructure.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import JsonRestructurer from './JsonRestructurer.js'
import ns from '../../utils/ns.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import rdf from 'rdf-ext'

class Restructure extends Processor {
    constructor(config) {
        super(config)
    }

    async getRenames(config, configKey, term) {



        const renamesRDF = GrapoiHelpers.listToArray(config, configKey, term)
        const dataset = this.config
        var renames = []
        for (let i = 0; i < renamesRDF.length; i++) {
            let rename = renamesRDF[i]
            let poi = rdf.grapoi({ dataset: dataset, term: rename })
            let pre = poi.out(ns.trm.pre).value
            let post = poi.out(ns.trm.post).value
            renames.push({ "pre": pre, "post": post })
        }
        return renames
    }

    async process(message) {
        logger.setLogLevel('info')


        var renames
        if (this.config.simples) {
            renames = this.config.rename
        } else {
            renames = await this.getRenames(this.config, this.configKey, ns.trm.rename)
        }





        this.restructurer = new JsonRestructurer({
            mappings: renames
        })
        try {
            logger.debug('Restructure processor executing...')



            const input = structuredClone(message)


            const restructured = this.restructurer.restructure(input)

            const type = typeof restructured




            for (const key of Object.keys(restructured)) {
                message[key] = restructured[key]
            }


            logger.debug('Restructure successful')
            return this.emit('message', message)

        } catch (err) {
            logger.error("Restructure processor error: " + err.message)
            logger.reveal(message)
            throw err
        }
    }
}

export default Restructure

================
File: src/processors/json/ValueConcat.js
================
import logger from '../../utils/Logger.js'
import rdf from 'rdf-ext'
import ns from '../../utils/ns.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import Processor from '../base/Processor.js'

class ValueConcat extends Processor {

    constructor(config) {
        super(config)
        logger.log('CREATING VALUECONCAT')
    }

}
export default ValueConcat

================
File: src/processors/markup/LinkFinder.js
================
import * as cheerio from 'cheerio'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class LinkFinder extends Processor {

    async process(message) {

        await this.extractLinks(message)

        if (data === '~~done~~') {
            logger.log('LF DONE*****************')
            return this.emitLocal('message', '~~done~~', message)
            return
        }
    }


    relocate(filename, extension) {
        const split = filename.split('.').slice(0, -1)
        return split.join('.') + extension
    }

    async extractLinks(htmlContent, message) {

        const $ = cheerio.load(htmlContent)
        let label = ''

        $('a, h1, h2, h3, h4, h5, h6').each((_, element) => {
            const tagName = element.tagName.toLowerCase()
            if (tagName.startsWith('h')) {
                const level = tagName.substring(1)
                const headerText = $(element).text()
                label = `\n\n${'#'.repeat(parseInt(level))} ${headerText}\n`
            } else if (tagName === 'a') {
                const linkText = $(element).text()

                let href = $(element).attr('href')

                if (!href || href.startsWith('#')) return

                if (href && !href.includes('://')) {

                    const baseURL = message.sourceURL

                    href = new URL(href, baseURL).toString()
                }
                label = `\n[${linkText}](${href})`

            }
            message.label = label
            return this.emit('message', message)
        })
    }
}

export default LinkFinder

================
File: src/processors/markup/MarkdownToHTML.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

import { marked } from 'marked'


import markedFootnote from 'marked-footnote'
import markedCodeFormat from 'marked-code-format'


class MarkdownToHTML extends Processor {


    async process(message) {
        const input = message.content


        message.content = await
            marked

                .use(markedFootnote())
                .use(
                    markedCodeFormat({

                    })
                )
                .parse(input.toString())

        return this.emit('message', message)
    }
}

export default MarkdownToHTML

================
File: src/processors/markup/MarkupProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import MetadataExtractor from './MetadataExtractor.js'
import LinkFinder from './LinkFinder.js'
import MarkdownToHTML from './MarkdownToHTML.js'

class MarkupProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.MetadataExtractor)) {
            return new MetadataExtractor(config)
        }
        if (type.equals(ns.t.MarkdownToHTML)) {
            return new MarkdownToHTML(config)
        }
        if (type.equals(ns.t.LinkFinder)) {
            return new LinkFinder(config)
        }
        return false
    }
}

export default MarkupProcessorsFactory

================
File: src/processors/markup/MetadataExtractor.js
================
import * as cheerio from 'cheerio'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class MetadataExtractor extends Processor {

    async process(message) {
        const filename = data.filename
        const content = data.content

        logger.debug("MetadataExtractor input file : " + filename)
        const targetFilename = this.relocate(filename)
        logger.debug("MetadataExtractor outputfile : " + targetFilename)

        const jsonData = this.convertEmailToJSON(content)

        const jsonString = JSON.stringify(jsonData)

        const output = { filename: targetFilename, content: jsonString }

        return this.emit('message', output, message)
    }

    relocate(filename) {

        const split = filename.split('.').slice(0, -1)
        var newFileName = split.join('.') + '.json'
        return newFileName
    }

    convertEmailToJSON(htmlContent) {
        const $ = cheerio.load(htmlContent)
        var subjectLine = $('H1').text().trim()
        var fromName = $('B').first().text().trim()
        var nextMessageLink = $('LINK[REL="Next"]').attr('HREF')
        var previousMessageLink = $('LINK[REL="Previous"]').attr('HREF')
        var messageText = $('PRE').text().trim()
        messageText = this.pruneContent(messageText)
        const jsonResult = {
            subjectLine: subjectLine,
            fromName: fromName,
            nextMessageLink: nextMessageLink,
            previousMessageLink: previousMessageLink,
            messageText: messageText

        }























        return jsonResult
    }

    pruneContent(content) {

        const regex1 = /(^|\n).*?:\n>/s
        content = content.replace(regex1, '$1')

        const regex2 = /\n>.*?\n/g




        content = content.replace(regex2, '\n')

        return content
    }
}



export default MetadataExtractor

================
File: src/processors/postcraft/EntryContentToPagePrep.js
================
import path from 'path'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class EntryContentToPagePrep extends Processor {

  constructor(config) {
    super(config)
  }

  async process(message) {
    if (message.done) {
      return this.emit('message', message)
      return
    }
    logger.setLogLevel('debug')







    message.templateFilename = path.join(message.applicationRootDir, message.entryContentToPage.templateFilename)

    logger.log("################ message.templateFilename = " + message.templateFilename)
    message.template = false

    message.contentBlocks.content = message.content

    if (message.entryContentToPage.targetDir.startsWith('/')) {
      message.filepath = path.join(message.entryContentToPage.targetDir, message.slug + '.html')
    } else {
      message.filepath = path.join(message.rootDir, message.entryContentToPage.targetDir, message.slug + '.html')
    }
    logger.debug('EntryContentToPagePrep, message.filepath = ' + message.filepath)

    this.emit('message', message)
  }

}

export default EntryContentToPagePrep

================
File: src/processors/postcraft/FrontPagePrep.js
================
import path from 'path'
import { readFile } from 'node:fs/promises'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'


class FrontPagePrep extends Processor {
  constructor(config) {
    super(config)
  }


  async process(message) {

    try {
      message.templateFilename = message.rootDir + '/' + message.indexPage.templateFilename
      logger.debug('Template = ' + message.templateFilename)

      const rawEntryPaths = this.resolveRawEntryPaths(message)
      message.content = ''

      // TODO tidy up
      const entryCount = Math.min(5, rawEntryPaths.length) // Limit to 5 entries or less
      logger.debug('FrontPagePrep, entryCount = ' + entryCount)

      const rangeStart = rawEntryPaths.length - entryCount
      const rangeEnd = rawEntryPaths.length - 1
      //     for (let i = 0; i < entryCount; i++) {
      for (let i = rangeEnd; i >= rangeStart; i--) {
        logger.debug('FrontPagePrep, i = ' + entryCount)
        const rawEntryPath = rawEntryPaths[i]
        if (rawEntryPath) {
          message.content += await readFile(rawEntryPath, 'utf8')
        } else {
          logger.warn(`Skipping undefined entry path at index ${i}`)
        }
      }

      message.contentBlocks.content = message.content

      message.filepath = message.rootDir + '/' + message.indexPage.filepath

      return this.emit('message', message)
    } catch (err) {
      logger.error('Error in FrontPagePrep')
      logger.error(err)
    }
  }

  resolveRawEntryPaths(message) {
    const paths = []
    const slugs = message.slugs || []
    const entryCount = slugs.length

    for (let i = 0; i < entryCount; i++) {
      const slug = slugs[i]
      if (slug) {

        const filePath = path.join(message.rootDir, message.entryContentMeta.targetDir, slug + '.html')
        paths.push(filePath)
      }
    }

    return paths
  }
}

export default FrontPagePrep

================
File: src/processors/postcraft/PostcraftDispatcher.js
================
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'











class PostcraftDispatcher extends Processor {





  constructor(config) {
    super(config)
  }






  async process(message) {
    const postcraftConfig = message.dataset
    message.template = data.toString()
    logger.log('PostcraftDispatcherPostcraftDispatcherPostcraftDispatcher ' + data)
    const poi = grapoi({ dataset: postcraftConfig })

    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.pc.ContentGroup)) {
        await this.processContentGroup(message, q.subject)
      }
    }
  }






  async processContentGroup(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })
    const sourceDir = groupPoi.out(ns.fs.sourceDirectory).term.value
    const targetDir = groupPoi.out(ns.fs.targetDirectory).term.value
    const templateFilename = groupPoi.out(ns.pc.template).term.value





    message.sourceDir = sourceDir
    message.targetDir = targetDir
    message.templateFilename = templateFilename
    message.loadContext = 'template'

    return this.emit('message', sourceDir, message)
  }
}

export default PostcraftDispatcher

================
File: src/processors/postcraft/PostcraftPrep.js
================
import path from 'path'
import logger from '../../utils/Logger.js'

import Processor from '../base/Processor.js'

class PostcraftPrep extends Processor {

  constructor(config) {
    super(config)
  }

  async process(message) {
    logger.setLogLevel("debug")


    message.slug = this.extractSlug(message)
    message.targetFilename = this.extractTargetFilename(message)
    message.contentBlocks = {}
    message.contentBlocks.relURL = this.extractRelURL(message)


    message.contentBlocks.link = 'entries/' + message.contentBlocks.relURL

    message.contentBlocks.title = this.extractTitle(message)

    const { created, updated } = this.extractDates(message)
    message.contentBlocks.created = created
    message.contentBlocks.updated = updated

    return this.emit('message', message)
  }


  extractSlug(message) {

    var slug = message.filename
    if (slug.includes('.')) {
      slug = slug.substr(0, slug.lastIndexOf("."))
    }
    return slug
  }

  extractTargetFilename(message) {
    return path.join(message.rootDir, message.entryContentMeta.targetDir, this.extractSlug(message) + '.html')
  }

  extractRelURL(message) {
    return this.extractSlug(message) + '.html'
  }

  extractDates(message) {
    const today = (new Date()).toISOString().split('T')[0]
    const dates = { created: today, updated: today }


    const nonExt = message.filename.split('.').slice(0, -1).join()
    const shreds = nonExt.split('_')
    if (Date.parse(shreds[0])) {
      dates.created = shreds[0]
    }
    return dates
  }




  extractTitle(message) {
    let title = 'Title'
    let match = message.content.toString().match(/^#(.*)$/m)
    let contentTitle = match ? match[1].trim() : null
    if (contentTitle) {
      title = contentTitle.replaceAll('#', '') // TODO make nicer
      return title
    }

    // derive from filename
    // eg. 2024-04-19_hello-postcraft.md
    try {
      const nonExt = message.filename.split('.').slice(0, -1).join()
      const shreds = nonExt.split('_')


      title = shreds[1].split('-')
        .map(word => word.charAt(0).toUpperCase() + word.slice(1))
        .join(' ')
    } catch (err) {
      title = message.filename
    }
    return title
  }
}

export default PostcraftPrep

================
File: src/processors/postcraft/PostcraftProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import PostcraftDispatcher from './PostcraftDispatcher.js'
import PostcraftPrep from './PostcraftPrep.js'
import EntryContentToPagePrep from './EntryContentToPagePrep.js'
import FrontPagePrep from './FrontPagePrep.js'

class PostcraftProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.PostcraftDispatcher)) {
            return new PostcraftDispatcher(config)
        }
        if (type.equals(ns.t.PostcraftPrep)) {
            return new PostcraftPrep(config)
        }
        if (type.equals(ns.t.EntryContentToPagePrep)) {
            return new EntryContentToPagePrep(config)
        }
        if (type.equals(ns.t.FrontPagePrep)) {
            return new FrontPagePrep(config)
        }

        return false
    }
}

export default PostcraftProcessorsFactory

================
File: src/processors/protocols/HttpGet.js
================
import axios from 'axios'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class HttpGet extends Processor {
    constructor(config) {
        super(config)
    }







    async process(url, message) {
        logger.setLogLevel('debug')
        logger.debug('HttpGet, url = ' + url)
        if (url === '~~done~~') {
            logger.log('HG DONE*****************')
            return this.emit('message', url, message)
            return
        }
        try {
            logger.log('HG GETTING*****************')
            const response = await axios.get(url)
            const content = response.data

            message.sourceURL = url
            return this.emit('message', content, message)
        } catch (error) {
            logger.error("HttpGet.execute error\n" + error)
        }
    }
}

export default HttpGet

================
File: src/processors/protocols/ProtocolsProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import HttpGet from './HttpGet.js'



class ProtocolsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.HttpGet)) {
            return new HttpGet(config)
        }

        return false
    }
}

export default ProtocolsProcessorsFactory

================
File: src/processors/rdf/ConfigMap.js
================
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ConfigMap extends Processor {
  constructor(config) {
    super(config)
  }






  async process(message) {
    logger.setLogLevel('debug')

    logger.debug(`ConfigMap, Using configKey ${this.configKey.value}`)

    const group = this.getPropertyFromMyConfig(ns.trm.group)
    const targetGroup = rdf.namedNode(group)
    logger.debug(`ConfigMap, group =  ${targetGroup}`)



    this.preProcess(message)
    const dataset = message.dataset
    const poi = grapoi({ dataset, factory: rdf })
    const quads = await poi.out(ns.rdf.type).quads()

    for (const q of quads) {
      const type = q.object


      if (type.equals(targetGroup)) {

        await this.processContentGroup(message, q.subject)
      }
    }

    return this.emit('message', message)
  }






  async processContentGroup(message, contentGroupID) {
    logger.debug('contentGroupID = ' + contentGroupID.value)
    switch (contentGroupID.value) {
      case ns.t.PostContent.value:
        await this.markdownToEntryContent(message, contentGroupID)
        break
      case ns.t.PostPages.value:
        await this.entryContentToPostPage(message, contentGroupID)
        break
      case ns.t.IndexPage.value:
        await this.indexPage(message, contentGroupID)
        break
      default:
        logger.log('Group not found in dataset: ' + contentGroupID.value)
    }
  }






  async markdownToEntryContent(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })



    message.filepath = groupPoi.out(ns.pc.template).term.value
    message.template = ' placeholer for debugging '

    message.entryContentMeta = {
      sourceDir: groupPoi.out(ns.fs.sourceDirectory).term.value,
      targetDir: groupPoi.out(ns.fs.targetDirectory).term.value,
      templateFilename: groupPoi.out(ns.pc.template).term.value
    }
  }






  async entryContentToPostPage(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })

    message.entryContentToPage = {
      targetDir: groupPoi.out(ns.fs.targetDirectory).term.value,
      templateFilename: groupPoi.out(ns.pc.template).term.value
    }
  }






  async indexPage(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })

    message.indexPage = {
      filepath: groupPoi.out(ns.fs.filepath).term.value,
      templateFilename: groupPoi.out(ns.pc.template).term.value
    }
  }
}

export default ConfigMap

================
File: src/processors/rdf/DatasetReader.js
================
import path from 'path'
import rdf from 'rdf-ext'
import { fromFile } from 'rdf-utils-fs'
import Processor from '../base/Processor.js'
import logger from '../../utils/Logger.js'

class DatasetReader extends Processor {
    constructor(config) {
        super(config)
    }





    getInputKeys() {
        return ['sdfsdf']
    }





    getOutputKeys() {
        return ['sdfsdfsdfdataset']
    }





    async process(message) {
        this.preProcess(message)
        var datasetName = 'dataset'
        var datasetFilename = path.join(message.rootDir, '/manifest.ttl')
        if (message.datasetFilename) {
            datasetFilename = message.datasetFilename
        }
        if (message.datasetName) {
            datasetName = message.datasetName
        }

        const stream = fromFile(datasetFilename)


        message[datasetName] = await rdf.dataset().import(stream)
        logger.debug(`DatasetReader, datasetName = ${datasetName}`)

        return this.emit('message', message)
    }
}

export default DatasetReader

================
File: src/processors/rdf/RDFProcessorsFactory.js
================
import ns from '../../utils/ns.js'
import DatasetReader from './DatasetReader.js'
import ConfigMap from './ConfigMap.js'

class RDFProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.DatasetReader)) {
            return new DatasetReader(config)
        }
        if (type.equals(ns.t.ConfigMap)) {
            return new ConfigMap(config)
        }
        return false
    }
}

export default RDFProcessorsFactory

================
File: src/processors/staging/MarkdownFormatter.js
================
import path from 'path'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class MarkdownFormatter extends Processor {
    constructor(config) {
        super(config)
    }

    async process(message) {

        if (message.done) return


        const dir = '/home/danny/github-danny/hyperdata/docs/postcraft/content-raw/chat-archives/md'

        const filename = `${message.content.created_at.substring(0, 10)}_${message.content.uuid.substring(0, 3)}.md`


        message.filepath = path.join(dir, message.meta.conv_uuid.substring(0, 4), filename)
        message.content = this.extractMarkdown(message)

        return this.emit('message', message)
    }



    extractMarkdown(message) {

        const urlBase = 'https://claude.ai/chat/'

        const lines = []
        lines.push(`# [${message.meta.conv_name}](${urlBase}${message.meta.conv_uuid})\n`)

        lines.push(`${message.content.uuid}\n`)

        lines.push(message.content.text)
        lines.push('\n---\n')

        for (const [key, value] of Object.entries(message)) {
            if (key !== 'content' && value !== null) {
                const v = typeof value === 'object' ? JSON.stringify(value, null, 2) : value.toString()
                lines.push(`* **${key}** : ${v}`)
            }
        }

        return lines.join('\n')
    }

}

export default MarkdownFormatter

================
File: src/processors/staging/StagingProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import MarkdownFormatter from './MarkdownFormatter.js'
import TurtleFormatter from './TurtleFormatter.js'




class StagingProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.t.MarkdownFormatter)) {
            return new MarkdownFormatter(config)
        }
        if (type.equals(ns.t.TurtleFormatter)) {
            return new TurtleFormatter(config)
        }
        return false
    }
}
export default StagingProcessorsFactory

================
File: src/processors/staging/TurtleFormatter.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class TurtleFormatter extends Processor {
    constructor(config) {
        super(config)
        this.baseURI = config.baseURI || 'http://example.org/'
    }

    async process(message) {
        try {
            const item = message.currentItem
            if (!item) {
                return
            }


            const turtle = this.formatTurtle(item)
            message.content = turtle
            message.targetFile = `${item.id}.ttl`

            this.emit('message', message)
        } catch (err) {
            logger.error("TurtleFormatter.execute error: " + err.message)
            throw err
        }
    }

    formatTurtle(item) {
        const lines = []
        lines.push('@prefix : <http://example.org/ns#> .')
        lines.push('@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .')
        lines.push('')

        const subject = `<${this.baseURI}${item.id}>`
        lines.push(`${subject} a :Item ;`)

        const entries = Object.entries(item)
        entries.forEach(([key, value], index) => {
            if (value !== null) {
                const isLast = index === entries.length - 1
                const literal = typeof value === 'string' ?
                    `"${value.replace(/"/g, '\\"')}"` :
                    `"${JSON.stringify(value)}"`
                lines.push(`    :${key} ${literal}${isLast ? ' .' : ' ;'}`)
            }
        })

        return lines.join('\n')
    }
}

export default TurtleFormatter

================
File: src/processors/system/EnvLoader.js
================
import 'dotenv/config'



import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'














class EnvLoader extends Processor {





    constructor(config) {
        super(config)
    }





    async process(message) {
        logger.setLogLevel('debug')


        this.config.whiteboard.env = process.env



        return super.handle(message)

    } catch(err) {
        logger.error("EnvLoader.execute error : " + err.message)
    }
}

export default EnvLoader

================
File: src/processors/system/SystemProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import EnvLoader from './EnvLoader.js'



class SystemsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.EnvLoader)) {
            return new EnvLoader(config)
        }
        return false
    }
}
export default SystemsProcessorsFactory

================
File: src/processors/templates/ProcessorsFactoryTemplate.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import ProcessorTemplate from './ProcessorTemplate.js'



class ProcessorsFactoryTemplate {
    static createProcessor(type, config) {

        if (type.equals(ns.t.ProcessorTemplate)) {
            return new ProcessorTemplate(config)
        }

        return false
    }
}
export default ProcessorsFactoryTemplate

================
File: src/processors/templates/ProcessorTemplate.js
================
import { readFile } from 'node:fs/promises'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'









class ProcessorExample extends Processor {





    constructor(config) {
        super(config)
    }





    async process(message) {
        logger.setLogLevel('debug')


        return this.emit('message', message)
    } catch(err) {
        logger.error("ProcessorExample.execute error : " + err.message)
    }
}

export default ProcessorExample

================
File: src/processors/test/AppendProcess.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class AppendProcess extends Processor {


    async process(message) {
        logger.debug("AppendProcess data : " + message.content)
        message.content = message.content + " world"
        return this.emit('message', message)
    }
}

export default AppendProcess

================
File: src/processors/test/FileSink.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import { writeFile } from 'node:fs/promises'
import footpath from '../../utils/footpath.js'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class FileSink extends Processor {

    constructor(config) {
        super(config)
        const dataset = this.config
        const poi = grapoi({ dataset })
        this.destinationFile = poi.out(ns.trm.destinationFile).value
    }


    async process(message) {
        const toRootDir = '../../../'
        const dataDir = path.join(toRootDir, message.dataDir)
        const df = footpath.resolve(import.meta.url, dataDir, this.destinationFile)
        logger.debug("FileSink to = " + df)
        await writeFile(df, message.content)
        return this.emit('message', message)
    }
}

export default FileSink

================
File: src/processors/test/FileSource.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import { readFile } from 'node:fs/promises'

import footpath from '../../utils/footpath.js'
import rdf from 'rdf-ext'

import grapoi from 'grapoi'
import ns from '../../utils/ns.js'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class FileSource extends Processor {

    constructor(config) {
        super(config)
        const dataset = this.config
        const poi = grapoi({ dataset })
        this.sourceFile = poi.out(ns.trm.sourceFile).value
    }


    async process(message) {
        try {
            const toRootDir = '../../../'
            const dataDir = toRootDir + message.dataDir
            const sf = footpath.resolve(import.meta.url, dataDir, this.sourceFile)
            logger.debug('FileSource file : ' + sf)
            const contents = await readFile(sf, { encoding: 'utf8' })
            logger.debug('FileSource data : ' + contents)
            return this.emit('message', { content: contents, ...message })
        } catch (err) {
            logger.error("FileSource.execute error : " + err.message)
        }
    }
}

export default FileSource

================
File: src/processors/test/StringSink.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class StringSink extends Processor {

    process(message) {
        logger.log("\n\nStringSink outputs : \"" + data + "\"\n\n")
    }
}

export default StringSink

================
File: src/processors/test/StringSource.js
================
import Processor from '../base/Processor.js'

class StringSource extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        console.log("message = " + message)
        console.log("data = " + data)
        return this.emit('message', message)
    }
}

export default StringSource

================
File: src/processors/test/TestProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import StringSource from './StringSource.js'
import StringSink from './StringSink.js'
import AppendProcess from './AppendProcess.js'
import FileSource from './FileSource.js'
import FileSink from './FileSink.js'



class TestProcessorsFactory {
    static createProcessor(type, config) {



        if (type.equals(ns.t.StringSource)) {
            return new StringSource(config)
        }
        if (type.equals(ns.t.StringSink)) {
            return new StringSink(config)
        }
        if (type.equals(ns.t.AppendProcess)) {
            return new AppendProcess(config)
        }


        if (type.equals(ns.t.FileSource)) {
            return new FileSource(config)
        }
        if (type.equals(ns.t.FileSink)) {
            return new FileSink(config)
        }

        return false
    }
}

export default TestProcessorsFactory

================
File: src/processors/text/LineReader.js
================
import { readFile } from 'node:fs/promises'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class LineReader extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {

        const text = data.toString()


        const lines = text.split('\n')
        for await (let line of lines) {
            if (line.trim() && !line.startsWith('#')) {
                logger.debug('Line = [[[' + line + ']]]')
                return this.emit('message', line, message)
            }
        }

        return this.emit('message', '~~done~~', message)
    }
}

export default LineReader

================
File: src/processors/text/StringFilter.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class StringFilter extends Processor {
    constructor(config) {
        super(config)
    }








    isAccepted(content, exclude, include) {

        if (content === undefined) {
            return false
        }


        if ((!exclude || exclude.length === 0) && (!include || include.length === 0)) {
            return true
        }


        if (this.isExcluded(content, exclude)) {
            return false
        }


        if (include && include.length > 0) {
            return this.isIncluded(content, include)
        }


        return true
    }








    isExcluded(content, exclude) {
        if (!exclude || exclude.length === 0) {
            return false
        }
        const patterns = Array.isArray(exclude) ? exclude : [exclude]
        return patterns.some(pattern => this.matchPattern(content, pattern))
    }







    isIncluded(content, include) {
        if (!include || include.length === 0) {
            return true
        }
        const patterns = Array.isArray(include) ? include : [include]
        return patterns.some(pattern => this.matchPattern(content, pattern))
    }







    matchPattern(content, pattern) {
        const regexPattern = pattern
            .replace(/\*/g, '.*')
            .replace(/\?/g, '.')
            .replace(/\[([^\]]+)\]/g, '[$1]')
        const regex = new RegExp(`^${regexPattern}$`)
        return regex.test(content)
    }

    async process(message) {
        logger.debug('\nStringFilter Input : \nmessage.content = ' + message.content)
        logger.debug('message.exclude = ')
        logger.reveal(message.exclude)
        logger.debug('message.include = ')
        logger.reveal(message.include)

        const accepted = this.isAccepted(message.content, message.exclude, message.include)

        if (accepted) {
            logger.debug('\nOutput : \nmessage.content = ' + message.content)
            return this.emit('message', message)
        } else {
            logger.debug('\nString filtered out')
        }
    }
}

export default StringFilter

================
File: src/processors/text/StringMerger.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'


class StringMerger extends Processor {
    constructor(config) {
        super(config)
        this.merged = ''
    }

    async process(message) {
        logger.log('SMDATA*********************************\n' + data)

        if (data === '~~done~~') {
            logger.log('SM  DONE**********************************\n' + this.merged)
            return this.emit('message', this.merged, message)
            return
        }
        this.merged = this.merged + data

    }
}

export default StringMerger

================
File: src/processors/text/StringReplace.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class StringReplace extends Processor {
    constructor(config) {
        super(config)
    }





    async process(message) {
        logger.setLogLevel('info')
        logger.debug('StringReplace input: ' + message.content)

        if (message.content && message.match && message.replace) {

            message.content = message.content.split(message.match).join(message.replace)
        } else {
            logger.warn('StringReplace: Missing required properties in message')
        }

        logger.debug('StringReplace output: ' + message.content)
        return this.emit('message', message)
    }
}

export default StringReplace

================
File: src/processors/text/Templater.js
================
import Processor from '../base/Processor.js'
import nunjucks from 'nunjucks'
import logger from '../../utils/Logger.js'

class Templater extends Processor {
    constructor(config) {
        super(config)
    }





    async process(message) {

        if (message.templateFilename) {

            const path = message.templateFilename.substr(0, message.templateFilename.lastIndexOf("/"))
            const filename = message.templateFilename.substr(message.templateFilename.lastIndexOf("/") + 1)

            logger.debug('Templater, path = ' + path)
            logger.debug('Templater, filename = ' + filename)

            nunjucks.configure(path, { autoescape: false })


            message.content = nunjucks.render(filename, message.contentBlocks)



        } else {

            nunjucks.configure({ autoescape: false })


            message.content = nunjucks.renderString(message.template, message.contentBlocks)
        }

        return this.emit('message', message)
    }
}
export default Templater

================
File: src/processors/text/TextProcessorsFactory.js
================
import ns from '../../utils/ns.js'

import LineReader from './LineReader.js'
import StringFilter from './StringFilter.js'
import StringMerger from './StringMerger.js'
import Templater from './Templater.js'

class TextProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.t.Templater)) {
            return new Templater(config)
        }
        if (type.equals(ns.t.LineReader)) {
            return new LineReader(config)
        }

        if (type.equals(ns.t.StringFilter)) {
            return new StringFilter(config)
        }

        if (type.equals(ns.t.StringMerger)) {
            return new StringMerger(config)
        }

        return false
    }
}

export default TextProcessorsFactory

================
File: src/processors/unsafe/chatgpt.md
================
User
What different ways of creating instances of classes in node js, including the esoteric ones?
ChatGPT
Creating instances of classes in Node.js primarily revolves around using the `new` operator with constructor functions or class definitions. However, there are several techniques, including some less commonly used (or esoteric) methods, to instantiate objects. Here's an overview:

### 1. Standard Instantiation with `new`

The most common method involves defining a class or constructor function and using the `new` keyword to create instances.

```javascript
class MyClass {
  constructor(name) {
    this.name = name;
  }
}

const instance = new MyClass("Example");
```

### 2. Factory Functions

Factory functions return a new object and can encapsulate the use of `new` internally or return object literals directly.

```javascript
function createInstance(name) {
  return new MyClass(name);
}
const instance = createInstance("Example");
```

### 3. Object.create()

`Object.create(proto[, propertiesObject])` creates a new object with the specified prototype object and properties.

```javascript
const prototype = {
  sayName() {
    console.log(this.name);
  },
};
const instance = Object.create(prototype);
instance.name = "Example";
```

### 4. Calling Constructor without `new` (Esoteric)

Using the constructor directly without `new` is not standard for creating instances and usually results in errors or unexpected behavior. However, some constructors might internally manage instantiation to allow this.

```javascript
function MyClass(name) {
  if (!(this instanceof MyClass)) {
    return new MyClass(name);
  }
  this.name = name;
}

const instance = MyClass("Example"); // Works due to internal handling
```

### 5. Clone Existing Instance

Creating a new instance by cloning an existing one, although not creating a fresh instance in the strictest sense, serves to produce a new object.

```javascript
const original = new MyClass("Original");
const clone = Object.assign(
  Object.create(Object.getPrototypeOf(original)),
  original
);
```

### 6. Deserialization

Creating an instance from a string representation, typically used in conjunction with serialization.

```javascript
const serialized = '{"name":"Example"}'; // Could be the result of JSON.stringify(instance)
const instance = Object.assign(new MyClass(), JSON.parse(serialized));
```

### 7. Function.prototype.bind()

You can set the `this` value in the constructor function, allowing for partial application or setting certain initial parameters ahead of time.

```javascript
function MyClass(name, message) {
  this.name = name;
  this.message = message;
}

const PreConfigured = MyClass.bind(null, "PreConfiguredName");
const instance = new PreConfigured("Hello World");
```

### 8. Proxy for Constructor (Esoteric)

Using a `Proxy` to intercept and customize the instantiation process.

```javascript
const MyClassProxy = new Proxy(MyClass, {
  construct(target, args) {
    console.log("MyClass instantiated with args:", args);
    return new target(...args);
  },
});

const instance = new MyClassProxy("Example");
```

These methods showcase the flexibility of object creation in JavaScript and Node.js, ranging from straightforward and common practices to more unusual and esoteric approaches. Each has its use cases, depending on the requirements for flexibility, pattern use, and object initialization specifics.
User
If all the object needs is a distinct name 'MyClass', the ability to set the value of a property 'config' in a constructor-like function where properties will be set, and a function process(data) which returns a processed version of data, what is the minimum code necessary? Could these objects be create with something like a const instance = create('MyName') kind of function?
ChatGPT
For a minimal implementation that meets the specified requirements, you can use a factory function to create instances. Each instance needs a distinct name (`MyClass`), the ability to set the value of a `config` property in a constructor-like function, and an `execute(data)` method that returns a processed version of the data. Here's a concise way to accomplish this:

```javascript
function createInstance(name) {
  return {
    name: name,
    config: null,
    process(data) {
      // Process and return the data
      // This example simply returns the data as-is; replace with actual processing logic
      return data;
    },
    setConfig(config) {
      this.config = config;
    },
  };
}

// Example usage:
const instance = createInstance("MyName");
instance.setConfig({ key: "value" }); // Setting the config
console.log(instance.process("some data")); // Executing and processing data
```

This approach uses a simple object literal inside the `createInstance` factory function to define the necessary properties and methods. Each created instance will have its own `name`, `config`, and `execute` method, as per the requirements.

- `name` is assigned based on the argument passed to `createInstance`.
- `config` can be set using the `setConfig` method, which simulates constructor-like behavior for setting initial properties.
- `execute(data)` method is defined to process and return data. You can replace the placeholder logic with actual data processing as needed.

This solution provides a lightweight and flexible way to create instances with the desired functionality, following your example of a `create('MyName')` style function call.
User
Could the execute function be created from a string?
ChatGPT
Yes, the `execute` function can be created from a string using the `new Function()` constructor. This method allows you to dynamically create a function from a string of JavaScript code. Here's how you could modify the previous example to include an `execute` function created from a string:

```javascript
function createInstance(name, executeFunctionString) {
  return {
    name: name,
    config: null,
    execute: new Function("data", executeFunctionString),
    setConfig(config) {
      this.config = config;
    },
  };
}

// Example usage:
const executeCode = 'return data + " processed";'; // String of code for the execute function
const instance = createInstance("MyName", executeCode);
instance.setConfig({ key: "value" }); // Setting the config

console.log(instance.process("some data")); // Outputs: "some data processed"
```

In this example, the `executeFunctionString` parameter is passed to `new Function('data', executeFunctionString)`, where `'data'` is the name of the argument that the function will receive, and `executeFunctionString` is the body of the function. This dynamically created function is then assigned to the `execute` property of the object returned by `createInstance`.

**Note of Caution:** Using `new Function()` to create functions from strings poses similar risks to `eval()` in terms of security and performance:

- **Security Risk:** If the string is derived from user input or an untrusted source, it could potentially introduce security vulnerabilities, such as code injection attacks.
- **Debugging Difficulty:** Debugging can be more challenging because the dynamically generated code may not be as readily traceable in debugging tools.
- **Performance:** The JavaScript engine can't optimize these functions as well as it can for statically declared functions.

Always validate and sanitize any input if you choose to use this approach, and consider whether the dynamic nature of function creation is necessary for your use case.
User
In RS6, can imports be done inside functions?
ChatGPT
In ES6 (ECMAScript 2015), the standard import syntax (`import ... from '...'`) cannot be used conditionally or inside functions, blocks, or any other statements that are not at the top level of a module. The ES6 `import` statements are static and must be at the top level of your JavaScript file or module, allowing for these imports to be statically analyzed by JavaScript engines and bundlers.

However, you can dynamically import modules using the `import()` function, which returns a promise. This feature is not part of ES6, but it was added later in the ES2020 (ECMAScript 11) specification. The `import()` function can be used inside functions, conditionals, loops, or anywhere you can execute a function. It is useful for lazy-loading modules only when they are needed, which can improve the performance of your application by reducing the initial load time.

Here's an example of how to use dynamic imports within a function:

```javascript
async function loadModuleDynamically() {
  const moduleName = "./myModule.js"; // The module path
  const module = await import(moduleName);
  module.myFunction(); // Use a function from the imported module
}

loadModuleDynamically().then(() => {
  console.log("Module loaded and function executed");
});
```

In this example, `import(moduleName)` dynamically imports the module specified by `moduleName`, and because `import()` returns a promise, you can use `await` within an async function to wait for the module to be loaded before using it.

This approach provides flexibility for loading modules based on conditions, user actions, or any logic that requires modules to be loaded dynamically at runtime.
User
How could you check at runtime to see if import() was supported?
ChatGPT
To check at runtime whether dynamic `import()` is supported, you can use a feature detection approach. This involves attempting to use `import()` with a data URI or a no-op (no operation) module and catching any errors if the feature is not supported. Since `import()` returns a promise, you can use a try-catch block within an asynchronous function to perform this check. Here's how you could implement it:

```javascript
async function supportsDynamicImport() {
  try {
    // Attempt to dynamically import a module using a no-op or data URI
    // For actual deployment, a more reliable check with a real module path may be preferable
    await new Function('return import("data:text/javascript;base64,Cg==")')();
    return true; // Dynamic import is supported
  } catch (error) {
    return false; // Dynamic import is not supported
  }
}

// Example usage
supportsDynamicImport().then((isSupported) => {
  if (isSupported) {
    console.log("Dynamic import is supported");
  } else {
    console.log("Dynamic import is not supported");
  }
});
```

This function works by dynamically creating a function that attempts to `import` a base64-encoded empty JavaScript module. If the browser or environment supports dynamic imports, this operation will succeed, and the function will return `true`. If not, the operation will throw an exception, and the function will return `false`.

Note: The use of `new Function()` allows this check to not cause a syntax error in environments that do not recognize `import()` syntax at all, as the `import()` call is within a string and not directly in the JavaScript code being parsed.

This method provides a straightforward way to determine the availability of dynamic imports, allowing you to conditionally implement fallback logic or alternative module loading strategies if necessary.
User
Can imports go in eval() strings?
ChatGPT
Using `import` statements directly within `eval()` strings in JavaScript does not work in the same way as using them at the top level of modules or through dynamic `import()` expressions. The static `import` statement is part of the module syntax and expected to be statically analyzable at the top level of module files, not dynamically executed within function bodies, `eval()`, or similar constructs.

However, dynamic imports using `import()` can be used within `eval()` because `import()` is a function that returns a promise. This allows for dynamically importing modules even in messages where static import statements are not syntactically valid. Here's an example:

```javascript
eval('import("./module.js").then(module => { console.log(module); })');
```

This uses `eval()` to dynamically execute a string that contains a dynamic import statement. Note that this approach does indeed work, but it combines two powerful and potentially risky features of JavaScript: `eval()` and dynamic imports. Both should be used cautiously because:

- `eval()` can execute arbitrary code, which poses significant security risks, especially with untrusted input. It's also generally slower and harder to debug compared to other methods of executing JavaScript.
- Dynamic imports can introduce complexity to your module loading strategy and potentially impact performance negatively if not used judyiciously.

Given these considerations, while you can use dynamic `import()` within `eval()` strings, it's essential to carefully assess whether this approach is necessary and ensure that the code being evaluated is from a trusted source to avoid security vulnerabilities.

================
File: src/processors/util/CaptureAll.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class CaptureAll extends Processor {

    constructor(config) {
        if (!config.whiteboard) {
            config.whiteboard = []
        }
        super(config)


        if (CaptureAll.singleInstance) {
            return CaptureAll.singleInstance
        }
        CaptureAll.singleInstance = this
    }


    async process(message) {
        logger.log('CaptureAll at (' + message.tags + ') ' + this.getTag())
        this.config.whiteboard.push(message)
        return this.emit('message', message)
    }


}

export default CaptureAll

================
File: src/processors/util/DeadEnd.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class DeadEnd extends Processor {

    async process(message) {
        logger.log('DeadEnd  at (' + message.tags + ') ' + this.getTag())
    }

}
export default DeadEnd

================
File: src/processors/util/Fork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'




class Fork extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        const nForks = message.nForks || 2

        logger.debug('forks = ' + nForks)

        for (let i = 0; i < nForks; i++) {
            var messageClone = structuredClone(message)
            messageClone.forkN = i
            logger.debug('--- emit --- ' + i)
            this.emit('message', messageClone)
        }

        message.done = true

        return this.emit('message', message)

    }

}

export default Fork

================
File: src/processors/util/Halt.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class Halt extends Processor {

    process(message) {
        logger.log('\n************************************************************************')
        logger.log('*** << Thou Hast Summoned HALT, the Mighty Stopper of All Things  >> ***')
        logger.log('*** <<                   ~~~ ALL IS GOOD ~~~                      >> ***')
        logger.log('*** <<                     Have a nice day!                       >> ***')
        logger.log('************************************************************************\n')
        logger.log('*** Pipeline was : ' + message.tags)
        logger.log('*** Context now : ')
        logger.reveal(message)
        process.exit()
    }
}

export default Halt

================
File: src/processors/util/NOP.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'

class NOP extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        logger.log('NOP at (' + message.tags + ') ' + this.getTag())
        return this.emit('message', message)
    }

    double(string) {
        return string + string
    }
}
export default NOP

================
File: src/processors/util/SetMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'

class SetMessage extends Processor {

    constructor(config) {
        super(config)
        logger.log('SetMessage constructor')
    }

    async process(message) {
        const setters = await this.getSetters(this.config, this.configKey, ns.trm.setValue)
        for (let i = 0; i < setters.length; i++) {
            message[setters[i].key] = setters[i].value
        }
        return this.emit('message', message)
    }

    async getSetters(config, configKey, term) {



        const settersRDF = GrapoiHelpers.listToArray(config, configKey, term)
        const dataset = this.config
        var setters = []
        for (let i = 0; i < settersRDF.length; i++) {
            let setter = settersRDF[i]
            let poi = rdf.grapoi({ dataset: dataset, term: setter })
            let key = poi.out(ns.trm.key).value
            let value = poi.out(ns.trm.value).value
            setters.push({ "key": key, "value": value })
        }
        return setters
    }
}

export default SetMessage

================
File: src/processors/util/ShowConfig.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ShowConfig extends Processor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async process(message) {



        if (this.verbose) logger.log("\n***  Show Config ***")


        logger.log("***************************")
        logger.log("***  Config")
        logger.reveal(this.config)
        logger.log("***************************")




        return this.emit('message', message)
    }
}

export default ShowConfig

================
File: src/processors/util/ShowMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ShowMessage extends Processor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async process(message) {



        if (this.verbose) logger.log("\n***  Show Message ***")

        logger.log("***************************")
        logger.log("***  Message")
        logger.reveal(message)
        logger.log("***************************")




        return this.emit('message', message)
    }
}

export default ShowMessage

================
File: src/processors/util/ShowTransmission.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ShowTransmission extends Processor {

    async process(message) {
        logger.log(this.transmission.toString())
        return this.emit('message', message)
    }
}

export default ShowTransmission

================
File: src/processors/util/Stash.js
================
import rdf from 'rdf-ext'
import { fromFile, toFile } from 'rdf-utils-fs'
import Processor from '../base/Processor.js'












class Stash extends Processor {





    constructor(config) {
        super(config)
    }






    async process(message) {
        const manifestFilename = rootDir + '/manifest.ttl'
        const stream = fromFile(manifestFilename)


        message.rootDir = rootDir
        message.dataset = await rdf.dataset().import(stream)
        return this.emit('message', message)
    }
}
export default Stash

================
File: src/processors/util/Unfork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import DeadEnd from './DeadEnd.js'






class Unfork extends Processor {

    constructor(config) {
        super(config)
        logger.setLogLevel("info")






    }

    async process(message) {

        if (message.done) {
            logger.debug(' - Unfork passing message')
            message.done = false
            return this.emit('message', message)
        } else {
            logger.debug(' - Unfork terminating pipe')
        }
    }

}

export default Unfork

================
File: src/processors/util/UtilProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import NOP from './NOP.js'
import DeadEnd from './DeadEnd.js'
import ShowMessage from './ShowMessage.js'
import Halt from './Halt.js'
import Restructure from '../json/Restructure.js'
import Unfork from './Unfork.js'
import Fork from './Fork.js'
import ShowTransmission from './ShowTransmission.js'
import CaptureAll from './CaptureAll.js'
import ShowConfig from './ShowConfig.js'
import WhiteboardToMessage from './WhiteboardToMessage.js'
import SetMessage from './SetMessage.js'

class UtilProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.NOP)) {
            return new NOP(config)
        }
        if (type.equals(ns.t.DeadEnd)) {
            return new DeadEnd(config)
        }
        if (type.equals(ns.t.ShowMessage)) {
            return new ShowMessage(config)
        }
        if (type.equals(ns.t.Halt)) {
            return new Halt(config)
        }
        if (type.equals(ns.t.Restructure)) {
            return new Restructure(config)
        }
        if (type.equals(ns.t.Fork)) {
            return new Fork(config)
        }
        if (type.equals(ns.t.Unfork)) {
            return new Unfork(config)
        }
        if (type.equals(ns.t.ShowTransmission)) {
            return new ShowTransmission(config)
        }
        if (type.equals(ns.t.CaptureAll)) {
            return new CaptureAll(config)
        }
        if (type.equals(ns.t.ShowConfig)) {
            return new ShowConfig(config)
        }
        if (type.equals(ns.t.WhiteboardToMessage)) {
            return new WhiteboardToMessage(config)
        }
        if (type.equals(ns.t.SetMessage)) {
            return new SetMessage(config)
        }

        return false

    }
}

export default UtilProcessorsFactory

================
File: src/processors/util/WhiteboardToMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class WhiteboardToMessage extends Processor {

    constructor(config) {
        super(config);
    }
    async process(message) {

        logger.log('WhiteboardToMessage at (' + message.tags + ') ' + this.getTag())

        const originalArray = this.config.whiteboard

        message.whiteboard = Object.keys(originalArray).reduce((acc, key) => {
            const value = originalArray[key];
            if (value !== undefined && value !== null) {
                Object.keys(value).forEach((prop) => {
                    if (!acc[prop]) {
                        acc[prop] = [];
                    }
                    acc[prop].push(value[prop]);
                });
            }
            return acc;
        }, {});

        return super.handle(message)

    }
}

export default WhiteboardToMessage

================
File: src/processors/about.md
================
# Creating a new Processor

- update repopacks for `transmissions` and `trans-apps`
- create a new chat session in existing Project
- upload repopacks to Claude, with anything else that might be relevant (handover from previous session?)
- follow the prompt model as in `/home/danny/workspaces_hkms-desktop/postcrafts-raw/transmissions/prompts/github-list.md`
- remember additions to `xProcessorsFactory.js` and `transmissions/src/engine/AbstractProcessorFactory.js`

#:todo add comment creation
#:todo check simples & application suitability
#:todo create document creation workflow
#:todo create manifest.ttl creation
#:todo make crossrefs.md, crossrefs.ttl
#:todo create manifest.ttl consumption
#:todo add test creation
#:todo wire to an API, include file creation ops
#:todo add support in #:hyperdata-desktop

#:todo dedicated transmissions model, fine-tuned on relevant docs

#:todo extract todos as something like :

```turtle
<http://hyperdata.it/transmissions/src/processors/about/nid123> a pv:ToDoItem ;
dc:source <http://hyperdata.it/transmissions/src/processors/about.md> ;
pv:semtag "#:todo" ;
dc:line "3" ;
dc:title "tbd" ;
dc:content "extract todos as something like :" .
```

================
File: src/simples/env-loader/about.md
================
node src/apps-simple/env-loader/env-loader.js

from:

:envy a trm:Pipeline ;

# trm:pipe (:SC :s10 :s20 :SM) .

trm:pipe (:p10 :p20 :SC) .
:p10 a :EnvLoader .
:p20 a :WhiteboardToMessage .

================
File: src/simples/env-loader/env-loader.js
================
import logger from '../../utils/Logger.js'
import EnvLoader from '../../processors/system/EnvLoader.js'
import WhiteboardToMessage from '../../processors/util/WhiteboardToMessage.js'

logger.log('EnvLoader simple')

const config = { whiteboard: [] }

const p10 = new EnvLoader(config)
p10.id = 'http://purls.org/stuff/#p10'

const p20 = new WhiteboardToMessage(config)
p10.id = 'http://purls.org/stuff/#p20'

var message = {
    "dataDir": "src/applications/env-loader-test/data",
    "rootDir": "[no key]",
    "tags": "SM"
}

const x = 3

message = await p10.process(message)

logger.log('p10 output ' + p10.getTag() + message)

message = await p20.process(message)

logger.log('p20 output ')

logger.reveal(message)

================
File: src/simples/nop/nop.js
================
import NOP from '../../processors/util/NOP.js'

const config = {
    "runmode": "functions",
    whiteboard: []
}

const nop = new NOP(config)

var message = { 'value': '42' }

message = await nop.process(message)

console.log('value = ' + message.value)

================
File: src/simples/nop/simple-runner.js
================
import NOP from '../../processors/util/NOP.js'
import Fork from '../../processors/util/Fork.js'








async function main() {
    const config = {}
    const nop = new NOP(config)
    const fork = new Fork(config)

    var message = { 'value': '42' }



    var outputs = await nop.process(message)
    console.log('NOP outputs:', outputs)


    message.nForks = 3
    outputs = await fork.process(message)
    console.log('Fork outputs:', outputs)
}

main().catch(console.error)

================
File: src/simples/set-message/set-message.js
================
import logger from '../../utils/Logger.js'
import SetMessage from '../../processors/util/SetMessage.js'

const config = {
    "runmode": "functions",
    whiteboard: []
}

const setm = new SetMessage(config)

var message = { 'value': '42' }

message = await setm.process(message)

logger.log('value = ' + message.value)

logger.reveal(message)

================
File: src/utils/footpath.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import logger from './Logger.js'







let footpath = {}

footpath.resolve = function footpath(here, relative, start) {

    const loggy = false
    if (loggy) {
        logger.debug("\n*** start footpath.resolve ***")
        logger.debug("process.cwd() = " + process.cwd())
        logger.debug("here = " + here)
        logger.debug("relative = " + relative)
        logger.debug("start = " + start)
    }

    const __filename = fileURLToPath(here)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, relative)
    const filePath = path.join(rootDir, start)

    if (loggy) {
        logger.debug("__filename = " + __filename)
        logger.debug("__dirname = " + __dirname)
        logger.debug("rootDir = " + rootDir)
        logger.debug("filePath = " + filePath)
        logger.debug("*** end footpath.resolve ***\n")
    }

    return filePath
}

footpath.urlLastPart = function footpath(url = 'http://example.org/not-a-url') {


    const urlObj = new URL(url);
    const hash = urlObj.hash;
    const path = urlObj.pathname;
    const lastPart = hash ? hash.replace(/^#/, '') : path.split('/').pop();
    // } catch {
    //  return 'not-a-url'

    return lastPart;
}

export default footpath

================
File: src/utils/GrapoiHelpers.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'
import ns from './ns.js'
import logger from './Logger.js'



class GrapoiHelpers {


    static async readDataset(filename) {
        const stream = fromFile(filename)
        const dataset = await rdf.dataset().import(stream)
        return dataset
    }

    static async writeDataset(dataset, filename) {
        await toFile(dataset.toStream(), filename)
    }


    static listToArray(dataset, term, property) {
        const poi = rdf.grapoi({ dataset: dataset, term: term })
        const first = poi.out(property).term

        let p = rdf.grapoi({ dataset, term: first })
        let object = p.out(ns.rdf.first).term

        const result = [object]

        while (true) {
            let restHead = p.out(ns.rdf.rest).term
            let p2 = rdf.grapoi({ dataset, term: restHead })
            let object = p2.out(ns.rdf.first).term

            if (restHead.equals(ns.rdf.nil)) break
            result.push(object)
            p = rdf.grapoi({ dataset, term: restHead })
        }
        return result
    }





    static listObjects(dataset, subjectList, predicate) {
        const objects = []
        for (const subject of subjectList) {
            logger.log("subject = " + subject.value)
            let p = rdf.grapoi({ dataset, term: subject })
            let object = p.out(predicate).term
            logger.log("object = " + object.value)
            objects.push(object)
        }
        return objects
    }
}
export default GrapoiHelpers

================
File: src/utils/Logger.js
================
import fs from 'fs'


let logger = {}

logger.logfile = 'latest.log'



const LOG_LEVELS = [
    "debug",
    "info",
    "log",
    "warn",
    "error",
]
const logComponent = "api.logger"

logger.appendLogToFile = function (message) {
    if (logger.logfile) {
        fs.appendFileSync(logger.logfile, message + '\n', 'utf8')
    }
}

logger.setLogLevel = function (logLevel = "warn") {


    logger.currentLogLevel = logLevel
}

logger.timestampISO = function () {
    let now = new Date()
    return now.toISOString()
}

logger.log = function (msg, level = "log") {
    const currentLevelIndex = LOG_LEVELS.indexOf(logger.currentLogLevel)
    const messageLevelIndex = LOG_LEVELS.indexOf(level)

    if (messageLevelIndex >= currentLevelIndex) {
        console[level](msg)
        const logMessage = `[${logger.timestampISO()}] [${level.toUpperCase()}] - ${msg}`
        logger.appendLogToFile(logMessage)
    }
}

logger.reveal = function (instance) {
    const serialized = {}

    logger.log('***    hidden keys :  ')
    for (const key in instance) {
        if (key === 'dataset') {
            logger.log('[[dataset found, skipping]]')
            continue
        }
        if (key.startsWith('_')) {


            logger.log(`       ${key}`)
            continue
        } else {
            if (instance.hasOwnProperty(key)) {
                let kiki = instance[key]

                if (kiki) {
                    if (Buffer.isBuffer(kiki)) {
                        kiki = kiki.toString()

                    }
                    if (kiki.length > 100) {
                        try {
                            kiki = kiki.substring(0, 100) + '...'
                        } catch (e) {
                            kiki = kiki.slice(0, 99)
                        }
                    }
                    serialized[key] = kiki
                } else {
                    serialized[key] = '[no key]'
                }
            }
        }
    }
    const props = JSON.stringify(serialized, null, 2)
    if (!instance) {
        throw new Error(` no instance defined`)
        return
    }
    logger.log(`Instance of ${instance.constructor.name} with properties - \n${props}`)

}

logger.debug = function (msg) {
    logger.log(msg, "debug")
}

logger.info = function (msg) {
    logger.log(msg, "info")
}

logger.warn = function (msg) {
    logger.log(msg, "warn")
}

logger.error = function (msg) {
    logger.log(msg, "error")
}

logger.poi = function exploreGrapoi(grapoi, predicates, objects, subjects) {

    console.log('Properties of the Grapoi object:')
    for (const prop in grapoi) {
        console.log(`\t${prop}: ${grapoi[prop]}`)
    }


    console.log('\nPath:')
    const path = grapoi.out(predicates, objects).in(predicates, subjects)
    for (const quad of path.quads()) {
        console.log(`\t${quad.predicate.value}: ${quad.object.value}`)
    }
}

function handleExit(options, exitCode) {
    if (options.cleanup) {

    }
    if (exitCode || exitCode === 0) console.log(exitCode)
    if (options.exit) process.exit()
}


process.on('exit', handleExit.bind(null, { cleanup: true }))
process.on('SIGINT', handleExit.bind(null, { exit: true }))
process.on('SIGUSR1', handleExit.bind(null, { exit: true }))
process.on('SIGUSR2', handleExit.bind(null, { exit: true }))
process.on('uncaughtException', handleExit.bind(null, { exit: true }))

export default logger

================
File: src/utils/ns.js
================
import rdf from 'rdf-ext'

const ns = {
    rdf: rdf.namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#'),
    rdfs: rdf.namespace('http://www.w3.org/2000/01/rdf-schema#'),
    dc: rdf.namespace('http://purl.org/dc/terms/'),
    schema: rdf.namespace('http://schema.org/'),
    xsd: rdf.namespace('http://www.w3.org/2001/XMLSchema#'),
    trm: rdf.namespace('http://purl.org/stuff/transmission/'),
    t: rdf.namespace('http://hyperdata.it/transmissions/'),
    fs: rdf.namespace('http://purl.org/stuff/filesystem/'),
    pc: rdf.namespace('http://purl.org/stuff/postcraft/')
}





ns.getShortname = function (url) {

    if (!url) return
    const lastSlashIndex = url.lastIndexOf('/');
    const lastHashIndex = url.lastIndexOf('#');
    const path = url.slice(lastSlashIndex + 1);
    return path.split('#')[0].split('?')[0];
}
export default ns

================
File: src/utils/t2j.js
================
import { Readable } from 'readable-stream'
import rdf from '@rdfjs/data-model'
import SerializerJsonld from '@rdfjs/serializer-jsonld'
import Serializer from '@rdfjs/serializer-turtle'
import N3Parser from '@rdfjs/parser-n3'
import { fromFile } from 'rdf-utils-fs'
import { toFile } from 'rdf-utils-fs'

const testTurtle = `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <https://hyperdata.it/transmissions/> . # for custom terms & instances

:simplepipe a trm:PipelineTransmission ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a trm:StringSource .
:s2 a trm:AppendProcess .
:s3 a trm:StringSink .
`
export class Turtle2JSONLD {
    static async convert(turtle) {

        let parser = new N3Parser({ factory: rdf })



        const input = Readable.from(turtle)

        const output = parser.import(input)

        const serializerJsonld = new SerializerJsonld()
        const jsonStream = serializerJsonld.import(output)





        const outputJson = await Turtle2JSONLD.streamToString(jsonStream)
        return outputJson
    }

    static stringToStream(str) {
        const stream = new Readable();
        stream.push(str);
        stream.push(null);
        return stream;
    }

    static streamToString(stream) {
        const chunks = [];
        return new Promise((resolve, reject) => {
            stream.on('data', (chunk) => {
                chunks.push(Buffer.from(chunk))
                console.log('chunk:', chunk)
            }
            );
            stream.on('error', (err) => reject(err));
            stream.on('end', () => {
                const result = Buffer.concat(chunks).toString('utf8')
                resolve(result)
                console.log('****************** result:', result)
            });
        })
    }
}



const testJson = await Turtle2JSONLD.convert(testTurtle)
console.log('')
console.log(testJson)

================
File: src/utils/test_runner.js
================
import fs from 'fs';
import path from 'path';

const testFiles = fs.readdirSync(__dirname).filter(file => file.startsWith('test_'));

testFiles.forEach(testFile => {
    console.log(`Running ${testFile}`);
    require(path.join(__dirname, testFile));
});

================
File: tests/helpers/reporter.js
================
import { SpecReporter } from 'jasmine-spec-reporter';

class CustomReporter {
    constructor() {
        this.specReporter = new SpecReporter({
            spec: {
                displayPending: true
            }
        });
    }

    jasmineStarted() {
        this.specReporter.jasmineStarted.apply(this.specReporter, arguments);
    }

    suiteStarted() {
        this.specReporter.suiteStarted.apply(this.specReporter, arguments);
    }

    specStarted() {
        this.specReporter.specStarted.apply(this.specReporter, arguments);
    }

    specDone() {
        this.specReporter.specDone.apply(this.specReporter, arguments);
    }

    suiteDone() {
        this.specReporter.suiteDone.apply(this.specReporter, arguments);
    }

    jasmineDone() {
        this.specReporter.jasmineDone.apply(this.specReporter, arguments);
    }
}

export default CustomReporter;

================
File: tests/integration/fork.spec.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'

describe('fork test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const logFile = path.join(__dirname, '../../latest.log')

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000

    it('should create correct number of message paths', (done) => {
        exec('node run.js test_fork', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {

                const logs = stdout.toString()
                const nopMatches = logs.match(/NOP at/g)
                const nopCount = nopMatches ? nopMatches.length : 0


                expect(nopCount).to.equal(3)
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })
})

================
File: tests/integration/fs-rw_simple.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import fs from 'fs/promises'

describe('fs-rw simple test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, '../../')

    const outputFile = path.join(rootDir, 'src/applications/test_fs-rw/data/output/output-01.md')
    const requiredFile = path.join(rootDir, 'src/applications/test_fs-rw/data/output/required-01.md')

    beforeEach(async () => {
        try {
            await fs.unlink(outputFile)
        } catch (error) {
            if (error.code !== 'ENOENT') throw error
        }
    })

    it('should process file correctly', async () => {

        await import('../../src/applications/test_fs-rw/simple.js')


        const output = await fs.readFile(outputFile, 'utf8')
        const required = await fs.readFile(requiredFile, 'utf8')

        expect(output.trim()).to.equal(required.trim())
    })
})

================
File: tests/integration/fs-rw.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs/promises'

describe('fs-rw test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const dataDir = path.join(__dirname, '../../src/applications/test_fs-rw/data')


    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000

    async function clearOutputFiles() {
        console.log('Clearing output files...')
        const outputDir = path.join(dataDir, 'output')
        const files = await fs.readdir(outputDir)
        for (const file of files) {
            if (file.startsWith('output-')) {
                await fs.unlink(path.join(outputDir, file))
                console.log(`Deleted ${file}`)
            }
        }
    }

    async function compareFiles(index) {
        const outputFile = path.join(dataDir, 'output', `output-${index}.md`)
        const requiredFile = path.join(dataDir, 'output', `required-${index}.md`)

        console.log(`Comparing files:`)
        console.log(`Output: ${outputFile}`)
        console.log(`Required: ${requiredFile}`)

        const output = await fs.readFile(outputFile, 'utf8')
        const required = await fs.readFile(requiredFile, 'utf8')

        if (output.trim() !== required.trim()) {
            console.log('Files differ:')
            console.log('Output:', output)
            console.log('Required:', required)
        }

        return output.trim() === required.trim()
    }

    beforeEach(async () => {
        await clearOutputFiles()
    })

    it('should process files correctly', (done) => {
        console.log('Running transmission...')
        exec('node run.js test_fs-rw', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {
                console.log('Transmission output:', stdout)
                if (stderr) console.error('Stderr:', stderr)

                const matched = await compareFiles('01')
                expect(matched).to.be.true
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })
})

================
File: tests/integration/restructure_simple.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import fs from 'fs/promises'

describe('restructure simple test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, '../../')

    const outputFile = path.join(rootDir, 'src/applications/test_restructure/data/output/output-01.json')
    const requiredFile = path.join(rootDir, 'src/applications/test_restructure/data/output/required-01.json')

    beforeEach(async () => {
        try {
            await fs.unlink(outputFile)
        } catch (error) {
            if (error.code !== 'ENOENT') throw error
        }
    })

    it('should process JSON file correctly', async () => {
        console.log('Running restructure test')

        await import('../../src/applications/test_restructure/simple.js')


        const output = JSON.parse(await fs.readFile(outputFile, 'utf8'))
        const required = JSON.parse(await fs.readFile(requiredFile, 'utf8'))


        expect(output).to.deep.equal(required)
    })
})

================
File: tests/integration/restructure.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs/promises'

describe('test_restructure', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const dataDir = path.join(__dirname, '../../src/applications/test_restructure/data')

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000

    async function clearOutputFiles() {
        console.log('Clearing output files...')
        const outputDir = path.join(dataDir, 'output')
        const files = await fs.readdir(outputDir)
        for (const file of files) {
            if (file.startsWith('output-')) {
                await fs.unlink(path.join(outputDir, file))
                console.log(`Deleted ${file}`)
            }
        }
    }

    async function compareFiles(index) {
        const outputFile = path.join(dataDir, 'output', `output-${index}.json`)
        const requiredFile = path.join(dataDir, 'output', `required-${index}.json`)

        console.log(`Comparing files:`)
        console.log(`Output: ${outputFile}`)
        console.log(`Required: ${requiredFile}`)

        const output = JSON.parse(await fs.readFile(outputFile, 'utf8'))
        const required = JSON.parse(await fs.readFile(requiredFile, 'utf8'))


        return JSON.stringify(output) === JSON.stringify(required)
    }

    beforeEach(async () => {
        await clearOutputFiles()
    })

    it('should process files correctly', (done) => {
        console.log('Running transmission...')
        exec('node run.js test_restructure', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {
                console.log('Transmission output:', stdout)
                if (stderr) console.error('Stderr:', stderr)

                const matched = await compareFiles('01')
                expect(matched).to.be.true
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })
})

================
File: tests/unit/NOP.spec.js
================
import NOP from '../../src/processors/util/NOP.js'
import { expect } from 'chai'

describe('NOP', function () {
    it('double() should return the input string concatenated with itself', function () {
        const nop = new NOP()
        const input = 'test'
        const expectedOutput = 'testtest'
        const output = nop.double(input)
        expect(output).to.equal(expectedOutput)
    })
})

================
File: tests/unit/PostcraftPrep.spec.js
================
import PostcraftPrep from '../../src/processors/postcraft/PostcraftPrep.js'
import { expect } from 'chai'

describe('PostcraftPrep', function () {
    beforeEach(function () {
        this.context = {
            content: 'only text',
            filename: 'minimal-filename.md'
        }
    })

    it('extractTitle(context) should lift the title from the filename', function () {
        this.context.filename = '2024-05-10_this-thing.md'
        const input = this.context
        const expectedOutput = 'This Thing'
        const pp = new PostcraftPrep()
        const output = pp.extractTitle(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractSlug(context) should return filename without path and extension', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        const input = this.context
        const expectedOutput = '2024-05-10_hello-postcraft'
        const pp = new PostcraftPrep()
        const output = pp.extractSlug(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractTargetFilename(context) should return the correct target filename', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        this.context.rootDir = '/root'
        this.context.entryContentMeta = {
            targetDir: 'target'
        }
        const input = this.context
        const expectedOutput = '/root/target/2024-05-10_hello-postcraft.html'
        const pp = new PostcraftPrep()
        const output = pp.extractTargetFilename(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractDates(context) should return the correct dates', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        const input = this.context
        const expectedOutput = { created: '2024-05-10', updated: (new Date()).toISOString().split('T')[0] }
        const pp = new PostcraftPrep()
        const output = pp.extractDates(input)
        expect(output).to.deep.equal(expectedOutput)
    })
})

================
File: tests/unit/StringFilter.spec.js
================
import StringFilter from '../../src/processors/text/StringFilter.js';
import { expect } from 'chai';

describe('StringFilter', function () {

    function compose(content, include, exclude) {
        return { content, include, exclude };
    }


    const contentSamples = [
        '/home/user/documents/',
        '/home/user/documents/file.txt',
        '/var/log/',
        '/etc/config.conf',
        '/usr/local/bin/app',
        '/home/user/pictures/vacation/',
        '/home/user/pictures/vacation/photo.jpg',
        '/opt/',
        '/tmp/temp.file',
        '/home/user/.config/',
        '',
        undefined
    ];

    const patternSamples = [
        '*.txt',
        '*.jpg',
        '/home/user/*',
        '/var/*',
        '*/bin/*',
        ['*.txt', '*.jpg'],
        ['/home/user/*', '/var/*'],
        ['*/bin/*', '*.conf'],
        ['*.file', '/tmp/*'],
        ['/opt/*', '/etc/*'],
        '',
        [],
        undefined
    ];

    describe('isAccepted()', function () {
        it('should accept all content when include and exclude are empty', function () {
            const filter = new StringFilter();
            contentSamples.forEach(content => {
                if (content !== undefined) {
                    const message = compose(content, '', '');
                    expect(filter.isAccepted(message.content, message.exclude, message.include)).to.be.true;
                }
            });
        });

        it('should reject undefined content', function () {
            const filter = new StringFilter();
            const message = compose(undefined, '', '');
            expect(filter.isAccepted(message.content, message.exclude, message.include)).to.be.false;
        });

        it('should correctly apply include patterns', function () {
            const filter = new StringFilter();
            const includeTests = [
                { content: '/home/user/documents/file.txt', include: '*.txt', expected: true },
                { content: '/home/user/pictures/vacation/photo.jpg', include: '*.jpg', expected: true },
                { content: '/var/log/', include: '/var/*', expected: true },
                { content: '/home/user/documents/', include: '/home/user/*', expected: true },
                { content: '/usr/local/bin/app', include: '*/bin/*', expected: true },
                { content: '/etc/config.conf', include: ['*.conf', '*.txt'], expected: true },
                { content: '/opt/', include: ['/var/*', '/opt/*'], expected: true },
                { content: '/tmp/temp.file', include: '*.doc', expected: false }
            ];

            includeTests.forEach(test => {
                const message = compose(test.content, test.include, '');
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });

        it('should correctly apply exclude patterns', function () {
            const filter = new StringFilter();
            const excludeTests = [
                { content: '/home/user/documents/file.txt', exclude: '*.txt', expected: false },
                { content: '/home/user/pictures/vacation/photo.jpg', exclude: '*.jpg', expected: false },
                { content: '/var/log/', exclude: '/var/*', expected: false },
                { content: '/home/user/documents/', exclude: '/home/user/*', expected: false },
                { content: '/usr/local/bin/app', exclude: '*/bin/*', expected: false },
                { content: '/etc/config.conf', exclude: ['*.conf', '*.txt'], expected: false },
                { content: '/opt/', exclude: ['/var/*', '/tmp/*'], expected: true },
                { content: '/tmp/temp.file', exclude: '*.doc', expected: true }
            ];

            excludeTests.forEach(test => {
                const message = compose(test.content, '', test.exclude);
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });

        it('should correctly apply both include and exclude patterns', function () {
            const filter = new StringFilter();
            const combinedTests = [
                { content: '/home/user/documents/file.txt', include: '*.txt', exclude: '/var/*', expected: true },
                { content: '/var/log/system.log', include: '*.log', exclude: '/var/*', expected: false },
                { content: '/home/user/pictures/vacation/photo.jpg', include: ['/home/user/*', '*.jpg'], exclude: '*.png', expected: true },
                { content: '/etc/config.conf', include: ['*.conf', '*.txt'], exclude: ['/home/*', '/var/*'], expected: true },
                { content: '/usr/local/bin/app', include: '*/bin/*', exclude: '*/local/*', expected: false }
            ];

            combinedTests.forEach(test => {
                const message = compose(test.content, test.include, test.exclude);
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });
    });
});

================
File: tests/unit/StringReplace.spec.js
================
import StringReplace from '../../src/processors/text/StringReplace.js'
import { expect } from 'chai'




describe('StringReplace', function () {



    it('execute() should replace all occurrences of the match string with the replace string', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: 'Hello world! Hello universe!',
            match: 'Hello',
            replace: 'Hi'
        }


        stringReplace.process(message)


        const expectedOutput = 'Hi world! Hi universe!'
        expect(message.content).to.equal(expectedOutput)
    })




    it('execute() should not modify the content if the match string is not found', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: 'Hello world!',
            match: 'Goodbye',
            replace: 'Hi'
        }


        stringReplace.process(message)


        const expectedOutput = 'Hello world!'
        expect(message.content).to.equal(expectedOutput)
    })




    it('execute() should handle empty content string', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: '',
            match: 'Hello',
            replace: 'Hi'
        }


        stringReplace.process(message)


        const expectedOutput = ''
        expect(message.content).to.equal(expectedOutput)
    })
})

================
File: tests/grapoi-raw-tests.js
================
import { namedNode, literal } from '@rdfjs/data-model';
import { Grapoi } from '@grapoi';
import { Term } from '@rdfjs/types';

describe('Grapoi', () => {
    let grapoi;

    beforeEach(() => {
        grapoi = new Grapoi();
    });

    describe('.addList', () => {
        it('should return a Grapoi instance', () => {

            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const items: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = grapoi.addList(predicates, items);

            expect(result).toBeInstanceOf(Grapoi);
        });
    });


});



import { namedNode, literal } from '@rdfjs/data-model';
import { Grapoi } from '@grapoi';
import { Term } from '@rdfjs/types';

describe('Grapoi', () => {
    let grapoi;

    beforeEach(() => {
        grapoi = new Grapoi();
    });

    describe('.addList', () => {
        it('should return a Grapoi instance', () => {
            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const items: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = grapoi.addList(predicates, items);

            expect(result).toBeInstanceOf(Grapoi);
        });
    });

    describe('.addOut', () => {
        it('should return a Grapoi instance', () => {
            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const objects: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = grapoi.addOut(predicates, objects);

            expect(result).toBeInstanceOf(Grapoi);
        });
    });


});




describe('.base', () => {
    it('should return a Grapoi instance', () => {
        const base: Term = namedNode('https://housemd.rdf-ext.org/person/allison-cameron');

        const result = grapoi.base(base);

        expect(result).toBeInstanceOf(Grapoi);
    });
});



import { namedNode, literal } from '@rdfjs/data-model';
import { Path } from '@grapoi';
import { Term } from '@rdfjs/types';

describe('Path', () => {
    let path;

    beforeEach(() => {
        path = new Path();
    });

    describe('.addList', () => {
        it('should return a Path instance', () => {
            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const items: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = path.addList(predicates, items);

            expect(result).toBeInstanceOf(Path);
        });
    });


});



import { namedNode, literal } from '@rdfjs/data-model';
import { PathList } from '@grapoi';
import { Term } from '@rdfjs/types';

describe('PathList', () => {
    let pathList;

    beforeEach(() => {
        pathList = new PathList();
    });

    describe('.addList', () => {
        it('should return a PathList instance', () => {
            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const items: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = pathList.addList(predicates, items);

            expect(result).toBeInstanceOf(PathList);
        });
    });


});





describe('.addOut', () => {
    it('should return a Path instance', () => {
        const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
        const objects: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

        const result = path.addOut(predicates, objects);

        expect(result).toBeInstanceOf(Path);
    });
});





describe('.addOut', () => {
    it('should return a PathList instance', () => {
        const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
        const objects: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

        const result = pathList.addOut(predicates, objects);

        expect(result).toBeInstanceOf(PathList);
    });
});





describe('.deleteIn', () => {
    it('should return a Path instance', () => {
        const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];

        const result = path.deleteIn(predicates);

        expect(result).toBeInstanceOf(Path);
    });
});





describe('.deleteIn', () => {
    it('should return a PathList instance', () => {
        const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];

        const result = pathList.deleteIn(predicates);

        expect(result).toBeInstanceOf(PathList);
    });
});

================
File: types/grapoi.d.ts
================
import { DatasetCore, Quad, Term } from "@rdfjs/types";


interface Grapoi extends PathList {
    addList(predicates: Term | Term[], items: Term | Term[]): Grapoi;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): Grapoi;
    base(base: Term | Term[]): Grapoi;
}


interface Edge {
    dataset: DatasetCore;
    end: string;
    quad: Quad;
    start: string;
    term: Term;
    graph: Term;
    startTerm: Term;
}


interface Instruction {
    operation?: string;
    quantifier?: string;
    start?: string;
    end?: string;
    subjects?: Term[];
    predicates?: Term[];
    objects?: Term[];
    graphs?: DatasetCore[];
    items?: Term[];
    callback?: (edge: Edge, ptr: Path | PathList) => Path | PathList;
}


interface Path {
    addList(predicates: Term | Term[], items: Term | Term[]): Path;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): Path;
    deleteIn(predicates: Term | Term[], subjects: Term | Term[]): Path;
    deleteList(predicates: Term | Term[]): Path;
    deleteOut(predicates: Term | Term[], objects: Term | Term[]): Path;
    extend(edge: Edge): Path;
    execute(instruction: Instruction): Path;
}


interface PathList {
    addList(predicates: Term | Term[], items: Term | Term[]): PathList;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): PathList;
    deleteIn(predicates: Term | Term[], subjects: Term | Term[]): PathList;
    deleteList(predicates: Term | Term[]): PathList;
    deleteOut(predicates: Term | Term[], objects: Term | Term[]): PathList;
    distinct(): PathList;
    in(predicates: Term | Term[], subjects: Term | Term[]): PathList;
    isAny(): boolean;
    isList(): boolean;
    list(): Iterator<Term> | undefined;
    map(callback: Function): PathList[];
    out(predicates: Term | Term[], objects: Term | Term[]): PathList;
    quads(): Iterator<Quad>;
    execute(instruction: Instruction): PathList[];
}

================
File: .babelrc
================
{
  "plugins": ["@babel/syntax-dynamic-import"],
  "presets": [
    [
      "@babel/preset-env",
      {
        "modules": false
      }
    ]
  ]
}

================
File: .gitignore
================
**/src-old
**/*\ copy.js

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

================
File: CC_2024-10-16T16:31:52.md
================
# Untitled

## 0

{
  "uuid": "e4f3df00-e887-42ff-965c-06b72e5e5cbc",
  "text": "I've made those changes but the generated form looks very different from what I asked for, the layout is messed up :",
  "sender": "human",
  "created_at": "2024-10-16T16:25:11.649199Z",
  "updated_at": "2024-10-16T16:25:11.649199Z",
  "attachments": [
    {
      "file_name": "paste.txt",
      "file_size": 5161,
      "file_type": "txt",
      "extracted_content": "\n<!DOCTYPE html>\n<html lang=\"en\">\n    \n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Form</title>\n        <link rel=\"stylesheet\" href=\"styles.css\">\n    <script defer src=\"main.bundle.js\"></script></head>\n    \n    <body>\n        <div class=\"container\">\n            <div class=\"column\">\n                <form id=\"inputForm\">\n                    <!-- Form elements will be inserted here -->\n                </form>\n            </div>\n            <div class=\"column\">\n                <form id=\"outputForm\">\n                    <label for=\"output\">Output</label>\n                    <textarea id=\"output\" rows=\"20\"></textarea>\n                    <button type=\"button\" id=\"submitButton\">Submit</button>\n                </form>\n            </div>\n        </div>\n        <script src=\"webpack/main.bundle.js\"></script>\n        <script>\n            // JavaScript code will be inserted here\n        </script>\n        <form>\n            <div class=\"form-group\">\n                <label>name</label>\n                <textarea data-term=\"name\" data-namespace=\"http://xmlns.com/foaf/0.1/\"></textarea>\n            </div>\n            <div class=\"form-group\">\n                <label>mbox</label>\n                <textarea data-term=\"mbox\" data-namespace=\"http://xmlns.com/foaf/0.1/\"></textarea>\n            </div>\n            <div class=\"form-group\">\n                <label>homepage</label>\n                <textarea data-term=\"homepage\" data-namespace=\"http://xmlns.com/foaf/0.1/\"></textarea>\n            </div>\n            <div class=\"form-group\">\n                <label>nick</label>\n                <textarea data-term=\"nick\" data-namespace=\"http://xmlns.com/foaf/0.1/\"></textarea>\n            </div>\n            <div class=\"form-group\">\n                <label>depiction</label>\n                <textarea data-term=\"depiction\" data-namespace=\"http://xmlns.com/foaf/0.1/\"></textarea>\n            </div>\n            <div class=\"form-group\">\n                <label>interest</label>\n                <textarea data-term=\"interest\" data-namespace=\"http://xmlns.com/foaf/0.1/\"></textarea>\n            </div>\n            <div class=\"form-group\">\n                <label>knows</label>\n                <fieldset>\n                    <legend>knows</legend>\n                    <div class=\"nested-entry\">\n                        <div class=\"form-group\">\n                            <label>name</label>\n                            <textarea data-term=\"name\" data-namespace=\"http://xmlns.com/foaf/0.1/\"></textarea>\n                        </div>\n                        <button type=\"button\">Remove</button>\n                    </div>\n                    <button type=\"button\">Add knows</button>\n                </fieldset>\n            </div>\n            <fieldset>\n                <legend>knows</legend>\n                <div class=\"form-group\">\n                    <label>name</label>\n                    <textarea data-term=\"name\" data-namespace=\"http://xmlns.com/foaf/0.1/\"></textarea>\n                </div>\n            </fieldset>\n            <div class=\"form-group\">\n                <label>maintainer-of</label>\n                <fieldset>\n                    <legend>maintainer-of</legend>\n                    <div class=\"nested-entry\">\n                        <div class=\"form-group\">\n                            <label>homepage</label>\n                            <textarea data-term=\"homepage\" data-namespace=\"http://usefulinc.com/ns/doap#\"></textarea>\n                        </div>\n                        <div class=\"form-group\">\n                            <label>description</label>\n                            <textarea data-term=\"description\" data-namespace=\"http://usefulinc.com/ns/doap#\"></textarea>\n                        </div>\n                        <button type=\"button\">Remove</button>\n                    </div>\n                    <button type=\"button\">Add maintainer-of</button>\n                </fieldset>\n            </div>\n            <fieldset>\n                <legend>maintainer-of</legend>\n                <div class=\"form-group\">\n                    <label>homepage</label>\n                    <textarea data-term=\"homepage\" data-namespace=\"http://usefulinc.com/ns/doap#\"></textarea>\n                </div>\n                <div class=\"form-group\">\n                    <label>description</label>\n                    <textarea data-term=\"description\" data-namespace=\"http://usefulinc.com/ns/doap#\"></textarea>\n                </div>\n            </fieldset>\n            <div class=\"form-group\">\n                <label>taskStatus</label>\n                <input type=\"checkbox\" data-term=\"taskStatus\" data-namespace=\"http://purl.org/stuff/prj/\">\n            </div>\n            <div class=\"form-group\">\n                <label>priority</label>\n                <input type=\"number\" data-term=\"priority\" data-namespace=\"http://purl.org/stuff/prj/\">\n            </div>\n            <button type=\"button\" id=\"submitButton\">Submit</button>\n            <label for=\"output\">Output</label>\n            <textarea id=\"output\" rows=\"10\"></textarea>\n        </form>\n    </body>\n\n</html>"
    }
  ],
  "files": [
    {
      "file_name": "paste.txt"
    }
  ]
}

## 1

{
  "uuid": "5cc59f8e-9b66-464f-bf16-ff8db5d64e88",
  "text": "I'd like the following changes which will impact ` html-template.html` which is used by `json-to-html.js` to generate forms like `foaf-form.html` and `style.css` it may also be necessary to modify `extractor.js`. Use modern best practices that will work cross-browser.\n1. Split the page into two columns with the input form on the lhs and the `Output` text area in a new form block on the rhs\n2. make the form more compact vertically and horizontally with a clear border on each form-group, with very slightly rounded corners\n3. give the buttons labelled `Add` something a class, so that they can be identified for the next step,\n4. implement the code necessary to add the expected functionality to the Add buttons, creating a new element.  The element's attributes should be derived from the parent group, as these may change from generated form to generated form\n5. suggest anything else that might improve aesthetics or functionality",
  "sender": "human",
  "created_at": "2024-10-16T16:11:11.394028Z",
  "updated_at": "2024-10-16T16:11:11.394028Z",
  "attachments": [],
  "files": []
}

## 2

{
  "uuid": "6209c39b-e3b3-4a62-ba2a-cc6e8cc31872",
  "text": " Certainly! I'll provide the changes for `html-template.html`, `style.css`, and `json-to-html.js` to implement these improvements. Here's an outline of the changes:\n\n1. Split page into two columns\n2. Compact form design with borders\n3. Add class to \"Add\" buttons\n4. Implement \"Add\" button functionality\n5. Additional improvements\n\nLet's start with the HTML template:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Form</title>\n    <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"column\">\n            <form id=\"inputForm\">\n                <!-- Form elements will be inserted here -->\n            </form>\n        </div>\n        <div class=\"column\">\n            <form id=\"outputForm\">\n                <label for=\"output\">Output</label>\n                <textarea id=\"output\" rows=\"20\"></textarea>\n                <button type=\"button\" id=\"submitButton\">Submit</button>\n            </form>\n        </div>\n    </div>\n    <script src=\"webpack/main.bundle.js\"></script>\n    <script>\n        // JavaScript code will be inserted here\n    </script>\n</body>\n</html>\n```\n\nNow, let's update the CSS:\n\n```css\nbody {\n    font-family: Arial, sans-serif;\n    line-height: 1.6;\n    color: #333;\n    margin: 0;\n    padding: 20px;\n    background-color: #f4f4f4;\n}\n\n.container {\n    display: flex;\n    gap: 20px;\n    max-width: 1200px;\n    margin: 0 auto;\n}\n\n.column {\n    flex: 1;\n}\n\nform {\n    background-color: #fff;\n    padding: 20px;\n    border-radius: 5px;\n    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n}\n\n.form-group {\n    border: 1px solid #ddd;\n    border-radius: 4px;\n    padding: 10px;\n    margin-bottom: 10px;\n}\n\nlabel {\n    display: block;\n    margin-bottom: 5px;\n    font-weight: bold;\n    color: #2c3e50;\n}\n\ninput[type=\"text\"],\ninput[type=\"number\"],\ntextarea {\n    width: 100%;\n    padding: 8px;\n    margin-bottom: 10px;\n    border: 1px solid #ddd;\n    border-radius: 4px;\n    box-sizing: border-box;\n}\n\ninput[type=\"checkbox\"] {\n    margin-bottom: 10px;\n}\n\nbutton {\n    background-color: #3498db;\n    color: #fff;\n    padding: 8px 12px;\n    border: none;\n    border-radius: 4px;\n    cursor: pointer;\n    transition: background-color 0.3s;\n}\n\nbutton:hover {\n    background-color: #2980b9;\n}\n\n.add-button {\n    background-color: #2ecc71;\n}\n\n.add-button:hover {\n    background-color: #27ae60;\n}\n\n#output {\n    width: 100%;\n    height: calc(100vh - 140px);\n    resize: vertical;\n}\n\n@media (max-width: 768px) {\n    .container {\n        flex-direction: column;\n    }\n}\n```\n\nNow, let's update `json-to-html.js` to add the \"Add\" button functionality:\n\n```javascript\nclass FormElementFactory {\n    // ... existing code ...\n\n    createAddButton(property) {\n        const addButton = this.document.createElement('button');\n        addButton.textContent = `Add ${property.term}`;\n        addButton.type = 'button';\n        addButton.classList.add('add-button');\n        addButton.dataset.term = property.term;\n        addButton.dataset.namespace = property.namespace;\n        addButton.onclick = () => this.addNestedEntry(addButton.closest('fieldset'), property);\n        return addButton;\n    }\n\n    // ... existing code ...\n}\n\nclass JsonToHtmlForm {\n    // ... existing code ...\n\n    createClientScript() {\n        const script = this.document.createElement('script');\n        script.textContent = `\n            document.addEventListener('DOMContentLoaded', function() {\n                console.log('DOM fully loaded');\n                var submitButton = document.getElementById('submitButton');\n                var outputElement = document.getElementById('output');\n                var loadingIndicator = document.createElement('div');\n                loadingIndicator.id = 'loadingIndicator';\n                loadingIndicator.textContent = 'Loading...';\n                loadingIndicator.style.display = 'none';\n                document.body.appendChild(loadingIndicator);\n\n                function checkExtractFunction() {\n                    if (typeof window.extract === 'function') {\n                        submitButton.disabled = false;\n                        loadingIndicator.style.display = 'none';\n                        console.log('Extract function is available');\n                    } else {\n                        setTimeout(checkExtractFunction, 100);\n                    }\n                }\n\n                function handleExtract() {\n                    submitButton.disabled = true;\n                    loadingIndicator.style.display = 'block';\n                    try {\n                        console.log('Calling extract function');\n                        var output = window.extract(document);\n                        outputElement.value = output;\n                    } catch (error) {\n                        console.error('Error during extraction:', error);\n                        outputElement.value = 'An error occurred: ' + error.message;\n                    } finally {\n                        submitButton.disabled = false;\n                        loadingIndicator.style.display = 'none';\n                    }\n                }\n\n                if (submitButton) {\n                    submitButton.disabled = true;\n                    submitButton.addEventListener('click', handleExtract);\n                    checkExtractFunction();\n                } else {\n                    console.error('Submit button not found');\n                }\n\n                // Add button functionality\n                document.addEventListener('click', function(event) {\n                    if (event.target.classList.contains('add-button')) {\n                        var fieldset = event.target.closest('fieldset');\n                        var term = event.target.dataset.term;\n                        var namespace = event.target.dataset.namespace;\n                        addNestedEntry(fieldset, { term, namespace });\n                    }\n                });\n\n                function addNestedEntry(fieldset, property) {\n                    var entryDiv = document.createElement('div');\n                    entryDiv.classList.add('nested-entry');\n\n                    var input = document.createElement('input');\n                    input.type = 'text';\n                    input.dataset.term = property.term;\n                    input.dataset.namespace = property.namespace;\n\n                    var removeButton = document.createElement('button');\n                    removeButton.textContent = 'Remove';\n                    removeButton.type = 'button';\n                    removeButton.onclick = function() {\n                        fieldset.removeChild(entryDiv);\n                    };\n\n                    entryDiv.appendChild(input);\n                    entryDiv.appendChild(removeButton);\n                    fieldset.insertBefore(entryDiv, event.target);\n                }\n            });\n        `;\n        return script;\n    }\n\n    // ... existing code ...\n}\n```\n\nThese changes implement the requested improvements:\n\n1. The page is split into two columns using flexbox.\n2. The form is more compact with clear borders and slightly rounded corners for each form-group.\n3. \"Add\" buttons now have a class `add-button` for easy identification.\n4. The \"Add\" button functionality is implemented, creating new elements based on the parent group's attributes.\n5. Additional improvements include:\n   - Responsive design for mobile devices\n   - Improved button styling with hover effects\n   - Full-height output textarea\n   - Loading indicator during extraction\n\nTo fully implement these changes, you'll need to update `extractor.js` to handle the new nested entries created by the \"Add\" buttons. This may involve modifying the `FormDataExtractor` class to recursively process nested entries.\n\nQ1: How should we handle validation for the dynamically added form elements?\nQ2: Would it be beneficial to add a \"Remove All\" button for each group of nested entries?\nQ3: Should we implement a way to reorder the nested entries using drag-and-drop functionality?\nQ4: How can we improve accessibility for users relying on screen readers or keyboard navigation?",
  "sender": "assistant",
  "created_at": "2024-10-16T16:11:11.394028Z",
  "updated_at": "2024-10-16T16:11:11.394028Z",
  "attachments": [],
  "files": []
}

## 3

{
  "uuid": "b10b7e30-4b24-4774-ad0c-13b8156502b3",
  "text": " I apologize for the confusion. It seems that the generated form is not correctly applying the new structure and styles. Let's revise the HTML template and adjust the JSON-to-HTML conversion process to ensure the layout is correct. Here's an updated version of the HTML template:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Form</title>\n    <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"column\">\n            <form id=\"inputForm\">\n                <!-- Form elements will be inserted here -->\n            </form>\n        </div>\n        <div class=\"column\">\n            <form id=\"outputForm\">\n                <label for=\"output\">Output</label>\n                <textarea id=\"output\" rows=\"20\"></textarea>\n                <button type=\"button\" id=\"submitButton\">Submit</button>\n            </form>\n        </div>\n    </div>\n    <script src=\"webpack/main.bundle.js\"></script>\n    <script>\n        // JavaScript code will be inserted here\n    </script>\n</body>\n</html>\n```\n\nNow, let's modify the `JsonToHtmlForm` class in `json-to-html.js` to ensure it generates the correct structure:\n\n```javascript\nclass JsonToHtmlForm {\n    // ... existing code ...\n\n    jsonToHtmlForm(jsonData) {\n        const inputForm = this.document.getElementById('inputForm');\n        inputForm.appendChild(this.createFormElements(jsonData.ROOT.properties));\n        return this.document.documentElement.outerHTML;\n    }\n\n    createFormElements(properties) {\n        const fragment = this.document.createDocumentFragment();\n        properties.forEach(property => {\n            const formGroup = this.document.createElement('div');\n            formGroup.classList.add('form-group');\n            const { label, input } = this.formElementFactory.createElement(property);\n            formGroup.appendChild(label);\n            formGroup.appendChild(input);\n            fragment.appendChild(formGroup);\n\n            if (property.children && property.children.properties) {\n                const fieldset = this.createNestedFieldset(property);\n                fragment.appendChild(fieldset);\n            }\n        });\n        return fragment;\n    }\n\n    // ... existing code ...\n\n    async jsonFileToHtmlForm(templatePath, filePath) {\n        try {\n            await this.initDOM(templatePath);\n\n            const jsonString = await fs.readFile(filePath, 'utf-8');\n            const jsonData = JSON.parse(jsonString);\n\n            if (!jsonData || !jsonData.ROOT || !Array.isArray(jsonData.ROOT.properties)) {\n                throw new Error('Invalid JSON structure: ROOT.properties array is missing');\n            }\n\n            this.jsonToHtmlForm(jsonData);\n            this.document.body.appendChild(this.createClientScript());\n\n            return this.dom.serialize();\n        } catch (error) {\n            console.error('Error in jsonFileToHtmlForm:', error);\n            throw error;\n        }\n    }\n}\n```\n\nThese changes should ensure that the form is generated with the correct structure, placing all form elements inside the `#inputForm` and maintaining the two-column layout.\n\nAlso, make sure that the `styles.css` file is correctly linked and contains the CSS provided earlier.\n\nLastly, update the `FormElementFactory` class to use the correct HTML structure for fieldsets:\n\n```javascript\nclass FormElementFactory {\n    // ... existing code ...\n\n    createFieldset(property) {\n        const fieldset = this.document.createElement('fieldset');\n        const legend = this.document.createElement('legend');\n        legend.textContent = property.term;\n        fieldset.appendChild(legend);\n\n        property.children.properties.forEach(childProp => {\n            const formGroup = this.document.createElement('div');\n            formGroup.classList.add('form-group');\n            const { label, input } = this.createElement(childProp);\n            formGroup.appendChild(label);\n            formGroup.appendChild(input);\n            fieldset.appendChild(formGroup);\n        });\n\n        const addButton = this.createAddButton(property);\n        fieldset.appendChild(addButton);\n\n        return fieldset;\n    }\n\n    // ... existing code ...\n}\n```\n\nThese changes should correct the layout issues and ensure that the generated form matches the requested structure and styling.\n\nQ1: Do you need any clarification on how to integrate these changes into your existing codebase?\nQ2: Would you like to see an example of how to handle the \"Add\" button functionality in the client-side JavaScript?\nQ3: Should we implement any additional error handling or validation in the form generation process?\nQ4: Would you like to see an example of how to style the nested entries differently from the top-level form groups?",
  "sender": "assistant",
  "created_at": "2024-10-16T16:25:11.649199Z",
  "updated_at": "2024-10-16T16:25:11.649199Z",
  "attachments": [],
  "files": []
}

## 4

{
  "uuid": "698b18cc-2eac-412f-bf15-71daba642a83",
  "text": " I apologize for the oversight. The error suggests that we're trying to append something that isn't a valid DOM node. Let's debug and fix this issue. The problem is likely in the `jsonToHtmlForm` method of the `JsonToHtmlForm` class. Here's a revised version of the relevant parts of the `json-to-html.js` file:\n\n```javascript\nclass JsonToHtmlForm {\n    // ... existing code ...\n\n    jsonToHtmlForm(jsonData) {\n        const inputForm = this.document.getElementById('inputForm');\n        if (!inputForm) {\n            console.error('Input form not found');\n            return;\n        }\n        const formElements = this.createFormElements(jsonData.ROOT.properties);\n        inputForm.appendChild(formElements);\n    }\n\n    createFormElements(properties) {\n        const fragment = this.document.createDocumentFragment();\n        properties.forEach(property => {\n            const formGroup = this.document.createElement('div');\n            formGroup.classList.add('form-group');\n            const { label, input } = this.formElementFactory.createElement(property);\n            formGroup.appendChild(label);\n            formGroup.appendChild(input);\n            fragment.appendChild(formGroup);\n\n            if (property.children && property.children.properties) {\n                const fieldset = this.createNestedFieldset(property);\n                fragment.appendChild(fieldset);\n            }\n        });\n        return fragment;\n    }\n\n    createNestedFieldset(property) {\n        const fieldset = this.document.createElement('fieldset');\n        const legend = this.document.createElement('legend');\n        legend.textContent = property.term;\n        fieldset.appendChild(legend);\n\n        property.children.properties.forEach(childProp => {\n            const formGroup = this.document.createElement('div');\n            formGroup.classList.add('form-group');\n            const { label, input } = this.formElementFactory.createElement(childProp);\n            formGroup.appendChild(label);\n            formGroup.appendChild(input);\n            fieldset.appendChild(formGroup);\n        });\n\n        const addButton = this.formElementFactory.createAddButton(property);\n        fieldset.appendChild(addButton);\n\n        return fieldset;\n    }\n\n    async jsonFileToHtmlForm(templatePath, filePath) {\n        try {\n            await this.initDOM(templatePath);\n\n            const jsonString = await fs.readFile(filePath, 'utf-8');\n            const jsonData = JSON.parse(jsonString);\n\n            if (!jsonData || !jsonData.ROOT || !Array.isArray(jsonData.ROOT.properties)) {\n                throw new Error('Invalid JSON structure: ROOT.properties array is missing');\n            }\n\n            this.jsonToHtmlForm(jsonData);\n            this.document.body.appendChild(this.createClientScript());\n\n            return this.dom.serialize();\n        } catch (error) {\n            console.error('Error in jsonFileToHtmlForm:', error);\n            throw error;\n        }\n    }\n}\n```\n\nMake sure that the `FormElementFactory` class is correctly implemented and that its `createElement` method returns an object with `label` and `input` properties:\n\n```javascript\nclass FormElementFactory {\n    // ... existing code ...\n\n    createElement(property) {\n        const label = this.createLabel(property.term);\n        const input = this.createInput(property);\n        return { label, input };\n    }\n\n    // ... other methods ...\n}\n```\n\nAlso, ensure that the HTML template file (`html-template.html`) contains the correct structure with the `inputForm` and `outputForm` elements:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Form</title>\n    <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"column\">\n            <form id=\"inputForm\">\n                <!-- Form elements will be inserted here -->\n            </form>\n        </div>\n        <div class=\"column\">\n            <form id=\"outputForm\">\n                <label for=\"output\">Output</label>\n                <textarea id=\"output\" rows=\"20\"></textarea>\n                <button type=\"button\" id=\"submitButton\">Submit</button>\n            </form>\n        </div>\n    </div>\n    <script src=\"webpack/main.bundle.js\"></script>\n</body>\n</html>\n```\n\nThese changes should resolve the \"appendChild\" error and correctly generate the form structure. If you're still encountering issues, please provide more details about the `FormElementFactory` class implementation and the contents of the `foaf-template.json` file.\n\nQ1: Are there any console logs or additional error messages that might provide more context about the issue?\nQ2: Can you confirm that the `FormElementFactory` class is correctly implemented and imported in the `json-to-html.js` file?\nQ3: Would you like to see a more detailed example of how to debug this issue step-by-step?\nQ4: Are there any specific parts of the code that you suspect might be causing the problem?",
  "sender": "assistant",
  "created_at": "2024-10-16T16:31:18.014203Z",
  "updated_at": "2024-10-16T16:31:18.014203Z",
  "attachments": [],
  "files": []
}

## 5

{
  "uuid": "a593f591-fb86-4a24-b418-3eb198c686fb",
  "text": "node src/index.js\nJSON file saved: src/templates/foaf-template.json\nError in jsonFileToHtmlForm: TypeError: Failed to execute 'appendChild' on 'Node': parameter 1 is not of type 'Node'.\n    at exports.convert (/home/danny/github-danny/hyperdata/packages/farelo/form-a-matic/node_modules/jsdom/lib/jsdom/living/generated/Node.js:25:9)\n    at HTMLDivElement.appendChild (/home/danny/github-danny/hyperdata/packages/farelo/form-a-matic/node_modules/jsdom/lib/jsdom/living/generated/Node.js:404:26)\n    at file:///home/danny/github-danny/hyperdata/packages/farelo/form-a-matic/src/json-to-html.js:148:23",
  "sender": "human",
  "created_at": "2024-10-16T16:31:18.014203Z",
  "updated_at": "2024-10-16T16:31:18.014203Z",
  "attachments": [],
  "files": []
}

## 6

{
  "uuid": "10f4ca7c-6ef9-43ac-86f4-a7cb2e379163",
  "text": " I apologize for the oversight. The error suggests that we're trying to append something that isn't a valid DOM node. Let's debug and fix this issue in the `JsonToHtmlForm` class. Here's a revised version of the relevant parts:\n\n```javascript\nclass JsonToHtmlForm {\n    // ... existing code ...\n\n    jsonToHtmlForm(jsonData) {\n        const inputForm = this.document.getElementById('inputForm');\n        if (!inputForm) {\n            console.error('Input form not found');\n            return;\n        }\n        const formElements = this.createFormElements(jsonData.ROOT.properties);\n        inputForm.appendChild(formElements);\n    }\n\n    createFormElements(properties) {\n        const fragment = this.document.createDocumentFragment();\n        properties.forEach(property => {\n            const formGroup = this.document.createElement('div');\n            formGroup.classList.add('form-group');\n            const { label, input } = this.formElementFactory.createElement(property);\n            formGroup.appendChild(label);\n            formGroup.appendChild(input);\n            fragment.appendChild(formGroup);\n\n            if (property.children && property.children.properties) {\n                const fieldset = this.createNestedFieldset(property);\n                fragment.appendChild(fieldset);\n            }\n        });\n        return fragment;\n    }\n\n    createNestedFieldset(property) {\n        const fieldset = this.document.createElement('fieldset');\n        const legend = this.document.createElement('legend');\n        legend.textContent = property.term;\n        fieldset.appendChild(legend);\n\n        property.children.properties.forEach(childProp => {\n            const formGroup = this.document.createElement('div');\n            formGroup.classList.add('form-group');\n            const { label, input } = this.formElementFactory.createElement(childProp);\n            formGroup.appendChild(label);\n            formGroup.appendChild(input);\n            fieldset.appendChild(formGroup);\n        });\n\n        const addButton = this.formElementFactory.createAddButton(property);\n        fieldset.appendChild(addButton);\n\n        return fieldset;\n    }\n\n    async jsonFileToHtmlForm(templatePath, filePath) {\n        try {\n            await this.initDOM(templatePath);\n\n            const jsonString = await fs.readFile(filePath, 'utf-8');\n            const jsonData = JSON.parse(jsonString);\n\n            if (!jsonData || !jsonData.ROOT || !Array.isArray(jsonData.ROOT.properties)) {\n                throw new Error('Invalid JSON structure: ROOT.properties array is missing');\n            }\n\n            this.jsonToHtmlForm(jsonData);\n            const script = this.createClientScript();\n            this.document.body.appendChild(script);\n\n            return this.dom.serialize();\n        } catch (error) {\n            console.error('Error in jsonFileToHtmlForm:', error);\n            throw error;\n        }\n    }\n\n    // ... existing code ...\n}\n```\n\nAlso, ensure that the `FormElementFactory` class is correctly implemented:\n\n```javascript\nclass FormElementFactory {\n    constructor(document) {\n        this.document = document;\n    }\n\n    createElement(property) {\n        const label = this.createLabel(property.term);\n        const input = this.createInput(property);\n        return { label, input };\n    }\n\n    createLabel(text) {\n        const label = this.document.createElement('label');\n        label.textContent = text;\n        return label;\n    }\n\n    createInput(property) {\n        let input;\n        if (property.type === 'LITERAL') {\n            if (property.subtype === 'BOOLEAN') {\n                input = this.document.createElement('input');\n                input.type = 'checkbox';\n            } else if (property.subtype === 'INTEGER') {\n                input = this.document.createElement('input');\n                input.type = 'number';\n            } else {\n                input = this.document.createElement('textarea');\n            }\n        } else if (property.type === 'URI') {\n            input = this.document.createElement('input');\n            input.type = 'url';\n        } else {\n            input = this.document.createElement('input');\n            input.type = 'text';\n        }\n\n        input.dataset.term = property.term;\n        input.dataset.namespace = property.namespace;\n        return input;\n    }\n\n    createAddButton(property) {\n        const button = this.document.createElement('button');\n        button.textContent = `Add ${property.term}`;\n        button.type = 'button';\n        button.classList.add('add-button');\n        button.dataset.term = property.term;\n        button.dataset.namespace = property.namespace;\n        return button;\n    }\n}\n```\n\nThese changes should resolve the TypeError and ensure that valid DOM nodes are being created and appended. The form structure should now be correctly generated within the `#inputForm` element.\n\nQ1: Does this resolve the error you were experiencing?\nQ2: Are there any other error messages or unexpected behaviors you're seeing?\nQ3: Would you like to see an example of how to test this form generation process?\nQ4: Should we implement any logging or debugging features to help troubleshoot future issues?",
  "sender": "assistant",
  "created_at": "2024-10-16T16:31:52.083281Z",
  "updated_at": "2024-10-16T16:31:52.083281Z",
  "attachments": [],
  "files": []
}

## 7

{
  "uuid": "7c9def3d-670d-407f-b1f6-48b236ae9bc8",
  "text": "node src/index.js\nJSON file saved: src/templates/foaf-template.json\nError in jsonFileToHtmlForm: TypeError: Failed to execute 'appendChild' on 'Node': parameter 1 is not of type 'Node'.\n    at exports.convert (/home/danny/github-danny/hyperdata/packages/farelo/form-a-matic/node_modules/jsdom/lib/jsdom/living/generated/Node.js:25:9)\n    at HTMLDivElement.appendChild (/home/danny/github-danny/hyperdata/packages/farelo/form-a-matic/node_modules/jsdom/lib/jsdom/living/generated/Node.js:404:26)\n    at file:///home/danny/github-danny/hyperdata/packages/farelo/form-a-matic/src/json-to-html.js:148:23",
  "sender": "human",
  "created_at": "2024-10-16T16:31:52.083281Z",
  "updated_at": "2024-10-16T16:31:52.083281Z",
  "attachments": [],
  "files": []
}

================
File: jasmine.json
================
{
    "spec_dir": "tests",
    "spec_files": [
        "**/*[sS]pec.js"
    ],
    "helpers": [
        "helpers/reporter.js"
    ],
    "stopSpecOnExpectationFailure": true,
    "random": false
}

================
File: jc.txt
================
CommandUtils.parseOrLoadContext(), contextArg = src/applications/claude-json-converter/data/input/conversations.json
*** Loading JSON from file...

+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/cjc>
| Create processor :p5 of type :SetMessage
1

================
File: jsconfig.json
================
{
    "compilerOptions": {
      "target": "ES6",
      "module": "commonjs",
      "allowSyntheticDefaultImports": true,
      "baseUrl": "./",
      "paths": {
        "*": ["node_modules/*", "types/*"]
      }
    },
    "include": ["src/**/*", "run.js", "../trans-apps/applications/git-apps/github_", "src-old/CommandUtils copy.js"],
    "exclude": ["node_modules", "**/node_modules/*"],
    "typeAcquisition": {
      "include": ["@rdfjs/types", "grapoi"]
    }
  }

================
File: jsdoc.json
================
{
    "source": {
        "include": [
            "src"
        ],
        "exclude": [
            "node_modules"
        ],
        "includePattern": ".+\\.js(doc|x)?$",
        "excludePattern": "(^|\\/|\\\\)_"
    },
    "opts": {
        "verbose": true,
        "recurse": true,
        "destination": "./docs/jsdoc"
    },
    "plugins": [
        "plugins/markdown"
    ]
}

================
File: LICENSE
================
MIT License

Copyright (c) 2024 Danny Ayers

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: package.json
================
{
  "type": "module",
  "version": "1.0.0",
  "description": "Transmissions",
  "name": "transmissions",
  "scripts": {
    "tests": "jasmine --config=jasmine.json --reporter=tests/helpers/reporter.js",
    "docs": "jsdoc -c jsdoc.json",
    "build": "webpack --mode=production --node-env=production",
    "build:dev": "webpack --mode=development",
    "build:prod": "webpack --mode=production --node-env=production",
    "rp": "repopack -c repopack.config-small.json . && repopack -c repopack.config-large.json . ",
    "watch": "webpack --watch",
    "serve": "webpack serve"
  },
  "devDependencies": {
    "@babel/core": "^7.23.7",
    "@babel/preset-env": "^7.23.8",
    "autoprefixer": "^10.4.17",
    "babel-loader": "^9.1.3",
    "chai": "^5.0.3",
    "css-loader": "^6.9.1",
    "html-webpack-plugin": "^5.6.0",
    "jasmine": "^5.1.0",
    "jasmine-browser-runner": "^2.3.0",
    "jasmine-core": "^5.1.1",
    "jasmine-spec-reporter": "^7.0.0",
    "jsdoc": "^4.0.2",
    "mini-css-extract-plugin": "^2.7.7",
    "nyc": "^17.1.0",
    "postcss": "^8.4.33",
    "postcss-loader": "^8.0.0",
    "prettier": "^3.2.4",
    "style-loader": "^3.3.4",
    "webpack": "^5.90.0",
    "webpack-cli": "^5.1.4",
    "webpack-dev-server": "^4.15.1",
    "workbox-webpack-plugin": "^7.0.0"
  },
  "dependencies": {
    "@dotenvx/dotenvx": "^1.14.2",
    "@rdfjs/formats": "^4.0.0",
    "axios": "^1.6.8",
    "cheerio": "^1.0.0-rc.12",
    "d3": "^7.9.0",
    "jsdom": "^25.0.0",
    "marked": "^12.0.1",
    "marked-code-format": "^1.1.6",
    "marked-custom-heading-id": "^2.0.10",
    "marked-footnote": "^1.2.4",
    "markmap-lib": "^0.17.0",
    "markmap-render": "^0.17.0",
    "markmap-toolbar": "^0.17.0",
    "markmap-view": "^0.17.0",
    "nunjucks": "^3.2.4",
    "queue": "^7.0.0",
    "rdf-ext": "^2.5.2",
    "rdf-utils-fs": "^3.0.0",
    "yargs": "^17.7.2"
  }
}

================
File: postcss.config.js
================
module.exports = {


  plugins: [["autoprefixer"]],
};

================
File: README.md
================
# transmissions

After _No Code_ and _Lo Code_ comes _Marginally Less Code_

**Transmissions** is a micro-framework intended to simplify construction of small pipeliney data processing applications in JavaScript (assuming you are already familiar with JavaScript and RDF).

The code is in active development, ie. **not stable**, subject to arbitrary changes.

A bit like `make` or a `package.json` builder. But much harder work (and fun).

Applications are defined in several places, the bits of interest are eg. Postcraft's [transmissions.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/transmissions.ttl) and [services.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/services.ttl).
The former defines the flow, the latter config of the services (under [src/services](https://github.com/danja/transmissions/tree/main/src/services)). The runtime instance of the application is given in the target [manifest.ttl](https://github.com/danja/postcraft/blob/main/danny.ayers.name/manifest.ttl).

### Installation etc.

This is not ready yet. But if you really must...

Make a fresh dir. Clone this repo and [Postcraft](https://github.com/danja/postcraft) into it.

```
cd transmissions
npm i
```

This may or may not work :

```
npm run test
```

Then if you do :

```
./trans postcraft /home/danny/github-danny/postcraft/danny.ayers.name
```

it may build a site (my blog - this is dogfooding to the max) under `public/home`

```
./trans
```

on its own should list the applications available. Most of these won't work, the code has been shapeshifting a lot.

### Status

**2024-09-02** Getting used as a serrrrriously over-engineered, feature-lacking static site builder, proof of concept is [Postcraft](https://github.com/danja/postcraft), as evinced by my [blog](https://danny.ayers.name/) (where, for now at least you will find update on this). But it mostly works as intended. Docs lagging. But now I have a documentation engine...

Documentation will be lagging behind code, be incomplete and out of date.

**2024-03-24** : a couple of simple data processing pipelines working and wired up as Jasmine e2e tests in place; started to develop actually useful pipelines for [foaf-archive](https://github.com/danja/foaf-archive) project

## Motivation

This was prompted by something of a ~~perfect storm~~ serendipitous convergence of ideas, requirements and environment.

- I had the realization that much of the software I want to write in the near future has the same basic high level **architecture** : pipeline-like, ie. a series of steps each of which transform data from one form into another ([Wikipedia](<https://en.wikipedia.org/wiki/Pipeline_(software)>)).
- **Pipelines** are approachable using concepts from Design Patterns like [Dependency Injection](https://en.wikipedia.org/wiki/Dependency_injection), [Inversion of control](https://en.wikipedia.org/wiki/Inversion_of_control) etc. A key benefit of such patterns is that, because the components are loosely coupled, they can largely be developed in isolation as small standalone units before integration into a larger application.
- **Large Language Models** currently struggle with large codebases and complex architectures (mostly due to [context window](https://en.wikipedia.org/wiki/Large_language_model#Prompt_engineering,_attention_mechanism,_and_context_window) limitations). But they are very good at building small, simple, self-contained units of code.

reused in different contexts.

_I'm writing this with GitHub Copilot enabled for markdown, which can be helpful, but sometimes it goes off on one... After the first point above it offered :_

```
This is true of a blog engine, a personal knowledge management system, a chatbot, a mail archive miner, a web scraper, a data visualizer, a data importer/exporter, a data transformer, a data validator, a data filter, a data aggregator, a data splitter, a data joiner, a data normalizer, a data summarizer, a data classifier, a data indexer, a data search engine, a data storage system, a data retrieval system, a data synchronization system, a data backup system, a data migration system, a data versioning system, a data sharing system, a data access control system, a data privacy system, a data security system, a data integrity system, a data quality system, a data governance system, a data provenance system, a data lineage system, a data lifecycle system, a data archiving system, a data retention system, a data deletion system, a data recovery system, a data discovery system, a data exploration system, a data analysis system, a data visualization system, a data reporting system, a data monitoring system, a data alerting system, a data forecasting system, a data prediction system, a data recommendation system, a data personalization system, a data optimization system, a data simulation system, a data modeling system, a data inference system, a data learning system, a data reasoning system, a data decision system, a data planning system, a data execution system, a data monitoring system, a data feedback system, a data control system, a data automation system, a data orchestration system, a data integration system, a data federation system, a data interoperability system, a data exchange system, a data transformation system, a data migration system, a data synchronization system, a data replication system, a data federation system, a data virtualization system, a data streaming system, a data batch system, a data real-time system, a data near-real-time system, a data historical system, a data temporal system, a data spatial system, a data geospatial system, a data network system, a data graph system, a data tree system, a data list system, a data set system, a data table system, a data document system, a data record system, a data entity system, a data attribute system, a data value system, a data type system, a data format system, a data encoding system, a data serialization
```

_... which is a little over the top, but it's a fair list of things that might be helped by a micro-framework like this. Remarkably the first 5 are pretty much my immediate intended uses for this._

**Docs below need updating**

for later :

The [Community Solid Server](https://github.com/CommunitySolidServer/CommunitySolidServer) uses [Components.js](https://componentsjs.readthedocs.io/en/latest/) to specify how modules and components need to be wired together at runtime. Components.js is a dependency injection framework for JavaScript applications.

## What it is

A low-level system for data processing pipelines/networks. Wherever convenient functionality will be defined declaratively with JSON-LD configuration files.

Dependency injection is used internally to allow loose coupling of components.

## What it isn't

There are several sophisticated frameworks for building interfaces between software applications and creating data processing networks. NodeRed, NoFlo etc. This is not one of them. This is much more basic and bare bones, down in the details.

See also [David Booth](https://github.com/dbooth-boston)'s [RDF Pipeline Framework](https://github.com/rdf-pipeline)

_I do eventually want to use this with NodeRed or whatever, but the entities created by transmissions will be at the level of nodes in such networks, not the network itself._

## Motivation

I'm in the process of writing yet another blog engine (Postcraft). I've also started working on a playground for interconnecting intelligent agents in an XMPP multiuser chat environment (Kia). I'm also revising a system for managing a personal knowledge base in the world of LLMs (HKMS). These all share functionality around connectivity to external data/messaging systems and internal data transformation. Might as well write this bit once only, and avoid thinking about software architecture more than I have to.

### Goals

To facilate :

- rapid development of small applications
- reuse of components in a loosely-couple environment
- versatility

### Soft Goals

- performance - low on the list
- scalability - ditto
- security - ditto

================
File: repopack.config-large.json
================
{
    "output": {
        "filePath": "./repopack-transmissions-large.txt",
        "headerText": "Transmissions source code",
        "removeComments": true
    },
    "ignore": {
        "useDefaultPatterns": true,
        "customPatterns": [
            ".env",
            "**/_*",
            ".git",
            "node_modules",
            "*.log",
            "docs",
            "**/*repopack*.txt",
            "**/*.html",
            "**/data/*",
            "**/*copy.js",
            "**/conversations.json"
        ]
    }
}

================
File: repopack.config-small.json
================
{
    "output": {
        "filePath": "./repopack-transmissions-small.txt",
        "format": "text",
        "headerText": "Transmissions source code",
        "removeComments": true
    },
    "ignore": {
        "useDefaultPatterns": true,
        "customPatterns": [
            ".env",
            ".git",
            "**/_*",
            "node_modules",
            "*.log",
            "docs",
            "**/*repopack*.txt",
            "**/*.html",
            "**/data/*",
            "**/*copy.js",
            "**/conversations.json"
        ]
    },
    "include": [
        "src/api/**/*.js",
        "src/engine/**/*.js",
        "src/processors/base/**/*.js",
        "src/processors/flow/**/*.js",
        "src/processors/util/**/*.js",
        "src/utils/**/*.js",
        "src/applications/string-pipeline/**/*",
        "src/applications/file-pipeline/**/*",
        "src/applications/test_fork/**/*",
        "package.json",
        "README.md",
        "run.js"
    ],
    "exclude": [
        "**/node_modules/**",
        "**/*.spec.js",
        "**/test/**",
        "**/__tests__/**",
        "**/coverage/**",
        "**/docs/**",
        "**/*.md",
        "**/*.log",
        "**/dist/**"
    ]
}

================
File: run.js
================
import yargs from 'yargs'
import { hideBin } from 'yargs/helpers'
import CommandUtils from './src/api/CommandUtils.js'
import WebRunner from './src/api/WebRunner.js'

const defaultApplicationsDir = 'src/applications'
const commandUtils = new CommandUtils(defaultApplicationsDir)

async function main() {
    await yargs(hideBin(process.argv))
        .usage('Usage: ./trans <application>[.subtask] [options] [target]')
        .option('message', {
            alias: 'm',
            describe: 'Input message as JSON',
            type: 'string',
            coerce: JSON.parse
        })
        .option('payload', {
            alias: 'P',
            describe: 'message.payload as a JSON string or a path to a JSON file',
            type: 'string',
        })
        .option('web', {
            alias: 'w',
            describe: 'Start web interface',
            type: 'boolean',
        })
        .option('port', {
            alias: 'p',
            describe: 'Port for web interface',
            type: 'number',
            default: 3000
        })
        .command('$0 [application] [target]', 'runs the specified application', (yargs) => {
            return yargs
                .positional('application', {
                    describe: 'the application to run'
                })
                .positional('target', {
                    describe: 'the target of the application'
                })
        }, async (argv) => {
            if (argv.web) {
                const webRunner = new WebRunner(applicationsDir, argv.port)
                webRunner.start()
                return
            }

            if (!argv.application) {
                console.log('Available applications:')
                const apps = await commandUtils.listApplications()
                console.log(apps.join('\n'))
                return
            }

            let message = {}
            if (argv.payload) {
                message = await CommandUtils.parseOrLoadContext(argv.payload)
            }

            await commandUtils.run(argv.application, argv.target, message)
        })
        .help('h')
        .alias('h', 'help')
        .argv
}

main().catch(console.error)

================
File: trans
================
#!/bin/bash

# run
# use 'chmod +x run' to make this executable


# Execute the Node.js script with Node
node run.js "$@"

================
File: users.json
================
[{"uuid": "dc67aa7d-f71f-4232-afb3-7f2688ac68f7", "full_name": "Danny Ayers", "email_address": "danny.ayers@gmail.com", "verified_phone_number": null}]

================
File: webpack.config.js
================
const path = require('path');
const HtmlWebpackPlugin = require('html-webpack-plugin');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');
const WorkboxWebpackPlugin = require('workbox-webpack-plugin');

const isProduction = process.env.NODE_ENV == 'production';


const stylesHandler = MiniCssExtractPlugin.loader;



const config = {
    entry: './src/index.js',
    output: {
        path: path.resolve(__dirname, 'dist'),
    },
    devServer: {
        open: true,
        host: 'localhost',
    },
    plugins: [
        new HtmlWebpackPlugin({
            template: 'index.html',
        }),

        new MiniCssExtractPlugin(),



    ],
    module: {
        rules: [
            {
                test: /\.(js|jsx)$/i,
                loader: 'babel-loader',
            },
            {
                test: /\.css$/i,
                use: [stylesHandler, 'css-loader', 'postcss-loader'],
            },
            {
                test: /\.(eot|svg|ttf|woff|woff2|png|jpg|gif)$/i,
                type: 'asset',
            },



        ],
    },
};

module.exports = () => {
    if (isProduction) {
        config.mode = 'production';


        config.plugins.push(new WorkboxWebpackPlugin.GenerateSW());

    } else {
        config.mode = 'development';
    }
    return config;
};
