[
  {
    "id": "job-1770197256305-57ijiw",
    "command": "src/apps/newsmonitor/subscribe-from-opml",
    "args": [
      "-m",
      "{\"sourceFile\":\"/tmp/newsmonitor-opml-jrUSG6/feeds-1770197256303.opml\"}"
    ],
    "status": "completed",
    "startedAt": "2026-02-04T09:27:36.305Z",
    "finishedAt": "2026-02-04T09:28:42.343Z",
    "exitCode": 0,
    "stdout": "[dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ“¡ add observability to secrets: https://dotenvx.com/ops\n[dotenv@17.2.3] injecting env (0) from trans-apps/apps/git-apps/.env -- tip: ðŸ”„ add secrets lifecycle management: https://dotenvx.com/ops\n[dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” prevent building .env in docker: https://dotenvx.com/prebuild\n\u001b[38;5;142m[dotenvx@1.51.2] injecting env (0) from .env\u001b[39m\n _____                              _            _\n|_   _|--------------------------> (_) -------> (_) ---------->\n  | |_ __ __ _ _ __  ___ _ __ ___  _ ___ ___ _  ___  _ __  ___\n  | | '__/ _` | '_ \\/ __| '_ ` _ \\| / __/ __| |/ _ \\| '_ \\/ __)\n  | | | | (_| | | | \\__ \\ | | | | | \\__ \\__ \\ | (_) | | | \\__ \\\n  \\_|_|  \\__,_|_| |_|___|_| |_| |_|_|___|___|_|\\___/|_| |_|___/\n   1.0.0 (dev)                                      2026-02-04\n\nAPP PATH = /home/danny/hyperdata/transmissions/src/apps/newsmonitor/subscribe-from-opml\n*** Module path = /home/danny/hyperdata/transmissions/src/apps/newsmonitor/subscribe-from-opml\n\n+ ***** Construct Transmission :  <http://purl.org/stuff/transmissions/subscribe-from-opml>\n  > Connect #0 [loadExistingFeeds] => [readFile]\n  > Connect #1 [readFile] => [extractFeeds]\n  > Connect #2 [extractFeeds] => [processEachURL]\n  > Connect #3 [processEachURL] => [normalizeUrl]\n  > Connect #4 [normalizeUrl] => [dedupeFeedUrl]\n  > Connect #5 [dedupeFeedUrl] => [fetchFeed]\n  > Connect #6 [fetchFeed] => [parseFeed]\n  > Connect #7 [parseFeed] => [generateFeedURI]\n  > Connect #8 [generateFeedURI] => [buildFeedRDF]\n  > Connect #9 [buildFeedRDF] => [storeFeed]\n\n+ Run Transmission :  <http://purl.org/stuff/transmissions/subscribe-from-opml>\n|-> loadExistingFeeds a SPARQLSelect\n    9 query hits\n|->  [loadExistingFeeds] -> readFile a FileReader\n|->  [loadExistingFeeds.readFile] -> extractFeeds a OpmlFeedExtractor\nOpmlFeedExtractor: Found 92 feed URLs\n|->  [loadExistingFeeds.readFile.extractFeeds] -> processEachURL a ForEach\nForEach: Processing 92 items\nForEach: Progress 1/92 (1%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 5/92 (5%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 9/92 (10%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 13/92 (14%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 17/92 (18%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 21/92 (23%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 25/92 (27%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 29/92 (32%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 33/92 (36%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 37/92 (40%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 41/92 (45%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 45/92 (49%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 49/92 (53%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 53/92 (58%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 57/92 (62%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 61/92 (66%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 65/92 (71%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 69/92 (75%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 73/92 (79%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 77/92 (84%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 81/92 (88%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 85/92 (92%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 89/92 (97%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 92/92 (100%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: All 92 messages dispatched\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\nFeedUrlDeduplicator: Skipping existing feed https://simonwillison.net/atom/everything\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\nAxiosError: Request failed with status code 400\nUpdate was :\nINSERT DATA {\n  GRAPH <?g> {\n<http://hyperdata.it/feeds/xhl9s5r8> a <http://rdfs.org/sioc/ns#Forum> ;\n    <http://purl.org/dc/elements/1.1/title> \"\" ;\n    \n    <http://rdfs.org/sioc/ns#link> <> ;\n    <http://rdfs.org/sioc/ns#feed_url> <https://krebsonsecurity.com/feed> ;\n    \n    <http://rdfs.org/sioc/ns#num_items>  ;\n    <http://rdfs.org/sioc/ns#format> \"\" ;\n    <http://purl.org/dc/elements/1.1/created> \"2026-02-04T09:27:39.571Z\"^^<http://www.w3.org/2001/XMLSchema#dateTime> .\n\n  }\n}\n\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 48 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 59 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 100 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\nAxiosError: Request failed with status code 400\nUpdate was :\nINSERT DATA {\n  GRAPH <?g> {\n<http://hyperdata.it/feeds/40zs9yor> a <http://rdfs.org/sioc/ns#Forum> ;\n    <http://purl.org/dc/elements/1.1/title> \"&#92;n&lt;antirez&gt;&#92;n\" ;\n    <http://purl.org/dc/elements/1.1/description> \"Description pending\" ;\n    <http://rdfs.org/sioc/ns#link> <\nhttp://antirez.com\n> ;\n    <http://rdfs.org/sioc/ns#feed_url> <http://antirez.com/rss> ;\n    \n    <http://rdfs.org/sioc/ns#num_items> 100 ;\n    <http://rdfs.org/sioc/ns#format> \"rss2.0\" ;\n    <http://purl.org/dc/elements/1.1/created> \"2026-02-04T09:27:41.150Z\"^^<http://www.w3.org/2001/XMLSchema#dateTime> .\n\n  }\n}\n\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 5 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 46 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 1 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 57 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\nAxiosError: Request failed with status code 400\nUpdate was :\nINSERT DATA {\n  GRAPH <?g> {\n<http://hyperdata.it/feeds/qd4ludvh> a <http://rdfs.org/sioc/ns#Forum> ;\n    <http://purl.org/dc/elements/1.1/title> \"\" ;\n    \n    <http://rdfs.org/sioc/ns#link> <> ;\n    <http://rdfs.org/sioc/ns#feed_url> <https://rachelbythebay.com/w/atom.xml> ;\n    \n    <http://rdfs.org/sioc/ns#num_items>  ;\n    <http://rdfs.org/sioc/ns#format> \"\" ;\n    <http://purl.org/dc/elements/1.1/created> \"2026-02-04T09:27:57.683Z\"^^<http://www.w3.org/2001/XMLSchema#dateTime> .\n\n  }\n}\n\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 31 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 219 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 59 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 4 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 5 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 137 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 14 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 337 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 40 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 43 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 62 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 16 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 28 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 28 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 37 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 12 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 72 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\nAxiosError: Request failed with status code 400\nUpdate was :\nINSERT DATA {\n  GRAPH <?g> {\n<http://hyperdata.it/feeds/m42fjwsw> a <http://rdfs.org/sioc/ns#Forum> ;\n    <http://purl.org/dc/elements/1.1/title> \"\" ;\n    \n    <http://rdfs.org/sioc/ns#link> <> ;\n    <http://rdfs.org/sioc/ns#feed_url> <https://www.tedunangst.com/flak/rss> ;\n    \n    <http://rdfs.org/sioc/ns#num_items>  ;\n    <http://rdfs.org/sioc/ns#format> \"\" ;\n    <http://purl.org/dc/elements/1.1/created> \"2026-02-04T09:28:41.877Z\"^^<http://www.w3.org/2001/XMLSchema#dateTime> .\n\n  }\n}\n\n0\n",
    "stderr": "FeedParser: Error parsing feed - Invalid character in tag name\nLine: 0\nColumn: 525\nChar: ?\nSPARQLUpdate: HTTP request failed - Request failed with status code 400\nUpdate http://purl.org/stuff/transmissions/storeFeed \nRequest failed with status code 400\nvvvvvvvv\n^^^^^^^^\nSPARQLUpdate: HTTP request failed - Request failed with status code 400\nUpdate http://purl.org/stuff/transmissions/storeFeed \nRequest failed with status code 400\nvvvvvvvv\n^^^^^^^^\nHttpClient: Error during HTTP request - The user aborted a request.\nFeedParser: No feed content found in input field\nSPARQLUpdate: HTTP request failed - Request failed with status code 400\nUpdate http://purl.org/stuff/transmissions/storeFeed \nRequest failed with status code 400\nvvvvvvvv\n^^^^^^^^\nHttpClient: Error during HTTP request - The user aborted a request.\nFeedParser: No feed content found in input field\nSPARQLUpdate: HTTP request failed - Request failed with status code 400\nUpdate http://purl.org/stuff/transmissions/storeFeed \nRequest failed with status code 400\nvvvvvvvv\n^^^^^^^^\n",
    "tempDir": "/tmp/newsmonitor-opml-jrUSG6",
    "tempFile": "/tmp/newsmonitor-opml-jrUSG6/feeds-1770197256303.opml",
    "cleanedUp": true
  },
  {
    "id": "job-1770197441153-7vqx2i",
    "command": "src/apps/newsmonitor/subscribe-from-opml",
    "args": [
      "-m",
      "{\"sourceFile\":\"/tmp/newsmonitor-opml-wxqkMf/feeds-1770197441153.opml\"}"
    ],
    "status": "completed",
    "startedAt": "2026-02-04T09:30:41.153Z",
    "finishedAt": "2026-02-04T09:31:35.434Z",
    "exitCode": 0,
    "stdout": "[dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ› ï¸  run anywhere with `dotenvx run -- yourcommand`\n[dotenv@17.2.3] injecting env (0) from trans-apps/apps/git-apps/.env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }\n[dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  write to custom object with { processEnv: myObject }\n\u001b[38;5;142m[dotenvx@1.51.2] injecting env (0) from .env\u001b[39m\n _____                              _            _\n|_   _|--------------------------> (_) -------> (_) ---------->\n  | |_ __ __ _ _ __  ___ _ __ ___  _ ___ ___ _  ___  _ __  ___\n  | | '__/ _` | '_ \\/ __| '_ ` _ \\| / __/ __| |/ _ \\| '_ \\/ __)\n  | | | | (_| | | | \\__ \\ | | | | | \\__ \\__ \\ | (_) | | | \\__ \\\n  \\_|_|  \\__,_|_| |_|___|_| |_| |_|_|___|___|_|\\___/|_| |_|___/\n   1.0.0 (dev)                                      2026-02-04\n\nAPP PATH = /home/danny/hyperdata/transmissions/src/apps/newsmonitor/subscribe-from-opml\n*** Module path = /home/danny/hyperdata/transmissions/src/apps/newsmonitor/subscribe-from-opml\n\n+ ***** Construct Transmission :  <http://purl.org/stuff/transmissions/subscribe-from-opml>\n  > Connect #0 [loadExistingFeeds] => [readFile]\n  > Connect #1 [readFile] => [extractFeeds]\n  > Connect #2 [extractFeeds] => [processEachURL]\n  > Connect #3 [processEachURL] => [normalizeUrl]\n  > Connect #4 [normalizeUrl] => [dedupeFeedUrl]\n  > Connect #5 [dedupeFeedUrl] => [fetchFeed]\n  > Connect #6 [fetchFeed] => [parseFeed]\n  > Connect #7 [parseFeed] => [generateFeedURI]\n  > Connect #8 [generateFeedURI] => [buildFeedRDF]\n  > Connect #9 [buildFeedRDF] => [storeFeed]\n\n+ Run Transmission :  <http://purl.org/stuff/transmissions/subscribe-from-opml>\n|-> loadExistingFeeds a SPARQLSelect\n    9 query hits\n|->  [loadExistingFeeds] -> readFile a FileReader\n|->  [loadExistingFeeds.readFile] -> extractFeeds a OpmlFeedExtractor\nOpmlFeedExtractor: Found 92 feed URLs\n|->  [loadExistingFeeds.readFile.extractFeeds] -> processEachURL a ForEach\nForEach: Processing 92 items\nForEach: Progress 1/92 (1%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 5/92 (5%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 9/92 (10%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 13/92 (14%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 17/92 (18%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 21/92 (23%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 25/92 (27%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 29/92 (32%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 33/92 (36%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 37/92 (40%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 41/92 (45%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 45/92 (49%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 49/92 (53%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 53/92 (58%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 57/92 (62%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 61/92 (66%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 65/92 (71%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 69/92 (75%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 73/92 (79%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 77/92 (84%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 81/92 (88%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 85/92 (92%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 89/92 (97%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 92/92 (100%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: All 92 messages dispatched\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\nFeedUrlDeduplicator: Skipping existing feed https://simonwillison.net/atom/everything\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 48 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 59 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 100 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\nAxiosError: Request failed with status code 400\nUpdate was :\nINSERT DATA {\n  GRAPH <?g> {\n<http://hyperdata.it/feeds/ceoz43b6> a <http://rdfs.org/sioc/ns#Forum> ;\n    <http://purl.org/dc/elements/1.1/title> \"&#92;n&lt;antirez&gt;&#92;n\" ;\n    <http://purl.org/dc/elements/1.1/description> \"Description pending\" ;\n    <http://rdfs.org/sioc/ns#link> <\nhttp://antirez.com\n> ;\n    <http://rdfs.org/sioc/ns#feed_url> <http://antirez.com/rss> ;\n    \n    <http://rdfs.org/sioc/ns#num_items> 100 ;\n    <http://rdfs.org/sioc/ns#format> \"rss2.0\" ;\n    <http://purl.org/dc/elements/1.1/created> \"2026-02-04T09:30:46.761Z\"^^<http://www.w3.org/2001/XMLSchema#dateTime> .\n\n  }\n}\n\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 5 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 46 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 1 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 57 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\nAxiosError: Request failed with status code 400\nUpdate was :\nINSERT DATA {\n  GRAPH <?g> {\n<http://hyperdata.it/feeds/dxgpnfr5> a <http://rdfs.org/sioc/ns#Forum> ;\n    <http://purl.org/dc/elements/1.1/title> \"\" ;\n    \n    <http://rdfs.org/sioc/ns#link> <> ;\n    <http://rdfs.org/sioc/ns#feed_url> <https://rachelbythebay.com/w/atom.xml> ;\n    \n    <http://rdfs.org/sioc/ns#num_items>  ;\n    <http://rdfs.org/sioc/ns#format> \"\" ;\n    <http://purl.org/dc/elements/1.1/created> \"2026-02-04T09:30:53.761Z\"^^<http://www.w3.org/2001/XMLSchema#dateTime> .\n\n  }\n}\n\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 31 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 219 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 59 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 4 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 5 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 137 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 14 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 337 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 40 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 43 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 62 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 16 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 28 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 28 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 37 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 12 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 72 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\nAxiosError: Request failed with status code 400\nUpdate was :\nINSERT DATA {\n  GRAPH <?g> {\n<http://hyperdata.it/feeds/0rou3ang> a <http://rdfs.org/sioc/ns#Forum> ;\n    <http://purl.org/dc/elements/1.1/title> \"\" ;\n    \n    <http://rdfs.org/sioc/ns#link> <> ;\n    <http://rdfs.org/sioc/ns#feed_url> <https://www.tedunangst.com/flak/rss> ;\n    \n    <http://rdfs.org/sioc/ns#num_items>  ;\n    <http://rdfs.org/sioc/ns#format> \"\" ;\n    <http://purl.org/dc/elements/1.1/created> \"2026-02-04T09:31:35.011Z\"^^<http://www.w3.org/2001/XMLSchema#dateTime> .\n\n  }\n}\n\n0\n",
    "stderr": "SPARQLUpdate: HTTP request failed - Request failed with status code 400\nUpdate http://purl.org/stuff/transmissions/storeFeed \nRequest failed with status code 400\nvvvvvvvv\n^^^^^^^^\nHttpClient: Error during HTTP request - request to https://rachelbythebay.com/w/atom.xml failed, reason: \nFeedParser: No feed content found in input field\nSPARQLUpdate: HTTP request failed - Request failed with status code 400\nUpdate http://purl.org/stuff/transmissions/storeFeed \nRequest failed with status code 400\nvvvvvvvv\n^^^^^^^^\nHttpClient: Error during HTTP request - The user aborted a request.\nFeedParser: No feed content found in input field\nSPARQLUpdate: HTTP request failed - Request failed with status code 400\nUpdate http://purl.org/stuff/transmissions/storeFeed \nRequest failed with status code 400\nvvvvvvvv\n^^^^^^^^\n",
    "tempDir": "/tmp/newsmonitor-opml-wxqkMf",
    "tempFile": "/tmp/newsmonitor-opml-wxqkMf/feeds-1770197441153.opml",
    "cleanedUp": true
  },
  {
    "id": "job-1770197621816-7v8ee9",
    "command": "src/apps/newsmonitor/subscribe-from-opml",
    "args": [
      "-m",
      "{\"sourceFile\":\"/tmp/newsmonitor-opml-Xdwqz2/feeds-1770197621814.opml\"}"
    ],
    "status": "completed",
    "startedAt": "2026-02-04T09:33:41.816Z",
    "finishedAt": "2026-02-04T09:34:37.862Z",
    "exitCode": 0,
    "stdout": "[dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” encrypt with Dotenvx: https://dotenvx.com\n[dotenv@17.2.3] injecting env (0) from trans-apps/apps/git-apps/.env -- tip: ðŸ—‚ï¸ backup and recover secrets: https://dotenvx.com/ops\n[dotenv@17.2.3] injecting env (0) from .env -- tip: âœ… audit secrets and track compliance: https://dotenvx.com/ops\n\u001b[38;5;142m[dotenvx@1.51.2] injecting env (0) from .env\u001b[39m\n _____                              _            _\n|_   _|--------------------------> (_) -------> (_) ---------->\n  | |_ __ __ _ _ __  ___ _ __ ___  _ ___ ___ _  ___  _ __  ___\n  | | '__/ _` | '_ \\/ __| '_ ` _ \\| / __/ __| |/ _ \\| '_ \\/ __)\n  | | | | (_| | | | \\__ \\ | | | | | \\__ \\__ \\ | (_) | | | \\__ \\\n  \\_|_|  \\__,_|_| |_|___|_| |_| |_|_|___|___|_|\\___/|_| |_|___/\n   1.0.0 (dev)                                      2026-02-04\n\nAPP PATH = /home/danny/hyperdata/transmissions/src/apps/newsmonitor/subscribe-from-opml\n*** Module path = /home/danny/hyperdata/transmissions/src/apps/newsmonitor/subscribe-from-opml\n\n+ ***** Construct Transmission :  <http://purl.org/stuff/transmissions/subscribe-from-opml>\n  > Connect #0 [loadExistingFeeds] => [readFile]\n  > Connect #1 [readFile] => [extractFeeds]\n  > Connect #2 [extractFeeds] => [processEachURL]\n  > Connect #3 [processEachURL] => [normalizeUrl]\n  > Connect #4 [normalizeUrl] => [dedupeFeedUrl]\n  > Connect #5 [dedupeFeedUrl] => [fetchFeed]\n  > Connect #6 [fetchFeed] => [parseFeed]\n  > Connect #7 [parseFeed] => [generateFeedURI]\n  > Connect #8 [generateFeedURI] => [buildFeedRDF]\n  > Connect #9 [buildFeedRDF] => [storeFeed]\n\n+ Run Transmission :  <http://purl.org/stuff/transmissions/subscribe-from-opml>\n|-> loadExistingFeeds a SPARQLSelect\n    9 query hits\n|->  [loadExistingFeeds] -> readFile a FileReader\n|->  [loadExistingFeeds.readFile] -> extractFeeds a OpmlFeedExtractor\nOpmlFeedExtractor: Found 92 feed URLs\n|->  [loadExistingFeeds.readFile.extractFeeds] -> processEachURL a ForEach\nForEach: Processing 92 items\nForEach: Progress 1/92 (1%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 5/92 (5%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 9/92 (10%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 13/92 (14%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 17/92 (18%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 21/92 (23%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 25/92 (27%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 29/92 (32%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 33/92 (36%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 37/92 (40%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 41/92 (45%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 45/92 (49%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 49/92 (53%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 53/92 (58%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 57/92 (62%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 61/92 (66%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 65/92 (71%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 69/92 (75%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 73/92 (79%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 77/92 (84%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 81/92 (88%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 85/92 (92%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 89/92 (97%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: Progress 92/92 (100%)\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\nForEach: All 92 messages dispatched\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL] -> normalizeUrl a URLNormalizer\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\nFeedUrlDeduplicator: Skipping existing feed https://simonwillison.net/atom/everything\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl] -> dedupeFeedUrl a FeedUrlDeduplicator\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl] -> fetchFeed a HttpClient\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 48 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 59 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 100 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\nAxiosError: Request failed with status code 400\nUpdate was :\nINSERT DATA {\n  GRAPH <http://hyperdata.it/feeds> {\n<http://hyperdata.it/feeds/j860epy7> a <http://rdfs.org/sioc/ns#Forum> ;\n    <http://purl.org/dc/elements/1.1/title> \"&#92;n&lt;antirez&gt;&#92;n\" ;\n    <http://purl.org/dc/elements/1.1/description> \"Description pending\" ;\n    <http://rdfs.org/sioc/ns#link> <\nhttp://antirez.com\n> ;\n    <http://rdfs.org/sioc/ns#feed_url> <http://antirez.com/rss> ;\n    \n    <http://rdfs.org/sioc/ns#num_items> 100 ;\n    <http://rdfs.org/sioc/ns#format> \"rss2.0\" ;\n    <http://purl.org/dc/elements/1.1/created> \"2026-02-04T09:33:46.736Z\"^^<http://www.w3.org/2001/XMLSchema#dateTime> .\n\n  }\n}\n\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 5 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 46 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 1 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 57 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 31 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 30 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 219 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 59 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 4 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 5 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 137 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 14 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 337 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 40 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 43 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 62 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 16 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 8 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 28 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 28 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 37 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 20 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 12 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 11 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 15 entries from atom feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 72 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 25 entries from unknown feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\nFeedParser: Parsed 10 entries from rss2.0 feed\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed] -> parseFeed a FeedParser\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed] -> generateFeedURI a ResourceMinter\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI] -> buildFeedRDF a RDFBuilder\n|->  [loadExistingFeeds.readFile.extractFeeds.processEachURL.normalizeUrl.dedupeFeedUrl.fetchFeed.parseFeed.generateFeedURI.buildFeedRDF] -> storeFeed a SPARQLUpdate\n0\n",
    "stderr": "SPARQLUpdate: HTTP request failed - Request failed with status code 400\nUpdate http://purl.org/stuff/transmissions/storeFeed \nRequest failed with status code 400\nvvvvvvvv\n^^^^^^^^\nHttpClient: Error during HTTP request - request to https://rachelbythebay.com/w/atom.xml failed, reason: \nFeedParser: No feed content found in input field\nHttpClient: Error during HTTP request - The user aborted a request.\nFeedParser: No feed content found in input field\n",
    "tempDir": "/tmp/newsmonitor-opml-Xdwqz2",
    "tempFile": "/tmp/newsmonitor-opml-Xdwqz2/feeds-1770197621814.opml",
    "cleanedUp": true
  }
]