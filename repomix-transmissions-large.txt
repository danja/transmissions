This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-09T15:47:07.233Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
- Code comments have been removed.

Additional Info:
----------------
User Provided Header:
-----------------------
Transmissions source code

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Directory Structure
================================================================
output/
  output-01.md
src/
  api/
    cli/
      about.md
      run.js
    common/
      CommandUtils.js
    http/
      WebRunner.js
    about.md
  applications/
    app-template/
      about.md
      config.ttl
      transmissions.ttl
    claude-json-converter/
      data/
        input/
          input-01.json
          users.json
      about.md
      config.ttl
      transmissions copy.ttl
      transmissions.ttl
    file-pipeline/
      config.ttl
      transmissions.ttl
    globbo/
      about.md
      config.ttl
      transmissions.ttl
    html-to-md/
      about.md
      config.ttl
      transmissions.ttl
    link-lister/
      about.md
      config.ttl
      transmissions.ttl
    packer/
      about.md
      config.ttl
      transmissions.ttl
    postcraft/
      about.md
      config.ttl
      transmissions.ttl
    postcraft-clear-cache/
      about.md
      config.ttl
      transmissions.ttl
    postcraft-previous/
      about.md
      config.ttl
      transmissions.ttl
    postcraft-render1/
      data/
        cache/
          2023-10-27_hello.md
          2025-01-08_hello-again.md
      about.md
      config.ttl
      transmissions.ttl
    postcraft-render2/
      data/
        cache/
          2023-10-27_hello.md
          2025-01-08_hello-again.md
      about.md
      config.ttl
      transmissions.ttl
    selfie/
      about.md
      config.ttl
      transmissions.ttl
    string-pipeline/
      config.ttl
      transmissions.ttl
    test_blanker/
      data/
        input/
          input-01.json
        output/
          output-01.json
          required-01.json
      about.md
      config.ttl
      transmissions.ttl
    test_configmap/
      data/
        input/
          input-01.md
        output/
          output-01.md
          required-01.md
      about.md
      config.ttl
      manifest.ttl
      simple.js
      test-config.json
      transmissions.ttl
    test_dirwalker/
      about.md
      config.ttl
      transmissions.ttl
    test_env-loader/
      about.md
      config.ttl
      transmissions.ttl
    test_file-copy-remove/
      data/
        several-full/
          one.txt
          two.txt
        single-full/
          one.txt
        start/
          one.txt
          two.txt
      about.md
      config.ttl
      init.sh
      transmissions.ttl
    test_filename-mapper/
      data/
        input/
          input-01.txt
        output/
          required-01.txt
      about.md
      config.ttl
      filename-mapper-simple.js
      transmissions.ttl
    test_foreach/
      about.md
      transmissions.ttl
    test_fork/
      about.md
      config.ttl
      transmissions.ttl
    test_fork-unfork/
      about.md
      config.ttl
      transmissions.ttl
    test_fs-rw/
      data/
        input/
          input-01.md
        output/
          required-01.md
      about.md
      config.ttl
      simple.js
      test-config.json
      transmissions.ttl
    test_http-server/
      data/
        input/
          index.html
          metrics.js
      about.md
      config.ttl
      shutdown-client-auth.js
      test-shutdown.js
      transmissions.ttl
    test_multi-pipe/
      config.ttl
      transmissions.ttl
    test_nop/
      about.md
      config.ttl
      transmissions.ttl
    test_ping/
      config.ttl
      transmissions.ttl
    test_restructure/
      data/
        input/
          input-01.json
        output/
          required-01.json
      about.md
      config.ttl
      simple.js
      transmissions.ttl
    test_runcommand/
      data/
        output/
          output-01.txt
          required-01.txt
      about.md
      config.ttl
      simple.js
      test-config.json
      transmissions.ttl
    test_two-transmissions/
      config.ttl
      transmissions.ttl
  core/
    ApplicationManager.js
    Director.js
    ModuleLoader.js
    ModuleLoaderFactory.js
    Procurer.js
    TransmissionBuilder.js
    WorkerPool.js
  engine/
    Application.js
    Connector.js
    Transmission.js
  processors/
    base/
      AbstractProcessorFactory.js
      Processor.js
    flow/
      DeadEnd.js
      FlowProcessorsFactory.js
      ForEach.js
      Fork.js
      Halt.js
      NOP.js
      Ping.js
      Unfork.js
    fs/
      DirWalker.js
      FileCopy.js
      FilenameMapper.js
      FileReader.js
      FileRemove.js
      FileWriter.js
      FsProcessorsFactory.js
    github/
      GitHubList_no-pag.js
      GitHubList.js
      GitHubProcessorsFactory.js
    http/
      services/
        MetricsService.js
        ShutdownService.js
      HttpClient.js
      HttpProcessorsFactory.js
      HttpProxy.js
      HttpServer.js
      HttpServerWorker.js
    json/
      Blanker.js
      JSONProcessorsFactory.js
      JsonRestructurer.js
      JSONWalker.js
      Restructure.js
      ValueConcat.js
    markup/
      LinkFinder.js
      MarkdownToHTML.js
      MarkupProcessorsFactory.js
      MetadataExtractor.js
    mcp/
      McpClient.js
      McpProcessorsFactory.js
      McpServer.js
    packer/
      CommentStripper.js
      FileContainer.js
      PackerProcessorsFactory.js
    postcraft/
      AtomFeedPrep.js
      EntryContentToPagePrep.js
      FrontPagePrep.js
      PostcraftDispatcher.js
      PostcraftPrep.js
      PostcraftProcessorsFactory.js
    protocols/
      HttpGet.js
      ProtocolsProcessorsFactory.js
    rdf/
      ConfigMap.js
      DatasetReader.js
      RDFConfig.js
      RDFProcessorsFactory.js
    staging/
      MarkdownFormatter.js
      StagingProcessorsFactory.js
      TurtleFormatter.js
    system/
      EnvLoader.js
      SystemProcessorsFactory.js
    templates/
      TemplateProcessor.js
      TemplateProcessorsFactory.js
    test/
      AppendProcess.js
      FileSink.js
      FileSource.js
      StringSink.js
      StringSource.js
      TestProcessorsFactory.js
    text/
      LineReader.js
      StringFilter.js
      StringMerger.js
      StringReplace.js
      Templater.js
      TextProcessorsFactory.js
    unsafe/
      RunCommand.js
      UnsafeProcessorsFactory.js
    util/
      CaptureAll.js
      SetMessage.js
      ShowConfig.js
      ShowMessage.js
      ShowTransmission.js
      Stash.js
      UtilProcessorsFactory.js
      WhiteboardToMessage.js
    xmpp/
      XmppClient.js
      XmppProcessorsFactory.js
    about.md
  simples/
    env-loader/
      about.md
      env-loader.js
    nop/
      nop.js
      simple-runner.js
    set-message/
      set-message.js
  utils/
    footpath.js
    GrapoiHelpers.js
    Logger.js
    ns.js
    t2j.js
    test_runner.js
staging/
  schema-documentation.md
  template-cli.js
  template-generator.js
  template-tool-docs.md
  transmissions-prompt-template.md
  transmissions-template-schema.json
  transmissions-template-turtle.txt
  transmissions-testing-template.md
test-failures/
  test_env-loader/
    2024-11-28T17-44-11.419Z/
      test-output.json
    2024-11-28T17-46-20.677Z/
      test-output.json
    2024-11-28T18-31-38.300Z/
      test-output.json
    2024-11-28T18-34-16.177Z/
      test-output.json
  test_http-server/
    2024-11-30T12-30-16.673Z/
      test-output.json
tests/
  examples/
    test-data-usage.js
  helpers/
    file-test-helper.js
    reporter.js
    test-data-generator.js
  integration/
    configmap.spec.js
    filename-mapper.spec.js
    fork.spec.js
    fs-rw_simple.spec.js
    fs-rw.spec.js
    http-server.spec.js
    markmap.spec.js
    restructure_simple.spec.js
    restructure.spec.js
    run-command.spec.js
    test_apps.spec.js
  support/
    jasmine-browser.json
  unit/
    filename-mapper.spec.js
    http-server_MetricsService.spec.js
    http-server_ShutdownService.spec.js
    markmap.spec..js
    NOP.spec.js
    PostcraftPrep.spec.js
    RunCommand.spec.js
    StringFilter.spec.js
    StringReplace.spec.js
    updated-shutdown-test.js
  about.md
types/
  grapoi.d.ts
.babelrc
.gitignore
jasmine.json
jsconfig.json
jsdoc.json
LICENSE
package.json
postcss.config.js
README.md
trans
users.json
webpack.config.js

================================================================
Files
================================================================

================
File: output/output-01.md
================
Hello!

================
File: src/api/cli/about.md
================
# About : CLI

`src/api/cli/*`

The CLI entry point `./trans` calls `src/api/cli/run.js` which uses [yargs](https://yargs.js.org/) - _tee hee_, they say it best :

> Yargs be a node.js library fer hearties tryin' ter parse optstrings.

`src/api/cli/run.js` then calls `src/api/common/CommandUtils.js`. That does a little bit of path-splitting and simple logic, calling on `src/core/ApplicationManager.js` to get things going.

================
File: src/api/cli/run.js
================
import yargs from 'yargs'
import { hideBin } from 'yargs/helpers'
import CommandUtils from '../common/CommandUtils.js'
import WebRunner from '../http/WebRunner.js'
import chalk from 'chalk'

const defaultApplicationsDir = 'src/applications'
const commandUtils = new CommandUtils(defaultApplicationsDir)

import { readFileSync } from 'fs';
import { dirname, join } from 'path';
import { fileURLToPath } from 'url';

const __dirname = dirname(fileURLToPath(import.meta.url));
const packageJson = JSON.parse(readFileSync(join(__dirname, '../../../package.json')));
const buildInfo = process.env.BUILD_INFO || 'dev';
const version = `${packageJson.version} (${buildInfo})`;
const banner = `
  _____
 |_   _| __ __ _ _ __  ___
   | || '__/ _\` | '_ \\/ __|
   | || | | (_| | | | \\__ \\
   |_||_|  \\__,_|_| |_|___/
             ${version.padStart(10).padEnd(20)}
         ${new Date().toISOString().split('T')[0]}
`;

async function main() {
    console.log(chalk.cyan(banner))
    const yargsInstance = yargs(hideBin(process.argv))
        .usage(chalk.cyan('Usage: ./trans [application][.subtask] [options] [target]\n  Run without arguments to list available applications.'))
        .option('verbose', {
            alias: 'v',
            describe: chalk.yellow('Enable verbose output'),
            type: 'boolean'
        })
        .option('silent', {
            alias: 's',
            describe: chalk.yellow('Suppress all output'),
            type: 'boolean'
        })
        .option('message', {
            alias: 'm',
            describe: chalk.yellow('Input message as JSON'),
            type: 'string',
            coerce: JSON.parse
        })
        .option('web', {
            alias: 'w',
            describe: chalk.yellow('Start web interface'),
            type: 'boolean',
        })
        .option('port', {
            alias: 'p',
            describe: chalk.yellow('Port for web interface'),
            type: 'number',
            default: 3000
        })
        .command('$0 [application] [target]', chalk.green('runs the specified application\n\nExample: ./trans process.convert -m \'{"text": "hello"}\'\n'), (yargs) => {
            return yargs
                .positional('application', {
                    describe: chalk.yellow('the application to run')
                })
                .positional('target', {
                    describe: chalk.yellow('the target of the application')
                })
        }, async (argv) => {
            if (argv.web) {
                const webRunner = new WebRunner(applicationsDir, argv.port)
                webRunner.start()
                return
            }

            if (!argv.application) {
                console.log(chalk.cyan('Available applications:'))
                const apps = await commandUtils.listApplications()
                console.log(chalk.green(apps.join('\n')))
                yargsInstance.showHelp()
                return
            }

            await commandUtils.begin(argv.application, argv.target, argv.message, argv.verbose)
        })
        .help('h')
        .alias('h', 'help')

    await yargsInstance.argv
}

main().catch(console.error)

================
File: src/api/common/CommandUtils.js
================
import path from 'path'
import fs from 'fs/promises'
import logger from '../../utils/Logger.js'

import ApplicationManager from '../../core/ApplicationManager.js'

class CommandUtils {

    #appManager

    constructor() {
        this.#appManager = new ApplicationManager();
    }

    async begin(application, target, message = {}, verbose, silent) {

        var debugLevel = verbose ? "debug" : "info"
        logger.setLogLevel(debugLevel)

        logger.debug('\nCommandUtils.begin()')
        logger.debug('CommandUtils.begin, process.cwd() = ' + process.cwd())
        logger.debug('CommandUtils.begin, debugLevel = ' + debugLevel)
        logger.debug('CommandUtils.begin, application = ' + application)
        logger.debug('CommandUtils.begin, target = ' + target)
        logger.debug(`CommandUtils.begin, message = ${message}`)


        if (target && !target.startsWith('/')) {
            target = path.join(process.cwd(), target)
        }

        var { appName, appPath, subtask } = CommandUtils.splitName(application)


        logger.debug(`\n
    after split :
    appName = ${appName}
    appPath = ${appPath}
    subtask = ${subtask}
    target = ${target}`)



        await this.#appManager.initialize(appName, appPath, subtask, target)

        return await this.#appManager.start(message)
    }

    static splitName(fullPath) {
        logger.debug(`\nCommandUtils.splitName, fullPath  = ${fullPath}`)
        const parts = fullPath.split(path.sep)
        logger.debug(`\nCommandUtils.splitName, parts  = ${parts}`)
        var lastPart = parts[parts.length - 1]

        var task = false
        if (lastPart.includes('.')) {
            const split = lastPart.split('.')
            task = split[1]
            lastPart = split[0]
        }
        var appPath = parts.slice(0, parts.length - 1).join(path.sep)
        appPath = path.join(appPath, lastPart)



        logger.debug(`CommandUtils.splitName, appName:${lastPart}, appPath:${appPath}, task:${task},`)

        return { appName: lastPart, appPath: appPath, task: task }
    }

    async listApplications() {
        return await this.#appManager.listApplications()
    }


    static async parseOrLoadContext(contextArg) {
        logger.debug(`CommandUtils.parseOrLoadContext(), contextArg = ${contextArg}`)
        let message = {}
        try {
            message.payload = JSON.parse(contextArg)
        } catch (err) {
            logger.debug('*** Loading JSON from file...')
            const filePath = path.resolve(contextArg)
            const fileContent = await fs.readFile(filePath, 'utf8')
            message.payload = JSON.parse(fileContent)
        }
        return message
    }
}

export default CommandUtils

================
File: src/api/http/WebRunner.js
================
import express from 'express'

import ApplicationManager from '../../core/ApplicationManager.js'
import logger from '../../utils/Logger.js'

class WebRunner {
    constructor(appsDir, port = 7247) {
        this.appManager = new ApplicationManager(appsDir)

        this.app = express()
        this.port = port

        this.setupRoutes()
    }

    setupRoutes() {
        this.app.use(express.json())

        this.app.get('/applications', async (req, res) => {
            const apps = await this.appManager.listApplications()
            res.json(apps)
        })

        this.app.post('/run/:application', async (req, res) => {
            const { application } = req.params
            const { target, message } = req.body

            try {


                await this.appManager.initialize()

                const result = await this.appManager.run({
                    ...config,
                    message,
                    target
                })

                res.json(result)
            } catch (error) {
                logger.error('Error running application:', error)
                res.status(500).json({
                    success: false,
                    error: error.message
                })
            }
        })
    }

    start() {
        this.app.listen(this.port, () => {
            logger.log(`Web interface running on port ${this.port}`)
        })
    }
}

export default WebRunner

================
File: src/api/about.md
================
# transmissions/src/api/

Interfaces for running transmissions.

================
File: src/applications/app-template/about.md
================
# App Template

## Runner

```sh
cd ~/github-danny/transmissions # my local path
./trans app-template
```

## Description

---

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/app-template/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:setDemo a trm:ServiceConfig ;
    trm:setValue (t:sv0)  . # consider using blank nodes
    t:sv0   trm:key    "demo" ;
            trm:value    "a test value"  .

================
File: src/applications/app-template/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:mini a trm:Transmission ;
    trm:pipe (:p10 :p20) .

:p10 a :SetMessage ;
     trm:configKey :setDemo .

:p20 a :ShowMessage .

================
File: src/applications/claude-json-converter/data/input/input-01.json
================
[
    {
        "uuid": "conversation 1 uuid",
        "name": "conversation 1 name",
        "created_at": "",
        "updated_at": "",
        "account": {
            "uuid": "account.uuid"
        },
        "chat_messages": [
            {
                "uuid": "c1 message 1 uuid",
                "text": "",
                "content": [
                    {
                        "type": "c1 message 1 type",
                        "text": "c1 message 1 text"
                    }
                ],
                "sender": "",
                "created_at": "",
                "updated_at": "",
                "attachments": [],
                "files": [
                    {
                        "file_name": ""
                    }
                ]
            },
            {
                "uuid": "c1 message 2 uuid",
                "text": "",
                "content": [
                    {
                        "type": "c1 message 2 type",
                        "text": "c1 message 2 text"
                    }
                ],
                "sender": "",
                "created_at": "",
                "updated_at": "",
                "attachments": [],
                "files": [
                    {
                        "file_name": ""
                    }
                ]
            }
        ]
    },
    {
        "uuid": "conversation 2 uuid",
        "name": "conversation 2 name",
        "created_at": "",
        "updated_at": "",
        "account": {
            "uuid": "account.uuid"
        },
        "chat_messages": [
            {
                "uuid": "c2 message 1 uuid",
                "text": "",
                "content": [
                    {
                        "type": "c2 message 1 type",
                        "text": "c2 message 1 text"
                    }
                ],
                "sender": "",
                "created_at": "",
                "updated_at": "",
                "attachments": [],
                "files": [
                    {
                        "file_name": ""
                    }
                ]
            },
            {
                "uuid": "c2 message 2 uuid",
                "text": "",
                "content": [
                    {
                        "type": "c2 message 2 type",
                        "text": "c2 message 2 text"
                    }
                ],
                "sender": "",
                "created_at": "",
                "updated_at": "",
                "attachments": [],
                "files": [
                    {
                        "file_name": ""
                    }
                ]
            }
        ]
    }
]

================
File: src/applications/claude-json-converter/data/input/users.json
================
[{"uuid": "dc67aa7d-f71f-4232-afb3-7f2688ac68f7", "full_name": "Danny Ayers", "email_address": "danny.ayers@gmail.com", "verified_phone_number": null}]

================
File: src/applications/claude-json-converter/about.md
================
```sh
cd ~/github-danny/transmissions/
./trans claude-json-converter

# -P src/applications/claude-json-converter/data/input/conversations.json
```

After `FileReader` (and `Blanker`):

```
{
    // system message bits,

    "content": [
        {
            "uuid": "",
            "name": "",
            "created_at": "",
            "updated_at": "",
            "account": {
                "uuid": ""
            },
            "chat_messages": [
                {
                    "uuid": "",
                    "text": "",
                    "content": [
                        {
                            "type": "",
                            "text": ""
                        }
                    ],
                    "sender": "",
                    "created_at": "",
                    "updated_at": "",
                    "attachments": [],
                    "files": [
                        {
                            "file_name": ""
                        }
                    ]
                },
                {
                    ...
                }
            ]
        }
}
```

`JSONWalker` fires off a message per-conversation.

These need `Restructure` to split off the common metadata as `message.content`, and move `chat_messages` to `message.content`, ready for -

`JSONWalker` fires off a message per-conversation.

================
File: src/applications/claude-json-converter/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

##################### only for testing
t:bContent a trm:ServiceConfig ;
    rdfs:label "Root node in JSON object for blanker" ;
    trm:configKey t:blankContent  ;
    trm:pointer "content"  .
    trm:preserve "content.payload.test.third" .

t:setDump a trm:ServiceConfig ;
    trm:setValue (t:sv0)  . # consider using blank nodes
    t:sv0   trm:key    "dump" ;
            trm:value  "true"  .
#########################################################################

# should really be in a manifest.ttl
t:ReadFile a trm:ServiceConfig ;
    rdfs:label "Read file" ;
    trm:configKey t:readFile ;
    trm:sourceFile "input/conversations.json" ;
  #  trm:sourceFile "input/input-01.json" ;
    trm:mediaType "application/json" .


t:ConversationsWalker a trm:ServiceConfig ;
    trm:key t:conversationsConfig ;
    trm:pointer "content" .


t:retreeConvs a trm:ServiceConfig ;
    trm:rename (t:pp100 t:pp101 t:pp102  t:pp103) .
    t:pp100     trm:pre     "content.uuid" ;
                trm:post    "meta.conv_uuid"  .
    t:pp101     trm:pre     "content.name" ;
                trm:post    "meta.conv_name"  .
    t:pp102     trm:pre     "content.updated_at" ;
                trm:post    "meta.updated_at"  .
    t:pp103     trm:pre     "content.chat_messages" ;
                trm:post    "content"  .

t:MessagesWalker a trm:ServiceConfig ;
    trm:key t:messagesConfig ;
    trm:pointer "content" .

# unused
t:retreeMsgs a trm:ServiceConfig ;
    trm:rename (t:pp200 t:pp201 t:pp202) .

    t:pp200     trm:pre     "content.item.chat_messages" ;
                trm:post    "channel"  .

    t:pp201     trm:pre     "content.item.uuid" ;
                trm:post    "filename"  .

    t:pp202     trm:pre     "content.item.name" ;
                trm:post    "title"  .

#      filepath = this.getPropertyFromMyConfig(ns.trm.destinationFile)

# unused
t:Writer a trm:ServiceConfig ;
    trm:key t:writer ;
    trm:destinationFile "DESTINATION" .

================
File: src/applications/claude-json-converter/transmissions copy.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one

# testing only - FileWriter will save message
:cb a trm:Transmission ;
     rdfs:label "cb" ;
     rdfs:comment "Claude blanker" ;
     trm:pipe (:cb10 :cb20 :cb30) .

:cb10 a :SetMessage ;
     trm:configKey :setDump .

:cb20 a :FileWriter .

:cb30 a :Blanker ; # clear values
     trm:configKey :blankContent .

# :UF :SD :FW :DE
####################################

:nop a trm:Transmission ;
    rdfs:label "nop" ;
    rdfs:comment "NOP for testing" ;
trm:pipe (:n10) .

:n10 a :NOP .

####### The thing

:ccc a trm:Transmission ;
    rdfs:label "ccc" ;
    rdfs:comment "Claude conversations.json converter" ;
     trm:pipe (:ccc10 :ccc20 :ccc30 :ccc40 :ccc50  :ccc60) .

# Start

:ccc10 a :FileReader ; # Claude conversations.json
       trm:configKey :readFile .

# Separates into conversations
:ccc20 a :JSONWalker ;
     trm:configKey :conversationsConfig .


:ccc30 a :Restructure ;
     trm:configKey :retreeConvs .

# Separates into messages
:ccc40 a :JSONWalker ;
     trm:configKey :messagesConfig .

#:p50 a :Restructure ;
 #    trm:configKey :retreeMsgs .

:ccc50 a :MarkdownFormatter .

:ccc60 a :FileWriter .

================
File: src/applications/claude-json-converter/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one

####  testing only
:nop a trm:Transmission ;
    rdfs:label "nop" ;
    rdfs:comment "NOP for testing" ;
trm:pipe (:n10) .

:n10 a :NOP .

# testing only - FileWriter will save message
:cb a trm:Transmission ;
     rdfs:label "cb" ;
     rdfs:comment "Claude blanker" ;
     trm:pipe (:ccc10   :cb10 :cb20 :cb30) .

:cb10 a :SetMessage ;
     trm:configKey :setDump .

:cb20 a :FileWriter .

:cb30 a :Blanker ; # clear values
     trm:configKey :blankContent .

#####################################
####### The thing

:ccc a trm:Transmission ;
     rdfs:label "ccc" ;
     rdfs:comment "Claude conversations.json converter" ;
     trm:pipe (:ccc10 :ccc20 :ccc30 :ccc40 :ccc50  :ccc60) .

# Start

:ccc10 a :FileReader ; # Claude conversations.json
       trm:configKey :readFile .

# Separates into conversations
:ccc20 a :JSONWalker ;
     trm:configKey :conversationsConfig .


:ccc30 a :Restructure ;
     trm:configKey :retreeConvs .

# Separates into messages
:ccc40 a :JSONWalker ;
     trm:configKey :messagesConfig .

#:p50 a :Restructure ;
 #    trm:configKey :retreeMsgs .

:ccc50 a :MarkdownFormatter .

:ccc60 a :FileWriter .

================
File: src/applications/file-pipeline/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:FilePipelineMap a trm:DataMap ;
    trm:sourceFile "input.txt" ;
    trm:destinationFile "output.txt" .

================
File: src/applications/file-pipeline/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:file_pipeline a trm:Transmission ;
    trm:pipe (:s1 :s2 :s3 :s4) .

:s1 a :FileSource .
:s2 a :AppendProcess .
:s3 a :AppendProcess .
:s4 a :FileSink .

================
File: src/applications/globbo/about.md
================
# Globbo

```
./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}'
```

there are more notes under

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-application-walkthrough.md

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-service-walkthrough.md

## Description

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/globbo/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/globbo/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:globbo a trm:Transmission ;
    trm:pipe (:s10 :s20 :s30 :s40 :SM) .

#:s40 a :Restructure ;
 #   trm:configKey :walkPrep .

:s10 a :DirWalker .
:s20 a :CaptureAll . # pushes all messages into config.whiteboard
:s30 a :Unfork .
:s40 a :WhiteboardToMessage .

================
File: src/applications/html-to-md/about.md
================
# HTML to Markdown

*a minimal application (that I need) which can also serve as an example in documentation*

```
./run html-to-md -c '{"rootDir": "./test-data/html-to-md", "filename":"webidl.html"}'
```

## Description

---

there are more notes under

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-application-walkthrough.md

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-service-walkthrough.md

## Description

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/html-to-md/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/html-to-md/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:h2m a trm:Transmission ;
    trm:pipe (:s10 :s20 :s30 :s40 :SM) .

#:s40 a :Restructure ;
 #   trm:configKey :walkPrep .

:s10 a :FileReader .
:s20 a :CaptureAll . # pushes all messages into config.whiteboard
:s30 a :Unfork .
:s40 a :WhiteboardToMessage .

================
File: src/applications/link-lister/about.md
================
run.js had

const here = import.meta.url
const message = { runScript: here }

transmission.process('', message)

================
File: src/applications/link-lister/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:linklister trm:hasDataMap t:llSourceMap .
t:linklister trm:hasDataMap t:llGotMap .
t:linklister trm:hasDataMap t:llLinkMap .

t:llSourceMap a trm:DataMap ;
    trm:key t:sourceFile ;
    trm:value "starter-links.md" .

t:llGotMap a trm:DataMap ;
    trm:key t:gotFile ;
    trm:value "got.html" .

t:llLinkMap a trm:DataMap ;
    trm:key t:linkFile ;
    trm:value "links.md" .

t:htmlMap a trm:DataMap ;
    trm:key t:htmlFile ;
    trm:value "links.html" .

================
File: src/applications/link-lister/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:linklister a trm:Transmission ;
    trm:pipe (:s1 :s2 :s3 :s4 :s5 :s6 :s7 :s8) .

:s1 a :FileReader ;
    trm:configKey :sourceFile .
    
:s2 a :LineReader .
:s3 a :HttpGet .

:s4 a :LinkFinder .

:s5 a :StringMerger .

:s6 a :FileWriter ;
    trm:configKey :linkFile .
# :s6 a :NOP .
:s7 a :MarkdownToHTML .

:s8 a :FileWriter ;
    trm:configKey :htmlFile .

#:s8 a :StringFilter .
#:s9 a :StringMerger .
#:s10 a :FileWriter 
#        trm:configKey :linkFile .
# :s4 a :NOP .
# :s4 a :FileWriter ;
#     trm:configKey :gotFile .

# :s5 a :NOP .

================
File: src/applications/packer/about.md
================
# Packer Application

_repopack/repomix equiv_

```sh
./trans packer path/to/repo

./trans packer ./

./trans packer
```

================
File: src/applications/packer/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:readConfig a trm:ServiceConfig ;
    rdfs:label "Read file" ;
    trm:configKey t:readConfig ;
    trm:mediaType "text/plain" .

t:filterConfig a trm:ServiceConfig ;
    rdfs:label "File filter config" ;
    trm:configKey t:filterConfig ;
    trm:include "*.txt,*.md,*.js,*.jsx,*.ts,*.tsx,*.json,*.html,*.css" ;
    trm:exclude "node_modules/*,dist/*,build/*,.git/*,.DS_Store,.env,*.log,coverage/*,tmp/*,*.min.js,*.map,.cache/*,__pycache__/*,*.pyc,*.pyo,*.pyd,test-failures/*,yarn.lock,package-lock.json" .

t:containerConfig a trm:ServiceConfig ;
    trm:configKey t:containerConfig ;
    trm:destination "repomix.json" .

t:writeConfig a trm:ServiceConfig ;
    rdfs:label "Write file" ;
    trm:configKey t:writeConfig ;
    trm:destinationFile "repomix.json" .

================
File: src/applications/packer/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################
:packer a trm:Transmission ;
    rdfs:label "Repository Packer" ;
    trm:pipe (:p10 :p20 :p30 :p40 :p50 :p60 :p70 :SM :p80) .

:p10 a :DirWalker .

:p20 a :StringFilter ;
    trm:configKey :filterConfig .

:p30 a :FileReader ;
    trm:configKey :readConfig .

:p40 a :FileContainer ;
    trm:configKey :containerConfig .

:p50 a :CaptureAll .

:p60 a :WhiteboardToMessage .

:p70 a :Unfork .

:p80 a :FileWriter ;
    trm:configKey :writeConfig .

================
File: src/applications/postcraft/about.md
================
```sh
cd ~/github-danny/transmissions

./trans postcraft ../postcraft/danny.ayers.name
```

================
File: src/applications/postcraft/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances
@prefix pc: <http://purl.org/stuff/postcraft/> .

### ConfigMap
t:PostcraftMap a trm:ServiceConfig ;
    trm:key t:postcraftMap ;
    trm:group pc:ContentGroup .

### clean

t:cacheRemove a trm:ServiceConfig ;
    trm:key t:removeCache ;
    trm:target "cache/" .

t:articlesRemove a trm:ServiceConfig ;
    trm:key t:removeArticles ;
    trm:target "public/home/articles" .

t:entriesRemove a trm:ServiceConfig ;
    trm:key t:removeEntries ;
    trm:target "public/home/entries" .

t:journalRemove a trm:ServiceConfig ;
    trm:key t:removeJournal ;
    trm:target "public/home/journal" .

t:todoRemove a trm:ServiceConfig ;
    trm:key t:removeTodo ;
    trm:target "public/home/todo" .

t:indexRemove a trm:ServiceConfig ;
    trm:key t:removeIndex ;
    trm:target "public/home/index.html" .

### copy #####################################

t:copyStatic a trm:ServiceConfig ;
    trm:key t:staticCopy ;
    trm:source "content-static" ;
    trm:destination "public/home/static" .
    # trm:destination "../../danny.ayers.name/static" .

t:copyMedia a trm:ServiceConfig ;
    trm:key t:mediaCopy ;
    trm:source "media" ;
    trm:destination "public/home/media" .

t:copyCSS a trm:ServiceConfig ;
    trm:key t:cssCopy ;
    trm:source "layouts/middlin/css" ;
    trm:destination "public/home/css" .

t:copyFonts a trm:ServiceConfig ;
    trm:key t:fontsCopy ;
    trm:source "layouts/middlin/fonts" ;
    trm:destination "public/home/fonts" .

t:copyJS a trm:ServiceConfig ;
    trm:key t:jsCopy ;
    trm:source "layouts/middlin/js" ;
    trm:destination "public/home/js" .

### render ##################################

# t:atomTemplate a trm:ServiceConfig ;
#    trm:templateFile "layouts/middlin/templates/atom_template.njk" .

trm:Describe  a trm:ServiceConfig ;
 trm:key trm:describe .

t:phaseOne a trm:ServiceConfig ;
    trm:key t:markdownToRawPosts ;
    trm:marker "Phase One" .

# TODO IS COPY, not rename!!
t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "contentGroup.PostContent.sourceDir" ;
            trm:post    "sourceDir" .
  #  t:pp2   trm:pre     "filename" ;
   #         trm:post    "filename"  .

t:entryRawPrep a trm:ReMap ;
    trm:rename (t:er1 t:er2 t:er3) .
   t:er1   trm:pre     "targetFilename" ;
            trm:post    "filepath" .
    t:er2   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .
    t:er3   trm:pre     "contentGroup.PostContent.templateFile" ;
            trm:post    "templateFilename" .

t:entryPagePrep a trm:ReMap ;
    trm:rename (t:ppp1) .
    t:ppp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

================
File: src/applications/postcraft/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s:
@prefix pc: <http://purl.org/stuff/postcraft/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################


## POSTCRAFT.CLEAN ##########################################

:clean a trm:Transmission ;
    rdfs:label "clean" ;
    rdfs:comment "directory cleaner" ;
    trm:pipe (:r10 :r20 :r30 :r40 :r50 :r60) .

:r10 a :FileRemove ;
    trm:configKey :removeCache .

:r20 a :FileRemove ;
    trm:configKey  :removeArticles .

:r30 a :FileRemove ;
    trm:configKey  :removeJournal .

:r40 a :FileRemove ;
    trm:configKey  :removeEntries .

:r50 a :FileRemove ;
    trm:configKey  :removeTodo .

:r60 a :FileRemove ;
    trm:configKey  :removeIndex .

## POSTCRAFT.COPY ##################################################################

#:copy a trm:Transmission ;
 #   rdfs:label "copy" ;
  #  rdfs:comment "dir/file copier" ;
   # trm:pipe (:cp10 :cp20 :cp30 :cp40 :cp50) .

### static dirs
:cp10 a :FileCopy ;
    trm:configKey :staticCopy .

### media dirs
:cp20 a :FileCopy ;
    trm:configKey :mediaCopy .

### layout dirs
:cp30 a :FileCopy ;
    trm:configKey :cssCopy .

:cp40 a :FileCopy ;
    trm:configKey :jsCopy .

:cp50 a :FileCopy ;
    trm:configKey :fontsCopy .

#####################

## POSTCRAFT.RENDER ###############################################################

:render a trm:Transmission ;
    rdfs:label "render" ;
    rdfs:comment "render the pages" ;
                 trm:pipe (:s20 :s30 :s40  :s50 :s60 :s70   :s80 :s90  :s100
              :s110  :s120  :s130 :s140 :s150 :s160 :s170 :s180 :s190 :s200 :s210) .


# :s10 a :DatasetReader . # read the manifest NO done in system
# trm:configKey trm:describe .

:s20 a :ConfigMap ;
    trm:configKey :postcraftMap .

:s30 a :FileReader ; # the template for raw entry content
    trm:describe trm:all .

:s40 a :Restructure ;
    trm:configKey :walkPrep .

:s50 a :DirWalker . # automatically forks

:s60 a :FileReader . # the markdown content

:s70 a :PostcraftPrep . # set up title, filenames etc

:s80 a :MarkdownToHTML .

:s90 a :Restructure ;
   trm:configKey :entryRawPrep .

:s100 a :Templater .

:s110 a :FileWriter .

############### entryContentToEntryPage
:s120 a :EntryContentToPagePrep .
#:s12 a :Restructure ;
 #  trm:configKey :entryPagePrep .

:s130 a :Templater .

:s140 a :FileWriter .

####################### index.html
:s150  a :Unfork .

:s160 a :FrontPagePrep .

:s170 a :Templater .

:s180 a :FileWriter .

###################### index.xml

:s190 a :AtomFeedPrep .

:s200 a :Templater .
 #   trm:configKey :atomTemplate .

:s210 a :FileWriter .

================
File: src/applications/postcraft-clear-cache/about.md
================
```sh
cd ~/github-danny/transmissions

./trans postcraft-clear-cache ../postcraft/danny.ayers.name
```

================
File: src/applications/postcraft-clear-cache/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances
@prefix pc: <http://purl.org/stuff/postcraft/> .

### ConfigMap
t:PostcraftMap a trm:ServiceConfig ;
    trm:key t:postcraftMap ;
    trm:group pc:ContentGroup .

### clean

t:cacheRemove a trm:ServiceConfig ;
    trm:key t:removeCache ;
    trm:target "cache/" .

t:articlesRemove a trm:ServiceConfig ;
    trm:key t:removeArticles ;
    trm:target "public/home/articles" .

t:entriesRemove a trm:ServiceConfig ;
    trm:key t:removeEntries ;
    trm:target "public/home/entries" .

t:journalRemove a trm:ServiceConfig ;
    trm:key t:removeJournal ;
    trm:target "public/home/journal" .

t:todoRemove a trm:ServiceConfig ;
    trm:key t:removeTodo ;
    trm:target "public/home/todo" .

t:indexRemove a trm:ServiceConfig ;
    trm:key t:removeIndex ;
    trm:target "public/home/index.html" .

### copy #####################################

t:copyStatic a trm:ServiceConfig ;
    trm:key t:staticCopy ;
    trm:source "content-static" ;
    trm:destination "public/home/static" .
    # trm:destination "../../danny.ayers.name/static" .

t:copyMedia a trm:ServiceConfig ;
    trm:key t:mediaCopy ;
    trm:source "media" ;
    trm:destination "public/home/media" .

t:copyCSS a trm:ServiceConfig ;
    trm:key t:cssCopy ;
    trm:source "layouts/middlin/css" ;
    trm:destination "public/home/css" .

t:copyFonts a trm:ServiceConfig ;
    trm:key t:fontsCopy ;
    trm:source "layouts/middlin/fonts" ;
    trm:destination "public/home/fonts" .

t:copyJS a trm:ServiceConfig ;
    trm:key t:jsCopy ;
    trm:source "layouts/middlin/js" ;
    trm:destination "public/home/js" .

### render ##################################

# t:atomTemplate a trm:ServiceConfig ;
#    trm:templateFile "layouts/middlin/templates/atom_template.njk" .

trm:Describe  a trm:ServiceConfig ;
 trm:key trm:describe .

t:phaseOne a trm:ServiceConfig ;
    trm:key t:markdownToRawPosts ;
    trm:marker "Phase One" .

# TODO IS COPY, not rename!!

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
  #  t:pp2   trm:pre     "filename" ;
   #         trm:post    "filename"  .

t:entryRawPrep a trm:ReMap ;
    trm:rename (t:er1 t:er3) .
   t:er1   trm:pre     "targetFilename" ;
            trm:post    "filepath" .
    t:er3   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

t:entryPagePrep a trm:ReMap ;
    trm:rename (t:ppp1) .
    t:ppp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

================
File: src/applications/postcraft-clear-cache/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s:
@prefix pc: <http://purl.org/stuff/postcraft/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################


## POSTCRAFT.CLEAN ##########################################

:clearCache a trm:Transmission ;
    rdfs:label "clear cache" ;
    rdfs:comment "directory cleaner" ;
    trm:pipe (:r10) .

:r10 a :FileRemove ;
    trm:configKey :removeCache .

================
File: src/applications/postcraft-previous/about.md
================
```sh
cd ~/github-danny/transmissions

./trans postcraft ../postcraft/danny.ayers.name
```

================
File: src/applications/postcraft-previous/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances
@prefix pc: <http://purl.org/stuff/postcraft/> .

### ConfigMap
t:PostcraftMap a trm:ServiceConfig ;
    trm:key t:postcraftMap ;
    trm:group pc:ContentGroup .

### clean

t:cacheRemove a trm:ServiceConfig ;
    trm:key t:removeCache ;
    trm:target "cache/" .

t:articlesRemove a trm:ServiceConfig ;
    trm:key t:removeArticles ;
    trm:target "public/home/articles" .

t:entriesRemove a trm:ServiceConfig ;
    trm:key t:removeEntries ;
    trm:target "public/home/entries" .

t:journalRemove a trm:ServiceConfig ;
    trm:key t:removeJournal ;
    trm:target "public/home/journal" .

t:todoRemove a trm:ServiceConfig ;
    trm:key t:removeTodo ;
    trm:target "public/home/todo" .

t:indexRemove a trm:ServiceConfig ;
    trm:key t:removeIndex ;
    trm:target "public/home/index.html" .

### copy #####################################

t:copyStatic a trm:ServiceConfig ;
    trm:key t:staticCopy ;
    trm:source "content-static" ;
    trm:destination "public/home/static" .
    # trm:destination "../../danny.ayers.name/static" .

t:copyMedia a trm:ServiceConfig ;
    trm:key t:mediaCopy ;
    trm:source "media" ;
    trm:destination "public/home/media" .

t:copyCSS a trm:ServiceConfig ;
    trm:key t:cssCopy ;
    trm:source "layouts/middlin/css" ;
    trm:destination "public/home/css" .

t:copyFonts a trm:ServiceConfig ;
    trm:key t:fontsCopy ;
    trm:source "layouts/middlin/fonts" ;
    trm:destination "public/home/fonts" .

t:copyJS a trm:ServiceConfig ;
    trm:key t:jsCopy ;
    trm:source "layouts/middlin/js" ;
    trm:destination "public/home/js" .

### render ##################################

trm:Describe  a trm:ServiceConfig ;
 trm:key trm:describe .

t:phaseOne a trm:ServiceConfig ;
    trm:key t:markdownToRawPosts ;
    trm:marker "Phase One" .

# TODO IS COPY, not rename!!

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
  #  t:pp2   trm:pre     "filename" ;
   #         trm:post    "filename"  .

t:entryRawPrep a trm:ReMap ;
    trm:rename (t:er1 t:er3) .
   t:er1   trm:pre     "targetFilename" ;
            trm:post    "filepath" .
    t:er3   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

t:entryPagePrep a trm:ReMap ;
    trm:rename (t:ppp1) .
    t:ppp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

================
File: src/applications/postcraft-previous/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s:
@prefix pc: <http://purl.org/stuff/postcraft/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################


## POSTCRAFT.CLEAN ##########################################

:clean a trm:Transmission ;
    rdfs:label "clean" ;
    rdfs:comment "directory cleaner" ;
    trm:pipe (:r10 :r20 :r30 :r40 :r50 :r60) .

:r10 a :FileRemove ;
    trm:configKey :removeCache .

:r20 a :FileRemove ;
    trm:configKey  :removeArticles .

:r30 a :FileRemove ;
    trm:configKey  :removeJournal .

:r40 a :FileRemove ;
    trm:configKey  :removeEntries .

:r50 a :FileRemove ;
    trm:configKey  :removeTodo .

:r60 a :FileRemove ;
    trm:configKey  :removeIndex .

## POSTCRAFT.COPY ##################################################################

:copy a trm:Transmission ;
    rdfs:label "copy" ;
    rdfs:comment "dir/file copier" ;
    trm:pipe (:cp10 :cp20 :cp30 :cp40 :cp50) .

### static dirs
:cp10 a :FileCopy ;
    trm:configKey :staticCopy .

### media dirs
:cp20 a :FileCopy ;
    trm:configKey :mediaCopy .

### layout dirs
:cp30 a :FileCopy ;
    trm:configKey :cssCopy .

:cp40 a :FileCopy ;
    trm:configKey :jsCopy .

:cp50 a :FileCopy ;
    trm:configKey :fontsCopy .

#####################

## POSTCRAFT.RENDER ###############################################################

:render a trm:Transmission ;
    rdfs:label "render" ;
    rdfs:comment "render the pages" ;
   trm:pipe (:s20  :s30 :s40  :s50 :s60 :s70   :s80 :s90  :s100
              :s110  :s120  :s130 :s140 :s150 :s160 :s170 :s180) .
 #  trm:pipe (:s10 :SM :s20 :SM2 :DE  :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100
  #               :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .

:s10 a :DatasetReader . # read the manifest NO done in system
# trm:configKey trm:describe .

:s20 a :ConfigMap ;
    trm:configKey :postcraftMap .

:s30 a :FileReader ; # the template for raw entry content
    trm:describe trm:all .

:s40 a :Restructure ;
    trm:configKey :walkPrep .

:s50 a :DirWalker . # automatically forks

:s60 a :FileReader . # the markdown content

:s70 a :PostcraftPrep . # set up title, filenames etc

:s80 a :MarkdownToHTML .

:s90 a :Restructure ;
   trm:configKey :entryRawPrep .

:s100 a :Templater .

:s110 a :FileWriter .

############### entryContentToEntryPage
:s120 a :EntryContentToPagePrep .
#:s12 a :Restructure ;
 #  trm:configKey :entryPagePrep .

:s130 a :Templater .

:s140 a :FileWriter .

#######################
:s150  a :Unfork .

:s160 a :FrontPagePrep .

:s170 a :Templater .

:s180 a :FileWriter .

================
File: src/applications/postcraft-render1/data/cache/2023-10-27_hello.md
================
<h1>Hello World! (again)</h1>
<p>lorem etc.</p>

================
File: src/applications/postcraft-render1/data/cache/2025-01-08_hello-again.md
================
<h1>Hello World! (again)</h1>
<p>lorem etc.</p>

================
File: src/applications/postcraft-render1/about.md
================
# Postcraft Render 1

walk source dirs for `.md`, render to `.html` in cache

```sh
cd ~/github-danny/transmissions

./trans postcraft-render1 ../postcraft/test-site

./trans postcraft-render1 ../postcraft/danny.ayers.name
```

================
File: src/applications/postcraft-render1/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances
@prefix pc: <http://purl.org/stuff/postcraft/> .

t:dirWalker a trm:ConfigSet ;
  trm:sourceDir "content-raw" .

t:templatePrep a trm:ReMap ;
    trm:rename (t:tp1 ).
    t:tp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .


t:contentTemplater a trm:ConfigSet ;
  trm:templateFilename "layouts/middlin/templates/entry-content_template.njk" .

t:filesRename a trm:ConfigSet ;
  trm:inputField "filename" ;
  trm:outputField "filepath" ;
  trm:match ".md";
  trm:replace ".html".

t:fileWriter a trm:ConfigSet ;
  trm:targetDir "cache" .

================
File: src/applications/postcraft-render1/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s:
@prefix pc: <http://purl.org/stuff/postcraft/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################


## ###############################################################

:render1 a trm:Transmission ;
    rdfs:label "render1" ;
    rdfs:comment "render raw entry pages" ;
   trm:pipe (:r110 :r120 :r130 :r140 :r150 :r160 :r170) .

:r110 a :DirWalker ; # automatically forks
    trm:config :dirWalker .

:r120 a :FileReader . # the markdown content

:r130 a :MarkdownToHTML .

:r140 a :Restructure ; # moves content into  contentBlocks
   trm:configKey :templatePrep .

:r150 a :Templater ; # section wrapper
    trm:config :contentTemplater .

:r160 a :StringReplace ; # *.md -> *.html
    trm:config :filesRename .

:r170 a :FileWriter ;
    trm:config :fileWriter .





#######################################################
# :s10 a :DatasetReader . # read the manifest NO done in system

:q10 a :ConfigMap ;
    trm:config :renderEntries .

:rq20 a :Restructure ;
    trm:configKey :walkPrep .

#######################################
:s50 a :DirWalker . # automatically forks

:s60 a :FileReader . # the markdown content

:s70 a :PostcraftPrep . # set up title, filenames etc

:s80 a :MarkdownToHTML .

:s90 a :Restructure ;
   trm:configKey :entryRawPrep .

:s100 a :Templater .

:s110 a :FileWriter .

############### entryContentToEntryPage
:s120 a :EntryContentToPagePrep .

:s130 a :Templater .

:s140 a :FileWriter .

#######################
:s150  a :Unfork .

:s160 a :FrontPagePrep .

:s170 a :Templater .

:s180 a :FileWriter .

================
File: src/applications/postcraft-render2/data/cache/2023-10-27_hello.md
================
<h1>Hello World! (again)</h1>
<p>lorem etc.</p>

================
File: src/applications/postcraft-render2/data/cache/2025-01-08_hello-again.md
================
<h1>Hello World! (again)</h1>
<p>lorem etc.</p>

================
File: src/applications/postcraft-render2/about.md
================
# Postcraft Render 2

walk cache dirs for `.html`, template to post pages

```sh
cd ~/github-danny/transmissions

./trans postcraft-render2 ../postcraft/test-site

./trans postcraft-render2 ../postcraft/danny.ayers.name
```

================
File: src/applications/postcraft-render2/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances
@prefix pc: <http://purl.org/stuff/postcraft/> .

t:cacheWalker a trm:ConfigSet ;
  trm:sourceDir "cache" ;
  trm:includeExtensions "['.html', '.md', '.ttl', '.js', '.json', '.txt',  '.css']" .

t:templatePrep a trm:ReMap ;
    trm:rename (t:tp1 ).
    t:tp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

t:pageTemplater a trm:ConfigSet ;
  trm:templateFilename "layouts/middlin/templates/entry-page_template.njk" .

t:writePrep a trm:ReMap ;
    trm:rename (t:wp1 ).
    t:wp1   trm:pre     "filename" ;
            trm:post    "filepath" .

t:fileWriter a trm:ConfigSet ;
  trm:targetDir "public" .

  ################

t:filesRename a trm:ConfigSet ;
  trm:inputField "filename" ;
  trm:outputField "filepath" ;
  trm:match ".md";
  trm:replace ".html".

================
File: src/applications/postcraft-render2/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s:
@prefix pc: <http://purl.org/stuff/postcraft/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################


## ###############################################################

:render2 a trm:Transmission ;
    rdfs:label "render2" ;
    rdfs:comment "template entry html to pages" ;
   trm:pipe (:r210 :r220 :r230 :r240 :r250 :r260
    :s150 :N :s160 :s170 :s180 :s190 :s200 :s210 ) .

:r210 a :DirWalker ; # automatically forks
    trm:config :cacheWalker .

:r220 a :FileReader . # the markdown content

:r230 a :Restructure ; # moves content into  contentBlocks
   trm:configKey :templatePrep .

:r240 a :Templater ; # for individual post pages
    trm:config :pageTemplater .

:r250 a :Restructure ; # moves content into  contentBlocks
   trm:configKey :writePrep .

:r260 a :FileWriter ;
    trm:config :fileWriter .

####################### index.html
:s150  a :Unfork .

:s160 a :FrontPagePrep .

:s170 a :Templater .

:s180 a :FileWriter .

###################### index.xml

:s190 a :AtomFeedPrep .

:s200 a :Templater .
 #   trm:configKey :atomTemplate .

:s210 a :FileWriter .

================
File: src/applications/selfie/about.md
================
# Selfie

Scan `transmissions`, generate self-descriptions - per-dir about.md, about.ttl

## Runner

```sh
cd ~/github-danny/transmissions # my local path
./trans app-template
```

## Description

---

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/selfie/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:setDemo a trm:ServiceConfig ;
    trm:setValue (t:sv0)  . # consider using blank nodes
    t:sv0   trm:key    "demo" ;
            trm:value    "a test value"  .

================
File: src/applications/selfie/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:mini a trm:Transmission ;
    trm:pipe (:p10 :p20) .

:p10 a :SetMessage ;
     trm:configKey :setDemo .

:p20 a :ShowMessage .

================
File: src/applications/string-pipeline/config.ttl
================
### NOT USED

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dc: <http://purl.org/dc/terms/> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:StringPipeline dc:title "Hello" .

================
File: src/applications/string-pipeline/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:stringpipe a trm:Transmission ;
    trm:pipe (:s1 :s2 :s3 :s4) .

:s1 a :StringSource .
:s2 a :AppendProcess .
:s3 a :AppendProcess .
:s4 a :StringSink .

================
File: src/applications/test_blanker/data/input/input-01.json
================
{
    "notpayload": "keep",
    "payload": {
        "nottest": "also keep",
        "test": {
            "first": "firststuff",
            "second": {
                "two": "twotwo"
            },
            "third": [
                "keep1",
                "keep2",
                "keep3"
            ],
            "fourth": [
                "31",
                "32",
                "33"
            ]
        }
    }
}

================
File: src/applications/test_blanker/data/output/output-01.json
================
{
    "notpayload": "keep",
    "payload": {
        "nottest": "also keep",
        "test": {
            "first": "",
            "second": {
                "two": ""
            },
            "third": [
                "keep1",
                "keep2",
                "keep3"
            ],
            "fourth": [
                "",
                "",
                ""
            ]
        }
    }
}

================
File: src/applications/test_blanker/data/output/required-01.json
================
{
    "notpayload": "keep",
    "payload": {
        "nottest": "also keep",
        "test": {
            "first": "",
            "second": {
                "two": ""
            },
            "third": [
                "keep1",
                "keep2",
                "keep3"
            ],
            "fourth": [
                "",
                "",
                ""
            ]
        }
    }
}

================
File: src/applications/test_blanker/about.md
================
# App Template

## Runner

```sh
cd ~/github-danny/transmissions # my local path
./trans test_blanker
```

## Description

================
File: src/applications/test_blanker/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances


t:r a trm:ServiceConfig ;
    rdfs:label "Read file" ;
    trm:configKey t:readFile ;
    trm:sourceFile "input/input-01.json" ;
    trm:mediaType "application/json" .

t:blanko a trm:ServiceConfig ;
    rdfs:label "Root node in JSON object" ;
    trm:configKey t:blankin ;
    trm:pointer "content.payload.test"  ; # "Root node in JSON object" ;
    trm:preserve "content.payload.test.third" .

t:w a trm:ServiceConfig ;
    rdfs:label "Write file" ;
    trm:configKey t:writeFile ;
    trm:destinationFile "output/output-01.json"  .

================
File: src/applications/test_blanker/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:testBlanker a trm:Transmission ;
    trm:pipe (:p10 :p20 :p30 ) .

:p10 a :FileReader ; # JSON test file
       trm:configKey :readFile .

:p20 a :Blanker ; # clear values
     trm:configKey :blankin .

:p30 a :FileWriter ; # save result
       trm:configKey :writeFile .

================
File: src/applications/test_configmap/data/input/input-01.md
================
Hello!

================
File: src/applications/test_configmap/data/output/output-01.md
================
Hello!

================
File: src/applications/test_configmap/data/output/required-01.md
================
Hello!

================
File: src/applications/test_configmap/about.md
================
# Application : test_fs-rw

```sh
cd ~/github-danny/transmissions/ # my local path

# run as application
./trans test_configmap
```

================
File: src/applications/test_configmap/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:readDataset a trm:ServiceConfig ;
    trm:configKey t:readDataset ;
    trm:datasetFile "manifest.ttl" .

t:configMapper a trm:ServiceConfig ;
    trm:configKey t:configMapper ;
    trm:pathMappings (
        t:postContent
    ) .

================
File: src/applications/test_configmap/manifest.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix pc: <http://purl.org/stuff/postcraft/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

<https://danny.ayers.name> a pc:Site ;
    rdfs:label "danny.ayers.name" ;
    dcterms:title "Rawer" ;
    pc:contains <https://danny.ayers.name/home> ;  # maybe
    pc:includes t:PostContent . # maybe

# this should maybe give the contentgroup a renderType, indirect with template etc

# ENTRIES CONTENT
t:PostContent a pc:ContentGroup ;
    rdfs:label "entries" ;
    pc:site <https://danny.ayers.name> ;
    pc:subdir "home" ; # better property name?
    fs:sourceDirectory "content-raw/entries" ; # SOURCE DIR HERE journal, entries
    fs:targetDirectory "cache/entries" ;
    pc:template "layouts/middlin/templates/entry-content_template.njk" .

================
File: src/applications/test_configmap/simple.js
================
import FileReader from '../../processors/fs/FileReader.js'
import FileWriter from '../../processors/fs/FileWriter.js'

const config = {
    "simples": "true",
    "sourceFile": "input/input-01.md",
    "destinationFile": "output/output-01.md"
}

var message = { "dataDir": "src/applications/test_fs-rw/data" }

const read = new FileReader(config)

message = await read.process(message)

const write = new FileWriter(config)

message = await write.process(message)

================
File: src/applications/test_configmap/test-config.json
================
{
    "transmissions": [
        {
            "name": "test_fs-rw",
            "message": {
                "content": "Hello World"
            },
            "requiredFiles": [
                "output-01.md"
            ]
        }
    ]
}

================
File: src/applications/test_configmap/transmissions.ttl
================
# src/applications/test_configmap/transmissions.ttl
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s:

:configtest a trm:Transmission ;
    trm:pipe (:s10 :s20 :s30) .

:s10 a :DatasetReader ;
    trm:configKey :readDataset .

:s20 a :ConfigMap ;
    trm:configKey :configMapper .

:s30 a :ShowMessage .

================
File: src/applications/test_dirwalker/about.md
================
# DirWalker

```sh
cd ~/github-danny/transmissions

./trans test_dirwalker
```

================
File: src/applications/test_dirwalker/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:DirWalkerConfig a trm:ServiceConfig ;
    trm:key t:dirwalker ;
    trm:sourceDir "." .  # subdirectory path

================
File: src/applications/test_dirwalker/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:dirwalk a trm:Transmission ;
    trm:pipe (:SM :s1 :s2 :s3) .

:s1 a :DirWalker .
#    trm:configKey :dirwalker . # specify in config.ttl

:s2 a :AppendProcess .
:s3 a :StringSink .

================
File: src/applications/test_env-loader/about.md
================
```sh
cd ~/github-danny/transmissions
./trans env-loader-test
```

================
File: src/applications/test_env-loader/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/test_env-loader/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:envy a trm:Transmission ;
    trm:pipe (:s10 :s20 :SM) .
# trm:pipe (:SC) .
:s10 a :EnvLoader .
:s20 a :WhiteboardToMessage .

================
File: src/applications/test_file-copy-remove/data/several-full/one.txt
================
Hello from One

================
File: src/applications/test_file-copy-remove/data/several-full/two.txt
================
Hello from Two

================
File: src/applications/test_file-copy-remove/data/single-full/one.txt
================
Hello from One

================
File: src/applications/test_file-copy-remove/data/start/one.txt
================
Hello from One

================
File: src/applications/test_file-copy-remove/data/start/two.txt
================
Hello from Two

================
File: src/applications/test_file-copy-remove/about.md
================
# file-copy-remove-test

run with :

```
# in transmissions dir

./run file-copy-remove-test
```

or

```
npm test -- tests/integration/file-copy-remove-test.spec.js
```

this should :

- copy `start/one.txt` into `single-empty/`
- copy `single-empty/one.txt` into `single-full/`
- remove `single-empty/one.txt`

- copy everything in `start/` into `several-empty/`
- copy everything in `several-empty/` into `several-full/`
- remove everything in `several-empty/`

Hmm, test services would be helpful to check before and after - or maybe just use regular test runner script from npm?

================
File: src/applications/test_file-copy-remove/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

#t:copyOneToSingleEmpty a trm:ServiceConfig ;
 #   trm:key t:copyOneToSingleEmpty ;
  #  trm:source "data/start/one.txt" ;
   # trm:destination "data/single-empty/one.txt" .

t:copyOneToSingleEmpty a trm:ServiceConfig ;
    trm:key t:copyOneToSingleEmpty ;
    trm:source "data/start/one.txt" ;
    trm:destination "data/single-empty/one.txt" .

t:copySingleEmptyToSingleFull a trm:ServiceConfig ;
    trm:key t:copySingleEmptyToSingleFull ;
    trm:source "data/single-empty/one.txt" ;
    trm:destination "data/single-full/one.txt" .

t:removeSingleEmpty a trm:ServiceConfig ;
    trm:key t:removeSingleEmpty ;
    trm:target "data/single-empty/one.txt" .

t:copyStartToSeveralEmpty a trm:ServiceConfig ;
    trm:key t:copyStartToSeveralEmpty ;
    trm:source "data/start" ;
    trm:destination "data/several-empty" .

t:copySeveralEmptyToSeveralFull a trm:ServiceConfig ;
    trm:key t:copySeveralEmptyToSeveralFull ;
    trm:source "data/several-empty" ;
    trm:destination "data/several-full" .

t:removeSeveralEmpty a trm:ServiceConfig ;
    trm:key t:removeSeveralEmpty ;
    trm:target "data/several-empty" .

================
File: src/applications/test_file-copy-remove/init.sh
================
rm -rf data/start
rm -rf data/single-empty
rm -rf data/single-full
rm -rf data/several-empty
rm -rf data/several-full



mkdir -p data/start






echo 'Hello from One' > data/start/one.txt


echo 'Hello from Two' > data/start/two.txt

tree data

================
File: src/applications/test_file-copy-remove/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:file_copy_remove_test a trm:Transmission ;
    trm:pipe (:s1 :s2 :s3 :s4 :s5 :s6) .

:s1 a :FileCopy ;
    trm:configKey :copyOneToSingleEmpty .

:s2 a :FileCopy ;
    trm:configKey :copySingleEmptyToSingleFull .

:s3 a :FileRemove ;
    trm:configKey :removeSingleEmpty .

:s4 a :FileCopy ;
    trm:configKey :copyStartToSeveralEmpty .

:s5 a :FileCopy ;
    trm:configKey :copySeveralEmptyToSeveralFull .

:s6 a :FileRemove ;
    trm:configKey :removeSeveralEmpty .

================
File: src/applications/test_filename-mapper/data/input/input-01.txt
================
Test content for filename mapping

================
File: src/applications/test_filename-mapper/data/output/required-01.txt
================
Test content for filename mapping

================
File: src/applications/test_filename-mapper/about.md
================
# Test Filename Mapper

## Runner

```sh
cd ~/github-danny/transmissions # my local path
./trans test_filename-mapper

npm test -- tests/unit/filename-mapper.spec.js
npm test -- tests/integration/filename-mapper.spec.js
```

## Description

Tests the FilenameMapper processor by:

1. Reading a file
2. Mapping its filename according to configuration
3. Writing the file with the new name

## Test Files

- Input: data/input/input-01.txt
- Expected: data/output/required-01.txt
- Output: data/output/output-01.txt

================
File: src/applications/test_filename-mapper/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:readFile a trm:ServiceConfig ;
    rdfs:label "Read file" ;
    trm:configKey t:readConfig ;
    trm:sourceFile "input/input-01.txt" ;
    trm:mediaType "text/plain" .

t:mapperConfig a trm:ServiceConfig ;
    rdfs:label "Filename mapper config" ;
    trm:configKey t:filenameConfig ;
    trm:extensions (t:ext1 t:ext2) .

t:ext1 
    trm:format "html" ;
    trm:extension ".mm.html" .

t:ext2
    trm:format "svg" ;
    trm:extension ".mm.svg" .

t:writeFile a trm:ServiceConfig ;
    rdfs:label "Write file" ;
    trm:configKey t:writeConfig ;
    trm:destinationFile "output/output-01.txt" .

================
File: src/applications/test_filename-mapper/filename-mapper-simple.js
================
import FilenameMapper from '../../processors/fs/FilenameMapper.js';
import FileReader from '../../processors/fs/FileReader.js';
import FileWriter from '../../processors/fs/FileWriter.js';

const config = {
    "simples": true,
    "sourceFile": "input/input-01.txt",
    "destinationFile": "output/output-01.txt",
    "extensions": {
        "html": ".mm.html",
        "svg": ".mm.svg"
    }
};

async function runPipeline() {
    var message = {
        "dataDir": "src/applications/test_filename-mapper/data",
        "format": "html"
    };


    const reader = new FileReader(config);
    message = await reader.process(message);


    const mapper = new FilenameMapper(config);
    message = await mapper.process(message);


    const writer = new FileWriter(config);
    message = await writer.process(message);

    return message;
}

runPipeline().catch(console.error);

================
File: src/applications/test_filename-mapper/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:test_filenameMapper a trm:Transmission ;
    trm:pipe (:p10 :SM :p20 :p30) .

:p10 a :FileReader ;
    trm:configKey :readConfig .

:p20 a :FilenameMapper ;
    trm:configKey :filenameConfig .

:p30 a :FileWriter ;
    trm:configKey :writeConfig .

================
File: src/applications/test_foreach/about.md
================
# ForEach processor module for Transmissions

```sh
./trans test_fork
```

Your Goal is to write a processor module for Transmissions that will initiate multiple processing pipelines based on a list provided in the incoming message. First review these instructions as a whole, and then identify the subgoals. Then, taking each subgoal in turn, break it down into a concrete series of tasks. Carry out the sequence of tasks.  
You have plenty of time, so don't rush, try to be as careful in understanding and operation as possible.
Existing source code may be found in the Project Knowledge files.

Two modules are required -

1. `ForEach` located in :

```sh
./transmissions/src/processors/flow/ForEach.js
```

modeled on :

```sh
./transmissions/src/processors/templates/ProcessorTemplate.js
```

2. `FlowProcessorsFactory` located in

```sh
./transmissions/src/processors/flow/FlowProcessorsFactory.js
```

modeled on :

```sh
/transmissions/src/processors/templates/TemplateProcessorsFactory.js
```

The input message will contain the list to be processed in the form of this example :

```json
{
  "foreach": ["item1", "item2", "item3"]
}
```

The behavior will be to emit the message to a subsequent processor using the existing engine infrastructure, like a simpler version of :

```sh
transmissions/src/processors/fs/DirWalker.js
```

Each message emitted will be a structuredClone of the input message.

Once this code is completed, create application definitions in the form of these examples :

```sh
transmissions/src/applications/test_fork/transmissions.ttl
transmissions/src/applications/test_fork/processors-config.ttl
```

After you have finished all these, re-read the high level Goal and taking each of your derived subgoals in turn, review your code to ensure that it fulfils the requirements.
Show me the full source of the implementations.

---

/home/danny/github-danny/postcraft/danny.ayers.name/content-raw/entries/2024-09-27_lively-distractions.md

https://github.com/github/rest-api-description

================
File: src/applications/test_foreach/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

:foreach_test a trm:Transmission ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a :ForEach .
:s2 a :ShowMessage .
:s3 a :DeadEnd .

================
File: src/applications/test_fork/about.md
================
# Test Fork/Unfork

```
./run test_fork | grep 's2 a NOP'
```

should show the number of forks + 1 (for `message.done`)

```
./run test_fork | grep s1.s2.s10.s11.s12.s13
```

should show just one

================
File: src/applications/test_fork/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

================
File: src/applications/test_fork/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:test_fork a :Transmission ;
   trm:contains :pipeA .

:pipeA a trm:Transmission ;
trm:pipe (:p10 :p20 :SM ) .

:p10 a :Fork .

# :s10 a :Unfork .
:p20 a :NOP .

================
File: src/applications/test_fork-unfork/about.md
================
# Test Fork/Unfork

```
./run test_fork | grep 's2 a NOP'
```

should show the number of forks + 1 (for `message.done`)

```
./run test_fork | grep s1.s2.s10.s11.s12.s13
```

should show just one

================
File: src/applications/test_fork-unfork/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

================
File: src/applications/test_fork-unfork/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s:

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one
#############################################################

:test_fork_unfork a :Transmission ;
   trm:contains :pipeA .

:pipeA a trm:Transmission ;
trm:pipe (:p10 :p20 :p30 :p40) .

:p10 a :Fork .

:p20 a :NOP .

:p30 a :Unfork .

:p40 a :ShowMessage .

================
File: src/applications/test_fs-rw/data/input/input-01.md
================
Hello!

================
File: src/applications/test_fs-rw/data/output/required-01.md
================
Hello!

================
File: src/applications/test_fs-rw/about.md
================
# Application : test_fs-rw

```sh
cd ~/github-danny/transmissions/ # my local path

# run as application
./trans test_fs-rw
```

---

Copies

```sh
src/applications/test_fs-rw/data/output/input-01.md
```

to

```sh
src/applications/test_fs-rw/data/output/output-01.md
```

the tests compare the new file with :

```sh
src/applications/test_fs-rw/data/output/required-01.md
```

```sh
cd ~/github-danny/transmissions/ # my local path

# run as application
./trans test_fs-rw

# run as simples
node src/applications/test_fs-rw/simple.js

## Tests in tests/integration

# test as application
npm test -- --filter="fs-rw test"

# test as simples
npm test -- --filter="fs-rw simple test"
```

---

```sh
cd ~/github-danny/transmissions/
./trans test_restructure -P ./src/applications/test_restructure/input/input-01.json
```

---

./trans claude-json-converter -P ./conversations.json

```turtle
:s40 a :Restructure ;
    trm:configKey :walkPrep .

...

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
```

================
File: src/applications/test_fs-rw/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .


t:inputFile a trm:Config ;
    trm:key t:input ; 
    trm:sourceFile  "input/input-01.md" .

t:outputFile a trm:Config ;
    trm:key t:output ;
    trm:destinationFile "output/output-01.md" .

 # http://purl.org/stuff/transmission/sourceFile

================
File: src/applications/test_fs-rw/simple.js
================
import FileReader from '../../processors/fs/FileReader.js'
import FileWriter from '../../processors/fs/FileWriter.js'

const config = {
    "simples": "true",
    "sourceFile": "input/input-01.md",
    "destinationFile": "output/output-01.md"
}

var message = { "dataDir": "src/applications/test_fs-rw/data" }

const read = new FileReader(config)

message = await read.process(message)

const write = new FileWriter(config)

message = await write.process(message)

================
File: src/applications/test_fs-rw/test-config.json
================
{
    "transmissions": [
        {
            "name": "test_fs-rw",
            "message": {
                "content": "Hello World"
            },
            "requiredFiles": [
                "output-01.md"
            ]
        }
    ]
}

================
File: src/applications/test_fs-rw/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:fsrw a trm:Transmission ;
trm:pipe (:SM :read :write ) .

:read a :FileReader ;
     trm:configKey :input .

:write a :FileWriter ;
     trm:configKey :output .

================
File: src/applications/test_http-server/data/input/index.html
================
<!DOCTYPE html>
<html>

<head>
    <title>HTTP Server Test</title>
    <style>
        .status {
            margin: 10px 0;
            padding: 10px;
            border-radius: 4px;
        }

        .online {
            background: #d4edda;
        }

        .offline {
            background: #f8d7da;
        }

        .metrics {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 10px;
            margin: 20px 0;
        }
    </style>
</head>

<body>
    <h1>HTTP Server Test Interface</h1>

    <div id="status" class="status"></div>

    <div class="metrics">
        <div>
            <h3>Server Metrics</h3>
            <div id="metrics"></div>
        </div>
        <div>
            <h3>Message Values</h3>
            <input type="text" id="key" placeholder="Key">
            <input type="text" id="value" placeholder="Value">
            <button onclick="addValue()">Add Value</button>
            <div id="currentValues"></div>
        </div>
    </div>

    <button onclick="shutdownServer()"
        style="background: #dc3545; color: white; padding: 10px; border: none; border-radius: 4px;">
        Shutdown Server
    </button>

    <script>
        let messageValues = {};
        let metrics = {
            startTime: Date.now(),
            requests: 0
        };

        function updateStatus(online) {
            const status = document.getElementById('status');
            status.textContent = online ? 'Server Online' : 'Server Offline';
            status.className = `status ${online ? 'online' : 'offline'}`;
        }

        function updateMetrics() {
            metrics.uptime = Math.floor((Date.now() - metrics.startTime) / 1000);
            metrics.requests++;

            document.getElementById('metrics').innerHTML = Object.entries(metrics)
                .map(([k, v]) => `<div>${k}: ${v}</div>`)
                .join('');
        }

        function addValue() {
            const key = document.getElementById('key').value;
            const value = document.getElementById('value').value;
            if (key && value) {
                messageValues[key] = value;
                updateValues();
            }
        }

        function updateValues() {
            document.getElementById('currentValues').innerHTML =
                Object.entries(messageValues)
                    .map(([k, v]) => `<div>${k}: ${v}</div>`)
                    .join('');
        }

        async function shutdownServer() {
            try {
                const response = await fetch('/shutdown', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(messageValues)
                });
                updateStatus(false);
            } catch (error) {
                console.error('Shutdown error:', error);
            }
        }

        // Initialize
        updateStatus(true);
        setInterval(updateMetrics, 1000);
    </script>
</body>

</html>

================
File: src/applications/test_http-server/data/input/metrics.js
================
class MetricsUI {
    constructor() {
        this.ws = null;
        this.token = null;
        this.setupWebSocket();
        this.setupAuth();
    }

    setupWebSocket() {
        this.ws = new WebSocket(`ws://${window.location.host}/metrics`);
        this.ws.onmessage = (event) => {
            const metrics = JSON.parse(event.data);
            this.updateMetricsDisplay(metrics);
        };
    }

    async setupAuth() {
        const response = await fetch('/admin/token');
        const { token } = await response.json();
        this.token = token;
    }

    updateMetricsDisplay(metrics) {
        const display = document.getElementById('metrics');
        display.innerHTML = `
            <div>Uptime: ${Math.floor(metrics.uptime)}s</div>
            <div>Connections: ${metrics.connections}</div>
            <div>Requests: ${metrics.requests}</div>
            <div>Memory Used: ${Math.floor(metrics.memory.used / 1024 / 1024)}MB</div>
            <div>CPU Load: ${metrics.cpu.load[0].toFixed(2)}</div>
        `;
    }

    async shutdown() {
        try {
            await fetch('/admin/shutdown', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${this.token}`
                }
            });
        } catch (error) {
            console.error('Shutdown failed:', error);
        }
    }
}

export default new MetricsUI();

================
File: src/applications/test_http-server/about.md
================
# App Template

## Runner

```sh
cd ~/github-danny/transmissions # my local path
./trans test_http-server

---
runs at :

http://localhost:4000/transmissions/test/

```

curl -X POST http://localhost:4000/shutdown

node src/applications/test_http-server/test-shutdown.js

npm test -- tests/unit/http-server_ShutdownService.spec.js

```

## Description

Test application for HttpServer processor that:

- Serves static files from data/input directory
- Listens on port 4000
- Shuts down on POST to /shutdown endpoint
- Base path: /transmissions/test/
```

================
File: src/applications/test_http-server/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:setDemo a trm:ServiceConfig ;
    rdfs:label "HTTP Server configuration" ;
    trm:configKey t:httpServer ;
    trm:port 4000 ;
    trm:basePath "/transmissions/test/" ;
    trm:staticPath "src/applications/test_http-server/data/input" .

================
File: src/applications/test_http-server/shutdown-client-auth.js
================
async function shutdownServer(baseUrl, username, password) {
    const credentials = Buffer.from(`${username}:${password}`).toString('base64');

    try {
        const response = await fetch(`${baseUrl}/admin/shutdown`, {
            method: 'POST',
            headers: {
                'Authorization': `Basic ${credentials}`
            }
        });

        if (!response.ok) {
            throw new Error(`Shutdown failed: ${response.statusText}`);
        }

        return await response.text();
    } catch (error) {
        console.error('Shutdown error:', error);
        throw error;
    }
}


try {
    const baseUrl = 'http://localhost:4000';
    const response = await fetch(`${baseUrl}/admin/credentials`);
    const { username, password } = await response.json();
    await shutdownServer(baseUrl, username, password);
} catch (error) {
    console.error('Error:', error);
}

================
File: src/applications/test_http-server/test-shutdown.js
================
import fetch from 'node-fetch';

async function testShutdown() {
    try {
        const response = await fetch('http://localhost:4000/shutdown', {
            method: 'POST'
        });
        console.log('Server response:', await response.text());
    } catch (error) {
        console.error('Error:', error.message);
    }
}

testShutdown();

================
File: src/applications/test_http-server/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:mini a trm:Transmission ;
    trm:pipe (:server :SM) .

:server a :HttpServer ;
    trm:configKey :httpServer .

:SM a :ShowMessage .

================
File: src/applications/test_multi-pipe/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:dirWalkerPosts a trm:ServiceConfig ;
    trm:key t:files .

t:postTemplateMap a trm:ReMap ;
   trm:rename (t:rn1) . # consider using blank nodes
     t:rn1    trm:pre     "content" ;
            trm:post    "template"  .

t:postSaver a trm:ReMap ;
    trm:rename (t:rn2) . 
    t:rn2   trm:pre     "targetFilename" ;
            trm:post    "filename" .

================
File: src/applications/test_multi-pipe/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

:test_multi-pipes a :Transmission ;
   trm:contains :pipeA, :pipeB, :pipeC .

:pipeA a trm:Transmission ;
trm:pipe (:s1 :s2 :s3 ) .

:pipeB  a trm:Transmission ;
 trm:pipe (:s3 :s104 :s105) .

:pipeC a trm:Transmission ;
trm:pipe (:s3 :s204 :s205) .

# :postcraft a trm:Transmission ;

:s1 a :NOP .
:s2 a :NOP .
:s3 a :NOP .

:s104 a :NOP .
:s105 a :NOP .

:s204 a :NOP .
:s205 a :ShowTransmission .

================
File: src/applications/test_nop/about.md
================
# nop

## Description

minimal for comparing with simple runner

---

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/test_nop/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

================
File: src/applications/test_nop/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:nope a trm:Transmission ;
    trm:pipe (:N :SC :SM) .

================
File: src/applications/test_ping/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:pingConfig a trm:ServiceConfig ;
    trm:interval 2000 ;         # Ping every 2 seconds
    trm:count 5 ;               # Stop after 5 pings
    trm:payload "HEARTBEAT" ;   # Custom payload
    trm:killSignal "STOP" ;     # Kill signal value
    trm:retryAttempts 3 ;       # Number of retry attempts on error
    trm:retryDelay 1000 .      # Delay between retries in ms

t:killConfig a trm:ServiceConfig ;
    trm:setValue (t:sv0) ;
    t:sv0 trm:key "kill" ;
          trm:value "STOP" .

================
File: src/applications/test_ping/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

:test_ping a trm:Transmission ;
    trm:pipe (:p10 :p20 :p30) .

:p10 a :Ping ;
    trm:configKey :pingConfig .

:p20 a :ShowMessage .

:p30 a :SetMessage ;
    trm:configKey :killConfig .

================
File: src/applications/test_restructure/data/input/input-01.json
================
{
    "item": {
        "uuid": "convo1",
        "name": "Name of this convo",
        "created_at": "2024-10-29T17:57:50.229169Z",
        "chat_messages": [
            {
                "uuid": "id1",
                "text": "Text one"
            },
            {
                "uuid": "id2",
                "text": "Text two"
            }
        ]
    }
}

================
File: src/applications/test_restructure/data/output/required-01.json
================
{
    "channel": [
        {
            "uuid": "id1",
            "text": "Text one"
        },
        {
            "uuid": "id2",
            "text": "Text two"
        }
    ],
    "filename": "convo1",
    "title": "Name of this convo"
}

================
File: src/applications/test_restructure/about.md
================
# Application : test_restructure

Run with :

```sh
cd ~/github-danny/transmissions/ # local path of repo
./trans test_restructure
```

#:todo make this into something like processor signature
#:todo make Turtle version

## Description

Reads :

```sh
src/applications/test_restructure/data/output/input-01.json
```

as a message, restructures it according to config, then writes the result to :

```sh
src/applications/test_restructure/data/output/output-01.json
```

the tests compare the new file with :

```sh
src/applications/test_restructure/data/output/required-01.json
```

```sh
cd ~/github-danny/transmissions/ # my local path

# run as application
./trans test_restructure

# run as simples
node src/applications/test_restructure/simple.js

## Tests in tests/integration

# test as application
npm test -- --filter="restructure test"

# test as simples
npm test -- --filter="restructure_simple test"
```

---

```sh
cd ~/github-danny/transmissions/
./trans test_restructure -P ./src/applications/test_restructure/input/input-01.json
```

---

./trans claude-json-converter -P ./conversations.json

```turtle
:s40 a :Restructure ;
    trm:configKey :walkPrep .

...

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
```

================
File: src/applications/test_restructure/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:jsonFileIn a trm:Config ;
    trm:key t:reader ;
    trm:sourceFile "input/input-01.json" ;
    trm:mediaType "application/json" .

t:retree a trm:ServiceConfig ;
    trm:rename (t:pp1 t:pp2 t:pp3) . # consider using blank nodes
    t:pp1   trm:pre     "content.item.chat_messages" ;
            trm:post    "content.channel"  .
    t:pp2   trm:pre     "content.item.uuid" ;
            trm:post    "content.filename"  . 
    t:pp3   trm:pre     "content.item.name" ;
            trm:post    "content.title"  .


t:jsonFileOut a trm:Config ;
    trm:key t:writer ;
    trm:destinationFile "output/output-01.json" .

================
File: src/applications/test_restructure/simple.js
================
import FileReader from '../../processors/fs/FileReader.js'
import Restructure from '../../processors/json/Restructure.js'
import FileWriter from '../../processors/fs/FileWriter.js'

const config = {
    "simples": "true",
    "sourceFile": "input/input-01.json",
    "destinationFile": "output/output-01.json",
    "mediaType": "application/json",
    "rename": [{
        "pre": "content.item.chat_messages",
        "post": "content.channel"
    }, {
        "pre": "content.item.uuid",
        "post": "content.filename"
    }, {
        "pre": "content.item.name",
        "post": "content.title"
    }]
}

var message = { "dataDir": "src/applications/test_restructure/data" }

const read = new FileReader(config)
message = await read.process(message)

const restructure = new Restructure(config)
message = await restructure.process(message)

const write = new FileWriter(config)
await write.process(message)

================
File: src/applications/test_restructure/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM1 a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:cjc a trm:Transmission ;


trm:pipe (:read :retree1 :SM :writer) .

:read a :FileReader ;
     trm:configKey :reader .

:retree1 a :Restructure ;
     trm:configKey :retree .

:writer a :FileWriter ;
     trm:configKey :writer .

================
File: src/applications/test_runcommand/data/output/output-01.txt
================
Hello from RunCommand!

================
File: src/applications/test_runcommand/data/output/required-01.txt
================
Hello from RunCommand!

================
File: src/applications/test_runcommand/about.md
================
# Application: test_runcommand

```sh
cd ~/github-danny/transmissions/ # my local path

# run as application
./trans test_runcommand
```

This test application demonstrates the RunCommand processor by executing a simple echo command and verifying its output.

The test runs a simple echo command defined in config.ttl and compares the output with the expected content in:

```sh
src/applications/test_runcommand/data/output/required-01.txt
```

```sh
# run as application
./trans test_runcommand

# run tests
npm test -- --filter="runcommand test"
```

================
File: src/applications/test_runcommand/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:runCommandConfig a trm:ServiceConfig ;
    rdfs:label "Run test command" ;
    trm:configKey t:runCommand ;
    trm:command "echo 'Hello from RunCommand!'" ;
    trm:allowedCommands ( "echo" "ls" ) ;
    trm:blockedPatterns ( "rm" ">" "|" ";" ) ;
    trm:destinationFile "output/output-01.txt" .

================
File: src/applications/test_runcommand/simple.js
================
import RunCommand from '../../processors/unsafe/RunCommand.js'
import FileWriter from '../../processors/fs/FileWriter.js'
import FileReader from '../../processors/fs/FileReader.js'
import CaptureAll from '../../processors/util/CaptureAll.js'
import WhiteboardToMessage from '../../processors/util/WhiteboardToMessage.js'

const config = {
    "simples": "true",
    "allowedCommands": ["ls", "echo", "pwd"],
    "blockedPatterns": ["rm", ">", "|", ";"],
    "sourceFile": "input/input-01.txt",
    "destinationFile": "output/output-01.txt",
    "whiteboard": []
}

async function runPipeline() {
    console.log('Starting pipeline test with whiteboard...')
    var message = { "dataDir": "src/applications/test_runcommand/data" }


    const capture = new CaptureAll(config)
    const whiteboardToMessage = new WhiteboardToMessage(config)


    const pwdCommand = new RunCommand({ ...config, command: "pwd" })
    message = await pwdCommand.process(message)
    message = await capture.process(message)


    const lsCommand = new RunCommand({ ...config, command: "ls -l" })
    message = await lsCommand.process(message)
    message = await capture.process(message)


    message = await whiteboardToMessage.process(message)
    const combinedOutput = message.whiteboard.commandResult
        .map(result => result.stdout)
        .join('\n')

    const echoCommand = new RunCommand({
        ...config,
        command: `echo 'Command Outputs:\n${combinedOutput}'`
    })
    message = await echoCommand.process(message)


    const write = new FileWriter(config)
    message = await write.process(message)


    const read = new FileReader(config)
    message = await read.process(message)
    console.log('Final output:', message.content)


    console.log('\nWhiteboard contents:')
    console.log(JSON.stringify(config.whiteboard, null, 2))
}

runPipeline().catch(console.error)

================
File: src/applications/test_runcommand/test-config.json
================
{
    "transmissions": [
        {
            "name": "test_runcommand",
            "message": {
                "dataDir": "src/applications/test_runcommand/data"
            },
            "requiredFiles": [
                "output-01.txt"
            ],
            "expectedOutput": "Hello from RunCommand!\n"
        }
    ]
}

================
File: src/applications/test_runcommand/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

:runcommand a trm:Transmission ;
    trm:pipe (:cmd :write) .

:cmd a :RunCommand ;
    trm:configKey :runCommand .

:write a :FileWriter ;
    trm:configKey :runCommand .

================
File: src/applications/test_two-transmissions/config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

#t:copyOneToSingleEmpty a trm:ServiceConfig ;
 #   trm:key t:copyOneToSingleEmpty ;
  #  trm:source "data/start/one.txt" ;
   # trm:destination "data/single-empty/one.txt" .

t:copyOneToSingleEmpty a trm:ServiceConfig ;
    trm:key t:copyOneToSingleEmpty ;
    trm:source "data/start/one.txt" ;
    trm:destination "data/single-empty/one.txt" .

t:copySingleEmptyToSingleFull a trm:ServiceConfig ;
    trm:key t:copySingleEmptyToSingleFull ;
    trm:source "data/single-empty/one.txt" ;
    trm:destination "data/single-full/one.txt" .

t:removeSingleEmpty a trm:ServiceConfig ;
    trm:key t:removeSingleEmpty ;
    trm:target "data/single-empty/one.txt" .

t:copyStartToSeveralEmpty a trm:ServiceConfig ;
    trm:key t:copyStartToSeveralEmpty ;
    trm:source "data/start" ;
    trm:destination "data/several-empty" .

t:copySeveralEmptyToSeveralFull a trm:ServiceConfig ;
    trm:key t:copySeveralEmptyToSeveralFull ;
    trm:source "data/several-empty" ;
    trm:destination "data/several-full" .

t:removeSeveralEmpty a trm:ServiceConfig ;
    trm:key t:removeSeveralEmpty ;
    trm:target "data/several-empty" .

================
File: src/applications/test_two-transmissions/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:transmission_one a trm:Transmission ;
    trm:pipe (:a1 :a2) .

:a1 a :NOP .

:a2 a :NOP .

:transmission_two a trm:Transmission ;
    trm:pipe (:b1 :b2 :b3) .

:b1 a :NOP .

:b2 a :NOP .

:b3 a :NOP .

================
File: src/core/ApplicationManager.js
================
import path from 'path'
import fs from 'fs/promises'
import _ from 'lodash'

import rdf from 'rdf-ext'
import { fromFile } from 'rdf-utils-fs'

import logger from '../utils/Logger.js'
import TransmissionBuilder from './TransmissionBuilder.js'
import ModuleLoaderFactory from './ModuleLoaderFactory.js'

class ApplicationManager {
    constructor() {
        this.appsDir = 'src/applications'
        this.transmissionFilename = 'transmissions.ttl'
        this.configFilename = 'config.ttl'
        this.moduleSubDir = 'processors'
        this.dataSubDir = 'data'
        this.manifestFilename = 'manifest.ttl'
    }

    async initialize(appName, appPath, subtask, target) {

        logger.debug(`\n\nApplicationManager.initialize appPath =  ${appPath} `)
        this.appPath = this.resolveApplicationPath(appPath)

        logger.debug(`\nApplicationManager.initialize this.appPath =  ${this.appPath} `)
        this.transmissionsFile = path.join(this.appPath, this.transmissionFilename)
        this.processorsConfigFile = path.join(this.appPath, this.configFilename)
        this.modulePath = path.join(this.appPath, this.moduleSubDir)

        this.moduleLoader = ModuleLoaderFactory.createApplicationLoader(this.modulePath)
        this.app = {
            appName: appName,
            appPath: appPath,
            subtask: subtask,
        }

        if (target) {
            this.app.manifestFilename = path.join(target, this.manifestFilename)
            this.app.dataset = await this.loadManifest(this.app.manifestFilename)
            this.app.targetPath = target
        }
    }


    async start(message) {

        logger.debug(`\nApplicationManager.start
    transmissionsFile : ${this.transmissionsFile},
    processorsConfigFile : ${this.processorsConfigFile}
    subtask : ${this.app.subtask}`)


        const transmissions = await TransmissionBuilder.build(
            this.transmissionsFile,
            this.processorsConfigFile,
            this.moduleLoader
        )



        message = _.merge(message, this.app)


        if (!message.rootDir) {
            message.rootDir = this.appPath
        }
        if (!message.dataDir) {
            message.dataDir = path.join(this.appPath, this.dataSubDir)
        }


        for (const transmission of transmissions) {
            if (!this.app.subtask || this.app.subtask === transmission.label) {
                await transmission.process(message)
            }
        }

        return { success: true }
    }

    async loadManifest(manifestFilename) {
        logger.debug(`ApplicationManager.loadManifest, try loading : ${manifestFilename}`)
        try {



            const stream = fromFile(manifestFilename)
            return await rdf.dataset().import(stream)



        } catch (err) {
            logger.debug(`ApplicationManager.loadManifest, ${manifestFilename} non-existent, creating empty dataset`)
            return rdf.dataset()
        }
    }

    async listApplications() {
        try {
            const entries = await fs.readdir(this.appsDir, { withFileTypes: true })
            const subdirChecks = entries
                .filter(dirent => dirent.isDirectory())
                .map(async (dirent) => {
                    const subdirPath = path.join(this.appsDir, dirent.name)
                    const files = await fs.readdir(subdirPath)
                    return files.includes('about.md') ? dirent.name : null
                })

            const validApps = (await Promise.all(subdirChecks)).filter(Boolean)
            return validApps
        } catch (err) {
            logger.error('Error listing applications:', err)
            return []
        }
    }


    resolveApplicationPath(appName) {
        logger.debug(`\nApplicationManager.resolveApplicationPath, appName = ${appName}`)

        if (appName.startsWith('/')) {
            return appName
        }

        if (appName.startsWith('..')) {

            const resolved = path.resolve(process.cwd(), appName)
            logger.debug(`ApplicationManager.resolveApplicationPath, resolved = ${resolved}`)
            return resolved
        }
        logger.debug(`ApplicationManager.resolveApplicationPath, this.appsDir = ${this.appsDir}`)


        return path.join(process.cwd(), this.appsDir, appName)
    }


}

export default ApplicationManager

================
File: src/core/Director.js
================
class Director {
    constructor() {
        this.builder = new TransmissionBuilder()
        this.runner = new TransmissionRunner()
        this.procurer = new Procurer()
        this.proctor = new Proctor()
    }

    async initializeApplication(args) {
        const application = new Application()
        await this.procurer.loadResources(application, args)
        await this.builder.buildTransmissions(application)
        return application
    }

    async applyToTarget(application, target) {
        await this.runner.execute(application, target)
    }
}

export default Director

================
File: src/core/ModuleLoader.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import logger from '../utils/Logger.js'

class ModuleLoader {
    constructor(classpath) {
        this.classpath = classpath















        this.moduleCache = new Map()
        logger.debug(`ModuleLoader initialized with paths :\n${this.classpath}`)
    }

    async loadModule(moduleName) {

        logger.debug(`\n\nModuleLoader.loadModule, moduleName = ${moduleName}`)
        logger.debug(`ModuleLoader.loadModule looking for module in classpath ${this.classpath} `)


        if (this.moduleCache.has(moduleName)) {
            logger.debug(`Retrieved ${moduleName} from cache`)
            return this.moduleCache.get(moduleName)
        }


        for (const basePath of this.classpath) {
            try {
                const fullPath = path.join(basePath, `${moduleName}.js`)
                logger.debug(`Trying path: ${fullPath}`)

                const module = await import(fullPath)
                this.moduleCache.set(moduleName, module)
                logger.debug(`Successfully loaded ${moduleName} from ${fullPath}`)
                return module
            } catch (error) {
                logger.debug(`Failed to load from ${basePath}: ${error.message}`)
                continue
            }
        }

        throw new Error(`Module ${moduleName} not found in paths: ${this.classpath.join(', ')}`)
    }

    clearCache() {
        this.moduleCache.clear()
    }

    addPath(newPath) {
        if (typeof newPath !== 'string') {
            throw new TypeError('Path must be a string')
        }
        this.classpath.push(path.normalize(newPath))
    }
}
export default ModuleLoader

================
File: src/core/ModuleLoaderFactory.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import logger from '../utils/Logger.js'
import ModuleLoader from './ModuleLoader.js'

class ModuleLoaderFactory {
    static instance = null;

    static createModuleLoader(classpath) {
        const __filename = fileURLToPath(import.meta.url)
        const __dirname = path.dirname(__filename)











        if (!ModuleLoaderFactory.instance) {
            ModuleLoaderFactory.instance = new ModuleLoader(classpath)
        }

        return ModuleLoaderFactory.instance
    }

    static createApplicationLoader(appPath) {
        logger.debug(`\nModuleLoaderFactory.createApplicationLoader called with ${appPath}`)
        if (!appPath) {
            throw new Error('Application path is required')
        }
        const __filename = fileURLToPath(import.meta.url)
        const __dirname = path.dirname(__filename)

        const normalizedPath = path.resolve(process.cwd(), appPath)

        const appProcessorsPath = normalizedPath
        const corePath = path.resolve(__dirname, '../processors')

        logger.debug(`ModuleLoaderFactory creating loader with paths:
      App: ${appProcessorsPath}
      Core: ${corePath}`)

        return this.createModuleLoader([appProcessorsPath, corePath])
    }

    static clearInstance() {
        ModuleLoaderFactory.instance = null
    }
}
export default ModuleLoaderFactory

================
File: src/core/Procurer.js
================
class Procurer {
    constructor() {
        this.moduleLoader = ModuleLoaderFactory.createModuleLoader()
    }

    async loadResources(application, args) {
        const config = await this.loadConfig(args.configPath)
        const manifest = await this.loadManifest(args.target)
        application.config = config
        application.manifest = manifest
    }
}

export default Procurer

================
File: src/core/TransmissionBuilder.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'

import ns from '../utils/ns.js'
import GrapoiHelpers from '../utils/GrapoiHelpers.js'
import logger from '../utils/Logger.js'


import AbstractProcessorFactory from "../processors/base/AbstractProcessorFactory.js"
import Transmission from '../engine/Transmission.js'




class TransmissionBuilder {

  constructor(moduleLoader) {
    this.moduleLoader = moduleLoader
  }

  static async build(transmissionConfigFile, processorsConfigFile, moduleLoader) {
    const transmissionConfig = await TransmissionBuilder.readDataset(transmissionConfigFile)
    const processorsConfig = await TransmissionBuilder.readDataset(processorsConfigFile)

    const builder = new TransmissionBuilder(moduleLoader)
    return builder.buildTransmissions(transmissionConfig, processorsConfig)
  }

  async buildTransmissions(transmissionConfig, processorsConfig) {
    const poi = grapoi({ dataset: transmissionConfig })
    const transmissions = []

    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.trm.Transmission)) {
        const transmissionID = q.subject

        transmissions.push(await this.constructTransmission(transmissionConfig, transmissionID, processorsConfig))
      }
    }
    return transmissions
  }

  async constructTransmission(transmissionConfig, transmissionID, processorsConfig) {
    processorsConfig.whiteboard = {}

    const transmission = new Transmission()
    transmission.id = transmissionID.value
    transmission.label = ''

    const transPoi = grapoi({ dataset: transmissionConfig, term: transmissionID })

    // TODO has grapoi got a first/single property method?
    for (const quad of transPoi.out(ns.rdfs.label).quads()) {
      transmission.label = quad.object.value
    }
    logger.log('\n+ ***** Construct Transmission : ' + transmission.label + ' <' + transmission.id + '>')

    let previousName = "nothing"

    // grapoi probably has a built-in for all this
    const pipenodes = GrapoiHelpers.listToArray(transmissionConfig, transmissionID, ns.trm.pipe)
    await this.createNodes(transmission, pipenodes, transmissionConfig, processorsConfig) // was await, bad Claude
    //    this.createNodes(transmission, pipenodes, transmissionConfig, processorsConfig); // was await, bad Claude
    this.connectNodes(transmission, pipenodes)
    return transmission
  }

  async createNodes(transmission, pipenodes, transmissionConfig, processorsConfig) {
    for (let i = 0; i < pipenodes.length; i++) {
      let node = pipenodes[i]
      let processorName = node.value

      if (!transmission.get(processorName)) {
        let np = rdf.grapoi({ dataset: transmissionConfig, term: node })
        let processorType = np.out(ns.rdf.type).term
        let processorConfig = np.out(ns.trm.configKey).term

        try {
          let name = ns.getShortname(processorName)
          let type = ns.getShortname(processorType.value)

          logger.log("| Create processor :" + name + " of type :" + type)
          let processor = await this.createProcessor(processorType, processorsConfig) // was await
          processor.id = processorName
          processor.type = processorType
          processor.transmission = transmission

          //    logger.log("| processorConfig :" + processorConfig)
          //  logger.reveal(processorConfig)
          if (processorConfig) {
            processor.configKey = processorConfig
          }
          transmission.register(processorName, processor)
        } catch (err) {
          logger.error('-> Can\'t resolve ' + processorName + ' (check transmission.ttl for typos!)\n')
          logger.error(err)
        }
      }
    }
  }

  async connectNodes(transmission, pipenodes) {
    for (let i = 0; i < pipenodes.length - 1; i++) {
      let leftNode = pipenodes[i]
      let leftProcessorName = leftNode.value
      let rightNode = pipenodes[i + 1]
      let rightProcessorName = rightNode.value
      logger.log("  > Connect #" + i + " [" + ns.getShortname(leftProcessorName) + "] => [" + ns.getShortname(rightProcessorName) + "]")
      transmission.connect(leftProcessorName, rightProcessorName)
    }
  }

  async createProcessor(type, config) {


    const coreProcessor = AbstractProcessorFactory.createProcessor(type, config)
    if (coreProcessor) {
      return coreProcessor
    }

    logger.debug(`TransmissionBuilder, core processor not found for ${type.value}. Trying remote module loader...`)


    try {
      const shortName = type.value.split('/').pop()
      logger.debug(`TransmissionBuilder, loading module: ${shortName}`)
      logger.log(this.moduleLoader)
      const ProcessorClass = await this.moduleLoader.loadModule(shortName)

      logger.debug(`Module loaded successfully: ${shortName}`)
      return new ProcessorClass.default(config)
    } catch (error) {

      logger.error(`TransmissionBuilder, failed to load processor ${type.value}`)
      process.exit(1)
    }
  }







  static async readDataset(filename) {
    const stream = fromFile(filename)
    const dataset = await rdf.dataset().import(stream)
    return dataset
  }

  static async writeDataset(dataset, filename) {
    await toFile(dataset.toStream(), filename)
  }


}

export default TransmissionBuilder

================
File: src/core/WorkerPool.js
================
import { Worker } from 'worker_threads'

class WorkerPool {
    constructor(module, size) {
        this.workers = [];
        this.queue = [];
        for (let i = 0; i < size; i++) {
            const worker = new Worker(module);
            worker.on('message', () => {

                this.markWorkerIdle(worker);
            });
            this.workers.push({ worker, busy: false });
        }
    }

    enqueueMessage(message) {
        this.queue.push(message);
        this.dispatch();
    }

    dispatch() {
        const idleWorkerWrapper = this.workers.find(wrapper => !wrapper.busy);
        if (idleWorkerWrapper && this.queue.length) {
            const message = this.queue.shift();
            idleWorkerWrapper.busy = true;
            idleWorkerWrapper.worker.postMessage(message);
        }
    }

    markWorkerIdle(workerWrapper) {
        workerWrapper.busy = false;
        this.dispatch();
    }
}

================
File: src/engine/Application.js
================
class Application {
    constructor() {
        this.transmissions = new Map()
        this.config = null
        this.manifest = null
    }

    addTransmission(id, transmission) {
        this.transmissions.set(id, transmission)
    }
}

================
File: src/engine/Connector.js
================
import { EventEmitter } from 'events'
import logger from '../utils/Logger.js'
import footpath from '../utils/footpath.js'

class Connector extends EventEmitter {


    constructor(fromName, toName) {
        super()
        this.fromName = fromName
        this.toName = toName
    }

    connect(processors) {
        logger.log(`Connector.connect this.fromName = ${this.fromName} this.toName =  ${this.toName}`)
        let fromProcessor = processors[this.fromName]
        let toProcessor = processors[this.toName]

        if (!fromProcessor) {
            throw new Error(`\nMissing processor : ${this.fromName}, going to ${this.toName} \n(check for typos in transmissions.ttl)\n`)
        }

        fromProcessor.on('message', (message) => {
            var tags = ''
            //     if (toProcessor.message) {
            tags = ' [' + fromProcessor.message.tags + '] '
            toProcessor.tags = tags

            const thisTag = footpath.urlLastPart(this.toName)
            logger.log("| Running >>> : " + tags + thisTag + " a " + toProcessor.constructor.name)

            toProcessor.receive(message)
        })

    }


}

export default Connector

================
File: src/engine/Transmission.js
================
import logger from '../utils/Logger.js'
import Connector from './Connector.js'

class Transmission {
  constructor() {
    this.processors = {}
    this.connectors = []

  }

  register(processorName, instance) {
    this.processors[processorName] = instance

  }

  get(processorName) {
    return this.processors[processorName]
  }

  connect(fromProcessorName, toProcessorName) {
    logger.log(`Transmission.connect from ${fromProcessorName} to ${fromProcessorName}`)
    let connector = new Connector(fromProcessorName, toProcessorName)
    this.connectors.push(connector)
    connector.connect(this.processors)
  }




  async process(message) {
    logger.log('\n+ ***** Execute Transmission : ' + this.label + ' <' + this.id + '>')
    const processorName = this.connectors[0]?.fromName || Object.keys(this.processors)[0]
    let processor = this.get(processorName)
    if (processor) {
      logger.log("| Running : " + processorName + " a " + processor.constructor.name)
      await processor.receive(message)
    } else {
      logger.error("No valid processor found to execute")
    }
  }






  toString() {
    let description = 'Transmission Structure:\n'


    description += 'Processors:\n'
    Object.keys(this.processors).forEach(processorName => {
      description += `  - ${processorName}\n`
    })


    description += 'Connectors:\n'
    this.connectors.forEach((connector, index) => {
      description += `  - Connector ${index + 1}: ${connector.fromName} -> ${connector.toName}\n`
    })

    return description
  }
}

export default Transmission

================
File: src/processors/base/AbstractProcessorFactory.js
================
import SystemProcessorsFactory from '../system/SystemProcessorsFactory.js'
import TestProcessorsFactory from '../test/TestProcessorsFactory.js'
import FsProcessorsFactory from '../fs/FsProcessorsFactory.js'
import MarkupProcessorsFactory from '../markup/MarkupProcessorsFactory.js'
import UtilProcessorsFactory from '../util/UtilProcessorsFactory.js'
import TextProcessorsFactory from '../text/TextProcessorsFactory.js'
import ProtocolsProcessorsFactory from '../protocols/ProtocolsProcessorsFactory.js'
import RDFProcessorsFactory from '../rdf/RDFProcessorsFactory.js'
import PostcraftProcessorsFactory from '../postcraft/PostcraftProcessorsFactory.js'
import FlowProcessorsFactory from '../flow/FlowProcessorsFactory.js'
import StagingProcessorsFactory from '../staging/StagingProcessorsFactory.js'
import GitHubProcessorsFactory from '../github/GitHubProcessorsFactory.js'
import JSONProcessorsFactory from '../json/JSONProcessorsFactory.js'
import PackerProcessorsFactory from '../packer/PackerProcessorsFactory.js'



import UnsafeProcessorsFactory from '../unsafe/UnsafeProcessorsFactory.js'
import HttpProcessorsFactory from '../http/HttpProcessorsFactory.js'
import McpProcessorsFactory from '../mcp/McpProcessorsFactory.js'
import XmppProcessorsFactory from '../xmpp/XmppProcessorsFactory.js'

class AbstractProcessorFactory {




    static createProcessor(type, config) {

        var processor = UnsafeProcessorsFactory.createProcessor(type, config)
        if (processor) return processor
        var processor = HttpProcessorsFactory.createProcessor(type, config)
        if (processor) return processor
        var processor = McpProcessorsFactory.createProcessor(type, config)
        if (processor) return processor
        var processor = XmppProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = TestProcessorsFactory.createProcessor(type, config)
        if (processor) return processor
        var processor = UtilProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = MarkupProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = TextProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = ProtocolsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = RDFProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = PostcraftProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = SystemProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FlowProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = GitHubProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = StagingProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = JSONProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = PackerProcessorsFactory.createProcessor(type, config);
        if (processor) return processor;

    }
}

export default AbstractProcessorFactory

================
File: src/processors/base/Processor.js
================
import logger from '../../utils/Logger.js'
import { EventEmitter } from 'events'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'





class Processor extends EventEmitter {





    constructor(config) {
        super()
        this.config = config
        this.messageQueue = []
        this.processing = false
        this.done = false
        this.outputs = []
    }

    async preProcess(message) {
        logger.debug("Processor.preProcess")
        return







        const processorPoi = rdf.grapoi({ dataset: this.config, term: this.configKey })




        if (this.configKey.value === ns.trm.describe.value) {
            this.describe()
        }



    }

    async postProcess(message) {
        logger.debug("Processor.postProcess")
        return
    }

    describe() {
        logger.log('describe')
        const inputs = this.getInputKeys()
        const outputs = this.getOutputKeys()
        for (var input of inputs) {
            logger.log('input = ' + input)
            logger.log(this.message[input] + ' = ' + this.message[input])
        }
        for (var output of outputs) {
            logger.log('output = ' + output)
            logger.log(this.message[output] + ' = ' + this.message[output])
        }
    }





















    getProperty(property, fallback) {
        const shortName = ns.getShortname(property)
        if (this.message[shortName]) return this.message[shortName]


        const maybe = this.getPropertyFromMyConfig(property)
        if (maybe) return maybe
        return fallback
    }


    getMyConfigNode() {
        const dataset = this.config
        const configNode = grapoi({ dataset, term: this.configKey }).in()
        return configNode.term
    }

    getMyPoi() {
        const dataset = this.config
        const myConfigNode = this.getMyConfigNode()
        const poi = grapoi({ dataset: dataset, term: myConfigNode })
        return poi
    }

    async addPropertyToMyConfig(predicate, value) {
        logger.log('addPropertyToMyConfig predicate = ' + predicate)
        logger.log('addPropertyToMyConfig value = ' + value)
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('addPropertyToMyConfig  myConfigNode.value = ' + myConfigNode.value)
        const dataset = this.config
        dataset.add(myConfigNode, predicate, value)
        this.config = dataset
    }

    showMyConfig() {
        const poi = this.getMyPoi()
        logger.log('POI = ')
        logger.poi(poi)
    }

    getPropertyFromMyConfig(property) {
        if (this.config.simples) {

            const shortName = ns.getShortname(property)
            logger.debug(`Processor (simples), property = ${shortName}`)
            const value = this.config[shortName]
            logger.debug(`Processor (simples), value = ${value}`)
            return value
        }
        const poi = this.getMyPoi()
        try {
            return poi.out(property).term.value
        } catch (err) {
            logger.debug('* Warn : Processor.getPropertyFromMyConfig(), property not defined : ' + property)

            return undefined
        }
    }

    async deletePropertyFromMyConfig(predicate, value) {
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('DELETING FROM ' + s)
        const dataset = this.config
        dataset.delete(myConfigNode, predicate, value)
        this.config = dataset
    }






    async receive(message) {
        await this.enqueue(message)
    }






    async enqueue(message) {
        this.messageQueue.push({ message })
        if (!this.processing) {
            this.executeQueue()
        }
    }

    cloneContext(baseContext) {
        const message = structuredClone(baseContext)
        if (baseContext.dataset) {


            message.dataset = baseContext.dataset
        }
        return message
    }





    async executeQueue() {
        this.processing = true
        while (this.messageQueue.length > 0) {
            let { message } = this.messageQueue.shift()

            message = this.cloneContext(message)
            this.message = message




            this.addTag(message)

            await this.preProcess(message)

            await this.process(message)
            await this.postProcess(message)
        }
        this.processing = false
    }

    addTag(message) {
        const tag = this.getTag()
        if (!message.tags) {
            message.tags = tag
            return
        }
        message.tags = message.tags + '.' + tag

    }

    getTag() {
        return footpath.urlLastPart(this.id)
    }







    async process(message) {
        throw new Error('execute method not implemented')
    }







    async doEmit(message) {
        this.emit(message)
    }

















    emit(event, message) {



        super.emit(event, message)
        return message


    }

    getOutputs() {
        const results = this.outputs
        this.outputs = []
        return results
    }
}

export default Processor

================
File: src/processors/flow/DeadEnd.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class DeadEnd extends Processor {

    async process(message) {
        logger.log('DeadEnd at [' + message.tags + '] ' + this.getTag())
    }

}
export default DeadEnd

================
File: src/processors/flow/FlowProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import ForEach from './ForEach.js'
import Ping from './Ping.js'
import NOP from '../flow/NOP.js'
import DeadEnd from '../flow/DeadEnd.js'
import Halt from '../flow/Halt.js'
import Unfork from '../flow/Unfork.js'
import Fork from '../flow/Fork.js'

class FlowProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.ForEach)) {
            logger.debug('FlowProcessorsFactory: Creating ForEach processor')
            return new ForEach(config)
        }
        if (type.equals(ns.t.Ping)) {
            logger.debug('FlowProcessorsFactory: Creating Ping processor')
            return new Ping(config)
        }
        if (type.equals(ns.t.NOP)) {
            return new NOP(config)
        }
        if (type.equals(ns.t.DeadEnd)) {
            return new DeadEnd(config)
        }
        if (type.equals(ns.t.Halt)) {
            return new Halt(config)
        }
        if (type.equals(ns.t.Fork)) {
            return new Fork(config)
        }
        if (type.equals(ns.t.Unfork)) {
            return new Unfork(config)
        }
        return false
    }
}

export default FlowProcessorsFactory

================
File: src/processors/flow/ForEach.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ForEach extends Processor {
    constructor(config) {
        super(config)
    }

    async process(message) {

        logger.debug('ForEach execute method called')

        if (!message.foreach || !Array.isArray(message.foreach)) {
            logger.error('ForEach: Invalid or missing foreach array in message')
            message.foreach = ["testing-testing", "one", "two", "three"]

        }

        for (const item of message.foreach) {
            const clonedMessage = structuredClone(message)
            clonedMessage.currentItem = item
            delete clonedMessage.foreach

            logger.debug(`ForEach: Emitting message for item: ${item}`)
            this.emit('message', clonedMessage)
        }

        logger.debug('ForEach: Finished processing all items')
    }
}
export default ForEach

================
File: src/processors/flow/Fork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'




class Fork extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {

        const nForks = message.nForks || 2

        logger.debug('forks = ' + nForks)

        for (let i = 0; i < nForks; i++) {
            var messageClone = structuredClone(message)
            messageClone.forkN = i
            logger.debug('--- emit --- ' + i)
            this.emit('message', messageClone)
        }

        message.done = true

        return this.emit('message', message)

    }

}

export default Fork

================
File: src/processors/flow/Halt.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class Halt extends Processor {

    process(message) {
        logger.log('\n************************************************************************')
        logger.log('*** << Thou Hast Summoned HALT, the Mighty Stopper of All Things  >> ***')
        logger.log('*** <<                   ~~~ ALL IS GOOD ~~~                      >> ***')
        logger.log('*** <<                     Have a nice day!                       >> ***')
        logger.log('************************************************************************\n')
        logger.log('*** Transmission was : ' + message.tags)
        logger.log('*** Context now : ')
        logger.reveal(message)
        process.exit()
    }
}

export default Halt

================
File: src/processors/flow/NOP.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'

class NOP extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        const done = message.done ? `DONE` : `NOT DONE`
        logger.log(`\nNOP at [${message.tags}] ${this.getTag()} (${done})`)

        return this.emit('message', message)
    }

    double(string) {
        return string + string
    }
}
export default NOP

================
File: src/processors/flow/Ping.js
================
import { Worker } from 'worker_threads';
import path from 'path';
import logger from '../../utils/Logger.js';
import Processor from '../base/Processor.js';
import ns from '../../utils/ns.js';

class Ping extends Processor {
    constructor(config) {
        super(config);
        this.worker = null;
        this.pingConfig = {
            interval: this.getPropertyFromMyConfig(ns.trm.interval) || 5000,
            count: this.getPropertyFromMyConfig(ns.trm.count) || 0,
            payload: this.getPropertyFromMyConfig(ns.trm.payload) || 'ping',
            killSignal: this.getPropertyFromMyConfig(ns.trm.killSignal) || 'STOP',
            retryAttempts: this.getPropertyFromMyConfig(ns.trm.retryAttempts) || 3,
            retryDelay: this.getPropertyFromMyConfig(ns.trm.retryDelay) || 1000
        };
    }

    async process(message) {
        try {

            if (message.kill === this.pingConfig.killSignal) {
                await this.shutdown();
                return this.emit('message', {
                    ...message,
                    pingStatus: 'stopped',
                    timestamp: Date.now()
                });
            }

            if (this.worker) {
                logger.warn('Ping worker already running, ignoring start request');
                return;
            }

            let retryCount = 0;
            const startWorker = async () => {
                try {
                    this.worker = new Worker(
                        path.join(process.cwd(), 'src/processors/flow/PingWorker.js')
                    );

                    this.worker.on('message', (msg) => {
                        switch (msg.type) {
                            case 'ping':
                                this.emit('message', {
                                    ...message,
                                    ping: {
                                        count: msg.count,
                                        timestamp: msg.timestamp,
                                        payload: msg.payload,
                                        status: 'running'
                                    }
                                });
                                break;
                            case 'complete':
                                this.emit('message', {
                                    ...message,
                                    pingComplete: true,
                                    timestamp: Date.now()
                                });
                                break;
                            case 'error':
                                this.handleWorkerError(msg.error, startWorker, retryCount);
                                break;
                        }
                    });

                    this.worker.on('error', (error) => {
                        this.handleWorkerError(error, startWorker, retryCount);
                    });

                    this.worker.on('exit', (code) => {
                        if (code !== 0) {
                            this.handleWorkerError(
                                new Error(`Worker stopped with exit code ${code}`),
                                startWorker,
                                retryCount
                            );
                        }
                        this.worker = null;
                    });

                    this.worker.postMessage({
                        type: 'start',
                        config: this.pingConfig
                    });

                } catch (error) {
                    this.handleWorkerError(error, startWorker, retryCount);
                }
            };

            await startWorker();

            return new Promise((resolve) => {
                this.worker.on('exit', () => {
                    resolve(message);
                });
            });

        } catch (error) {
            logger.error(`Failed to start ping processor: ${error}`);
            throw error;
        }
    }

    async handleWorkerError(error, retryFn, retryCount) {
        logger.error(`Ping worker error: ${error}`);

        if (retryCount < this.pingConfig.retryAttempts) {
            retryCount++;
            logger.info(`Retrying ping worker (attempt ${retryCount}/${this.pingConfig.retryAttempts})`);
            setTimeout(retryFn, this.pingConfig.retryDelay);
        } else {
            logger.error('Max retry attempts reached, stopping ping worker');
            this.emit('error', error);
            await this.shutdown();
        }
    }

    async shutdown() {
        if (this.worker) {
            this.worker.postMessage({ type: 'stop' });
            this.worker = null;
        }
    }
}

export default Ping;

================
File: src/processors/flow/Unfork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import DeadEnd from './DeadEnd.js'






class Unfork extends Processor {

    constructor(config) {
        super(config)







    }

    async process(message) {
        logger.setLogLevel("debug")
        logger.debug('Unfork ----')
        if (message.done) {
            logger.debug(' - Unfork passing message')
            message.done = false
            return this.emit('message', message)
        } else {
            logger.debug(' - Unfork terminating pipe')
            return
        }
    }
}
export default Unfork

================
File: src/processors/fs/DirWalker.js
================
import { readdir } from 'fs/promises';

import path from 'path';
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js';
import Processor from '../base/Processor.js';

class DirWalker extends Processor {
    constructor(config) {
        super(config);
        this.includeExtensions = ['.md'];

        this.excludePrefixes = ['_', '.'];
    }

    async process(message) {

        logger.debug('\nDirWalker.process');


        message.counter = 0;
        message.slugs = [];
        message.done = false;

        const sourceDirProperty = this.getProperty(ns.trm.sourceDir)
        var sourceDir = sourceDirProperty

        var includeExtensions = this.getProperty(ns.trm.includeExtensions)
        if (includeExtensions) {
            logger.debug(`includeExtensions = ${includeExtensions}`)
            includeExtensions = includeExtensions.replaceAll('\'', '"')
            logger.debug(`includeExtensions = ${includeExtensions}`)
            this.includeExtensions = JSON.parse(includeExtensions);
        }


        if (!message.sourceDir) message.sourceDir = sourceDir
        logger.log(sourceDir)


















        logger.debug('\n\nDirWalker, message.targetPath = ' + message.targetPath)
        logger.debug('DirWalker, message.rootDir = ' + message.rootDir)
        logger.debug('DirWalker, message.sourceDir = ' + message.sourceDir)

        let dirPath
        if (path.isAbsolute(sourceDir)) {
            dirPath = sourceDir
        } else {
            if (message.targetPath) {
                dirPath = path.join(message.targetPath, sourceDir)
            } else {
                dirPath = path.join(message.rootDir, sourceDir)
            }
        }
        logger.debug('DirWalker, dirPath = ' + dirPath)



        if (!message.sourceDir) {
            message.sourceDir = sourceDirProperty
        }

        await this.walkDirectory(dirPath, message);


        const finalMessage = structuredClone(message);
        finalMessage.done = true;

        return this.emit('message', finalMessage);
    }

    async walkDirectory(dir, baseMessage) {

        logger.debug(`DirWalker.walkDirectory, dir = ${dir}`)

        const entries = await readdir(dir, { withFileTypes: true });

        for (const entry of entries) {
            const fullPath = path.join(dir, entry.name);
            logger.debug(`DirWalker.walkDirectory, fullPath = ${fullPath}`)
            if (entry.isDirectory() && !this.excludePrefixes.includes(entry.name[0])) {
                await this.walkDirectory(fullPath, baseMessage);
            } else if (entry.isFile()) {
                const extension = path.extname(entry.name);
                const prefix = entry.name[0];

                if (!this.excludePrefixes.includes(prefix) &&
                    this.includeExtensions.includes(extension)) {

                    const message = structuredClone(baseMessage);
                    message.filename = entry.name;





                    message.subdir = path.dirname(path.relative(message.targetPath, fullPath)).split(path.sep)[1];


                    message.fullPath = fullPath;
                    message.filepath = path.relative(baseMessage.targetPath || baseMessage.rootDir, fullPath);
                    message.done = false;
                    message.counter++;

                    logger.debug(`DirWalker emitting file:
                            filename: ${message.filename}
                            fullPath: ${message.fullPath}
                            filepath: ${message.filepath}`);

                    this.emit('message', message);
                }
            }
        }



    }
}

export default DirWalker;

================
File: src/processors/fs/FileCopy.js
================
import { copyFile, mkdir, readdir, stat } from 'node:fs/promises'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../base/Processor.js'


class FileCopy extends Processor {
    constructor(config) {
        super(config)
    }





    async process(message) {


        logger.debug("message.rootDir = " + message.rootDir)
        var source, destination


        if (this.configKey === 'undefined') {
            logger.debug('FileCopy: using message.source and message.destination')
            source = message.source
            destination = message.destination
        } else {
            logger.debug(`FileCopy: using configKey ${this.configKey.value}`)
            source = this.getPropertyFromMyConfig(ns.trm.source)
            destination = this.getPropertyFromMyConfig(ns.trm.destination)
            if (message.targetPath) {
                source = path.join(message.targetPath, source)
                destination = path.join(message.targetPath, destination)
            } else {
                source = path.join(message.rootDir, source)
                destination = path.join(message.rootDir, destination)
            }

        }

        logger.debug(`Source: ${source}`)
        logger.debug(`Destination: ${destination}`)

        try {
            await this.ensureDirectoryExists(path.dirname(destination))
            const sourceStat = await stat(source)

            if (sourceStat.isFile()) {
                logger.debug(`Copying file from ${source} to ${destination}`)
                await copyFile(source, destination)
            } else if (sourceStat.isDirectory()) {
                logger.debug(`Copying directory from ${source} to ${destination}`)
                await this.copyDirectory(source, destination)
            }
        } catch (err) {
            logger.error(`Error in FileCopy: ${err.message}`)
            logger.error(`Source: ${source}`)
            logger.error(`Destination: ${destination}`)
        }

        return this.emit('message', message)
    }





    async ensureDirectoryExists(dirPath) {
        logger.debug(`Ensuring directory exists: ${dirPath}`)
        try {
            await mkdir(dirPath, { recursive: true })
            logger.debug(`Directory created/ensured: ${dirPath}`)
        } catch (err) {
            logger.debug(`Error creating directory ${dirPath}: ${err.message}`)
            throw err
        }
    }






    async copyDirectory(source, destination) {
        logger.debug(`Copying directory: ${source} to ${destination}`)
        try {
            await this.ensureDirectoryExists(destination)
            const entries = await readdir(source, { withFileTypes: true })

            for (const entry of entries) {
                const srcPath = path.join(source, entry.name)
                const destPath = path.join(destination, entry.name)

                logger.debug(`Processing: ${srcPath} to ${destPath}`)

                if (entry.isDirectory()) {
                    await this.copyDirectory(srcPath, destPath)
                } else {
                    await copyFile(srcPath, destPath)
                    logger.debug(`File copied: ${srcPath} to ${destPath}`)
                }
            }
        } catch (err) {
            logger.debug(`Error in copyDirectory: ${err.message}`)
            throw err
        }
    }
}

export default FileCopy

================
File: src/processors/fs/FilenameMapper.js
================
import Processor from '../base/Processor.js';
import path from 'path';
import logger from '../../utils/Logger.js';

class FilenameMapper extends Processor {
    constructor(config) {
        super(config);
        this.extensions = {
            html: '.mm.html',
            svg: '.mm.svg'
        };
    }

    async process(message) {
        const format = message.format || 'html';
        const extension = this.extensions[format];

        if (!extension) {
            throw new Error(`Unknown format: ${format}`);
        }

        const parsedPath = path.parse(message.filepath);
        message.filepath = path.join(
            parsedPath.dir,
            parsedPath.name + extension
        );

        return this.emit('message', message);
    }
}

export default FilenameMapper;

================
File: src/processors/fs/FileReader.js
================
import { readFile } from 'node:fs/promises';
import { access, constants } from 'node:fs';
import path from 'path';
import logger from '../../utils/Logger.js';
import ns from '../../utils/ns.js';
import Processor from '../base/Processor.js';

class FileReader extends Processor {
    constructor(config) {
        super(config);
    }

    async process(message) {







        logger.debug(`\n\nFileReader.process(), this.getTag() =  ${this.getTag()}`);

        logger.debug(`\n\n1 FileReader.process(), message.fullPath =  ${message.fullPath}`);
        logger.debug(`FileReader.process(), message.filepath = ${message.filepath}`);
        logger.debug(`FileReader.process(), message.targetPath = ${message.targetPath}`);

        try {







            var filePath


            if (!message.fullPath) {
                if (!message.filepath) return
                if (message.targetPath && !path.isAbsolute(message.filepath)) {
                    logger.debug(`\n\n2 FileReader.process(), message.filepath = ${message.filepath}`);
                    logger.debug(`FileReader.process(), message.targetPath = ${message.targetPath}`);
                    filePath = path.join(message.targetPath, message.filepath)
                } else {
                    filePath = message.filepath
                }
            } else {
                filePath = message.fullPath
            }

            logger.debug(`\n\n3 FileReader.process(), reading file: ${filePath}`);
            logger.debug(`FileReader.process(), process.cwd() = ${process.cwd()}`)

            await new Promise((resolve, reject) => {
                access(filePath, constants.R_OK, (err) => {
                    if (err) {
                        reject(new Error(`File ${filePath} is not readable: ${err.message}`));
                    }
                    resolve();
                });
            });


            const content = await readFile(filePath, 'utf8');
            message.content = content;

            logger.debug(`FileReader successfully read file: ${filePath}`);
            return this.emit('message', message);

        } catch (err) {
            logger.error('FileReader error:', err);
            throw err;
        }
    }
}

export default FileReader;

================
File: src/processors/fs/FileRemove.js
================
import { unlink, readdir, stat, rm } from 'node:fs/promises'
import path from 'path'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class FileRemove extends Processor {
    constructor(config) {
        super(config)
    }





    async process(message) {



        this.ignoreDotfiles = true

        var target




        if (this.configKey === 'undefined') {
            logger.debug('FileRemove no configKey from transmission, using message.target')
            target = message.target
        } else {
            logger.debug('FileRemove this.configKey = ' + this.configKey.value)
            target = this.getPropertyFromMyConfig(ns.trm.target)

            target = path.join(message.rootDir, target)
        }

        logger.debug('FileRemove, target = ' + target)
        try {
            const removeStat = await stat(target)

            if (removeStat.isFile()) {
                await this.removeFile(target)
            } else if (removeStat.isDirectory()) {
                await this.removeDirectoryContents(target)
            }
        } catch (err) {

            logger.debug('FileRemove, target stat caused err : ' + target)
        }

        return this.emit('message', message)
    }





    async removeFile(filePath) {
        await unlink(filePath)
    }





    async removeDirectoryContents(dirPath) {
        logger.debug('FileRemove, dirPath = ' + dirPath)
        const entries = await readdir(dirPath, { withFileTypes: true })

        for (const entry of entries) {
            if (this.ignoreDotfiles && (entry.name.charAt(0) === ".")) {
                continue
            }
            const entryPath = path.join(dirPath, entry.name)

            if (entry.isDirectory()) {
                await this.removeDirectoryContents(entryPath)
            } else {
                await unlink(entryPath)
            }
        }
    }
}

export default FileRemove

================
File: src/processors/fs/FileWriter.js
================
import path from 'path'
import { access, constants } from 'node:fs'
import ns from '../../utils/ns.js'
import { writeFile } from 'node:fs/promises'
import { dirname, join } from 'node:path'
import { mkdir, mkdirSync } from 'node:fs'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'














class FileWriter extends Processor {





    constructor(config) {
        super(config)
    }





    async process(message) {
        logger.setLogLevel('debug')
        logger.debug(`\n\nFileWriter.process, message.done = ${message.done}`)
        if (message.done) {
            message.done = false
            return this.emit('message', message)
        }

        if (message.dump) {


            const filename = 'message.json'
            const f = path.join(message.dataDir, filename)
            const content = JSON.stringify(message)

            access(f, constants.W_OK, (err) => {
                if (err) {
                    logger.error(`FileWriter error : ${f} is not writable.`)
                    logger.reveal(message)
                }
            })
            return this.doWrite(f, content, message)
        }

        logger.debug("Filewriter, message.filepath = " + message.filepath)

        var destinationFile = this.getProperty(ns.trm.destinationFile)
        var filepath = message.filepath
        if (message.subdir) {
            filepath = path.join(message.subdir, filepath)
        }

        logger.debug(`Filewriter, 1 filepath = ${filepath}`)


        if (!destinationFile) {
            var targetDir = message.targetDir ?
                message.targetDir : this.getProperty(ns.trm.targetDir)
            targetDir = targetDir ? targetDir : '.'

            filepath = path.join(targetDir, filepath)
        }

        if (!path.isAbsolute(filepath)) {
            filepath = path.join(message.targetPath, filepath)
        }

        logger.debug(`Filewriter, filepath = ${filepath}`)
        const dirName = dirname(filepath)
        logger.debug("Filewriter, dirName = " + dirName)














        var content = message.content



        this.mkdirs(dirName)

        return await this.doWrite(filepath, content, message)
    }

    async doWrite(f, content, message) {
        logger.log(' - FileWriter writing : ' + f)
        await writeFile(f, content)
        return this.emit('message', message)
    }

    mkdirs(dir) {
        mkdirSync(dir, { recursive: true })





    }
}

export default FileWriter

================
File: src/processors/fs/FsProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import DirWalker from './DirWalker.js'
import FileReader from './FileReader.js'
import FileWriter from './FileWriter.js'
import FileCopy from './FileCopy.js'
import FileRemove from './FileRemove.js'
import FilenameMapper from './FilenameMapper.js'

class FsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.DirWalker)) {
            return new DirWalker(config)
        }
        if (type.equals(ns.t.FileReader)) {
            return new FileReader(config)
        }
        if (type.equals(ns.t.FileWriter)) {
            return new FileWriter(config)
        }
        if (type.equals(ns.t.FileCopy)) {
            return new FileCopy(config)
        }
        if (type.equals(ns.t.FileRemove)) {
            return new FileRemove(config)
        }
        if (type.equals(ns.t.FilenameMapper)) {
            return new FilenameMapper(config)
        }
        return false
    }
}

export default FsProcessorsFactory

================
File: src/processors/github/GitHubList_no-pag.js
================
import { Octokit } from '@octokit/rest'
import dotenv from 'dotenv'
import Processor from '../base/Processor.js'
import logger from '../../utils/Logger.js'

dotenv.config({ path: './trans-apps/applications/git-apps/.env' })

class GitHubList extends Processor {
    constructor(config) {
        super(config)
        logger.debug('GitHubList constructor called')
        this.octokit = new Octokit({ auth: process.env.GITHUB_TOKEN })
        logger.debug('Octokit instance created')
    }

    async process(message) {
        logger.debug('GitHubList execute method called')
        logger.debug('Input message:', JSON.stringify(message, null, 2))

        if (!message.github || !message.github.name) {
            logger.error('GitHub username not provided in the message')
            throw new Error('GitHub username not provided in the message')
        }

        const username = message.github.name
        logger.debug(`Fetching repositories for username: ${username}`)

        try {
            logger.debug('Calling GitHub API')
            const { data } = await this.octokit.repos.listForUser({ username })
            logger.debug(`Fetched ${data.length} repositories`)

            const repositories = data.map(repo => repo.name)
            logger.debug('Extracted repository names:' + repositories)

            message.github.repositories = repositories
            logger.debug('Updated message:', JSON.stringify(message, null, 2))

            this.emit('message', message)
            logger.debug('Emitted updated message')
        } catch (error) {
            logger.error(`Error fetching repositories for ${username}:`, error)
            logger.debug('Error details:', JSON.stringify(error, null, 2))
            if (error.status === 403) {
                logger.warn('Possible rate limit exceeded. Check GitHub API rate limits.')
            }
            throw error
        }
    }
}

export default GitHubList

================
File: src/processors/github/GitHubList.js
================
import { Octokit } from '@octokit/rest'
import dotenv from 'dotenv'
import Processor from '../../../../transmissions/src/processors/base/Processor.js'
import logger from '../../../../transmissions/src/utils/Logger.js'

dotenv.config({ path: './trans-apps/applications/git-apps/.env' })

class GitHubList extends Processor {
    constructor(config) {
        super(config)
        logger.debug('GitHubList constructor called')
        this.octokit = new Octokit({ auth: process.env.GITHUB_TOKEN })
        logger.debug('Octokit instance created')
    }

    async process(message) {

        logger.debug('GitHubList process method called')

        try {

            if (!message.payload) {
                message.payload = {}
            }
            if (!message.payload.github) {
                message.payload.github = {}
            }


            const username = message.github.name
            logger.debug(`Processing for username: ${username}`)

            logger.debug('Calling GitHub API with pagination')
            logger.info(`Starting repository fetch for ${username}`)

            const repositories = await this.fetchAllRepositories(username)
            logger.debug(`Setting ${repositories.length} repositories in payload`)


            message.payload.github.repositories = repositories
            message.payload.github.totalRepos = repositories.length

            return this.emit('message', message)
        } catch (error) {
            this.handleError(error, username)
        }
    }

    async fetchAllRepositories(username) {
        const repositories = []
        let page = 1
        const delay = ms => new Promise(resolve => setTimeout(resolve, ms))

























        while (true) {
            const response = await this.octokit.repos.listForUser({
                username,
                per_page: 100,
                page
            })

            let data = response.data


            logger.debug(`Page ${page}: Got ${data.length} repos`)

            repositories.push(...data.map(repo => repo.name))

            if (data.length < 100) break
            page++


            await new Promise(r => setTimeout(r, 5000))
        }

        logger.debug(`Total repositories found: ${repositories.length}`)

        return repositories
    }

    checkRateLimit(headers) {
        const remaining = headers['x-ratelimit-remaining']
        const resetTime = new Date(headers['x-ratelimit-reset'] * 1000)
        logger.info(`Rate limit remaining: ${remaining}, Reset time: ${resetTime}`)

        if (remaining < 10) {
            logger.warn(`Rate limit is low. Only ${remaining} requests left. Reset at ${resetTime}`)
        }
    }

    createDetailedError(error, message) {
        const detailedError = new Error(`${message}: ${error.message}`)
        detailedError.status = error.status
        detailedError.response = error.response
        return detailedError
    }

    handleError(error, username) {
        logger.error(`Error fetching repositories for ${username}:`, error.message)
        logger.debug('Error details:', JSON.stringify(error, null, 2))

        if (error.status === 403) {
            logger.warn('Rate limit exceeded. Check GitHub API rate limits.')
            throw new Error('GitHub API rate limit exceeded')
        } else if (error.status === 404) {
            logger.warn(`User ${username} not found on GitHub`)
            throw new Error(`GitHub user ${username} not found`)
        } else {
            throw new Error(`Failed to fetch GitHub repositories: ${error.message}`)
        }
    }
}

export default GitHubList

================
File: src/processors/github/GitHubProcessorsFactory.js
================
import logger from '../../../../transmissions/src/utils/Logger.js';
import ns from '../../../../transmissions/src/utils/ns.js';
import GitHubList from './GitHubList.js';

class GitHubProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.GitHubList)) {
            return new GitHubList(config);
        }
        return false;
    }
}

export default GitHubProcessorsFactory;

================
File: src/processors/http/services/MetricsService.js
================
import WebSocket from 'ws';
import os from 'os';

class MetricsService {
    constructor(server) {
        this.wss = new WebSocket.Server({ server });
        this.metrics = {
            startTime: Date.now(),
            requests: 0,
            connections: 0,
            memory: {},
            cpu: {}
        };
        this.setupWebSocket();
        this.startMetricsCollection();
    }

    setupWebSocket() {
        this.wss.on('connection', (ws) => {
            this.metrics.connections++;
            ws.on('close', () => this.metrics.connections--);
        });
    }

    startMetricsCollection() {
        setInterval(() => {
            this.updateMetrics();
            this.broadcastMetrics();
        }, 1000);
    }

    updateMetrics() {
        this.metrics.uptime = (Date.now() - this.metrics.startTime) / 1000;
        this.metrics.memory = {
            used: process.memoryUsage().heapUsed,
            total: os.totalmem(),
            free: os.freemem()
        };
        this.metrics.cpu = {
            load: os.loadavg(),
            cores: os.cpus().length
        };
    }

    broadcastMetrics() {
        const data = JSON.stringify(this.metrics);
        this.wss.clients.forEach(client => {
            if (client.readyState === WebSocket.OPEN) {
                client.send(data);
            }
        });
    }

    incrementRequests() {
        this.metrics.requests++;
    }
}

export default MetricsService;

================
File: src/processors/http/services/ShutdownService.js
================
import crypto from 'crypto';

class ShutdownService {
    constructor() {

        this.username = crypto.randomBytes(16).toString('hex');
        this.password = crypto.randomBytes(32).toString('hex');
    }

    setupMiddleware(app) {
        app.use('/admin', (req, res, next) => {
            const authHeader = req.headers.authorization;
            if (!this.validateAuth(authHeader)) {
                res.setHeader('WWW-Authenticate', 'Basic');
                return res.status(401).send('Authentication required');
            }
            next();
        });
    }

    validateAuth(authHeader) {
        if (!authHeader || !authHeader.startsWith('Basic ')) {
            return false;
        }
        const base64Credentials = authHeader.split(' ')[1];
        const credentials = Buffer.from(base64Credentials, 'base64').toString('utf-8');
        const [username, password] = credentials.split(':');

        return username === this.username && password === this.password;
    }

    setupEndpoints(app, shutdownCallback) {
        app.get('/admin/credentials', (req, res) => {
            res.json({ username: this.username, password: this.password });
        });

        app.post('/admin/shutdown', (req, res) => {
            res.send('Shutdown initiated');
            shutdownCallback();
        });
    }
}

export default ShutdownService;

================
File: src/processors/http/HttpClient.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'









class HttpClient extends Processor {





    constructor(config) {
        super(config)
    }





    async process(message) {



        return this.emit('message', message)
    }
}

export default HttpClient

================
File: src/processors/http/HttpProcessorsFactory.js
================
import ns from '../../utils/ns.js'

import HttpServer from './HttpServer.js'
import HttpClient from './HttpClient.js'
import HttpProxy from './HttpProxy.js'

class HttpProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.t.HttpServer)) {
            return new HttpServer(config)
        }
        if (type.equals(ns.t.HttpClient)) {
            return new HttpClient(config)
        }
        if (type.equals(ns.t.HttpProxy)) {
            return new HttpProxy(config)
        }

        return false
    }
}
export default HttpProcessorsFactory

================
File: src/processors/http/HttpProxy.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'









class HttpProxy extends Processor {





    constructor(config) {
        super(config)
    }





    async process(message) {



        return this.emit('message', message)
    }
}

export default HttpProxy

================
File: src/processors/http/HttpServer.js
================
import path from 'path';
import { Worker } from 'worker_threads';
import logger from '../../utils/Logger.js';
import Processor from '../base/Processor.js';
import ns from '../../utils/ns.js';

class HttpServer extends Processor {
    constructor(config) {
        super(config);
        this.worker = null;
        this.serverConfig = {
            port: this.getPropertyFromMyConfig(ns.trm.port) || 4000,
            basePath: this.getPropertyFromMyConfig(ns.trm.basePath) || '/transmissions/test/',
            staticPath: this.getPropertyFromMyConfig(ns.trm.staticPath),
            cors: this.getPropertyFromMyConfig(ns.trm.cors) || false,
            timeout: this.getPropertyFromMyConfig(ns.trm.timeout) || 30000,
            maxRequestSize: this.getPropertyFromMyConfig(ns.trm.maxRequestSize) || '1mb',
            rateLimit: {
                windowMs: 15 * 60 * 1000,
                max: 100
            }
        };
    }

    async process(message) {
        try {
            this.worker = new Worker(
                path.join(process.cwd(), 'src/processors/http/HttpServerWorker.js')
            );

            this.worker.on('message', (msg) => {
                switch (msg.type) {
                    case 'status':
                        if (msg.status === 'running') {
                            logger.info(`Server running on port ${msg.port}`);
                        } else if (msg.status === 'stopped') {
                            this.emit('message', { ...message, serverStopped: true });
                        }
                        break;
                    case 'error':
                        logger.error(`Server error: ${msg.error}`);
                        this.emit('error', new Error(msg.error));
                        break;
                }
            });

            this.worker.on('error', (error) => {
                logger.error(`Worker error: ${error}`);
                this.emit('error', error);
            });

            this.worker.postMessage({
                type: 'start',
                config: this.serverConfig
            });

            return new Promise((resolve) => {
                this.worker.on('exit', () => {
                    resolve(message);
                });
            });

        } catch (error) {
            logger.error(`Failed to start server: ${error}`);
            throw error;
        }
    }

    async shutdown() {
        if (this.worker) {
            this.worker.postMessage({ type: 'stop' });
        }
    }
}

export default HttpServer;

================
File: src/processors/http/HttpServerWorker.js
================
import { parentPort } from 'worker_threads';
import express from 'express';
import path from 'path';
import logger from '../../utils/Logger.js';

class ServerWorker {
    constructor(config) {
        this.app = express();
        this.server = null;
        this.config = config;
        this.setupMessageHandling();
    }

    setupMessageHandling() {
        parentPort.on('message', (message) => {
            switch (message.type) {
                case 'start':
                    this.start(message.config);
                    break;
                case 'stop':
                    this.stop();
                    break;
                default:
                    logger.warn(`Unknown message type: ${message.type}`);
            }
        });
    }

    async start(config) {
        try {
            const { port = 4000, basePath = '/transmissions/test/', staticPath } = config;

            if (staticPath) {
                this.app.use(basePath, express.static(staticPath));
            }

            this.app.post('/shutdown', (req, res) => {
                res.send('Server shutting down...');
                this.stop();
            });

            this.server = this.app.listen(port, () => {
                parentPort.postMessage({
                    type: 'status',
                    status: 'running',
                    port: port
                });
            });

        } catch (error) {
            parentPort.postMessage({
                type: 'error',
                error: error.message
            });
        }
    }

    async stop() {
        if (this.server) {
            try {
                await new Promise((resolve, reject) => {
                    this.server.close((err) => {
                        if (err) reject(err);
                        resolve();
                    });
                });

                parentPort.postMessage({
                    type: 'status',
                    status: 'stopped'
                });

                process.exit(0);
            } catch (error) {
                parentPort.postMessage({
                    type: 'error',
                    error: error.message
                });
                process.exit(1);
            }
        }
    }
}

const worker = new ServerWorker();

================
File: src/processors/json/Blanker.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'

class Blanker extends Processor {
    constructor(config) {
        super(config)

        logger.debug(config.blankValue)
        this.blankValue = config.blankValue || ''
    }

    async process(message) {
        const pointer = this.getPropertyFromMyConfig(ns.trm.pointer)
        const preserve = this.getPropertyFromMyConfig(ns.trm.preserve)

        var preservePath = preserve.value ? preserve.value : 'nonono'

        logger.debug(`Blanker.process,  typeof preservePath = ${typeof preservePath}, preservePath = ${preservePath}`)
        logger.reveal(preservePath)
        if (!pointer) {
            if (preservePath) {
                message = this.blankValues(message, '', preservePath)
            } else {
                message = this.blankAllValues(message)
            }
        } else {
            const parts = pointer.toString().split('.')
            let target = message

            for (let i = 0; i < parts.length - 1; i++) {
                target = target[parts[i]]
                if (!target) break
            }

            if (target && target[parts[parts.length - 1]]) {
                if (preservePath) {
                    target[parts[parts.length - 1]] =
                        this.blankValues(target[parts[parts.length - 1]], parts.join('.'), preservePath)
                } else {
                    target[parts[parts.length - 1]] =
                        this.blankAllValues(target[parts[parts.length - 1]])
                }
            }
        }

        return this.emit('message', message)
    }

    shouldPreserve(path, preservePath) {
        logger.debug(`Blanker.shouldPreserve path = ${path}, preservePath = ${preservePath}`)
        if (!preservePath) return false
        const pathParts = path.split('.')
        const preserveParts = preservePath.split('.')

        if (pathParts.length < preserveParts.length) return false

        for (let i = 0; i < preserveParts.length; i++) {
            if (pathParts[i] !== preserveParts[i]) return false
        }
        return true
    }


    blankAllValues(obj) {
        if (Array.isArray(obj)) {
            return obj.map(item => this.blankAllValues(item))
        } else if (typeof obj === 'object' && obj !== null) {
            const result = {}
            for (const [key, value] of Object.entries(obj)) {
                result[key] = this.blankAllValues(value)
            }
            return result
        } else if (typeof obj === 'string') {
            return ''
        }
        return obj
    }

    blankValues(obj, currentPath = '', preservePath = '') {
        if (Array.isArray(obj)) {
            return obj.map((item, index) =>
                this.blankValues(item, `${currentPath}[${index}]`, preservePath)
            )
        } else if (typeof obj === 'object' && obj !== null) {
            const result = {}
            for (const [key, value] of Object.entries(obj)) {
                const newPath = currentPath ? `${currentPath}.${key}` : key
                logger.debug(`Blanker.blankValues 1 newPath = ${newPath}, preservePath = ${preservePath}`)
                if (this.shouldPreserve(newPath, preservePath)) {
                    result[key] = value
                } else {
                    result[key] = this.blankValues(value, newPath, preservePath)
                }
            }
            return result
        } else if (typeof obj === 'string') {
            logger.debug(`Blanker.blankValues 2 currentPath = ${currentPath}, preservePath = ${preservePath}`)
            return this.shouldPreserve(currentPath, preservePath) ? obj : ''
        }
        return obj
    }
}

export default Blanker

================
File: src/processors/json/JSONProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import JSONWalker from './JSONWalker.js'
import Restructure from './Restructure.js'
import ValueConcat from './ValueConcat.js'
import Blanker from './Blanker.js'

class JSONProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.Restructure)) {
            return new Restructure(config)
        }
        if (type.equals(ns.t.JSONWalker)) {
            return new JSONWalker(config)
        }
        if (type.equals(ns.t.ValueConcat)) {
            return new ValueConcat(config)
        }
        if (type.equals(ns.t.Blanker)) {
            return new Blanker(config)
        }
        return false
    }

}
export default JSONProcessorsFactory

================
File: src/processors/json/JsonRestructurer.js
================
import logger from '../../utils/Logger.js'

class JsonRestructurer {
    constructor(mappings) {
        logger.setLogLevel('info')
        if (!mappings?.mappings || !Array.isArray(mappings.mappings)) {
            throw new Error('Invalid mapping structure')
        }
        this.mappings = mappings.mappings
        logger.debug('JsonRestructurer,  this.mappings = ' + this.mappings)

    }

    getValueByPath(obj, path) {







        try {
            const sp = path.split('.')
            logger.debug('JsonRestructurer, sp = ' + sp)
            const reduced = sp.reduce((acc, part) => acc[part], obj)
            logger.debug('JsonRestructurer, reduced = ' + reduced)
            return reduced
        } catch (e) {
            logger.warn(`Warning: Path ${path} not found`)
            return undefined
        }
    }

    setValueByPath(obj, path, value) {
        logger.debug(`JsonRestructurer.setValueByPath, obj = ${obj}, path = ${path}, value = ${value}`)
        const parts = path.split('.')
        const last = parts.pop()
        const target = parts.reduce((acc, part) => {
            acc[part] = acc[part] || {}
            return acc[part]
        }, obj)
        logger.debug(`JsonRestructurer.setValueByPath, target = ${target}, last = ${last}, value = ${value}`)
        target[last] = value
    }

    restructure(inputData) {

        if (typeof inputData === 'string') {
            try {
                inputData = JSON.parse(inputData)
            } catch (e) {
                throw new Error('Invalid JSON string provided')
            }
        }


        const result = {}
        this.mappings.forEach(({ pre, post }) => {

            const value = this.getValueByPath(inputData, pre)

            if (value !== undefined) {
                this.setValueByPath(result, post, value)
            }
        })



        return result
    }
}
export default JsonRestructurer

================
File: src/processors/json/JSONWalker.js
================
import path from 'path'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'

class JSONWalker extends Processor {
    constructor(config) {
        super(config)
    }







    async process(message) {
        try {
            message.done = false
            var pointer = this.getPropertyFromMyConfig(ns.trm.pointer)

            logger.debug(`JSONWalker pointer =  ${pointer}`)

            const content = structuredClone(message.content)
            message.content = {}


            for (var i = 0; i < content.length; i++) {
                const newMessage = structuredClone(message)
                newMessage.content = content[i]
                this.emit('message', newMessage)
            }

            var finalMessage = structuredClone(message)
            finalMessage.content = content[content.length - 1]













            finalMessage.done = true
            this.emit('message', finalMessage)
        } catch (err) {
            logger.error("JSONWalker.process error: " + err.message)
            throw err
        }
    }
}

export default JSONWalker

================
File: src/processors/json/Restructure.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import JsonRestructurer from './JsonRestructurer.js'
import ns from '../../utils/ns.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import rdf from 'rdf-ext'

class Restructure extends Processor {
    constructor(config) {
        super(config)
    }

    async getRenames(config, configKey, term) {



        const renamesRDF = GrapoiHelpers.listToArray(config, configKey, term)
        const dataset = this.config
        var renames = []
        for (let i = 0; i < renamesRDF.length; i++) {
            let rename = renamesRDF[i]
            let poi = rdf.grapoi({ dataset: dataset, term: rename })
            let pre = poi.out(ns.trm.pre).value
            let post = poi.out(ns.trm.post).value
            renames.push({ "pre": pre, "post": post })
        }
        return renames
    }

    async process(message) {
        logger.setLogLevel('info')


        var renames
        if (this.config.simples) {
            renames = this.config.rename
        } else {
            renames = await this.getRenames(this.config, this.configKey, ns.trm.rename)
        }





        this.restructurer = new JsonRestructurer({
            mappings: renames
        })
        try {
            logger.debug('Restructure processor executing...')



            const input = structuredClone(message)


            const restructured = this.restructurer.restructure(input)

            const type = typeof restructured




            for (const key of Object.keys(restructured)) {
                message[key] = restructured[key]
            }


            logger.debug('Restructure successful')
            return this.emit('message', message)

        } catch (err) {
            logger.error("Restructure processor error: " + err.message)
            logger.reveal(message)
            throw err
        }
    }
}

export default Restructure

================
File: src/processors/json/ValueConcat.js
================
import logger from '../../utils/Logger.js'
import rdf from 'rdf-ext'
import ns from '../../utils/ns.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import Processor from '../base/Processor.js'

class ValueConcat extends Processor {

    constructor(config) {
        super(config)
        logger.log('CREATING VALUECONCAT')
    }

}
export default ValueConcat

================
File: src/processors/markup/LinkFinder.js
================
import * as cheerio from 'cheerio'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class LinkFinder extends Processor {

    async process(message) {

        await this.extractLinks(message)

        if (data === '~~done~~') {
            logger.log('LF DONE*****************')
            return this.emitLocal('message', '~~done~~', message)
            return
        }
    }


    relocate(filename, extension) {
        const split = filename.split('.').slice(0, -1)
        return split.join('.') + extension
    }

    async extractLinks(htmlContent, message) {

        const $ = cheerio.load(htmlContent)
        let label = ''

        $('a, h1, h2, h3, h4, h5, h6').each((_, element) => {
            const tagName = element.tagName.toLowerCase()
            if (tagName.startsWith('h')) {
                const level = tagName.substring(1)
                const headerText = $(element).text()
                label = `\n\n${'#'.repeat(parseInt(level))} ${headerText}\n`
            } else if (tagName === 'a') {
                const linkText = $(element).text()

                let href = $(element).attr('href')

                if (!href || href.startsWith('#')) return

                if (href && !href.includes('://')) {

                    const baseURL = message.sourceURL

                    href = new URL(href, baseURL).toString()
                }
                label = `\n[${linkText}](${href})`

            }
            message.label = label
            return this.emit('message', message)
        })
    }
}

export default LinkFinder

================
File: src/processors/markup/MarkdownToHTML.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

import { marked } from 'marked'


import markedFootnote from 'marked-footnote'
import markedCodeFormat from 'marked-code-format'


class MarkdownToHTML extends Processor {


    async process(message) {
        const input = message.content


        message.content = await
            marked

                .use(markedFootnote())
                .use(
                    markedCodeFormat({

                    })
                )
                .parse(input.toString())

        return this.emit('message', message)
    }
}

export default MarkdownToHTML

================
File: src/processors/markup/MarkupProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import MetadataExtractor from './MetadataExtractor.js'
import LinkFinder from './LinkFinder.js'
import MarkdownToHTML from './MarkdownToHTML.js'

class MarkupProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.MetadataExtractor)) {
            return new MetadataExtractor(config)
        }
        if (type.equals(ns.t.MarkdownToHTML)) {
            return new MarkdownToHTML(config)
        }
        if (type.equals(ns.t.LinkFinder)) {
            return new LinkFinder(config)
        }
        return false
    }
}

export default MarkupProcessorsFactory

================
File: src/processors/markup/MetadataExtractor.js
================
import * as cheerio from 'cheerio'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class MetadataExtractor extends Processor {

    async process(message) {
        const filename = data.filename
        const content = data.content

        logger.debug("MetadataExtractor input file : " + filename)
        const targetFilename = this.relocate(filename)
        logger.debug("MetadataExtractor outputfile : " + targetFilename)

        const jsonData = this.convertEmailToJSON(content)

        const jsonString = JSON.stringify(jsonData)

        const output = { filename: targetFilename, content: jsonString }

        return this.emit('message', output, message)
    }

    relocate(filename) {

        const split = filename.split('.').slice(0, -1)
        var newFileName = split.join('.') + '.json'
        return newFileName
    }

    convertEmailToJSON(htmlContent) {
        const $ = cheerio.load(htmlContent)
        var subjectLine = $('H1').text().trim()
        var fromName = $('B').first().text().trim()
        var nextMessageLink = $('LINK[REL="Next"]').attr('HREF')
        var previousMessageLink = $('LINK[REL="Previous"]').attr('HREF')
        var messageText = $('PRE').text().trim()
        messageText = this.pruneContent(messageText)
        const jsonResult = {
            subjectLine: subjectLine,
            fromName: fromName,
            nextMessageLink: nextMessageLink,
            previousMessageLink: previousMessageLink,
            messageText: messageText

        }























        return jsonResult
    }

    pruneContent(content) {

        const regex1 = /(^|\n).*?:\n>/s
        content = content.replace(regex1, '$1')

        const regex2 = /\n>.*?\n/g




        content = content.replace(regex2, '\n')

        return content
    }
}



export default MetadataExtractor

================
File: src/processors/mcp/McpClient.js
================
import logger from "../../utils/Logger.js";
import Processor from "../base/Processor.js";









class McpClient extends Processor {




  constructor(config) {
    super(config)
  }





  async process(message) {
    logger.setLogLevel("debug")


    return this.emit("message", message)
  }
}

export default McpClient

================
File: src/processors/mcp/McpProcessorsFactory.js
================
import logger from "../../utils/Logger.js";
import ns from "../../utils/ns.js";

import ProcessorTemplate from "./McpClient.js";



class ProcessorsFactoryTemplate {
  static createProcessor(type, config) {
    if (type.equals(ns.t.ProcessorTemplate)) {
      return new ProcessorTemplate(config);
    }

    return false;
  }
}
export default ProcessorsFactoryTemplate;

================
File: src/processors/mcp/McpServer.js
================
import logger from "../../utils/Logger.js";
import Processor from "../base/Processor.js";









class McpServer extends Processor {




  constructor(config) {
    super(config)
  }





  async process(message) {
    logger.setLogLevel("debug")


    return this.emit("message", message)
  }
}

export default McpServer

================
File: src/processors/packer/CommentStripper.js
================
import path from 'path';

const LANGUAGE_PATTERNS = {
    js: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    jsx: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    ts: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    tsx: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    py: {
        single: '#',
        multi: { start: '"""', end: '"""' }
    },
    java: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    cpp: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    c: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    },
    h: {
        single: '//',
        multi: { start: '/*', end: '*/' }
    }
};

export function commentStripper(content, filepath) {
    const ext = path.extname(filepath).toLowerCase().slice(1);
    const patterns = LANGUAGE_PATTERNS[ext];

    if (!patterns) {
        return content;
    }

    let lines = content.split('\n');
    let inMultiLineComment = false;
    let result = [];

    for (let i = 0; i < lines.length; i++) {
        let line = lines[i].trim();

        if (inMultiLineComment) {
            if (line.includes(patterns.multi.end)) {
                inMultiLineComment = false;
                line = line.split(patterns.multi.end)[1];
            } else {
                continue;
            }
        }

        if (patterns.multi && line.includes(patterns.multi.start)) {
            const parts = line.split(patterns.multi.start);
            if (!parts[1].includes(patterns.multi.end)) {
                inMultiLineComment = true;
                line = parts[0];
            } else {
                line = parts[0] + parts[1].split(patterns.multi.end)[1];
            }
        }

        if (patterns.single && line.startsWith(patterns.single)) {
            continue;
        }

        if (patterns.single) {
            const commentIndex = line.indexOf(patterns.single);
            if (commentIndex >= 0) {
                line = line.substring(0, commentIndex).trim();
            }
        }

        if (line.trim()) {
            result.push(line);
        }
    }

    return result.join('\n');
}

================
File: src/processors/packer/FileContainer.js
================
import Processor from '../base/Processor.js';
import logger from '../../utils/Logger.js';
import ns from '../../utils/ns.js';
import path from 'path';

class FileContainer extends Processor {
    constructor(config) {
        super(config);
        this.container = {
            files: {},
            summary: {
                totalFiles: 0,
                fileTypes: {},
                timestamp: new Date().toISOString()
            }
        };
    }

    async process(message) {
        if (message.done) {
            message.content = JSON.stringify(this.container, null, 2);
            message.filepath = this.getPropertyFromMyConfig(ns.trm.destination);
            return this.emit('message', message);
        }

        if (!message.filepath || !message.content) {
            logger.warn('FileContainer: Missing filepath or content');
            return;
        }


        const targetDir = message.targetPath || message.rootDir;
        const relativePath = path.relative(targetDir, message.filepath);


        this.container.files[relativePath] = {
            content: message.content,
            type: path.extname(message.filepath),
            timestamp: new Date().toISOString()
        };


        this.container.summary.totalFiles++;
        const fileType = path.extname(message.filepath) || 'unknown';
        this.container.summary.fileTypes[fileType] = (this.container.summary.fileTypes[fileType] || 0) + 1;

        return this.emit('message', message);
    }
}

export default FileContainer;

================
File: src/processors/packer/PackerProcessorsFactory.js
================
import logger from '../../utils/Logger.js';
import ns from '../../utils/ns.js';
import FileContainer from './FileContainer.js';

class PackerProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.FileContainer)) {
            logger.debug('PackerProcessorsFactory: Creating FileContainer processor');
            return new FileContainer(config);
        }
        return false;
    }
}

export default PackerProcessorsFactory;

================
File: src/processors/postcraft/AtomFeedPrep.js
================
import fs from 'fs/promises';
import path from 'path';
import Processor from '../base/Processor.js';
import logger from '../../utils/Logger.js';

class AtomFeedPrep extends Processor {
    constructor(config) {
        super(config);
    }

    async process(message) {

        const entries = message.slugs || [];
        const siteUrl = message.site?.url || 'https://danny.ayers.name';

        if (message.targetPath) {
            message.templateFilename = path.join(message.targetPath, message.atomFeed.templateFilename)
        } else {
            message.templateFilename = path.join(message.rootDir, message.atomFeed.templateFilename)
        }

        const feed = {
            title: message.site?.title || "Danny Ayers' Blog",
            subtitle: message.site?.subtitle || '',
            updated: new Date().toISOString(),
            id: siteUrl,
            link: siteUrl,
            author: {
                name: "Danny Ayers",
                email: "danny.ayers@gmail.com"
            },
            entries: []
        };

        // Get same number of entries as front page
        const entryCount = Math.min(5, entries.length);
        const rangeStart = entries.length - entryCount;
        const rangeEnd = entries.length - 1;

        for (let i = rangeEnd; i >= rangeStart; i--) {
            const slug = entries[i];
            if (slug) {
                let filePath;
                if (message.targetPath) {
                    filePath = path.join(message.targetPath, message.entryContentMeta.targetDir, slug + '.html');
                } else {
                    filePath = path.join(message.rootDir, message.entryContentMeta.targetDir, slug + '.html');
                }

                const entry = {
                    title: `Entry ${slug}`,
                    id: `${siteUrl}/entries/${slug}.html`,
                    link: `${siteUrl}/entries/${slug}.html`,
                    updated: message.contentBlocks?.updated || new Date().toISOString(),
                    content: await this.getEntryContent(filePath)
                };

                feed.entries.push(entry);
            }
        }

        message.contentBlocks = feed;
        message.filepath = path.join(message.targetPath || message.rootDir, 'public/home/atom.xml');

        return this.emit('message', message);
    }

    async getEntryContent(filePath) {
        try {
            return await fs.readFile(filePath, 'utf8');
        } catch (error) {
            logger.error(`Error reading entry file ${filePath}: ${error}`);
            return '';
        }
    }
}

export default AtomFeedPrep;

================
File: src/processors/postcraft/EntryContentToPagePrep.js
================
import path from 'path'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class EntryContentToPagePrep extends Processor {

  constructor(config) {
    super(config)
  }

  async process(message) {
    if (message.done) {
      return this.emit('message', message)
      return
    }
    logger.setLogLevel('debug')














    message.contentBlocks.content = message.content













    message.filepath = path.join(message.contentGroup.PostPages.targetDir, message.slug + '.html')
    logger.debug('EntryContentToPagePrep, message.filepath = ' + message.filepath)

    this.emit('message', message)
  }

}

export default EntryContentToPagePrep

================
File: src/processors/postcraft/FrontPagePrep.js
================
import path from 'path'
import { readFile } from 'node:fs/promises'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'


class FrontPagePrep extends Processor {
  constructor(config) {
    super(config)
  }


  async process(message) {
    logger.setLogLevel('debug')

    if (message.targetPath) {
      message.templateFilename = path.join(message.targetPath, message.indexPage.templateFilename)
    } else {
      message.templateFilename = path.join(message.rootDir, message.indexPage.templateFilename)
    }

    logger.debug('FrontPagePrep, Template = ' + message.templateFilename)


    const rawEntryPaths = this.resolveRawEntryPaths(message)
    message.content = ''

    // TODO tidy up
    const entryCount = Math.min(5, rawEntryPaths.length) // Limit to 5 entries or less
    logger.debug('FrontPagePrep, entryCount = ' + entryCount)

    const rangeStart = rawEntryPaths.length - entryCount
    const rangeEnd = rawEntryPaths.length - 1
    //     for (let i = 0; i < entryCount; i++) {
    for (let i = rangeEnd; i >= rangeStart; i--) {
      logger.debug('FrontPagePrep, i = ' + entryCount)
      const rawEntryPath = rawEntryPaths[i]
      if (rawEntryPath) {
        message.content += await readFile(rawEntryPath, 'utf8')
      } else {
        logger.warn(`Skipping undefined entry path at index ${i}`)
      }
    }

    message.contentBlocks.content = message.content

    if (message.targetPath) {
      message.filepath = path.join(message.targetPath, message.indexPage.filepath)
    } else {
      message.filepath = path.join(message.rootDir, message.indexPage.filepath)
    }
    return this.emit('message', message)

  }

  resolveRawEntryPaths(message) {
    const paths = []
    const slugs = message.slugs || []
    const entryCount = slugs.length

    for (let i = 0; i < entryCount; i++) {
      const slug = slugs[i]
      if (slug) {

        let filePath
        if (message.targetPath) {
          filePath = path.join(message.targetPath, message.entryContentMeta.targetDir, slug + '.html')
        } else {
          filePath = path.join(message.rootDir, message.entryContentMeta.targetDir, slug + '.html')
        }
        paths.push(filePath)
      }
    }

    return paths
  }
}

export default FrontPagePrep

================
File: src/processors/postcraft/PostcraftDispatcher.js
================
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'











class PostcraftDispatcher extends Processor {





  constructor(config) {
    super(config)
  }






  async process(message) {

    const postcraftConfig = message.dataset
    message.template = data.toString()
    logger.debug('PostcraftDispatcherPostcraftDispatcherPostcraftDispatcher ' + data)
    process.exit(0)
    const poi = grapoi({ dataset: postcraftConfig })

    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.pc.ContentGroup)) {
        await this.processContentGroup(message, q.subject)
      }
    }
  }






  async processContentGroup(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })
    const sourceDir = groupPoi.out(ns.fs.sourceDirectory).term.value
    const targetDir = groupPoi.out(ns.fs.targetDirectory).term.value
    const templateFilename = groupPoi.out(ns.pc.template).term.value





    message.sourceDir = sourceDir
    message.targetDir = targetDir
    message.templateFilename = templateFilename
    message.loadContext = 'template'

    return this.emit('message', sourceDir, message)
  }
}

export default PostcraftDispatcher

================
File: src/processors/postcraft/PostcraftPrep.js
================
import path from 'path'
import logger from '../../utils/Logger.js'

import Processor from '../base/Processor.js'

class PostcraftPrep extends Processor {

  constructor(config) {
    super(config)
  }

  async process(message) {
    logger.setLogLevel("debug")

    if (message.done) {
      return this.emit('message', message)
    }
    message.slug = this.extractSlug(message)
    message.targetFilename = this.extractTargetFilename(message)
    message.contentBlocks = {}
    message.contentBlocks.relURL = this.extractRelURL(message)


    message.contentBlocks.link = 'entries/' + message.contentBlocks.relURL

    message.contentBlocks.title = this.extractTitle(message)

    const { created, updated } = this.extractDates(message)
    message.contentBlocks.created = created
    message.contentBlocks.updated = updated

    return this.emit('message', message)
  }


  extractSlug(message) {

    var slug = message.filename

    if (slug.includes('.')) {
      slug = slug.substr(0, slug.lastIndexOf("."))
    }
    return slug
  }

  extractTargetFilename(message) {









    logger.reveal(message)








    return path.join(message.contentGroup.PostPages.targetDir, this.extractSlug(message) + '.html')
  }

  extractRelURL(message) {
    return this.extractSlug(message) + '.html'
  }

  extractDates(message) {
    const today = (new Date()).toISOString().split('T')[0]
    const dates = { created: today, updated: today }


    const nonExt = message.filename.split('.').slice(0, -1).join()
    const shreds = nonExt.split('_')
    if (Date.parse(shreds[0])) {
      dates.created = shreds[0]
    }
    return dates
  }




  extractTitle(message) {
    let title = 'Title'
    let match = message.content.toString().match(/^#(.*)$/m)
    let contentTitle = match ? match[1].trim() : null
    if (contentTitle) {
      title = contentTitle.replaceAll('#', '') // TODO make nicer
      return title
    }

    // derive from filename
    // eg. 2024-04-19_hello-postcraft.md
    try {
      const nonExt = message.filename.split('.').slice(0, -1).join()
      const shreds = nonExt.split('_')


      title = shreds[1].split('-')
        .map(word => word.charAt(0).toUpperCase() + word.slice(1))
        .join(' ')
    } catch (err) {
      title = message.filename
    }
    return title
  }
}

export default PostcraftPrep

================
File: src/processors/postcraft/PostcraftProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import PostcraftDispatcher from './PostcraftDispatcher.js'
import PostcraftPrep from './PostcraftPrep.js'
import EntryContentToPagePrep from './EntryContentToPagePrep.js'
import FrontPagePrep from './FrontPagePrep.js'
import AtomFeedPrep from './AtomFeedPrep.js'

class PostcraftProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.PostcraftDispatcher)) {
            return new PostcraftDispatcher(config)
        }
        if (type.equals(ns.t.PostcraftPrep)) {
            return new PostcraftPrep(config)
        }
        if (type.equals(ns.t.EntryContentToPagePrep)) {
            return new EntryContentToPagePrep(config)
        }
        if (type.equals(ns.t.FrontPagePrep)) {
            return new FrontPagePrep(config)
        }
        if (type.equals(ns.t.AtomFeedPrep)) {
            return new AtomFeedPrep(config)
        }
        return false
    }
}

export default PostcraftProcessorsFactory

================
File: src/processors/protocols/HttpGet.js
================
import axios from 'axios'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class HttpGet extends Processor {
    constructor(config) {
        super(config)
    }







    async process(url, message) {

        logger.debug('HttpGet, url = ' + url)
        if (url === '~~done~~') {
            logger.log('HG DONE*****************')
            return this.emit('message', url, message)
            return
        }
        try {
            logger.log('HG GETTING*****************')
            const response = await axios.get(url)
            const content = response.data

            message.sourceURL = url
            return this.emit('message', content, message)
        } catch (error) {
            logger.error("HttpGet.execute error\n" + error)
        }
    }
}

export default HttpGet

================
File: src/processors/protocols/ProtocolsProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import HttpGet from './HttpGet.js'



class ProtocolsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.HttpGet)) {
            return new HttpGet(config)
        }

        return false
    }
}

export default ProtocolsProcessorsFactory

================
File: src/processors/rdf/ConfigMap.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import path from 'path'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ConfigMap extends Processor {
  constructor(config) {
    super(config)
  }

  async process(message) {








    logger.debug(`ConfigMap.process`)
    this.showMyConfig()

    const basePath = message.targetPath || message.rootDir
    logger.debug(`ConfigMap using base path: ${basePath}`)

    const dataset = message.dataset
    const poi = grapoi({ dataset })


    for (const quad of poi.out(ns.rdf.type, ns.pc.ConfigSet).quads()) {
      const groupID = quad.subject

      let groupName = ns.getShortname(groupID.value)

      logger.debug(`*** groupName = ${groupName} `)


      const groupPoi = grapoi({ dataset, term: groupID })

      if (!message.contentGroup) message.contentGroup = {}

      if (groupPoi.out(ns.fs.sourceDirectory).term) {
        let sourceDir = this.resolvePath(
          basePath,
          groupPoi.out(ns.fs.sourceDirectory).term.value)

        if (!message.contentGroup[groupName]) message.contentGroup[groupName] = {}
        message.contentGroup[groupName].sourceDir = sourceDir
      }

      if (groupPoi.out(ns.fs.targetDirectory).term) {
        let targetDir = this.resolvePath(
          basePath,
          groupPoi.out(ns.fs.targetDirectory).term.value
        )
        if (!message.contentGroup[groupName]) message.contentGroup[groupName] = {}
        message.contentGroup[groupName].targetDir = targetDir
      }

      if (groupPoi.out(ns.pc.template).term) {
        let templateFile = this.resolvePath(
          basePath,
          groupPoi.out(ns.pc.template).term.value
        )
        if (!message.contentGroup[groupName]) message.contentGroup[groupName] = {}
        message.contentGroup[groupName].templateFile = templateFile
      }













    }


    return this.emit('message', message)
  }

  resolvePath(basePath, relativePath) {
    if (!basePath || !relativePath) {
      throw new Error('Base path and relative path required')
    }

    const resolved = path.isAbsolute(relativePath)
      ? relativePath
      : path.join(basePath, relativePath)

    return path.normalize(resolved)
  }
}

export default ConfigMap

================
File: src/processors/rdf/DatasetReader.js
================
import path from 'path';
import rdf from 'rdf-ext';
import { fromFile } from 'rdf-utils-fs';
import ns from '../../utils/ns.js';
import logger from '../../utils/Logger.js';
import Processor from '../base/Processor.js';

class DatasetReader extends Processor {
    constructor(config) {
        super(config);
    }

    async process(message) {
        try {
            const datasetFile = this.getPropertyFromMyConfig(ns.trm.datasetFile);
            const datasetPath = path.join(message.rootDir, datasetFile);

            logger.debug(`Reading dataset from ${datasetPath}`);
            const stream = fromFile(datasetPath);
            message.dataset = await rdf.dataset().import(stream);

            if (message.dataset.size === 0) {
                logger.warn('Empty dataset loaded');
            } else {
                logger.debug(`Loaded dataset with ${message.dataset.size} quads`);
            }

            return this.emit('message', message);
        } catch (err) {
            logger.error('Failed to read dataset:', err);
            throw err;
        }
    }
}

export default DatasetReader;

================
File: src/processors/rdf/RDFConfig.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class RDFConfig extends Processor {
  constructor(config) {
    super(config)
    this.configMap = new Map()
  }

  async process(message) {
    if (!message.dataset) {
      throw new Error('No RDF dataset provided')
    }

    const dataset = message.dataset
    const poi = grapoi({ dataset })


    for (const configGroup of poi.out(ns.rdf.type, ns.trm.ConfigGroup).terms) {
      const groupPoi = grapoi({ dataset, term: configGroup })


      const mappings = {}
      for (const quad of groupPoi.quads()) {
        if (!quad.predicate.equals(ns.rdf.type)) {
          mappings[quad.predicate.value] = this.resolveValue(quad.object)
        }
      }

      this.configMap.set(configGroup.value, mappings)
      message.configMap = this.configMap
    }


    if (message.configPatterns) {
      for (const pattern of message.configPatterns) {
        const config = this.configMap.get(pattern)
        if (config) {
          Object.assign(message, config)
        }
      }
    }

    return this.emit('message', message)
  }

  resolveValue(term) {

    if (term.termType === 'NamedNode') {
      return term.value
    } else if (term.termType === 'Literal') {
      const value = term.value

      return isNaN(value) ? value : Number(value)
    }
    return term.value
  }

  getConfig(groupId) {
    return this.configMap.get(groupId)
  }
}

export default RDFConfig

================
File: src/processors/rdf/RDFProcessorsFactory.js
================
import ns from '../../utils/ns.js'
import DatasetReader from './DatasetReader.js'
import ConfigMap from './ConfigMap.js'
import RDFConfig from './RDFConfig.js'

class RDFProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.DatasetReader)) {
            return new DatasetReader(config)
        }
        if (type.equals(ns.t.ConfigMap)) {
            return new ConfigMap(config)
        }
        if (type.equals(ns.t.RDFConfig)) {
            return new RDFConfig(config)
        }
        return false
    }
}

export default RDFProcessorsFactory

================
File: src/processors/staging/MarkdownFormatter.js
================
import path from 'path'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class MarkdownFormatter extends Processor {
    constructor(config) {
        super(config)
    }

    async process(message) {

        if (message.done) return


        const dir = '/home/danny/github-danny/hyperdata/docs/postcraft/content-raw/chat-archives/md'

        const filename = `${message.content.created_at.substring(0, 10)}_${message.content.uuid.substring(0, 3)}.md`

        message.filepath = path.join(dir, message.meta.conv_uuid.substring(0, 4), filename)
        message.content = this.extractMarkdown(message)

        return this.emit('message', message)
    }



    extractMarkdown(message) {

        const urlBase = 'https://claude.ai/chat/'

        const lines = []
        lines.push(`# [${message.meta.conv_name}](${urlBase}${message.meta.conv_uuid})\n`)

        lines.push(`${message.content.uuid}\n`)

        lines.push(message.content.text)
        lines.push('\n---\n')

        for (const [key, value] of Object.entries(message)) {
            if (key !== 'content' && value !== null) {
                if (value) {
                    const v = typeof value === 'object' ? JSON.stringify(value, null, 2) : value.toString()
                    lines.push(`* **${key}** : ${v}`)
                } else {
                    lines.push(`* **${key}** : [undefined]`)
                }
            }
        }

        return lines.join('\n')
    }

}

export default MarkdownFormatter

================
File: src/processors/staging/StagingProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import MarkdownFormatter from './MarkdownFormatter.js'
import TurtleFormatter from './TurtleFormatter.js'




class StagingProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.t.MarkdownFormatter)) {
            return new MarkdownFormatter(config)
        }
        if (type.equals(ns.t.TurtleFormatter)) {
            return new TurtleFormatter(config)
        }
        return false
    }
}
export default StagingProcessorsFactory

================
File: src/processors/staging/TurtleFormatter.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class TurtleFormatter extends Processor {
    constructor(config) {
        super(config)
        this.baseURI = config.baseURI || 'http://example.org/'
    }

    async process(message) {
        try {
            const item = message.currentItem
            if (!item) {
                return
            }


            const turtle = this.formatTurtle(item)
            message.content = turtle
            message.targetFile = `${item.id}.ttl`

            this.emit('message', message)
        } catch (err) {
            logger.error("TurtleFormatter.execute error: " + err.message)
            throw err
        }
    }

    formatTurtle(item) {
        const lines = []
        lines.push('@prefix : <http://example.org/ns#> .')
        lines.push('@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .')
        lines.push('')

        const subject = `<${this.baseURI}${item.id}>`
        lines.push(`${subject} a :Item ;`)

        const entries = Object.entries(item)
        entries.forEach(([key, value], index) => {
            if (value !== null) {
                const isLast = index === entries.length - 1
                const literal = typeof value === 'string' ?
                    `"${value.replace(/"/g, '\\"')}"` :
                    `"${JSON.stringify(value)}"`
                lines.push(`    :${key} ${literal}${isLast ? ' .' : ' ;'}`)
            }
        })

        return lines.join('\n')
    }
}

export default TurtleFormatter

================
File: src/processors/system/EnvLoader.js
================
import 'dotenv/config'



import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'














class EnvLoader extends Processor {





    constructor(config) {
        super(config)
    }





    async process(message) {



        this.config.whiteboard.env = process.env

        return this.emit("message", message)
    }
}

export default EnvLoader

================
File: src/processors/system/SystemProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import EnvLoader from './EnvLoader.js'



class SystemsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.EnvLoader)) {
            return new EnvLoader(config)
        }
        return false
    }
}
export default SystemsProcessorsFactory

================
File: src/processors/templates/TemplateProcessor.js
================
import logger from "../../utils/Logger.js";
import Processor from "../base/Processor.js";









class TemplateProcessor extends Processor {




  constructor(config) {
    super(config);
  }





  async process(message) {



    return this.emit("message", message);
  }
}

export default TemplateProcessor;

================
File: src/processors/templates/TemplateProcessorsFactory.js
================
import logger from "../../utils/Logger.js";
import ns from "../../utils/ns.js";

import ProcessorTemplate from "./_Processor.js";



class ProcessorsFactoryTemplate {
  static createProcessor(type, config) {
    if (type.equals(ns.t.ProcessorTemplate)) {
      return new ProcessorTemplate(config);
    }

    return false;
  }
}
export default ProcessorsFactoryTemplate;

================
File: src/processors/test/AppendProcess.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class AppendProcess extends Processor {


    async process(message) {
        logger.debug("AppendProcess data : " + message.content)
        message.content = message.content + " world"
        return this.emit('message', message)
    }
}

export default AppendProcess

================
File: src/processors/test/FileSink.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import { writeFile } from 'node:fs/promises'
import footpath from '../../utils/footpath.js'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class FileSink extends Processor {

    constructor(config) {
        super(config)
        const dataset = this.config
        const poi = grapoi({ dataset })
        this.destinationFile = poi.out(ns.trm.destinationFile).value
    }


    async process(message) {
        const toRootDir = '../../../'
        const dataDir = path.join(toRootDir, message.dataDir)
        const df = footpath.resolve(import.meta.url, dataDir, this.destinationFile)
        logger.debug("FileSink to = " + df)
        await writeFile(df, message.content)
        return this.emit('message', message)
    }
}

export default FileSink

================
File: src/processors/test/FileSource.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import { readFile } from 'node:fs/promises'

import footpath from '../../utils/footpath.js'
import rdf from 'rdf-ext'

import grapoi from 'grapoi'
import ns from '../../utils/ns.js'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class FileSource extends Processor {

    constructor(config) {
        super(config)
        const dataset = this.config
        const poi = grapoi({ dataset })
        this.sourceFile = poi.out(ns.trm.sourceFile).value
    }


    async process(message) {
        try {
            const toRootDir = '../../../'
            const dataDir = toRootDir + message.dataDir
            const sf = footpath.resolve(import.meta.url, dataDir, this.sourceFile)
            logger.debug('FileSource file : ' + sf)
            const contents = await readFile(sf, { encoding: 'utf8' })
            logger.debug('FileSource data : ' + contents)
            return this.emit('message', { content: contents, ...message })
        } catch (err) {
            logger.error("FileSource.execute error : " + err.message)
        }
    }
}

export default FileSource

================
File: src/processors/test/StringSink.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class StringSink extends Processor {

    process(message) {
        logger.log("\n\nStringSink outputs : \"" + message + "\"\n\n")
    }
}

export default StringSink

================
File: src/processors/test/StringSource.js
================
import Processor from '../base/Processor.js'

class StringSource extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {
        console.log("message = " + message)
        console.log("data = " + data)
        return this.emit('message', message)
    }
}

export default StringSource

================
File: src/processors/test/TestProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import StringSource from './StringSource.js'
import StringSink from './StringSink.js'
import AppendProcess from './AppendProcess.js'
import FileSource from './FileSource.js'
import FileSink from './FileSink.js'



class TestProcessorsFactory {
    static createProcessor(type, config) {



        if (type.equals(ns.t.StringSource)) {
            return new StringSource(config)
        }
        if (type.equals(ns.t.StringSink)) {
            return new StringSink(config)
        }
        if (type.equals(ns.t.AppendProcess)) {
            return new AppendProcess(config)
        }


        if (type.equals(ns.t.FileSource)) {
            return new FileSource(config)
        }
        if (type.equals(ns.t.FileSink)) {
            return new FileSink(config)
        }

        return false
    }
}

export default TestProcessorsFactory

================
File: src/processors/text/LineReader.js
================
import { readFile } from 'node:fs/promises'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class LineReader extends Processor {

    constructor(config) {
        super(config)
    }

    async process(message) {

        const text = data.toString()


        const lines = text.split('\n')
        for await (let line of lines) {
            if (line.trim() && !line.startsWith('#')) {
                logger.debug('Line = [[[' + line + ']]]')
                return this.emit('message', line, message)
            }
        }

        return this.emit('message', '~~done~~', message)
    }
}

export default LineReader

================
File: src/processors/text/StringFilter.js
================
import fs from 'fs/promises';
import path from 'path';
import Processor from '../base/Processor.js';
import logger from '../../utils/Logger.js';
import ns from '../../utils/ns.js';

class StringFilter extends Processor {
    constructor(config) {
        super(config);
        this.gitignorePatterns = [];
        this.initialized = false;
        this.initialize();
    }

    async initialize() {
        if (this.initialized) return;

        try {

            if (this.configKey) {
                this.includePatterns = this.getPropertyFromMyConfig(ns.trm.include)?.split(',') || [];
                this.excludePatterns = this.getPropertyFromMyConfig(ns.trm.exclude)?.split(',') || [];
            } else {
                this.includePatterns = this.config.include?.split(',') || [];
                this.excludePatterns = this.config.exclude?.split(',') || [];
            }


            const gitignorePath = this.config.gitignorePath;
            if (gitignorePath) {
                try {
                    const gitignoreContent = await fs.readFile(gitignorePath, 'utf8');
                    this.gitignorePatterns = gitignoreContent
                        .split('\n')
                        .map(line => line.trim())
                        .filter(line => line && !line.startsWith('#'));
                } catch (err) {
                    logger.warn(`Could not load gitignore from ${gitignorePath}: ${err.message}`);
                }
            }
        } catch (err) {
            logger.error('Error initializing StringFilter:', err);
        }

        this.initialized = true;
    }

    async process(message) {
        await this.initialize();

        if (!message.filepath) {
            logger.warn('StringFilter: No filepath provided');
            return;
        }

        const relativePath = message.filepath;
        logger.debug(`StringFilter, relative path = ${relativePath}`);


        if (this.gitignorePatterns.some(pattern => this.matchPattern(relativePath, pattern))) {
            return;
        }


        if (this.excludePatterns.some(pattern => this.matchPattern(relativePath, pattern))) {
            return;
        }


        if (this.includePatterns.length > 0 &&
            !this.includePatterns.some(pattern => this.matchPattern(relativePath, pattern))) {
            return;
        }

        return this.emit('message', message);
    }

    matchPattern(filePath, pattern) {

        const regexPattern = pattern
            .replace(/\./g, '\\.')
            .replace(/\*/g, '.*')
            .replace(/\?/g, '.');
        const regex = new RegExp(`^${regexPattern}$`);


        const filename = path.basename(filePath);
        return regex.test(filename) || regex.test(filePath);
    }
}

export default StringFilter;

================
File: src/processors/text/StringMerger.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'


class StringMerger extends Processor {
    constructor(config) {
        super(config)
        this.merged = ''
    }

    async process(message) {
        logger.log('SMDATA*********************************\n' + data)

        if (data === '~~done~~') {
            logger.log('SM  DONE**********************************\n' + this.merged)
            return this.emit('message', this.merged, message)
            return
        }
        this.merged = this.merged + data

    }
}

export default StringMerger

================
File: src/processors/text/StringReplace.js
================
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class StringReplace extends Processor {
    constructor(config) {
        super(config)
    }





    async process(message) {

        const inputField = this.getProperty(ns.trm.inputField)
        const outputField = this.getProperty(ns.trm.outputField)

        var match = message.match ? message.match : this.getProperty(ns.trm.match)
        var replace = message.replace ? message.replace : this.getProperty(ns.trm.replace)

        var input = message.input ? message.input : message[inputField]
        if (!input) {
            input = message.content
        }

        logger.debug('StringReplace.process input = ' + input)


        const output = input.split(match).join(replace)

        logger.debug('StringReplace output: ' + output)
        try {
            message[outputField] = output
        } catch {
            message.content = output
        }
        return this.emit('message', message)
    }
}

export default StringReplace

================
File: src/processors/text/Templater.js
================
import Processor from '../base/Processor.js'
import nunjucks from 'nunjucks'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

class Templater extends Processor {
    constructor(config) {
        super(config)
    }





    async process(message) {


        var templateFilename = this.getProperty(ns.trm.templateFilename)

        if (templateFilename) {
            logger.debug(`\nTemplater.process, templateFilename = ${templateFilename}`)


            var targetPath = templateFilename.substr(0, templateFilename.lastIndexOf("/"))
            const filename = templateFilename.substr(templateFilename.lastIndexOf("/") + 1)

            if (!path.isAbsolute(targetPath)) {
                targetPath = path.join(this.getProperty(ns.trm.targetPath, message.rootDir), targetPath)
            }

            logger.debug('\nTemplater, targetPath = ' + targetPath)
            logger.debug('Templater, filename = ' + filename)


            nunjucks.configure(targetPath, { autoescape: false })



            message.content = nunjucks.render(filename, message.contentBlocks)

            logger.debug(`content POST = ${message.content}`)


        } else {


            nunjucks.configure({ autoescape: false })



            message.content = nunjucks.renderString(message.template, message.contentBlocks)
        }

        return this.emit('message', message)
    }
}
export default Templater

================
File: src/processors/text/TextProcessorsFactory.js
================
import ns from '../../utils/ns.js'

import LineReader from './LineReader.js'
import StringFilter from './StringFilter.js'
import StringMerger from './StringMerger.js'
import StringReplace from './StringReplace.js'
import Templater from './Templater.js'

class TextProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.t.Templater)) {
            return new Templater(config)
        }
        if (type.equals(ns.t.LineReader)) {
            return new LineReader(config)
        }

        if (type.equals(ns.t.StringFilter)) {
            return new StringFilter(config)
        }

        if (type.equals(ns.t.StringMerger)) {
            return new StringMerger(config)
        }

        if (type.equals(ns.t.StringReplace)) {
            return new StringReplace(config)
        }





        return false
    }
}

export default TextProcessorsFactory

================
File: src/processors/unsafe/RunCommand.js
================
import { exec } from 'child_process';
import logger from '../../utils/Logger.js';
import Processor from '../base/Processor.js';
import ns from '../../utils/ns.js';

class RunCommand extends Processor {
    constructor(config) {
        super(config);
        this.allowedCommands = config.allowedCommands || [];
        this.blockedPatterns = config.blockedPatterns || [];
        this.timeout = config.timeout || 5000;
        this.initializeSecurity();
    }

    async initializeSecurity() {
        if (this.configKey) {
            const allowed = await this.getPropertyFromMyConfig(ns.trm.allowedCommands);
            this.allowedCommands = allowed ? allowed.split(',') : [];

            const blocked = await this.getPropertyFromMyConfig(ns.trm.blockedPatterns);
            this.blockedPatterns = blocked ? blocked.split(',') : [];
        }
    }

    validateCommand(command) {
        if (!command) {
            throw new Error('No command specified');
        }

        const commandName = command.split(' ')[0];
        const isAllowed = this.allowedCommands.length === 0 ||
            this.allowedCommands.includes(commandName);

        if (!isAllowed) {
            throw new Error(`Command '${commandName}' not in allowed list`);
        }

        const hasBlocked = this.blockedPatterns.some(pattern =>
            command.includes(pattern)
        );
        if (hasBlocked) {
            throw new Error('Command contains blocked pattern');
        }
    }

    async process(message) {
        let command = message.command;
        if (!command) {
            command = this.getPropertyFromMyConfig(ns.trm.command);
        }

        try {
            this.validateCommand(command);
            const result = await this.executeCommand(command);
            message.content = result.stdout;
            message.commandResult = result;
            logger.debug(`Command executed successfully: ${command}`);
        } catch (error) {
            logger.error(`Command error: ${error.message}`);
            message.commandError = error.message;
            message.content = error.message;
            throw error;
        }

        return this.emit('message', message);
    }

    executeCommand(command) {
        return new Promise((resolve, reject) => {
            const child = exec(command, {
                timeout: this.timeout
            }, (error, stdout, stderr) => {
                if (error) {
                    if (error.signal === 'SIGTERM') {
                        reject(new Error('Command timeout'));
                    } else {
                        reject(error);
                    }
                    return;
                }
                resolve({
                    stdout: stdout.toString(),
                    stderr: stderr.toString(),
                    code: 0
                });
            });
        });
    }
}

export default RunCommand;

================
File: src/processors/unsafe/UnsafeProcessorsFactory.js
================
import ns from '../../utils/ns.js'


import RunCommand from './RunCommand.js'


class UnsafeProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.t.RunCommand)) {
            return new RunCommand(config)
        }

        return false
    }
}
export default UnsafeProcessorsFactory

================
File: src/processors/util/CaptureAll.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class CaptureAll extends Processor {
    constructor(config) {

        if (!config.whiteboard || !Array.isArray(config.whiteboard)) {
            config.whiteboard = [];
        }
        super(config)

        if (CaptureAll.singleInstance) {
            return CaptureAll.singleInstance
        }
        CaptureAll.singleInstance = this
    }

    async process(message) {
        logger.log('CaptureAll at [' + message.tags + '] ' + this.getTag())
        this.config.whiteboard.push(message)
        return this.emit('message', message)
    }
}

export default CaptureAll

================
File: src/processors/util/SetMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'

class SetMessage extends Processor {

    constructor(config) {
        super(config)
        logger.log('SetMessage constructor')
    }

    async process(message) {
        const setters = await this.getSetters(this.config, this.configKey, ns.trm.setValue)
        for (let i = 0; i < setters.length; i++) {
            message[setters[i].key] = setters[i].value
        }
        return this.emit('message', message)
    }

    async getSetters(config, configKey, term) {



        const settersRDF = GrapoiHelpers.listToArray(config, configKey, term)
        const dataset = this.config
        var setters = []
        for (let i = 0; i < settersRDF.length; i++) {
            let setter = settersRDF[i]
            let poi = rdf.grapoi({ dataset: dataset, term: setter })
            let key = poi.out(ns.trm.key).value
            let value = poi.out(ns.trm.value).value
            setters.push({ "key": key, "value": value })
        }
        return setters
    }
}

export default SetMessage

================
File: src/processors/util/ShowConfig.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ShowConfig extends Processor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async process(message) {



        if (this.verbose) logger.log("\n***  Show Config ***")


        logger.log("***************************")
        logger.log("***  Config")
        logger.reveal(this.config)
        logger.log("***************************")




        return this.emit('message', message)
    }
}

export default ShowConfig

================
File: src/processors/util/ShowMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ShowMessage extends Processor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async process(message) {



        if (this.verbose) logger.log("\n***  Show Message ***")

        logger.log("***************************")
        logger.log("***  Message")
        logger.reveal(message)
        logger.log("***************************")




        return this.emit('message', message)
    }
}

export default ShowMessage

================
File: src/processors/util/ShowTransmission.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ShowTransmission extends Processor {

    async process(message) {
        logger.log(this.transmission.toString())
        return this.emit('message', message)
    }
}

export default ShowTransmission

================
File: src/processors/util/Stash.js
================
import rdf from 'rdf-ext'
import { fromFile, toFile } from 'rdf-utils-fs'
import Processor from '../base/Processor.js'












class Stash extends Processor {





    constructor(config) {
        super(config)
    }






    async process(message) {
        const manifestFilename = rootDir + '/manifest.ttl'
        const stream = fromFile(manifestFilename)


        message.rootDir = rootDir
        message.dataset = await rdf.dataset().import(stream)
        return this.emit('message', message)
    }
}
export default Stash

================
File: src/processors/util/UtilProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import ShowMessage from './ShowMessage.js'
import ShowTransmission from './ShowTransmission.js'
import CaptureAll from './CaptureAll.js'
import ShowConfig from './ShowConfig.js'
import WhiteboardToMessage from './WhiteboardToMessage.js'
import SetMessage from './SetMessage.js'

class UtilProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.t.ShowMessage)) {
            return new ShowMessage(config)
        }
        if (type.equals(ns.t.ShowTransmission)) {
            return new ShowTransmission(config)
        }
        if (type.equals(ns.t.CaptureAll)) {
            return new CaptureAll(config)
        }
        if (type.equals(ns.t.ShowConfig)) {
            return new ShowConfig(config)
        }
        if (type.equals(ns.t.WhiteboardToMessage)) {
            return new WhiteboardToMessage(config)
        }
        if (type.equals(ns.t.SetMessage)) {
            return new SetMessage(config)
        }

        return false
    }
}
export default UtilProcessorsFactory

================
File: src/processors/util/WhiteboardToMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class WhiteboardToMessage extends Processor {

    constructor(config) {
        super(config);
    }
    async process(message) {

        logger.log('WhiteboardToMessage at [' + message.tags + '] ' + this.getTag())

        const originalArray = this.config.whiteboard

        message.whiteboard = Object.keys(originalArray).reduce((acc, key) => {
            const value = originalArray[key];
            if (value !== undefined && value !== null) {
                Object.keys(value).forEach((prop) => {
                    if (!acc[prop]) {
                        acc[prop] = [];
                    }
                    acc[prop].push(value[prop]);
                });
            }
            return acc;
        }, {});

        return this.emit('message', message)
    }
}

export default WhiteboardToMessage

================
File: src/processors/xmpp/XmppClient.js
================
import logger from "../../utils/Logger.js";
import Processor from "../base/Processor.js";









class XmppClient extends Processor {




  constructor(config) {
    super(config);
  }





  async process(message) {
    logger.setLogLevel("debug");


    return this.emit("message", message);
  }
}

export default XmppClient

================
File: src/processors/xmpp/XmppProcessorsFactory.js
================
import logger from "../../utils/Logger.js";
import ns from "../../utils/ns.js";

import ProcessorTemplate from "./XmppClient.js";



class XmppProcessorsFactory {
  static createProcessor(type, config) {
    if (type.equals(ns.t.XmppClient)) {
      return new XmppClient(config)
    }
    return false
  }
}
export default XmppProcessorsFactory

================
File: src/processors/about.md
================
# Creating a new Processor

- update repopacks for `transmissions` and `trans-apps`
- create a new chat session in existing Project
- upload repopacks to Claude, with anything else that might be relevant (handover from previous session?)
- follow the prompt model as in `/home/danny/workspaces_hkms-desktop/postcrafts-raw/transmissions/prompts/github-list.md`
- remember additions to `xProcessorsFactory.js` and `transmissions/src/engine/AbstractProcessorFactory.js`

#:todo add comment creation
#:todo check simples & application suitability
#:todo create document creation workflow
#:todo create manifest.ttl creation
#:todo make crossrefs.md, crossrefs.ttl
#:todo create manifest.ttl consumption
#:todo add test creation
#:todo wire to an API, include file creation ops
#:todo add support in #:hyperdata-desktop

#:todo dedicated transmissions model, fine-tuned on relevant docs

#:todo extract todos as something like :

```turtle
<http://hyperdata.it/transmissions/src/processors/about/nid123> a pv:ToDoItem ;
dc:source <http://hyperdata.it/transmissions/src/processors/about.md> ;
pv:semtag "#:todo" ;
dc:line "3" ;
dc:title "tbd" ;
dc:content "extract todos as something like :" .
```

================
File: src/simples/env-loader/about.md
================
node src/apps-simple/env-loader/env-loader.js

from:

:envy a trm:Pipeline ;

# trm:pipe (:SC :s10 :s20 :SM) .

trm:pipe (:p10 :p20 :SC) .
:p10 a :EnvLoader .
:p20 a :WhiteboardToMessage .

================
File: src/simples/env-loader/env-loader.js
================
import logger from '../../utils/Logger.js'
import EnvLoader from '../../processors/system/EnvLoader.js'
import WhiteboardToMessage from '../../processors/util/WhiteboardToMessage.js'

logger.log('EnvLoader simple')

const config = { whiteboard: [] }

const p10 = new EnvLoader(config)
p10.id = 'http://purls.org/stuff/#p10'

const p20 = new WhiteboardToMessage(config)
p10.id = 'http://purls.org/stuff/#p20'

var message = {
    "dataDir": "src/applications/env-loader-test/data",
    "rootDir": "[no key]",
    "tags": "SM"
}

const x = 3

message = await p10.process(message)

logger.log('p10 output ' + p10.getTag() + message)

message = await p20.process(message)

logger.log('p20 output ')

logger.reveal(message)

================
File: src/simples/nop/nop.js
================
import NOP from '../../processors/flow/NOP.js'

const config = {
    "runmode": "functions",
    whiteboard: []
}

const nop = new NOP(config)

var message = { 'value': '42' }

message = await nop.process(message)

console.log('value = ' + message.value)

================
File: src/simples/nop/simple-runner.js
================
import NOP from '../../processors/flow/NOP.js'
import Fork from '../../processors/flow/Fork.js'








async function main() {
    const config = {}
    const nop = new NOP(config)
    const fork = new Fork(config)

    var message = { 'value': '42' }



    var outputs = await nop.process(message)
    console.log('NOP outputs:', outputs)


    message.nForks = 3
    outputs = await fork.process(message)
    console.log('Fork outputs:', outputs)
}

main().catch(console.error)

================
File: src/simples/set-message/set-message.js
================
import logger from '../../utils/Logger.js'
import SetMessage from '../../processors/util/SetMessage.js'

const config = {
    "runmode": "functions",
    whiteboard: []
}

const setm = new SetMessage(config)

var message = { 'value': '42' }

message = await setm.process(message)

logger.log('value = ' + message.value)

logger.reveal(message)

================
File: src/utils/footpath.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import logger from './Logger.js'







let footpath = {}

footpath.resolve = function footpath(here, relative, start) {

    const loggy = false
    if (loggy) {
        logger.debug("\n*** start footpath.resolve ***")
        logger.debug("process.cwd() = " + process.cwd())
        logger.debug("here = " + here)
        logger.debug("relative = " + relative)
        logger.debug("start = " + start)
    }

    const __filename = fileURLToPath(here)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, relative)
    const filePath = path.join(rootDir, start)

    if (loggy) {
        logger.debug("__filename = " + __filename)
        logger.debug("__dirname = " + __dirname)
        logger.debug("rootDir = " + rootDir)
        logger.debug("filePath = " + filePath)
        logger.debug("*** end footpath.resolve ***\n")
    }

    return filePath
}

footpath.urlLastPart = function footpath(url = 'http://example.org/not-a-url') {


    const urlObj = new URL(url);
    const hash = urlObj.hash;
    const path = urlObj.pathname;
    const lastPart = hash ? hash.replace(/^#/, '') : path.split('/').pop();
    // } catch {
    //  return 'not-a-url'

    return lastPart;
}

export default footpath

================
File: src/utils/GrapoiHelpers.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'
import ns from './ns.js'
import logger from './Logger.js'



class GrapoiHelpers {


    static async readDataset(filename) {
        const stream = fromFile(filename)
        const dataset = await rdf.dataset().import(stream)
        return dataset
    }

    static async writeDataset(dataset, filename) {
        await toFile(dataset.toStream(), filename)
    }


    static listToArray(dataset, term, property) {
        const poi = rdf.grapoi({ dataset: dataset, term: term })
        const first = poi.out(property).term

        let p = rdf.grapoi({ dataset, term: first })
        let object = p.out(ns.rdf.first).term

        const result = [object]

        while (true) {
            let restHead = p.out(ns.rdf.rest).term
            let p2 = rdf.grapoi({ dataset, term: restHead })
            let object = p2.out(ns.rdf.first).term

            if (restHead.equals(ns.rdf.nil)) break
            result.push(object)
            p = rdf.grapoi({ dataset, term: restHead })
        }
        return result
    }





    static listObjects(dataset, subjectList, predicate) {
        const objects = []
        for (const subject of subjectList) {
            logger.log("subject = " + subject.value)
            let p = rdf.grapoi({ dataset, term: subject })
            let object = p.out(predicate).term
            logger.log("object = " + object.value)
            objects.push(object)
        }
        return objects
    }
}
export default GrapoiHelpers

================
File: src/utils/Logger.js
================
import fs from 'fs'


let logger = {}

logger.logfile = 'latest.log'



const LOG_LEVELS = [
    "debug",
    "info",
    "log",
    "warn",
    "error",
]
const logComponent = "api.logger"

logger.appendLogToFile = function (message) {
    if (logger.logfile) {
        fs.appendFileSync(logger.logfile, message + '\n', 'utf8')
    }
}

logger.setLogLevel = function (logLevel = "warn") {


    logger.currentLogLevel = logLevel
}

logger.timestampISO = function () {
    let now = new Date()
    return now.toISOString()
}

logger.log = function (msg, level = "log") {
    const currentLevelIndex = LOG_LEVELS.indexOf(logger.currentLogLevel)
    const messageLevelIndex = LOG_LEVELS.indexOf(level)

    if (messageLevelIndex >= currentLevelIndex) {
        console[level](msg)
        const logMessage = `[${logger.timestampISO()}] [${level.toUpperCase()}] - ${msg}`
        logger.appendLogToFile(logMessage)
    }
}

logger.reveal = function (instance) {
    const serialized = {}

    logger.log('***    hidden keys :  ')
    for (const key in instance) {
        if (key === 'dataset') {
            logger.log('[[dataset found, skipping]]')
            continue
        }
        if (key.startsWith('_')) {


            logger.log(`       ${key}`)
            continue
        } else {
            if (instance.hasOwnProperty(key)) {
                let kiki = instance[key]

                if (kiki) {
                    if (Buffer.isBuffer(kiki)) {
                        kiki = kiki.toString()

                    }
                    if (kiki.length > 100) {
                        try {
                            kiki = kiki.substring(0, 100) + '...'
                        } catch (e) {
                            kiki = kiki.slice(0, 99)
                        }
                    }
                    serialized[key] = kiki
                } else {
                    serialized[key] = '[no key]'
                }
            }
        }
    }
    const props = JSON.stringify(serialized, null, 2)
    if (!instance) {
        logger.log(` no instance defined`)
        return
    }
    logger.log(`Instance of ${instance.constructor.name} with properties - \n${props}`)

}

logger.debug = function (msg) {
    logger.log(msg, "debug")
}

logger.info = function (msg) {
    logger.log(msg, "info")
}

logger.warn = function (msg) {
    logger.log(msg, "warn")
}

logger.error = function (msg) {
    logger.log(msg, "error")
}

logger.poi = function exploreGrapoi(grapoi, predicates, objects, subjects) {

    console.log('Properties of the Grapoi object:')
    for (const prop in grapoi) {
        console.log(`\t${prop}: ${grapoi[prop]}`)
    }


    console.log('\nPath:')
    const path = grapoi.out(predicates, objects).in(predicates, subjects)
    for (const quad of path.quads()) {
        console.log(`\t${quad.predicate.value}: ${quad.object.value}`)
    }
}

function handleExit(options, exitCode) {
    if (options.cleanup) {

    }
    if (exitCode || exitCode === 0) console.log(exitCode)
    if (options.exit) process.exit()
}


process.on('exit', handleExit.bind(null, { cleanup: true }))
process.on('SIGINT', handleExit.bind(null, { exit: true }))
process.on('SIGUSR1', handleExit.bind(null, { exit: true }))
process.on('SIGUSR2', handleExit.bind(null, { exit: true }))
process.on('uncaughtException', handleExit.bind(null, { exit: true }))

export default logger

================
File: src/utils/ns.js
================
import rdf from 'rdf-ext'

const ns = {
    rdf: rdf.namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#'),
    rdfs: rdf.namespace('http://www.w3.org/2000/01/rdf-schema#'),
    dc: rdf.namespace('http://purl.org/dc/terms/'),
    schema: rdf.namespace('http://schema.org/'),
    xsd: rdf.namespace('http://www.w3.org/2001/XMLSchema#'),
    trm: rdf.namespace('http://purl.org/stuff/transmission/'),
    t: rdf.namespace('http://hyperdata.it/transmissions/'),
    fs: rdf.namespace('http://purl.org/stuff/filesystem/'),
    pc: rdf.namespace('http://purl.org/stuff/postcraft/')
}





ns.getShortname = function (url) {

    if (!url) return
    url = url.toString()
    const lastSlashIndex = url.lastIndexOf('/');
    const lastHashIndex = url.lastIndexOf('#');
    const path = url.slice(lastSlashIndex + 1);
    return path.split('#')[0].split('?')[0];
}
export default ns

================
File: src/utils/t2j.js
================
import { Readable } from 'readable-stream'
import rdf from '@rdfjs/data-model'
import SerializerJsonld from '@rdfjs/serializer-jsonld'
import Serializer from '@rdfjs/serializer-turtle'
import N3Parser from '@rdfjs/parser-n3'
import { fromFile } from 'rdf-utils-fs'
import { toFile } from 'rdf-utils-fs'

const testTurtle = `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <https://hyperdata.it/transmissions/> . # for custom terms & instances

:simplepipe a trm:TransmissionTransmission ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a trm:StringSource .
:s2 a trm:AppendProcess .
:s3 a trm:StringSink .
`
export class Turtle2JSONLD {
    static async convert(turtle) {

        let parser = new N3Parser({ factory: rdf })



        const input = Readable.from(turtle)

        const output = parser.import(input)

        const serializerJsonld = new SerializerJsonld()
        const jsonStream = serializerJsonld.import(output)





        const outputJson = await Turtle2JSONLD.streamToString(jsonStream)
        return outputJson
    }

    static stringToStream(str) {
        const stream = new Readable();
        stream.push(str);
        stream.push(null);
        return stream;
    }

    static streamToString(stream) {
        const chunks = [];
        return new Promise((resolve, reject) => {
            stream.on('data', (chunk) => {
                chunks.push(Buffer.from(chunk))
                console.log('chunk:', chunk)
            }
            );
            stream.on('error', (err) => reject(err));
            stream.on('end', () => {
                const result = Buffer.concat(chunks).toString('utf8')
                resolve(result)
                console.log('****************** result:', result)
            });
        })
    }
}



const testJson = await Turtle2JSONLD.convert(testTurtle)
console.log('')
console.log(testJson)

================
File: src/utils/test_runner.js
================
import fs from 'fs';
import path from 'path';

const testFiles = fs.readdirSync(__dirname).filter(file => file.startsWith('test_'));

testFiles.forEach(testFile => {
    console.log(`Running ${testFile}`);
    require(path.join(__dirname, testFile));
});

================
File: staging/schema-documentation.md
================
# Transmissions Templates Schema Documentation

## JSON Schema
The JSON schema provides a strict validation structure for application definitions:

### Core Components
1. `appName`: String identifier used in paths & configurations
2. `purpose`: Object describing application goals
   - `primaryGoal`: Single sentence description
   - `inputs`/`outputs`: Array of expected formats
   - `behavior`: Expected processing behavior 

3. `processingRequirements`: Object defining data flow
   - `input`: Message & file specifications
   - `steps`: Array of processing stages
   - `output`: Expected results format

4. `components`: Required implementation pieces
   - `newProcessors`: New code needed
   - `configFiles`: Configuration files
   - `existingProcessors`: Reused components

5. `testing`: Test specifications
   - `unitTests`: Component-level tests
   - `integrationTests`: Pipeline tests

## RDF Schema
The RDF schema models the application definition as linked data:

### Core Classes
1. `trm:ApplicationDefinition`
   - Links requirements, components, testing
   - Provides metadata about application

2. `trm:Requirements` 
   - Models input/output specifications
   - Defines processing steps
   - Links to configurations

3. `trm:ComponentList`
   - Catalogs needed processors
   - Specifies configurations
   - References existing code

4. `trm:TestingRequirements`
   - Defines test scenarios
   - Specifies test data
   - Documents expectations

### Additional Properties
```turtle
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix prj: <http://purl.org/stuff/project/> .

trm:ApplicationDefinition
    trm:hasVersion "1.0" ;
    trm:requiresTransmissionsVersion "2.0" ;
    trm:category "data-processing" ;
    prj:status "development" ;
    prj:priority "medium" ;
    prj:estimatedEffort "2d" ;
    prj:dependencies [
        a prj:DependencyList ;
        prj:requires "markmap-lib", "rdf-ext"
    ] ;
    prj:documentation [
        a prj:DocumentationRequirements ;
        prj:requiresAPIDoc true ;
        prj:requiresUserGuide true
    ] ;
    prj:deployment [
        a prj:DeploymentRequirements ;
        prj:environment "node16+" ;
        prj:memoryRequirements "512MB"
    ] .
```

================
File: staging/template-cli.js
================
#!/usr/bin/env node
import TemplateGenerator from './TemplateGenerator.js';

const generator = new TemplateGenerator();
generator.run();

================
File: staging/template-generator.js
================
import fs from 'fs/promises';
import path from 'path';
import { Command } from 'commander';
import inquirer from 'inquirer';
import { rdf, namespace } from '@rdfjs/data-model';
import { Writer } from 'n3';

const ns = {
    trm: namespace('http://purl.org/stuff/transmission/'),
    prj: namespace('http://purl.org/stuff/project/'),
    app: namespace('http://example.org/app/')
};

class TemplateGenerator {
    constructor() {
        this.program = new Command();
        this.setupCommands();
    }

    setupCommands() {
        this.program
            .name('trans-template')
            .description('Generate Transmissions application templates')
            .version('1.0.0');

        this.program
            .command('create')
            .description('Create new application templates')
            .argument('<name>', 'Application name')
            .option('-f, --format <format>', 'Output format (json|turtle|markdown)', 'json')
            .action(async (name, options) => {
                const answers = await this.promptForDetails(name);
                await this.generateTemplates(name, answers, options.format);
            });
    }

    async promptForDetails(name) {
        return inquirer.prompt([
            {
                type: 'input',
                name: 'primaryGoal',
                message: 'What is the primary goal of this application?'
            },
            {
                type: 'input',
                name: 'inputs',
                message: 'Input formats (comma-separated):',
                filter: input => input.split(',').map(i => i.trim())
            },
            {
                type: 'input',
                name: 'outputs',
                message: 'Output formats (comma-separated):',
                filter: input => input.split(',').map(i => i.trim())
            },
            {
                type: 'input',
                name: 'processors',
                message: 'Required processors (comma-separated):',
                filter: input => input.split(',').map(i => i.trim())
            },
            {
                type: 'confirm',
                name: 'needsTests',
                message: 'Generate test templates?',
                default: true
            }
        ]);
    }

    async generateTemplates(name, answers, format) {
        const outputDir = path.join(process.cwd(), name);
        await fs.mkdir(outputDir, { recursive: true });

        const templates = {
            json: () => this.generateJSON(name, answers),
            turtle: () => this.generateTurtle(name, answers),
            markdown: () => this.generateMarkdown(name, answers)
        };

        const content = templates[format]();
        const fileExt = format === 'turtle' ? 'ttl' : format;

        await fs.writeFile(
            path.join(outputDir, `app-definition.${fileExt}`),
            content
        );


        await this.generateFileStructure(outputDir, answers);

        console.log(`Generated ${format} template in ${outputDir}`);
    }

    generateJSON(name, answers) {
        return JSON.stringify({
            appName: name,
            purpose: {
                primaryGoal: answers.primaryGoal,
                inputs: answers.inputs,
                outputs: answers.outputs
            },
            processingRequirements: {
                steps: answers.processors.map(p => ({
                    name: p,
                    processor: p,
                    config: {}
                }))
            },
            testing: {
                unitTests: answers.processors.map(p => ({
                    component: p,
                    cases: ['basic', 'error']
                }))
            }
        }, null, 2);
    }

    generateTurtle(name, answers) {
        const writer = new Writer();
        const app = ns.app(name);

        writer.addQuad(
            app,
            ns.trm('title'),
            rdf.literal(name)
        );

        writer.addQuad(
            app,
            ns.trm('primaryGoal'),
            rdf.literal(answers.primaryGoal)
        );

        answers.processors.forEach(p => {
            const proc = ns.app(p);
            writer.addQuad(
                app,
                ns.trm('hasProcessor'),
                proc
            );
        });

        return writer.toString();
    }

    generateMarkdown(name, answers) {
        return `# ${name}

## Purpose
${answers.primaryGoal}

## Inputs
${answers.inputs.map(i => `- ${i}`).join('\n')}

## Outputs
${answers.outputs.map(o => `- ${o}`).join('\n')}

## Processors
${answers.processors.map(p => `- ${p}`).join('\n')}

## Testing
${answers.needsTests ? '- Unit tests required\n- Integration tests required' : 'No tests specified'}
`;
    }

    async generateFileStructure(outputDir, answers) {
        const dirs = [
            'processors',
            'tests',
            'config'
        ];

        for (const dir of dirs) {
            await fs.mkdir(path.join(outputDir, dir), { recursive: true });
        }


        const files = {
            'transmissions.ttl': '',
            'config.ttl': '',
            'about.md': `# ${path.basename(outputDir)}\n\n${answers.primaryGoal}`
        };

        for (const [file, content] of Object.entries(files)) {
            await fs.writeFile(
                path.join(outputDir, file),
                content
            );
        }
    }

    run() {
        this.program.parse();
    }
}

export default TemplateGenerator;

================
File: staging/template-tool-docs.md
================
# Transmissions Template Generator

## Overview
Command-line tool to generate scaffold for new Transmissions applications.

## Installation
```bash
npm install -g trans-template
```

## Usage
```bash
# Generate new application template
trans-template create my-app

# Specify output format
trans-template create my-app --format turtle

# Help
trans-template --help
```

## Generated Structure
```
my-app/
 processors/      # New processors
 tests/          # Test files
 config/         # Configuration files
 transmissions.ttl  # Pipeline definition
 config.ttl         # Service configuration
 about.md          # Application documentation
```

## Template Formats

### JSON
- Full application definition
- Validates against JSON schema
- Used for tooling/automation

### Turtle
- RDF representation
- Linked data model
- Integration with semantic tools

### Markdown
- Human-readable format
- Documentation focus
- GitHub-friendly

## Environment Variables
- `TRANS_TEMPLATE_PATH`: Base path for templates
- `TRANS_CONFIG_PATH`: Path to configuration

## Error Handling
- Validates input parameters
- Creates missing directories
- Reports detailed errors

## Extension
Custom templates can be added in:
```bash
~/.config/trans-template/templates/
```

================
File: staging/transmissions-prompt-template.md
================
# Transmissions Application Definition Template

## Application Name
[Short name for the application, will be used in file paths]

## Purpose
- Primary goal in one sentence
- Key inputs and outputs
- Expected behavior

## Technical Context
- Base paths:
  - Transmissions core: ~/github-danny/transmissions
  - Applications: ~/github-danny/trans-apps

## Processing Requirements 
1. Input Format
   - Message structure
   - File formats/paths
   - Required fields

2. Processing Steps
   - List processing stages in sequence
   - Note any existing processors to use
   - Identify new processors needed

3. Output Format
   - Expected message structure
   - File formats/paths
   - Required fields

## Required Components
- New processors to create [list]
- Configuration files needed [list]
- Existing processors to reuse [list]

## Example Usage
```bash
./trans [app-name] [example command line arguments]
```

## Success Criteria
- List specific conditions that indicate successful implementation
- Example outputs or results

## Technical Constraints
- Note any performance requirements
- Special error handling needs
- Specific processor features needed

## Reference Material
- Links to example code
- Related processors
- Documentation needed

================
File: staging/transmissions-template-schema.json
================
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Transmissions Application Definition",
  "type": "object",
  "required": ["appName", "purpose", "technicalContext", "processingRequirements", "components", "testing"],
  "properties": {
    "appName": {
      "type": "string",
      "description": "Short name for the application"
    },
    "purpose": {
      "type": "object",
      "required": ["primaryGoal", "inputs", "outputs", "behavior"],
      "properties": {
        "primaryGoal": { "type": "string" },
        "inputs": { "type": "array", "items": { "type": "string" }},
        "outputs": { "type": "array", "items": { "type": "string" }},
        "behavior": { "type": "string" }
      }
    },
    "technicalContext": {
      "type": "object",
      "required": ["transmissionsPath", "applicationsPath"],
      "properties": {
        "transmissionsPath": { "type": "string" },
        "applicationsPath": { "type": "string" }
      }
    },
    "processingRequirements": {
      "type": "object",
      "required": ["input", "steps", "output"],
      "properties": {
        "input": {
          "type": "object",
          "properties": {
            "messageStructure": { "type": "object" },
            "fileFormats": { "type": "array", "items": { "type": "string" }},
            "requiredFields": { "type": "array", "items": { "type": "string" }}
          }
        },
        "steps": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": { "type": "string" },
              "processor": { "type": "string" },
              "config": { "type": "object" }
            }
          }
        },
        "output": {
          "type": "object",
          "properties": {
            "messageStructure": { "type": "object" },
            "fileFormats": { "type": "array", "items": { "type": "string" }},
            "requiredFields": { "type": "array", "items": { "type": "string" }}
          }
        }
      }
    },
    "components": {
      "type": "object",
      "required": ["newProcessors", "configFiles", "existingProcessors"],
      "properties": {
        "newProcessors": { 
          "type": "array", 
          "items": { "type": "string" }
        },
        "configFiles": { 
          "type": "array", 
          "items": { "type": "string" }
        },
        "existingProcessors": { 
          "type": "array", 
          "items": { "type": "string" }
        }
      }
    },
    "testing": {
      "type": "object",
      "required": ["unitTests", "integrationTests"],
      "properties": {
        "unitTests": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "component": { "type": "string" },
              "cases": { 
                "type": "array",
                "items": { "type": "string" }
              }
            }
          }
        },
        "integrationTests": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "scenario": { "type": "string" },
              "steps": { 
                "type": "array",
                "items": { "type": "string" }
              }
            }
          }
        }
      }
    }
  }
}

================
File: staging/transmissions-template-turtle.txt
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix prov: <http://www.w3.org/ns/prov#> .
@prefix prj: <http://purl.org/stuff/project/> .
@prefix app: <http://example.org/app/> .

app:Application a trm:ApplicationDefinition ;
    dcterms:title "Application Name" ;
    dcterms:description "Primary goal description" ;
    trm:basePath "/path/to/application" ;
    trm:hasRequirement app:ProcessingRequirements ;
    trm:hasComponent app:Components ;
    trm:hasTesting app:Testing .

app:ProcessingRequirements a trm:Requirements ;
    trm:input [
        a trm:InputRequirement ;
        trm:messageStructure "JSON structure" ;
        trm:fileFormat "format specification" ;
        trm:requiredField "field1", "field2"
    ] ;
    trm:processing [
        a trm:ProcessingStep ;
        trm:order 1 ;
        trm:processor "ProcessorName" ;
        trm:configuration app:ProcessorConfig
    ] ;
    trm:output [
        a trm:OutputRequirement ;
        trm:messageStructure "JSON structure" ;
        trm:fileFormat "format specification" ;
        trm:requiredField "field1", "field2"
    ] .

app:Components a trm:ComponentList ;
    trm:newProcessor [
        a trm:Processor ;
        dcterms:title "Processor Name" ;
        trm:class "ProcessorClass" ;
        trm:sourcePath "/path/to/source"
    ] ;
    trm:configFile [
        a trm:Configuration ;
        dcterms:title "Config Name" ;
        trm:format "Turtle" ;
        trm:path "/path/to/config"
    ] ;
    trm:existingProcessor [
        a trm:Processor ;
        dcterms:title "Existing Processor" ;
        trm:class "ProcessorClass"
    ] .

app:Testing a trm:TestingRequirements ;
    trm:unitTest [
        a trm:UnitTest ;
        dcterms:title "Test Name" ;
        trm:component "ComponentName" ;
        trm:testCase "test description"
    ] ;
    trm:integrationTest [
        a trm:IntegrationTest ;
        dcterms:title "Test Scenario" ;
        trm:step "step description" ;
        trm:expectedResult "expected outcome"
    ] ;
    trm:testData [
        a trm:TestData ;
        trm:input "/path/to/test/input" ;
        trm:expected "/path/to/test/output"
    ] .

================
File: staging/transmissions-testing-template.md
================
# Transmissions Testing Requirements Template

## Unit Tests
1. Individual Processors
   - Input validation tests
   - Core processing tests 
   - Error handling tests
   - Edge case tests
   - Sample data needed

2. Configuration Tests
   - Config file loading
   - Config validation
   - Default values
   - Error conditions

## Integration Tests
1. Pipeline Tests
   - Full transmission flow
   - Inter-processor communication
   - Message transformations
   - File I/O operations

2. System Tests
   - CLI interface testing
   - File system interactions
   - Error recovery
   - Resource cleanup

## Test Data Requirements
1. Input Test Files
   - Sample files needed
   - File formats
   - Edge cases
   - Invalid data samples

2. Expected Outputs
   - Reference output files
   - Validation criteria
   - Format specifications
   - Error conditions

## Test Environment
1. Setup Requirements
   - Directory structure
   - Required permissions
   - External dependencies
   - Configuration files

2. Cleanup Procedures
   - File cleanup
   - Resource cleanup
   - State reset
   - Verification steps

## Documentation
1. Test Coverage
   - Required coverage metrics
   - Critical paths
   - Exception paths
   - Performance criteria

2. Test Reports
   - Required metrics
   - Format specifications
   - Success criteria
   - Failure analysis

================
File: test-failures/test_env-loader/2024-11-28T17-44-11.419Z/test-output.json
================
{
  "result": {
    "stdout": "\nCommandUtils.run()\nCommandUtils.run, process.cwd() = /home/danny/github-danny/transmissions\nCommandUtils.run, application = test_env-loader\nCommandUtils.run, target = undefined\n\nCommandUtils.splitName, fullPath  = test_env-loader\n\nCommandUtils.splitName, parts  = test_env-loader\nCommandUtils.splitName, appName:test_env-loader, appPath:test_env-loader, task:false,\n\n\n    CommandUtils.run, \n    appName = test_env-loader\n    appPath = test_env-loader\n    subtask = undefined\n    target = undefined\n\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/envy>\n| Create processor :s10 of type :EnvLoader\n| Create processor :s20 of type :WhiteboardToMessage\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [s10] => [s20]\nTransmission.connect from http://hyperdata.it/transmissions/s10 to http://hyperdata.it/transmissions/s10\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s10 this.toName =  http://hyperdata.it/transmissions/s20\n  > Connect #1 [s20] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/s20 to http://hyperdata.it/transmissions/s20\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s20 this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/envy>\n| Running : http://hyperdata.it/transmissions/s10 a EnvLoader\nTypeError: (intermediate value).handle is not a function\n    at EnvLoader.process (file:///home/danny/github-danny/transmissions/src/processors/system/EnvLoader.js:43:22)\n    at EnvLoader.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at EnvLoader.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n    at EnvLoader.receive (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:145:20)\n    at Transmission.process (file:///home/danny/github-danny/transmissions/src/engine/Transmission.js:36:23)\n    at ApplicationManager.start (file:///home/danny/github-danny/transmissions/src/core/ApplicationManager.js:76:36)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async CommandUtils.begin (file:///home/danny/github-danny/transmissions/src/api/common/CommandUtils.js:40:16)\n    at async Object.handler (file:///home/danny/github-danny/transmissions/src/api/cli/run.js:69:13)\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.771"
  },
  "config": {
    "name": "test_env-loader"
  }
}

================
File: test-failures/test_env-loader/2024-11-28T17-46-20.677Z/test-output.json
================
{
  "result": {
    "stdout": "\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/envy>\n| Create processor :s10 of type :EnvLoader\n| Create processor :s20 of type :WhiteboardToMessage\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [s10] => [s20]\nTransmission.connect from http://hyperdata.it/transmissions/s10 to http://hyperdata.it/transmissions/s10\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s10 this.toName =  http://hyperdata.it/transmissions/s20\n  > Connect #1 [s20] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/s20 to http://hyperdata.it/transmissions/s20\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s20 this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/envy>\n| Running : http://hyperdata.it/transmissions/s10 a EnvLoader\nTypeError: (intermediate value).handle is not a function\n    at EnvLoader.process (file:///home/danny/github-danny/transmissions/src/processors/system/EnvLoader.js:43:22)\n    at EnvLoader.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at EnvLoader.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n    at EnvLoader.receive (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:145:20)\n    at Transmission.process (file:///home/danny/github-danny/transmissions/src/engine/Transmission.js:36:23)\n    at ApplicationManager.start (file:///home/danny/github-danny/transmissions/src/core/ApplicationManager.js:76:36)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async CommandUtils.begin (file:///home/danny/github-danny/transmissions/src/api/common/CommandUtils.js:40:16)\n    at async Object.handler (file:///home/danny/github-danny/transmissions/src/api/cli/run.js:69:13)\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.709"
  },
  "config": {
    "name": "test_env-loader"
  }
}

================
File: test-failures/test_env-loader/2024-11-28T18-31-38.300Z/test-output.json
================
{
  "result": {
    "stdout": "\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/envy>\n| Create processor :s10 of type :EnvLoader\n| Create processor :s20 of type :WhiteboardToMessage\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [s10] => [s20]\nTransmission.connect from http://hyperdata.it/transmissions/s10 to http://hyperdata.it/transmissions/s10\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s10 this.toName =  http://hyperdata.it/transmissions/s20\n  > Connect #1 [s20] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/s20 to http://hyperdata.it/transmissions/s20\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s20 this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/envy>\n| Running : http://hyperdata.it/transmissions/s10 a EnvLoader\nTypeError: (intermediate value).handle is not a function\n    at EnvLoader.process (file:///home/danny/github-danny/transmissions/src/processors/system/EnvLoader.js:43:22)\n    at EnvLoader.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at EnvLoader.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n    at EnvLoader.receive (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:145:20)\n    at Transmission.process (file:///home/danny/github-danny/transmissions/src/engine/Transmission.js:36:23)\n    at ApplicationManager.start (file:///home/danny/github-danny/transmissions/src/core/ApplicationManager.js:76:36)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async CommandUtils.begin (file:///home/danny/github-danny/transmissions/src/api/common/CommandUtils.js:40:16)\n    at async Object.handler (file:///home/danny/github-danny/transmissions/src/api/cli/run.js:69:13)\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.723"
  },
  "config": {
    "name": "test_env-loader"
  }
}

================
File: test-failures/test_env-loader/2024-11-28T18-34-16.177Z/test-output.json
================
{
  "result": {
    "stdout": "\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/envy>\n| Create processor :s10 of type :EnvLoader\n| Create processor :s20 of type :WhiteboardToMessage\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [s10] => [s20]\nTransmission.connect from http://hyperdata.it/transmissions/s10 to http://hyperdata.it/transmissions/s10\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s10 this.toName =  http://hyperdata.it/transmissions/s20\n  > Connect #1 [s20] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/s20 to http://hyperdata.it/transmissions/s20\nConnector.connect this.fromName = http://hyperdata.it/transmissions/s20 this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/envy>\n| Running : http://hyperdata.it/transmissions/s10 a EnvLoader\n| Running >>> :  (s10) s20 a WhiteboardToMessage\nWhiteboardToMessage at (s10.s20) s20\nTypeError: (intermediate value).handle is not a function\n    at WhiteboardToMessage.process (file:///home/danny/github-danny/transmissions/src/processors/util/WhiteboardToMessage.js:28:22)\n    at WhiteboardToMessage.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at WhiteboardToMessage.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n    at WhiteboardToMessage.receive (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:145:20)\n    at EnvLoader.<anonymous> (file:///home/danny/github-danny/transmissions/src/engine/Connector.js:32:25)\n    at EnvLoader.emit (node:events:518:28)\n    at EnvLoader.emit (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:246:15)\n    at EnvLoader.process (file:///home/danny/github-danny/transmissions/src/processors/system/EnvLoader.js:41:21)\n    at EnvLoader.executeQueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:187:24)\n    at EnvLoader.enqueue (file:///home/danny/github-danny/transmissions/src/processors/base/Processor.js:156:18)\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.708"
  },
  "config": {
    "name": "test_env-loader"
  }
}

================
File: test-failures/test_http-server/2024-11-30T12-30-16.673Z/test-output.json
================
{
  "result": {
    "stdout": "\n+ ***** Construct Transmission :  <http://hyperdata.it/transmissions/mini>\n| Create processor :server of type :HttpServer\n| Create processor :SM of type :ShowMessage\n  > Connect #0 [server] => [SM]\nTransmission.connect from http://hyperdata.it/transmissions/server to http://hyperdata.it/transmissions/server\nConnector.connect this.fromName = http://hyperdata.it/transmissions/server this.toName =  http://hyperdata.it/transmissions/SM\n\n+ ***** Execute Transmission :  <http://hyperdata.it/transmissions/mini>\n| Running : http://hyperdata.it/transmissions/server a HttpServer\nError: listen EADDRINUSE: address already in use :::4000\n    at Server.setupListenHandle [as _listen2] (node:net:1872:16)\n    at listenInCluster (node:net:1920:12)\n    at Server.listen (node:net:2008:7)\n    at Function.listen (/home/danny/github-danny/transmissions/node_modules/express/lib/application.js:635:24)\n    at ServerWorker.start (file:///home/danny/github-danny/transmissions/src/processors/http/HttpServerWorker.js:42:36)\n    at MessagePort.<anonymous> (file:///home/danny/github-danny/transmissions/src/processors/http/HttpServerWorker.js:18:26)\n    at [nodejs.internal.kHybridDispatch] (node:internal/event_target:826:20)\n    at exports.emitMessage (node:internal/per_context/messageport:23:28) {\n  code: 'EADDRINUSE',\n  errno: -98,\n  syscall: 'listen',\n  address: '::',\n  port: 4000\n}\n0\n",
    "stderr": "",
    "code": 0,
    "signal": null,
    "success": false,
    "duration": "0.898"
  },
  "config": {
    "name": "test_http-server"
  }
}

================
File: tests/examples/test-data-usage.js
================
import TestDataGenerator from '../helpers/TestDataGenerator.js';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

async function generateTestData() {

    const generator = new TestDataGenerator(
        path.join(__dirname, '../../src/applications/test_markmap/data')
    );

    try {

        await generator.init();


        const files = await generator.generateMarkdownFiles(3);
        console.log('Generated basic test files:', files);


        await generator.generateNestedStructure();
        console.log('Generated nested structure');


        await generator.generateEdgeCases();
        console.log('Generated edge cases');


        await generator.generateRequiredOutputs(
            path.join(generator.baseDir, 'input')
        );
        console.log('Generated required outputs');

    } catch (error) {
        console.error('Error generating test data:', error);
    }
}


generateTestData().catch(console.error);

================
File: tests/helpers/file-test-helper.js
================
import fs from 'fs/promises';
import path from 'path';
import logger from '../../src/utils/Logger.js';

class FileTestHelper {
    constructor(baseDir) {
        this.baseDir = baseDir;
    }

    async setup() {
        await fs.mkdir(this.baseDir, { recursive: true });
        await fs.mkdir(path.join(this.baseDir, 'input'), { recursive: true });
        await fs.mkdir(path.join(this.baseDir, 'output'), { recursive: true });
    }

    async cleanup() {
        try {
            await fs.rm(this.baseDir, { recursive: true, force: true });
        } catch (error) {
            logger.error('Cleanup error:', error);
        }
    }

    async createTestFile(subPath, content) {
        const filePath = path.join(this.baseDir, subPath);
        await fs.mkdir(path.dirname(filePath), { recursive: true });
        await fs.writeFile(filePath, content);
        return filePath;
    }

    async compareFiles(actualPath, expectedPath) {
        try {
            const actual = await fs.readFile(actualPath, 'utf8');
            const expected = await fs.readFile(expectedPath, 'utf8');
            return {
                match: actual.trim() === expected.trim(),
                actual: actual.trim(),
                expected: expected.trim()
            };
        } catch (error) {
            logger.error('File comparison error:', error);
            return {
                match: false,
                error: error.message
            };
        }
    }

    async clearOutputFiles(pattern = 'output-*') {
        const outputDir = path.join(this.baseDir, 'output');
        const files = await fs.readdir(outputDir);
        for (const file of files) {
            if (file.match(pattern)) {
                await fs.unlink(path.join(outputDir, file));
            }
        }
    }

    async fileExists(filePath) {
        try {
            await fs.access(path.join(this.baseDir, filePath));
            return true;
        } catch {
            return false;
        }
    }
}

export default FileTestHelper;

================
File: tests/helpers/reporter.js
================
import { SpecReporter } from 'jasmine-spec-reporter';

class CustomReporter {
    constructor() {
        this.specReporter = new SpecReporter({
            spec: {
                displayPending: true
            }
        });
    }

    jasmineStarted() {
        this.specReporter.jasmineStarted.apply(this.specReporter, arguments);
    }

    suiteStarted() {
        this.specReporter.suiteStarted.apply(this.specReporter, arguments);
    }

    specStarted() {
        this.specReporter.specStarted.apply(this.specReporter, arguments);
    }

    specDone() {
        this.specReporter.specDone.apply(this.specReporter, arguments);
    }

    suiteDone() {
        this.specReporter.suiteDone.apply(this.specReporter, arguments);
    }

    jasmineDone() {
        this.specReporter.jasmineDone.apply(this.specReporter, arguments);
    }
}

export default CustomReporter;

================
File: tests/helpers/test-data-generator.js
================
import path from 'path';
import fs from 'fs/promises';
import logger from '../../src/utils/Logger.js';

class TestDataGenerator {
    constructor(baseDir) {
        this.baseDir = baseDir;
    }

    async init() {
        await fs.mkdir(this.baseDir, { recursive: true });
        await fs.mkdir(path.join(this.baseDir, 'input'), { recursive: true });
        await fs.mkdir(path.join(this.baseDir, 'output'), { recursive: true });
    }

    async generateMarkdownFiles(count = 5) {
        const files = [];
        for (let i = 1; i <= count; i++) {
            const content = this.generateMarkdownContent(i);
            const filename = `test-${String(i).padStart(2, '0')}.md`;
            const filepath = path.join(this.baseDir, 'input', filename);

            await fs.writeFile(filepath, content);
            files.push(filepath);
        }
        return files;
    }

    generateMarkdownContent(depth = 3) {
        const content = [];
        content.push(`# Test Document ${depth}`);

        for (let i = 1; i <= depth; i++) {
            content.push(`\n${'#'.repeat(i + 1)} Section ${i}`);
            content.push(this.generateListItems(i));

            if (i < depth) {
                content.push(this.generateParagraph(i));
            }
        }

        return content.join('\n');
    }

    generateListItems(count) {
        const items = [];
        for (let i = 1; i <= count; i++) {
            items.push(`* List item ${i}`);
            if (Math.random() > 0.5) {
                for (let j = 1; j <= 2; j++) {
                    items.push(`  * Nested item ${i}.${j}`);
                }
            }
        }
        return items.join('\n');
    }

    generateParagraph(seed) {
        const sentences = [
            "Lorem ipsum dolor sit amet.",
            "Consectetur adipiscing elit.",
            "Sed do eiusmod tempor incididunt.",
            "Ut labore et dolore magna aliqua.",
            "Ut enim ad minim veniam."
        ];

        return sentences.slice(0, seed + 1).join(' ');
    }

    async generateNestedStructure(depth = 3) {
        for (let i = 1; i <= depth; i++) {
            const dirPath = path.join(this.baseDir, 'input', 'nested',
                ...Array(i).fill(0).map((_, idx) => `level-${idx + 1}`));

            await fs.mkdir(dirPath, { recursive: true });

            const content = this.generateMarkdownContent(i);
            const filepath = path.join(dirPath, `nested-${i}.md`);
            await fs.writeFile(filepath, content);
        }
    }

    async generateEdgeCases() {
        const cases = {
            'empty.md': '',
            'only-title.md': '# Solo Title',
            'special-chars.md': '# Test & < > " \' Document',
            'very-deep.md': this.generateDeepStructure(10),
            'wide.md': this.generateWideStructure(10)
        };

        const edgeCaseDir = path.join(this.baseDir, 'input', 'edge-cases');
        await fs.mkdir(edgeCaseDir, { recursive: true });

        for (const [filename, content] of Object.entries(cases)) {
            await fs.writeFile(path.join(edgeCaseDir, filename), content);
        }
    }

    generateDeepStructure(depth) {
        return Array(depth)
            .fill(0)
            .map((_, i) => `${'#'.repeat(i + 1)} Level ${i + 1}`)
            .join('\n');
    }

    generateWideStructure(width) {
        const content = ['# Wide Document'];
        for (let i = 1; i <= width; i++) {
            content.push(`## Section ${i}`);
            for (let j = 1; j <= width; j++) {
                content.push(`* Item ${i}.${j}`);
            }
        }
        return content.join('\n');
    }

    async generateRequiredOutputs(sourceDir) {
        const files = await fs.readdir(sourceDir);
        for (const file of files) {
            if (file.endsWith('.md')) {
                const content = await fs.readFile(path.join(sourceDir, file));


                await fs.writeFile(
                    path.join(this.baseDir, 'output', `required-${file.replace('.md', '.mm.html')}`),
                    this.wrapHTML(content.toString())
                );


                await fs.writeFile(
                    path.join(this.baseDir, 'output', `required-${file.replace('.md', '.mm.svg')}`),
                    this.generateSVG(content.toString())
                );
            }
        }
    }

    wrapHTML(content) {
        return `<!DOCTYPE html>
<html>
<head>
    <title>Markmap</title>
</head>
<body>
    <div class="markmap">
        ${content}
    </div>
</body>
</html>`;
    }

    generateSVG(content) {

        return `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 600">
    <text x="10" y="20">Test SVG for: ${content.split('\n')[0]}</text>
</svg>`;
    }

    async cleanup() {
        try {
            await fs.rm(this.baseDir, { recursive: true, force: true });
        } catch (error) {
            logger.error('Cleanup error:', error);
        }
    }
}

export default TestDataGenerator;

================
File: tests/integration/configmap.spec.js
================
import { expect } from 'chai';
import rdf from 'rdf-ext';
import ConfigMap from '../../src/processors/rdf/ConfigMap.js';
import ns from '../../src/utils/ns.js';

describe('ConfigMap Integration Tests', () => {
  let configMap;
  let message;
  const testBasePath = '/test/base';

  beforeEach(() => {
    configMap = new ConfigMap({});
    message = {
      rootDir: testBasePath,
      dataset: rdf.dataset()
    };
  });

  function addTestData(predicates) {
    const subject = rdf.namedNode('http://hyperdata.it/transmissions/Content');
    message.dataset.add(rdf.quad(
      subject,
      ns.rdf.type,
      ns.pc.ConfigSet
    ));

    for (const [pred, obj] of Object.entries(predicates)) {
      message.dataset.add(rdf.quad(
        subject,
        ns.fs[pred],
        rdf.literal(obj)
      ));
    }
  }

  it('should resolve paths from ContentGroup', async () => {
    addTestData({
      sourceDirectory: 'content/src',
      targetDirectory: 'content/out'
    });

    await configMap.process(message);
    expect(message.contentGroup?.Content?.sourceDir).to.equal('/test/base/content/src');
    expect(message.contentGroup?.Content?.targetDir).to.equal('/test/base/content/out');
  });

  it('should preserve absolute paths', async () => {
    addTestData({
      sourceDirectory: '/abs/path/src',
      targetDirectory: '/abs/path/out'
    });

    await configMap.process(message);
    expect(message.contentGroup?.Content?.sourceDir).to.equal('/abs/path/src');
    expect(message.contentGroup?.Content?.targetDir).to.equal('/abs/path/out');
  });

  it('should handle missing paths', async () => {
    addTestData({});
    await configMap.process(message);
    expect(message.contentGroup?.Content?.sourceDir).to.be.undefined;
    expect(message.contentGroup?.Content?.targetDir).to.be.undefined;
  });

  it('should normalize paths', async () => {
    addTestData({
      sourceDirectory: 'content/../src'
    });
    await configMap.process(message);
    expect(message.contentGroup?.Content?.sourceDir).to.equal('/test/base/src');
  });
});

================
File: tests/integration/filename-mapper.spec.js
================
import { expect } from 'chai';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

describe('filename-mapper test', () => {
    const dataDir = path.join(__dirname, '../../src/applications/test_filename-mapper/data');
    const inputDir = path.join(dataDir, 'input');
    const outputDir = path.join(dataDir, 'output');

    async function setupTestFiles() {
        await fs.mkdir(outputDir, { recursive: true });
        const inputContent = 'Test content for filename mapping';
        await fs.writeFile(path.join(inputDir, 'input-01.txt'), inputContent);
    }

    async function cleanup() {
        try {
            const files = await fs.readdir(outputDir);
            for (const file of files) {
                if (file.startsWith('output-')) {
                    await fs.unlink(path.join(outputDir, file));
                }
            }
        } catch (err) {
            if (err.code !== 'ENOENT') throw err;
        }
    }

    beforeEach(async () => {
        await cleanup();
        await setupTestFiles();
    });

    afterAll(async () => {
        await cleanup();
    });

    async function compareFiles() {
        const outputFile = path.join(outputDir, 'output-01.txt');
        const requiredFile = path.join(outputDir, 'required-01.txt');

        const [output, required] = await Promise.all([
            fs.readFile(outputFile, 'utf8'),
            fs.readFile(requiredFile, 'utf8')
        ]);

        return output.trim() === required.trim();
    }

    it('should process files correctly', async () => {
        const { exec } = await import('child_process');
        const util = await import('util');
        const execAsync = util.promisify(exec);

        const result = await execAsync('node src/api/cli/run.js test_filename-mapper', {
            cwd: path.resolve(__dirname, '../..')
        });

        const matched = await compareFiles();
        expect(matched).to.be.true;
    });
});

================
File: tests/integration/fork.spec.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'

describe('fork test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const logFile = path.join(__dirname, '../../latest.log')

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000

    it('should create correct number of message paths', (done) => {
        exec('node src/api/cli/run.js test_fork', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {

                const logs = stdout.toString()
                const nopMatches = logs.match(/NOP at/g)
                const nopCount = nopMatches ? nopMatches.length : 0


                expect(nopCount).to.equal(3)
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })
})

================
File: tests/integration/fs-rw_simple.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import fs from 'fs/promises'

describe('fs-rw simple test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, '../../')

    const outputFile = path.join(rootDir, 'src/applications/test_fs-rw/data/output/output-01.md')
    const requiredFile = path.join(rootDir, 'src/applications/test_fs-rw/data/output/required-01.md')

    beforeEach(async () => {
        try {
            await fs.unlink(outputFile)
        } catch (error) {
            if (error.code !== 'ENOENT') throw error
        }
    })

    it('should process file correctly', async () => {

        await import('../../src/applications/test_fs-rw/simple.js')


        const output = await fs.readFile(outputFile, 'utf8')
        const required = await fs.readFile(requiredFile, 'utf8')

        expect(output.trim()).to.equal(required.trim())
    })
})

================
File: tests/integration/fs-rw.spec.js
================
import { expect } from 'chai';
import rdf from 'rdf-ext';
import ConfigMap from '../../src/processors/rdf/ConfigMap.js';
import ns from '../../src/utils/ns.js';

describe('ConfigMap Integration Tests', () => {
    let configMap;
    let message;
    const testBasePath = '/test/base';

    beforeEach(() => {
        configMap = new ConfigMap({});
        message = {
            rootDir: testBasePath,
            dataset: rdf.dataset()
        };
    });

    function addTestData(predicates) {
        const subject = rdf.namedNode('http://hyperdata.it/transmissions/Content');
        message.dataset.add(rdf.quad(
            subject,
            ns.rdf.type,
            ns.pc.ConfigSet
        ));

        for (const [pred, obj] of Object.entries(predicates)) {
            message.dataset.add(rdf.quad(
                subject,
                ns.fs[pred],
                rdf.literal(obj)
            ));
        }
    }

    it('should resolve paths from ContentGroup', async () => {
        addTestData({
            sourceDirectory: 'content/src',
            targetDirectory: 'content/out'
        });

        await configMap.process(message);
        expect(message.contentGroup?.Content?.sourceDir).to.equal('/test/base/content/src');
        expect(message.contentGroup?.Content?.targetDir).to.equal('/test/base/content/out');
    });

    it('should preserve absolute paths', async () => {
        addTestData({
            sourceDirectory: '/abs/path/src',
            targetDirectory: '/abs/path/out'
        });

        await configMap.process(message);
        expect(message.contentGroup?.Content?.sourceDir).to.equal('/abs/path/src');
        expect(message.contentGroup?.Content?.targetDir).to.equal('/abs/path/out');
    });

    it('should handle missing paths', async () => {
        addTestData({});
        await configMap.process(message);
        expect(message.contentGroup?.Content?.sourceDir).to.be.undefined;
        expect(message.contentGroup?.Content?.targetDir).to.be.undefined;
    });

    it('should normalize paths', async () => {
        addTestData({
            sourceDirectory: 'content/../src'
        });
        await configMap.process(message);
        expect(message.contentGroup?.Content?.sourceDir).to.equal('/test/base/src');
    });
});

================
File: tests/integration/http-server.spec.js
================
import { expect } from 'chai';
import fetch from 'node-fetch';
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);

describe('HTTP Server Integration', () => {
    const SERVER_URL = 'http://localhost:4000';
    const TEST_VALUES = { testKey: 'testValue' };
    let serverProcess;

    before(async () => {
        serverProcess = exec('node src/api/cli/run.js test_http-server');
        await new Promise(resolve => setTimeout(resolve, 1000));
    });

    after(async () => {
        try {
            await fetch(`${SERVER_URL}/shutdown`, {
                method: 'POST'
            });
        } catch (e) {
            console.log('Server already stopped');
        }
    });

    it('should serve static files', async () => {
        const response = await fetch(`${SERVER_URL}/transmissions/test/`);
        expect(response.status).to.equal(200);
        const html = await response.text();
        expect(html).to.include('HTTP Server Test Interface');
    });

    it('should accept message values and shutdown', async () => {
        const response = await fetch(`${SERVER_URL}/shutdown`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(TEST_VALUES)
        });
        expect(response.status).to.equal(200);
    });
});

================
File: tests/integration/markmap.spec.js
================
import path from 'path';
import { fileURLToPath } from 'url';
import { expect } from 'chai';
import fs from 'fs/promises';
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

describe('Markmap Integration', () => {
    const testDir = path.join(__dirname, '../../src/applications/markmap/data/test');
    const testFiles = ['test1.md', 'test2.md'];

    beforeAll(async () => {

        await fs.mkdir(testDir, { recursive: true });


        await fs.writeFile(
            path.join(testDir, 'test1.md'),
            '# Test 1\n## Section 1\n* Item 1\n* Item 2'
        );

        await fs.writeFile(
            path.join(testDir, 'test2.md'),
            '# Test 2\n## Section 2\n* Item A\n* Item B'
        );
    });

    afterAll(async () => {

        await fs.rm(testDir, { recursive: true, force: true });
    });

    it('should process multiple markdown files through ForEach', async () => {
        const message = {
            paths: testFiles.map(f => path.join(testDir, f))
        };

        const result = await execAsync(
            `./trans markmap -m '${JSON.stringify(message)}'`
        );


        for (const file of testFiles) {
            const basePath = path.join(testDir, path.parse(file).name);


            const htmlPath = `${basePath}.mm.html`;
            const htmlExists = await fs.access(htmlPath)
                .then(() => true)
                .catch(() => false);
            expect(htmlExists).to.be.true;


            const html = await fs.readFile(htmlPath, 'utf8');
            expect(html).to.include('<html');
            expect(html).to.include(`Test ${file[4]}`);


            const svgPath = `${basePath}.mm.svg`;
            const svgExists = await fs.access(svgPath)
                .then(() => true)
                .catch(() => false);
            expect(svgExists).to.be.true;


            const svg = await fs.readFile(svgPath, 'utf8');
            expect(svg).to.include('<svg');
            expect(svg).to.include(`Test ${file[4]}`);
        }
    });

    it('should handle empty input paths array', async () => {
        const message = { paths: [] };

        const result = await execAsync(
            `./trans markmap -m '${JSON.stringify(message)}'`
        );


        const files = await fs.readdir(testDir);
        expect(files.filter(f => f.endsWith('.mm.html') || f.endsWith('.mm.svg')))
            .to.have.lengthOf(0);
    });

    it('should handle invalid markdown files gracefully', async () => {

        const invalidPath = path.join(testDir, 'invalid.md');
        await fs.writeFile(invalidPath, '# Title\n## [Invalid markdown');

        const message = {
            paths: [invalidPath]
        };

        try {
            await execAsync(`./trans markmap -m '${JSON.stringify(message)}'`);
        } catch (error) {
            expect(error.message).to.include('Error processing markdown');
        }
    });
});

================
File: tests/integration/restructure_simple.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import fs from 'fs/promises'

describe('restructure simple test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, '../../')

    const outputFile = path.join(rootDir, 'src/applications/test_restructure/data/output/output-01.json')
    const requiredFile = path.join(rootDir, 'src/applications/test_restructure/data/output/required-01.json')

    beforeEach(async () => {
        try {
            await fs.unlink(outputFile)
        } catch (error) {
            if (error.code !== 'ENOENT') throw error
        }
    })

    it('should process JSON file correctly', async () => {
        console.log('Running restructure test')

        await import('../../src/applications/test_restructure/simple.js')


        const output = JSON.parse(await fs.readFile(outputFile, 'utf8'))
        const required = JSON.parse(await fs.readFile(requiredFile, 'utf8'))


        expect(output).to.deep.equal(required)
    })
})

================
File: tests/integration/restructure.spec.js
================
import footpath from '../../src/utils/footpath.js'
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs/promises'

describe('test_restructure', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const dataDir = path.join(__dirname, '../../src/applications/test_restructure/data')

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000

    async function clearOutputFiles() {
        console.log('Clearing output files...')
        const outputDir = path.join(dataDir, 'output')
        const files = await fs.readdir(outputDir)
        for (const file of files) {
            if (file.startsWith('output-')) {
                await fs.unlink(path.join(outputDir, file))
                console.log(`Deleted ${file}`)
            }
        }
    }

    async function compareFiles(index) {
        const outputFile = path.join(dataDir, 'output', `output-${index}.json`)
        const requiredFile = path.join(dataDir, 'output', `required-${index}.json`)

        console.log(`Comparing files:`)
        console.log(`Output: ${outputFile}`)
        console.log(`Required: ${requiredFile}`)

        const output = JSON.parse(await fs.readFile(outputFile, 'utf8'))
        const required = JSON.parse(await fs.readFile(requiredFile, 'utf8'))


        return JSON.stringify(output) === JSON.stringify(required)
    }

    beforeEach(async () => {
        await clearOutputFiles()
    })

    it('should process files correctly', (done) => {
        console.log('Running transmission...')
        exec('node src/api/cli/run.js test_restructure', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {
                console.log('Transmission output:', stdout)
                if (stderr) console.error('Stderr:', stderr)

                const matched = await compareFiles('01')
                expect(matched).to.be.true
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })
})

================
File: tests/integration/run-command.spec.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs/promises'

describe('run-command test', function () {
    const __filename = fileURLToPath(import.meta.url)
    const __dirname = path.dirname(__filename)
    const testDir = path.resolve(__dirname, '../../src/applications/test_run-command')

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 10000


    beforeAll(async function () {
        try {
            await fs.mkdir(testDir, { recursive: true })

            const configTtl = `@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

t:RunCommandConfig a trm:ServiceConfig ;
    trm:configKey t:runCommand ;
    trm:command "echo \\"test\\"" .`

            await fs.writeFile(path.join(testDir, 'config.ttl'), configTtl)

            const transmissionsTtl = `@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

:test_run_command a trm:Transmission ;
    trm:pipe (:p10 :p20) .

:p10 a :RunCommand ;
    trm:configKey :runCommand .

:p20 a :ShowMessage .`

            await fs.writeFile(path.join(testDir, 'transmissions.ttl'), transmissionsTtl)
        } catch (err) {
            console.error('Setup error:', err)
            throw err
        }
    })

    it('should execute command successfully', (done) => {
        exec('node src/api/cli/run.js test_run-command', async (error, stdout, stderr) => {
            if (error) {
                console.error('Exec error:', error)
                done(error)
                return
            }

            try {
                expect(stdout).to.include('test')
                expect(stderr).to.be.empty
                done()
            } catch (err) {
                console.error('Test error:', err)
                done(err)
            }
        })
    })


    afterAll(async function () {
        try {
            await fs.rm(testDir, { recursive: true, force: true })
        } catch (err) {
            console.error('Cleanup error:', err)
        }
    })
})

================
File: tests/integration/test_apps.spec.js
================
import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs/promises'
import { glob } from 'glob'
import { existsSync } from 'fs'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const rootDir = path.resolve(__dirname, '../../')

async function runCommand(command, options) {
    return new Promise((resolve) => {
        const startTime = process.hrtime()
        const proc = exec(`./trans ${command}`, { ...options, cwd: rootDir })
        let stdout = '', stderr = ''

        proc.stdout.on('data', (data) => {
            stdout += data
            process.stdout.write(data)
        })

        proc.stderr.on('data', (data) => {
            stderr += data
            process.stderr.write(data)
        })

        proc.on('exit', (code, signal) => {
            const endTime = process.hrtime(startTime)
            const duration = (endTime[0] + endTime[1] / 1e9).toFixed(3)


            const hasError = stdout.includes('TypeError:') ||
                stdout.includes('Error:') ||
                stderr.includes('TypeError:') ||
                stderr.includes('Error:')

            const result = {
                stdout,
                stderr,
                code,
                signal,
                success: code === 0 && !hasError,
                duration
            }
            resolve(result)
        })
    })
}

describe('Application Integration Tests', function () {
    it('should run test applications', async function () {
        const testApps = await glob(path.join(rootDir, 'src/applications/test_*'))
        expect(testApps.length).to.be.greaterThan(0)

        for (const appDir of testApps) {
            const appName = path.basename(appDir)
            console.log(`\nTesting ${appName}`)

            const configPath = path.join(appDir, 'test-config.json')
            const config = existsSync(configPath) ?
                JSON.parse(await fs.readFile(configPath, 'utf8')) :
                { transmissions: [{ name: appName }] }

            for (const tx of config.transmissions) {
                let cmd = tx.name
                if (tx.message) cmd += ` -m '${JSON.stringify(tx.message)}'`

                const result = await runCommand(cmd)

                if (!result.success) {
                    console.error('\n' + '='.repeat(80))
                    console.error(` Test failed for ${cmd}`)
                    console.error('='.repeat(80))
                    console.error('\nExecution Details:')
                    console.error('-'.repeat(40))
                    console.error(`Duration: ${result.duration}s`)
                    console.error('Exit code:', result.code)
                    console.error('Signal:', result.signal)

                    if (result.error) {
                        console.error('\nError Details:')
                        console.error('-'.repeat(40))
                        console.error('Message:', result.error.message)
                        console.error('Stack:', result.error.stack)
                    }

                    if (result.stderr) {
                        console.error('\nStderr Output:')
                        console.error('-'.repeat(40))
                        console.error(result.stderr)
                    }

                    console.error('\nStdout Output:')
                    console.error('-'.repeat(40))
                    console.error(result.stdout || '(no stdout output)')

                    console.error('\nTest Configuration:')
                    console.error('-'.repeat(40))
                    console.error(JSON.stringify(tx, null, 2))
                    console.error('\n' + '='.repeat(80))

                    try {
                        const failuresDir = path.join(rootDir, 'test-failures', appName, new Date().toISOString().replace(/:/g, '-'))
                        await fs.mkdir(failuresDir, { recursive: true })
                        await fs.writeFile(
                            path.join(failuresDir, 'test-output.json'),
                            JSON.stringify({ result, config: tx }, null, 2)
                        )
                        console.error(`Failure details saved to: ${failuresDir}`)
                    } catch (err) {
                        console.error('Failed to save failure details:', err)
                    }
                } else {
                    console.log(` ${cmd} completed successfully (${result.duration}s)`)
                }

                expect(result.success, `Command failed: ${cmd} with exit code ${result.code}`).to.be.true

                if (tx.requiredFiles) {
                    for (const pattern of tx.requiredFiles) {
                        const outputFiles = await glob(path.join(appDir, 'data/output', pattern))
                        for (const outputFile of outputFiles) {
                            const requiredFile = outputFile.replace('output-', 'required-')
                            const [output, required] = await Promise.all([
                                fs.readFile(outputFile, 'utf8'),
                                fs.readFile(requiredFile, 'utf8')
                            ])
                            expect(output.trim()).to.equal(required.trim())
                        }
                    }
                }
            }
        }
    })
})

================
File: tests/support/jasmine-browser.json
================
{
  "srcDir": "src",
  "srcFiles": [
    "**/*.js"
  ],
  "specDir": "spec",
  "specFiles": [
    "**/*[sS]pec.js"
  ],
  "helpers": [
    "helpers/**/*.js"
  ],
  "env": {
    "stopSpecOnExpectationFailure": false,
    "stopOnSpecFailure": false,
    "random": true
  },
  "browser": {
    "name": "firefox"
  }
}

================
File: tests/unit/filename-mapper.spec.js
================
import FilenameMapper from '../../src/processors/fs/FilenameMapper.js';
import { expect } from 'chai';

describe('FilenameMapper', () => {
    let filenameMapper;

    beforeEach(() => {
        filenameMapper = new FilenameMapper({
            extensions: {
                html: '.mm.html',
                svg: '.mm.svg'
            }
        });
    });

    it('should map HTML extension correctly', async () => {
        const message = {
            filepath: '/test/example.md',
            format: 'html'
        };

        let outputMessage;
        filenameMapper.on('message', (msg) => {
            outputMessage = msg;
        });

        await filenameMapper.process(message);
        expect(outputMessage.filepath).to.equal('/test/example.mm.html');
    });

    it('should map SVG extension correctly', async () => {
        const message = {
            filepath: '/test/example.md',
            format: 'svg'
        };

        let outputMessage;
        filenameMapper.on('message', (msg) => {
            outputMessage = msg;
        });

        await filenameMapper.process(message);
        expect(outputMessage.filepath).to.equal('/test/example.mm.svg');
    });

    it('should throw error for missing filepath', async () => {
        const message = {
            format: 'html'
        };

        try {
            await filenameMapper.process(message);
            expect.fail('Should have thrown error');
        } catch (error) {
            expect(error.message).to.equal('No filepath provided in message');
        }
    });

    it('should throw error for unknown format', async () => {
        const message = {
            filepath: '/test/example.md',
            format: 'unknown'
        };

        try {
            await filenameMapper.process(message);
            expect.fail('Should have thrown error');
        } catch (error) {
            expect(error.message).to.equal('Unknown format: unknown');
        }
    });

    it('should preserve directory structure', async () => {
        const message = {
            filepath: '/deep/nested/path/example.md',
            format: 'html'
        };

        let outputMessage;
        filenameMapper.on('message', (msg) => {
            outputMessage = msg;
        });

        await filenameMapper.process(message);
        expect(outputMessage.filepath).to.equal('/deep/nested/path/example.mm.html');
    });
});

================
File: tests/unit/http-server_MetricsService.spec.js
================
import { expect } from 'chai';
import WebSocket from 'ws';
import MetricsService from '../../src/processors/http/services/MetricsService.js';
import http from 'http';

describe('MetricsService', () => {
    let metricsService;
    let server;
    let wsClient;

    beforeEach((done) => {
        server = http.createServer();
        server.listen(0, () => {
            metricsService = new MetricsService(server);
            const port = server.address().port;
            wsClient = new WebSocket(`ws://localhost:${port}`);
            wsClient.on('open', done);
        });
    });

    afterEach((done) => {
        wsClient.close();
        server.close(done);
    });

    it('should send metrics updates', (done) => {
        wsClient.on('message', (data) => {
            const metrics = JSON.parse(data.toString());
            expect(metrics).to.have.property('uptime');
            expect(metrics).to.have.property('requests');
            expect(metrics).to.have.property('connections');
            expect(metrics).to.have.property('memory');
            expect(metrics).to.have.property('cpu');
            done();
        });
    });

    it('should increment requests counter', () => {
        const initialRequests = metricsService.metrics.requests;
        metricsService.incrementRequests();
        expect(metricsService.metrics.requests).to.equal(initialRequests + 1);
    });

    it('should track connections', (done) => {
        const newClient = new WebSocket(`ws://localhost:${server.address().port}`);
        newClient.on('open', () => {
            expect(metricsService.metrics.connections).to.equal(2);
            newClient.close();
            setTimeout(() => {
                expect(metricsService.metrics.connections).to.equal(1);
                done();
            }, 100);
        });
    });
});

================
File: tests/unit/http-server_ShutdownService.spec.js
================
import { expect } from 'chai';
import express from 'express';
import ShutdownService from '../../src/processors/http/services/ShutdownService.js';

describe('ShutdownService', () => {
    let app;
    let shutdownService;
    let shutdownCalled = false;

    beforeEach(() => {
        app = express();
        shutdownService = new ShutdownService();
        shutdownService.setupMiddleware(app);
        shutdownService.setupEndpoints(app, () => { shutdownCalled = true; });
    });

    it('should reject requests without auth', (done) => {
        const mockReq = { headers: {} };
        const mockRes = {
            setHeader: jasmine.createSpy('setHeader'),
            status: function (code) {
                expect(code).toBe(401);
                return { send: function () { } };
            }
        };

        app._router.handle(mockReq, mockRes, () => { });
        expect(mockRes.setHeader).toHaveBeenCalledWith('WWW-Authenticate', 'Basic');
        done();
    });

    it('should accept valid credentials', (done) => {
        const credentials = Buffer.from(`${shutdownService.username}:${shutdownService.password}`).toString('base64');
        const mockReq = {
            headers: {
                authorization: `Basic ${credentials}`
            }
        };
        const mockRes = {
            status: jasmine.createSpy('status'),
            send: jasmine.createSpy('send')
        };
        const nextSpy = jasmine.createSpy('next');

        app._router.handle(mockReq, mockRes, nextSpy);
        expect(nextSpy).toHaveBeenCalled();
        done();
    });
});

================
File: tests/unit/markmap.spec..js
================
import MarkMap from '../../../src/applications/markmap/processors/MarkMap.js';
import { expect } from 'chai';

describe('MarkMap', () => {
    let markMap;

    beforeEach(() => {
        markMap = new MarkMap({});
    });

    it('should transform markdown to HTML and SVG', async () => {
        const message = {
            filepath: '/test/example.md',
            content: '# Test Heading\n## Subheading\n* Item 1\n* Item 2'
        };

        let htmlMessage, svgMessage;

        markMap.on('message', (msg) => {
            if (msg.filepath.endsWith('.mm.html')) {
                htmlMessage = msg;
            } else if (msg.filepath.endsWith('.mm.svg')) {
                svgMessage = msg;
            }
        });

        await markMap.process(message);

        expect(htmlMessage).to.exist;
        expect(htmlMessage.content).to.include('<html');
        expect(htmlMessage.content).to.include('Test Heading');
        expect(htmlMessage.filepath).to.equal('/test/example.mm.html');

        expect(svgMessage).to.exist;
        expect(svgMessage.content).to.include('<svg');
        expect(svgMessage.content).to.include('Test Heading');
        expect(svgMessage.filepath).to.equal('/test/example.mm.svg');
    });

    it('should handle empty content', async () => {
        const message = {
            filepath: '/test/empty.md',
            content: ''
        };

        try {
            await markMap.process(message);
            expect.fail('Should have thrown error');
        } catch (error) {
            expect(error.message).to.equal('No content provided in message');
        }
    });
});

================
File: tests/unit/NOP.spec.js
================
import NOP from '../../src/processors/util/NOP.js'
import { expect } from 'chai'

describe('NOP', function () {
    it('double() should return the input string concatenated with itself', function () {
        const nop = new NOP()
        const input = 'test'
        const expectedOutput = 'testtest'
        const output = nop.double(input)
        expect(output).to.equal(expectedOutput)
    })
})

================
File: tests/unit/PostcraftPrep.spec.js
================
import PostcraftPrep from '../../src/processors/postcraft/PostcraftPrep.js'
import { expect } from 'chai'

describe('PostcraftPrep', function () {
    beforeEach(function () {
        this.context = {
            content: 'only text',
            filename: 'minimal-filename.md'
        }
    })

    it('extractTitle(context) should lift the title from the filename', function () {
        this.context.filename = '2024-05-10_this-thing.md'
        const input = this.context
        const expectedOutput = 'This Thing'
        const pp = new PostcraftPrep()
        const output = pp.extractTitle(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractSlug(context) should return filename without path and extension', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        const input = this.context
        const expectedOutput = '2024-05-10_hello-postcraft'
        const pp = new PostcraftPrep()
        const output = pp.extractSlug(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractTargetFilename(context) should return the correct target filename', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        this.context.rootDir = '/root'
        this.context.entryContentMeta = {
            targetDir: 'target'
        }
        const input = this.context
        const expectedOutput = '/root/target/2024-05-10_hello-postcraft.html'
        const pp = new PostcraftPrep()
        const output = pp.extractTargetFilename(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractDates(context) should return the correct dates', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        const input = this.context
        const expectedOutput = { created: '2024-05-10', updated: (new Date()).toISOString().split('T')[0] }
        const pp = new PostcraftPrep()
        const output = pp.extractDates(input)
        expect(output).to.deep.equal(expectedOutput)
    })
})

================
File: tests/unit/RunCommand.spec.js
================
import RunCommand from '../../src/processors/unsafe/RunCommand.js';
import { expect } from 'chai';
import fs from 'fs/promises';
import path from 'path';

describe('RunCommand', function () {
    let runCommand;
    const dataDir = 'src/applications/test_runcommand/data';

    beforeEach(function () {
        jasmine.DEFAULT_TIMEOUT_INTERVAL = 3000;
        runCommand = new RunCommand({
            simples: true,
            allowedCommands: ['echo', 'ls'],
            blockedPatterns: ['rm', '|', ';'],
            timeout: 50
        });
    });

    it('should validate command output against required file', async function () {
        const requiredPath = path.join(dataDir, 'output', 'required-01.txt');
        const required = await fs.readFile(requiredPath, 'utf8');
        const message = { command: 'echo "Hello from RunCommand!"' };

        const result = await runCommand.process(message);
        expect(result.content.trim()).to.equal(required.trim());
    });

    it('should handle timeouts', async function () {

        const neverEndingCommand = `echo "test" && while true; do :; done`;
        try {
            await runCommand.executeCommand(neverEndingCommand);
            expect.fail('Should have timed out');
        } catch (error) {
            expect(error.message).to.equal('Command timeout');
        }
    });

    it('should block disallowed commands', async function () {
        const message = { command: 'rm -rf /' };
        try {
            await runCommand.process(message);
            expect.fail('Should have blocked dangerous command');
        } catch (error) {
            expect(error.message).to.include('not in allowed list');
        }
    });

    it('should block commands with dangerous patterns', async function () {
        const message = { command: 'echo "test" | grep test' };
        try {
            await runCommand.process(message);
            expect.fail('Should have blocked command with pipe');
        } catch (error) {
            expect(error.message).to.include('blocked pattern');
        }
    });
});

================
File: tests/unit/StringFilter.spec.js
================
import StringFilter from '../../src/processors/text/StringFilter.js';
import { expect } from 'chai';

describe('StringFilter', function () {

    function compose(content, include, exclude) {
        return { content, include, exclude };
    }


    const contentSamples = [
        '/home/user/documents/',
        '/home/user/documents/file.txt',
        '/var/log/',
        '/etc/config.conf',
        '/usr/local/bin/app',
        '/home/user/pictures/vacation/',
        '/home/user/pictures/vacation/photo.jpg',
        '/opt/',
        '/tmp/temp.file',
        '/home/user/.config/',
        '',
        undefined
    ];

    const patternSamples = [
        '*.txt',
        '*.jpg',
        '/home/user/*',
        '/var/*',
        '*/bin/*',
        ['*.txt', '*.jpg'],
        ['/home/user/*', '/var/*'],
        ['*/bin/*', '*.conf'],
        ['*.file', '/tmp/*'],
        ['/opt/*', '/etc/*'],
        '',
        [],
        undefined
    ];

    describe('isAccepted()', function () {
        it('should accept all content when include and exclude are empty', function () {
            const filter = new StringFilter();
            contentSamples.forEach(content => {
                if (content !== undefined) {
                    const message = compose(content, '', '');
                    expect(filter.isAccepted(message.content, message.exclude, message.include)).to.be.true;
                }
            });
        });

        it('should reject undefined content', function () {
            const filter = new StringFilter();
            const message = compose(undefined, '', '');
            expect(filter.isAccepted(message.content, message.exclude, message.include)).to.be.false;
        });

        it('should correctly apply include patterns', function () {
            const filter = new StringFilter();
            const includeTests = [
                { content: '/home/user/documents/file.txt', include: '*.txt', expected: true },
                { content: '/home/user/pictures/vacation/photo.jpg', include: '*.jpg', expected: true },
                { content: '/var/log/', include: '/var/*', expected: true },
                { content: '/home/user/documents/', include: '/home/user/*', expected: true },
                { content: '/usr/local/bin/app', include: '*/bin/*', expected: true },
                { content: '/etc/config.conf', include: ['*.conf', '*.txt'], expected: true },
                { content: '/opt/', include: ['/var/*', '/opt/*'], expected: true },
                { content: '/tmp/temp.file', include: '*.doc', expected: false }
            ];

            includeTests.forEach(test => {
                const message = compose(test.content, test.include, '');
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });

        it('should correctly apply exclude patterns', function () {
            const filter = new StringFilter();
            const excludeTests = [
                { content: '/home/user/documents/file.txt', exclude: '*.txt', expected: false },
                { content: '/home/user/pictures/vacation/photo.jpg', exclude: '*.jpg', expected: false },
                { content: '/var/log/', exclude: '/var/*', expected: false },
                { content: '/home/user/documents/', exclude: '/home/user/*', expected: false },
                { content: '/usr/local/bin/app', exclude: '*/bin/*', expected: false },
                { content: '/etc/config.conf', exclude: ['*.conf', '*.txt'], expected: false },
                { content: '/opt/', exclude: ['/var/*', '/tmp/*'], expected: true },
                { content: '/tmp/temp.file', exclude: '*.doc', expected: true }
            ];

            excludeTests.forEach(test => {
                const message = compose(test.content, '', test.exclude);
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });

        it('should correctly apply both include and exclude patterns', function () {
            const filter = new StringFilter();
            const combinedTests = [
                { content: '/home/user/documents/file.txt', include: '*.txt', exclude: '/var/*', expected: true },
                { content: '/var/log/system.log', include: '*.log', exclude: '/var/*', expected: false },
                { content: '/home/user/pictures/vacation/photo.jpg', include: ['/home/user/*', '*.jpg'], exclude: '*.png', expected: true },
                { content: '/etc/config.conf', include: ['*.conf', '*.txt'], exclude: ['/home/*', '/var/*'], expected: true },
                { content: '/usr/local/bin/app', include: '*/bin/*', exclude: '*/local/*', expected: false }
            ];

            combinedTests.forEach(test => {
                const message = compose(test.content, test.include, test.exclude);
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });
    });
});

================
File: tests/unit/StringReplace.spec.js
================
import StringReplace from '../../src/processors/text/StringReplace.js'
import { expect } from 'chai'




describe('StringReplace', function () {



    it('execute() should replace all occurrences of the match string with the replace string', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: 'Hello world! Hello universe!',
            match: 'Hello',
            replace: 'Hi'
        }


        stringReplace.process(message)


        const expectedOutput = 'Hi world! Hi universe!'
        expect(message.content).to.equal(expectedOutput)
    })




    it('execute() should not modify the content if the match string is not found', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: 'Hello world!',
            match: 'Goodbye',
            replace: 'Hi'
        }


        stringReplace.process(message)


        const expectedOutput = 'Hello world!'
        expect(message.content).to.equal(expectedOutput)
    })




    it('execute() should handle empty content string', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: '',
            match: 'Hello',
            replace: 'Hi'
        }


        stringReplace.process(message)


        const expectedOutput = ''
        expect(message.content).to.equal(expectedOutput)
    })
})

================
File: tests/unit/updated-shutdown-test.js
================
import { expect } from 'chai';
import express from 'express';
import jwt from 'jsonwebtoken';
import ShutdownService from '../../src/processors/http/services/ShutdownService.js';

describe('ShutdownService', () => {
    let app;
    let shutdownService;
    let shutdownCalled = false;

    beforeEach(() => {
        app = express();
        app.use(express.json());
        shutdownService = new ShutdownService();
        shutdownService.setupMiddleware(app);
        shutdownService.setupEndpoints(app, () => { shutdownCalled = true; });
    });

    it('should generate valid JWT tokens', (done) => {
        const token = shutdownService.generateToken();
        const decoded = jwt.verify(token, shutdownService.secret);
        expect(decoded).to.have.property('action', 'shutdown');
        done();
    });

    it('should require valid token for shutdown', (done) => {
        const validToken = shutdownService.generateToken();


        const mockReq = {
            headers: { authorization: `Bearer ${validToken}` }
        };
        const mockRes = {
            status: function(code) {
                return { send: function(msg) {} };
            }
        };
        const nextSpy = jasmine.createSpy('next');

        app._router.handle(mockReq, mockRes, nextSpy);
        expect(nextSpy).toHaveBeenCalled();
        done();
    });

    it('should reject expired tokens', (done) => {
        const expiredToken = jwt.sign(
            { action: 'shutdown' },
            shutdownService.secret,
            { expiresIn: '0s' }
        );

        setTimeout(() => {
            const mockReq = {
                headers: { authorization: `Bearer ${expiredToken}` }
            };
            const mockRes = {
                status: function(code) {
                    expect(code).toBe(403);
                    return {
                        send: function(msg) {
                            expect(msg).toBe('Invalid token');
                        }
                    };
                }
            };

            app._router.handle(mockReq, mockRes, () => {});
            done();
        }, 100);
    });
});

================
File: tests/about.md
================
npm test -- tests/unit/RunCommand.spec.js

npm test -- tests/integration/configmap.spec.js

================
File: types/grapoi.d.ts
================
import { DatasetCore, Quad, Term } from "@rdfjs/types";


interface Grapoi extends PathList {
    addList(predicates: Term | Term[], items: Term | Term[]): Grapoi;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): Grapoi;
    base(base: Term | Term[]): Grapoi;
}


interface Edge {
    dataset: DatasetCore;
    end: string;
    quad: Quad;
    start: string;
    term: Term;
    graph: Term;
    startTerm: Term;
}


interface Instruction {
    operation?: string;
    quantifier?: string;
    start?: string;
    end?: string;
    subjects?: Term[];
    predicates?: Term[];
    objects?: Term[];
    graphs?: DatasetCore[];
    items?: Term[];
    callback?: (edge: Edge, ptr: Path | PathList) => Path | PathList;
}


interface Path {
    addList(predicates: Term | Term[], items: Term | Term[]): Path;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): Path;
    deleteIn(predicates: Term | Term[], subjects: Term | Term[]): Path;
    deleteList(predicates: Term | Term[]): Path;
    deleteOut(predicates: Term | Term[], objects: Term | Term[]): Path;
    extend(edge: Edge): Path;
    execute(instruction: Instruction): Path;
}


interface PathList {
    addList(predicates: Term | Term[], items: Term | Term[]): PathList;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): PathList;
    deleteIn(predicates: Term | Term[], subjects: Term | Term[]): PathList;
    deleteList(predicates: Term | Term[]): PathList;
    deleteOut(predicates: Term | Term[], objects: Term | Term[]): PathList;
    distinct(): PathList;
    in(predicates: Term | Term[], subjects: Term | Term[]): PathList;
    isAny(): boolean;
    isList(): boolean;
    list(): Iterator<Term> | undefined;
    map(callback: Function): PathList[];
    out(predicates: Term | Term[], objects: Term | Term[]): PathList;
    quads(): Iterator<Quad>;
    execute(instruction: Instruction): PathList[];
}

================
File: .babelrc
================
{
  "plugins": ["@babel/syntax-dynamic-import"],
  "presets": [
    [
      "@babel/preset-env",
      {
        "modules": false
      }
    ]
  ]
}

================
File: .gitignore
================
**/src-old
**/*\ copy.js

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

================
File: jasmine.json
================
{
    "spec_dir": "tests",
    "spec_files": [
        "**/*[sS]pec.js"
    ],
    "helpers": [
        "helpers/reporter.js"
    ],
    "stopSpecOnExpectationFailure": true,
    "random": false
}

================
File: jsconfig.json
================
{
  "compilerOptions": {
    "target": "ES6",
    "module": "commonjs",
    "allowSyntheticDefaultImports": true,
    "baseUrl": "./",
    "paths": {
      "*": ["node_modules/*", "types/*"]
    }
  },
  "include": [
    "src/**/*",
    "src/api/cli/run.js",
    "../trans-apps/applications/git-apps/github_",
    "src-old/CommandUtils copy.js",
    "../trans-apps/applications/markmap"
  ],
  "exclude": ["node_modules", "**/node_modules/*"],
  "typeAcquisition": {
    "include": ["@rdfjs/types", "grapoi"]
  }
}

================
File: jsdoc.json
================
{
    "source": {
        "include": [
            "src"
        ],
        "exclude": [
            "node_modules"
        ],
        "includePattern": ".+\\.js(doc|x)?$",
        "excludePattern": "(^|\\/|\\\\)_"
    },
    "opts": {
        "verbose": true,
        "recurse": true,
        "destination": "./docs/jsdoc"
    },
    "plugins": [
        "plugins/markdown"
    ]
}

================
File: LICENSE
================
MIT License

Copyright (c) 2024 Danny Ayers

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: package.json
================
{
  "type": "module",
  "version": "1.0.0",
  "description": "Transmissions",
  "name": "transmissions",
  "scripts": {
    "test": "jasmine --config=jasmine.json --reporter=tests/helpers/reporter.js",
    "cov": "nyc -a --include=src --reporter=lcov npm run test",
    "docs": "jsdoc -c jsdoc.json",
    "build": "webpack --mode=production --node-env=production",
    "build:dev": "webpack --mode=development",
    "build:prod": "webpack --mode=production --node-env=production",
    "rp": "node --no-warnings $(which repomix) -c repomix.config-small.json . && node --no-warnings $(which repomix) -c repomix.config-large.json .",
    "watch": "webpack --watch",
    "serve": "webpack serve"
  },
  "nyc": {
    "report-dir": "spec/coverage",
    "exclude": [
      "spec/**/*"
    ]
  },
  "devDependencies": {
    "@babel/core": "^7.23.7",
    "@babel/preset-env": "^7.23.8",
    "autoprefixer": "^10.4.17",
    "babel-loader": "^9.1.3",
    "chai": "^5.0.3",
    "css-loader": "^6.9.1",
    "html-webpack-plugin": "^5.6.0",
    "jasmine": "^5.1.0",
    "jasmine-browser-runner": "^2.3.0",
    "jasmine-core": "^5.1.1",
    "jasmine-spec-reporter": "^7.0.0",
    "jsdoc": "^4.0.2",
    "mini-css-extract-plugin": "^2.7.7",
    "nyc": "^17.1.0",
    "postcss": "^8.4.33",
    "postcss-loader": "^8.0.0",
    "prettier": "^3.2.4",
    "style-loader": "^3.3.4",
    "webpack": "^5.90.0",
    "webpack-cli": "^5.1.4",
    "webpack-dev-server": "^4.15.1",
    "workbox-webpack-plugin": "^7.0.0"
  },
  "dependencies": {
    "@dotenvx/dotenvx": "^1.14.2",
    "@rdfjs/formats": "^4.0.0",
    "@rdfjs/parser-n3": "^2.0.2",
    "axios": "^1.6.8",
    "cheerio": "^1.0.0-rc.12",
    "d3": "^7.9.0",
    "ignore": "^7.0.0",
    "jsdom": "^25.0.0",
    "lodash": "^4.17.21",
    "marked": "^12.0.1",
    "marked-code-format": "^1.1.6",
    "marked-custom-heading-id": "^2.0.10",
    "marked-footnote": "^1.2.4",
    "markmap-lib": "^0.17.0",
    "markmap-render": "^0.17.0",
    "markmap-toolbar": "^0.17.0",
    "markmap-view": "^0.17.0",
    "nunjucks": "^3.2.4",
    "queue": "^7.0.0",
    "rdf-ext": "^2.5.2",
    "rdf-utils-fs": "^3.0.0",
    "repomix": "^0.2.12",
    "string-to-stream": "^3.0.1",
    "yargs": "^17.7.2"
  }
}

================
File: postcss.config.js
================
module.exports = {


  plugins: [["autoprefixer"]],
};

================
File: README.md
================
# transmissions

After _No Code_ and _Lo Code_ comes _Marginally Less Code_

**Transmissions** is a micro-framework intended to simplify construction of small pipeliney data processing applications in JavaScript (assuming you are already familiar with JavaScript and RDF).

The code is in active development, ie. **not stable**, subject to arbitrary changes.

A bit like `make` or a `package.json` builder. But much harder work (and fun).

Applications are defined in several places, the bits of interest are eg. Postcraft's [transmissions.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/transmissions.ttl) and [services.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/services.ttl).
The former defines the flow, the latter config of the services (under [src/services](https://github.com/danja/transmissions/tree/main/src/services)). The runtime instance of the application is given in the target [manifest.ttl](https://github.com/danja/postcraft/blob/main/danny.ayers.name/manifest.ttl).

### Installation etc.

This is not ready yet. But if you really must...

Make a fresh dir. Clone this repo and [Postcraft](https://github.com/danja/postcraft) into it.

```
cd transmissions
npm i
```

This may or may not work :

```
npm run test
```

Then if you do :

```
./trans postcraft /home/danny/github-danny/postcraft/danny.ayers.name
```

it may build a site (my blog - this is dogfooding to the max) under `public/home`

```
./trans
```

on its own should list the applications available. Most of these won't work, the code has been shapeshifting a lot.

### Status

**2024-09-02** Getting used as a serrrrriously over-engineered, feature-lacking static site builder, proof of concept is [Postcraft](https://github.com/danja/postcraft), as evinced by my [blog](https://danny.ayers.name/) (where, for now at least you will find update on this). But it mostly works as intended. Docs lagging. But now I have a documentation engine...

Documentation will be lagging behind code, be incomplete and out of date.

**2024-03-24** : a couple of simple data processing pipelines working and wired up as Jasmine e2e tests in place; started to develop actually useful pipelines for [foaf-archive](https://github.com/danja/foaf-archive) project

## Motivation

This was prompted by something of a ~~perfect storm~~ serendipitous convergence of ideas, requirements and environment.

- I had the realization that much of the software I want to write in the near future has the same basic high level **architecture** : pipeline-like, ie. a series of steps each of which transform data from one form into another ([Wikipedia](<https://en.wikipedia.org/wiki/Pipeline_(software)>)).
- **Pipelines** are approachable using concepts from Design Patterns like [Dependency Injection](https://en.wikipedia.org/wiki/Dependency_injection), [Inversion of control](https://en.wikipedia.org/wiki/Inversion_of_control) etc. A key benefit of such patterns is that, because the components are loosely coupled, they can largely be developed in isolation as small standalone units before integration into a larger application.
- **Large Language Models** currently struggle with large codebases and complex architectures (mostly due to [context window](https://en.wikipedia.org/wiki/Large_language_model#Prompt_engineering,_attention_mechanism,_and_context_window) limitations). But they are very good at building small, simple, self-contained units of code.

reused in different contexts.

_I'm writing this with GitHub Copilot enabled for markdown, which can be helpful, but sometimes it goes off on one... After the first point above it offered :_

```
This is true of a blog engine, a personal knowledge management system, a chatbot, a mail archive miner, a web scraper, a data visualizer, a data importer/exporter, a data transformer, a data validator, a data filter, a data aggregator, a data splitter, a data joiner, a data normalizer, a data summarizer, a data classifier, a data indexer, a data search engine, a data storage system, a data retrieval system, a data synchronization system, a data backup system, a data migration system, a data versioning system, a data sharing system, a data access control system, a data privacy system, a data security system, a data integrity system, a data quality system, a data governance system, a data provenance system, a data lineage system, a data lifecycle system, a data archiving system, a data retention system, a data deletion system, a data recovery system, a data discovery system, a data exploration system, a data analysis system, a data visualization system, a data reporting system, a data monitoring system, a data alerting system, a data forecasting system, a data prediction system, a data recommendation system, a data personalization system, a data optimization system, a data simulation system, a data modeling system, a data inference system, a data learning system, a data reasoning system, a data decision system, a data planning system, a data execution system, a data monitoring system, a data feedback system, a data control system, a data automation system, a data orchestration system, a data integration system, a data federation system, a data interoperability system, a data exchange system, a data transformation system, a data migration system, a data synchronization system, a data replication system, a data federation system, a data virtualization system, a data streaming system, a data batch system, a data real-time system, a data near-real-time system, a data historical system, a data temporal system, a data spatial system, a data geospatial system, a data network system, a data graph system, a data tree system, a data list system, a data set system, a data table system, a data document system, a data record system, a data entity system, a data attribute system, a data value system, a data type system, a data format system, a data encoding system, a data serialization
```

_... which is a little over the top, but it's a fair list of things that might be helped by a micro-framework like this. Remarkably the first 5 are pretty much my immediate intended uses for this._

**Docs below need updating**

for later :

The [Community Solid Server](https://github.com/CommunitySolidServer/CommunitySolidServer) uses [Components.js](https://componentsjs.readthedocs.io/en/latest/) to specify how modules and components need to be wired together at runtime. Components.js is a dependency injection framework for JavaScript applications.

## What it is

A low-level system for data processing pipelines/networks. Wherever convenient functionality will be defined declaratively with JSON-LD configuration files.

Dependency injection is used internally to allow loose coupling of components.

## What it isn't

There are several sophisticated frameworks for building interfaces between software applications and creating data processing networks. NodeRed, NoFlo etc. This is not one of them. This is much more basic and bare bones, down in the details.

See also [David Booth](https://github.com/dbooth-boston)'s [RDF Pipeline Framework](https://github.com/rdf-pipeline)

_I do eventually want to use this with NodeRed or whatever, but the entities created by transmissions will be at the level of nodes in such networks, not the network itself._

## Motivation

I'm in the process of writing yet another blog engine (Postcraft). I've also started working on a playground for interconnecting intelligent agents in an XMPP multiuser chat environment (Kia). I'm also revising a system for managing a personal knowledge base in the world of LLMs (HKMS). These all share functionality around connectivity to external data/messaging systems and internal data transformation. Might as well write this bit once only, and avoid thinking about software architecture more than I have to.

### Goals

To facilate :

- rapid development of small applications
- reuse of components in a loosely-couple environment
- versatility

### Soft Goals

- performance - low on the list
- scalability - ditto
- security - ditto

================
File: trans
================
#!/bin/bash

# use 'chmod +x run' to make this executable

# Execute the Node.js script with Node
node src/api/cli/run.js "$@"

================
File: users.json
================
[{"uuid": "dc67aa7d-f71f-4232-afb3-7f2688ac68f7", "full_name": "Danny Ayers", "email_address": "danny.ayers@gmail.com", "verified_phone_number": null}]

================
File: webpack.config.js
================
const path = require('path');
const HtmlWebpackPlugin = require('html-webpack-plugin');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');
const WorkboxWebpackPlugin = require('workbox-webpack-plugin');

const isProduction = process.env.NODE_ENV == 'production';


const stylesHandler = MiniCssExtractPlugin.loader;



const config = {
    entry: './src/index.js',
    output: {
        path: path.resolve(__dirname, 'dist'),
    },
    devServer: {
        open: true,
        host: 'localhost',
    },
    plugins: [
        new HtmlWebpackPlugin({
            template: 'index.html',
        }),

        new MiniCssExtractPlugin(),



    ],
    module: {
        rules: [
            {
                test: /\.(js|jsx)$/i,
                loader: 'babel-loader',
            },
            {
                test: /\.css$/i,
                use: [stylesHandler, 'css-loader', 'postcss-loader'],
            },
            {
                test: /\.(eot|svg|ttf|woff|woff2|png|jpg|gif)$/i,
                type: 'asset',
            },



        ],
    },
};

module.exports = () => {
    if (isProduction) {
        config.mode = 'production';


        config.plugins.push(new WorkboxWebpackPlugin.GenerateSW());

    } else {
        config.mode = 'development';
    }
    return config;
};
