This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-17T20:47:53.517Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
- Code comments have been removed.

Additional Info:
----------------
User Provided Header:
-----------------------
Transmissions source code

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
layouts/
  mediocre/
    templates/
      entry-content_template.njk
    post-page_template.njk
raw-src/
  markmap/
    01.js
    02.js
  viz/
    test-data/
      foaf-template.ttl
    jsonld-turtle.js
    jsonld-vis.css
    jsonld-vis.js
    playground-examples.js
    viz-turtle.js
  containsAny.js
  crawl-fs.js
  grapoi-bits.js
  postcraft-transmission.ttl
  README.md
  regex-play.js
  structure-play.js
spec/
  support/
    jasmine-browser.json
src/
  applications/
    dirwalker-test/
      about.md
      processors-config.ttl
      transmissions.ttl
    env-loader-test/
      about.md
      processors-config.ttl
      transmissions.ttl
    file-copy-remove-test/
      data/
        several-full/
          one.txt
          two.txt
        single-full/
          one.txt
        start/
          one.txt
          two.txt
      about.md
      init.sh
      processors-config.ttl
      transmissions.ttl
    file-pipeline/
      processors-config.ttl
      transmissions.ttl
    globbo/
      about.md
      processors-config.ttl
      transmissions.ttl
    html-to-md/
      about.md
      processors-config.ttl
      transmissions.ttl
    link-lister/
      about.md
      processors-config.ttl
      transmissions.ttl
    nop-test/
      about.md
      processors-config.ttl
      transmissions.ttl
    postcraft/
      processors-config.ttl
      transmissions.ttl
    postcraft-init/
      about.md
      processors-config.ttl
      transmissions.ttl
    postcraft-previous/
      processors.ttl
      transmission.ttl
    string-pipeline/
      processors-config.ttl
      transmissions.ttl
    test_foreach/
      about.md
      transmissions.ttl
    test_fork/
      about.md
      processors-config.ttl
      transmissions.ttl
    test_multi-pipe/
      processors-config.ttl
      transmissions.ttl
    two-transmissions/
      processors-config.ttl
      transmissions.ttl
  engine/
    A.js
    AbstractProcessorFactory.js
    B.js
    Connector.js
    ModuleLoader.js
    Transmission.js
    TransmissionBuilder.js
    WorkerPool.js
  processors/
    base/
      Processor copy.js
      Processor.js
      ProcessProcessor.js
      SinkProcessor.js
      SourceProcessor.js
    flow/
      FlowProcessorsFactory.js
      ForEach.js
    fs/
      DirWalker.js
      FileCopy.js
      FileReader.js
      FileRemove.js
      FileWriter.js
      FsProcessorsFactory.js
    markup/
      LinkFinder.js
      MarkdownToHTML.js
      MarkupProcessorsFactory.js
      MetadataExtractor.js
    postcraft/
      EntryContentToPagePrep.js
      FrontPagePrep copy.js
      FrontPagePrep.js
      PostcraftDispatcher.js
      PostcraftPrep.js
      PostcraftProcessorsFactory.js
    protocols/
      HttpGet.js
      ProtocolsProcessorsFactory.js
    rdf/
      ConfigMap.js
      DatasetReader.js
      RDFProcessorsFactory.js
    system/
      EnvLoader.js
      SystemProcessorsFactory.js
    templates/
      ProcessorTemplate.js
      TemplateProcessorsFactory.js
    test/
      AppendProcess.js
      FileSink.js
      FileSource.js
      StringSink.js
      StringSource.js
      TestProcessorsFactory.js
    text/
      LineReader.js
      StringFilter.js
      StringMerger.js
      StringReplace.js
      Templater.js
      TextProcessorsFactory.js
    unsafe/
      chatgpt.md
    util/
      CaptureAll.js
      DeadEnd.js
      Fork.js
      Halt.js
      NOP.js
      RemapContext.js
      ShowConfig.js
      ShowMessage.js
      ShowTransmission.js
      Stash.js
      Unfork.js
      UtilProcessorsFactory.js
      WhiteboardToMessage.js
    about.md
  simples/
    env-loader/
      about.md
      env-loader.js
    nop/
      nop copy.js
      nop.js
      simple-runner.js
  utils/
    CommandUtils.js
    footpath.js
    GrapoiHelpers.js
    Logger.js
    ns.js
    t2j.js
    test_runner.js
tests/
  helpers/
    reporter.js
  integration/
    file-copy-remove-test.spec.js
    file-pipeline.spec.js
    string-pipeline.speeec.js
  unit/
    NOP.spec.js
    PostcraftPrep.spec.js
    StringFilter.spec.js
    StringReplace.spec.js
  grapoi-raw-tests.js
types/
  grapoi.d.ts
vocabs/
  dbooth-pipeline.n3.txt
  info-store.rdf
  info-store.ttl
  notes.md
  slug-config.rdf
  slug-config.ttl
.babelrc
.gitignore
jasmine.json
jsconfig.json
jsdoc.json
kiki.txt
LICENSE
links.md
package.json
postcss.config.js
README.md
run.js
runs.txt
trans
webpack.config.js

================================================================
Repository Files
================================================================

================
File: layouts/mediocre/templates/entry-content_template.njk
================
<!-- POST CONTENT TEMPLATE -->
<p class="post-title">
    <a href="https://danny.ayers.name/blog/2024-05-03_one.html">
        One
    </a>
</p>
<article class="post-content">
    
</article>
<em>2024-05-14</em>

================
File: layouts/mediocre/post-page_template.njk
================
<!DOCTYPE html>
<html lang="en">

<head>
    <title>The Title</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <link rel="stylesheet" href="css/fonts.css" type="text/css" />
    <link rel="stylesheet" href="css/grid-columns.css" type="text/css" />
    <link rel="stylesheet" href="css/style.css" type="text/css" />

</head>

<body>
    <header id="main-header">
        <h1>
            
        </h1>
    </header>
    <div class="grid-container">
        <div class="main-grid-item directory">
            <h2>directory</h2>
            <p>i cinque secoli, ma anche al passaggio alla videoimp</p>
        </div>
        <div class="main-grid-item articles">
            <h2>articles</h2>
            <article>
                
            </article>
        </div>
        <div class="main-grid-item about">
            <h2>About</h2>
            
        </div>
    </div>
</body>

</html>

================
File: raw-src/markmap/01.js
================
import { Transformer } from 'markmap-lib';
import { fillTemplate } from 'markmap-render';

const input = `# A markdown string
## another

* bullet1
* bullet2`

function expandNode(node, depth = 0) {
    const indent = '  '.repeat(depth);
    console.log(`${indent}content: ${JSON.stringify(node.content)}`);
    if (node.payload) {
        console.log(`${indent}payload: ${JSON.stringify(node.payload)}`);
    }
    if (node.children && node.children.length > 0) {
        console.log(`${indent}children:`);
        node.children.forEach(child => expandNode(child, depth + 1));
    }
}


const transformer = new Transformer();
const A = transformer.transform(input);


const B = transformer.getAssets();


const C = fillTemplate(A.root, B);







expandNode(A.root);

================
File: raw-src/markmap/02.js
================
import { Transformer } from 'markmap-lib';
import { fillTemplate } from 'markmap-render';
import fs from 'fs/promises';

const input = '# A markdown string\n## Subheading\n- List item 1\n- List item 2';

async function generateMarkmap() {

    const transformer = new Transformer();
    const { root, features } = transformer.transform(input);


    const assets = transformer.getUsedAssets(features);


    const html = fillTemplate(root, assets);


    const svgMatch = html.match(/<svg[^>]*>[\s\S]*?<\/svg>/i);
    const svg = svgMatch ? svgMatch[0] : '';

    // Save outputs
    await fs.writeFile('output.html', html);
    await fs.writeFile('output.svg', svg);

    console.log('HTML and SVG files have been generated.');
}

generateMarkmap().catch(console.error);

================
File: raw-src/viz/test-data/foaf-template.ttl
================
# foaf-template.ttl sample for form-a-matic

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .

<#ROOT>
    a foaf:Person ;
    foaf:name "LITERAL" ;
    foaf:mbox <#URI> ;
    foaf:homepage <#URI> ;
    foaf:nick "LITERAL" ;
    foaf:depiction <#URI> ;
    foaf:interest <#URI> ;
    foaf:knows [
        a foaf:Person ;
        foaf:name "LITERAL"
    ] .

================
File: raw-src/viz/jsonld-turtle.js
================
(function() {









function getProperties(obj) {

  var rval = [];
  for(var key in obj) {
    if(key !== '@id') {
      rval.push(key);
    }
  }
  return rval;
};








function isBnode(iri) {
  return iri.indexOf('_:') === 0;
};










function iriToTurtle(iri) {

  return isBnode(iri) ? iri : ('<' + iri + '>');
};









function objectToString(obj) {
  var rval = '';

  if(obj instanceof Array) {
    // if the object is an array, convert each object in the list
    var firstItem = true;
    for(i in obj) {
      if(firstItem) {
        firstItem = false;
        rval = '\n      ';
      }
      else {
        rval += ',\n      ';
      }
      rval += objectToString(obj[i]);
    }
  }
  else if(obj instanceof Object) {
    if('@value' in obj) {
      rval = '"' + obj['@value'] + '"';

      if('@type' in obj) {

        rval += '^^<' + obj['@type'] + '>';
      }
      else if('@language' in obj) {

        rval += '@' + obj['@language'];
      }
    }
    else if('@id' in obj) {
      rval = iriToTurtle(obj['@id']);
    }
  }
  else {

    rval = '"' + obj + '"';
  }

  return rval;
};








jsonld.turtle = function(input, options, callback) {
  jsonld.normalize(input, options, function(err, normalized) {
    var output = '';
    for(s in normalized) {
      // print out each key in the normalized array (the subjects)
      var subject = normalized[s];
      var iri = subject['@id'];


      if(Object.keys(subject).length === 1) {
        continue;
      }

      output += iriToTurtle(iri) + '\n';


      var properties = getProperties(subject);
      var numProperties = properties.length;


      var count = numProperties;
      for(p in properties) {

        property = properties[p];
        if(property === '@type') {
          output += '   <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ';
        }
        else {
          output += '   <' + property + '> ';
        }
        output += objectToString(subject[property]);

        if(count === 1) {

          output += '.\n';
        }
        else {

          output += ';\n';
        }
        count -= 1;
      }
    }
    callback(null, output);
  });
};

})();

================
File: raw-src/viz/jsonld-vis.css
================
@import url("https://fonts.googleapis.com/css?family=Open+Sans:300,400,600");

svg {
  border: none;
}

.node {
  cursor: pointer;
}

.node text {
  font-size: 12px;
  font-family: 'Open Sans', 'Helvetica Neue', Helvetica, sans-serif;
  fill: #333333;
}

.d3-tip {
  font-size: 14px;
  font-family: 'Open Sans', 'Helvetica Neue', Helvetica, sans-serif;
  color: #333333;
  border: 1px solid #CCCCCC;
  border-radius: 5px;
  padding: 10px 20px;
  max-width: 250px;
  word-wrap: break-word;
  background-color: rgba(255, 255, 255, 0.9);
  text-align: left;
}

.link {
  fill: none;
  stroke: #DADFE1;
  stroke-width: 1px;
}

================
File: raw-src/viz/jsonld-vis.js
================
(function() {
  'use strict';

  function jsonldVis(jsonld, selector, config) {
    if (!arguments.length) return jsonldVis;
    config = config || {};

    var h = config.h || 600
      , w = config.w || 800
      , maxLabelWidth = config.maxLabelWidth || 250
      , transitionDuration = config.transitionDuration || 750
      , transitionEase = config.transitionEase || 'cubic-in-out'
      , minRadius = config.minRadius || 5
      , scalingFactor = config.scalingFactor || 2;

    var i = 0;

    var tree = d3.layout.tree()
      .size([h, w]);

    var diagonal = d3.svg.diagonal()
      .projection(function(d) { return [d.y, d.x]; });

    var svg = d3.select(selector).append('svg')
      .attr('width', w)
      .attr('height', h)
      .append('g')
      .attr('transform', 'translate(' + maxLabelWidth + ',0)');

    var tip = d3.tip()
      .direction(function(d) {
        return d.children || d._children ? 'w' : 'e';
      })
      .offset(function(d) {
        return d.children || d._children ? [0, -3] : [0, 3];
      })
      .attr('class', 'd3-tip')
      .html(function(d) {
        return '<span>' + d.valueExtended + '</span>';
      });

    svg.call(tip);

    var root = jsonldTree(jsonld);
    root.x0 = h / 2;
    root.y0 = 0;
    root.children.forEach(collapse);

    function changeSVGWidth(newWidth) {
      if (w !== newWidth) {
        d3.select(selector + ' > svg').attr('width', newWidth);
      }
    }

    function jsonldTree(source) {
      var tree = {};

      if ('@id' in source) {
        tree.isIdNode = true;
        tree.name = source['@id'];
        if (tree.name.length > maxLabelWidth / 9) {
          tree.valueExtended = tree.name;
          tree.name = '...' + tree.valueExtended.slice(-Math.floor(maxLabelWidth / 9));
        }
      } else {
        tree.isIdNode = true;
        tree.isBlankNode = true;

        tree.name = '_' + Math.random().toString(10).slice(-7);
      }

      var children = [];
      Object.keys(source).forEach(function(key) {
        if (key === '@id' || key === '@context' || source[key] === null) return;

        var valueExtended, value;
        if (typeof source[key] === 'object' && !Array.isArray(source[key])) {
          children.push({
            name: key,
            children: [jsonldTree(source[key])]
          });
        } else if (Array.isArray(source[key])) {
          children.push({
            name: key,
            children: source[key].map(function(item) {
              if (typeof item === 'object') {
                return jsonldTree(item);
              } else {
                return { name: item };
              }
            })
          });
        } else {
          valueExtended = source[key];
          value = valueExtended;
          if (value.length > maxLabelWidth / 9) {
            value = value.slice(0, Math.floor(maxLabelWidth / 9)) + '...';
            children.push({
              name: key,
              value: value,
              valueExtended: valueExtended
            });
          } else {
            children.push({
              name: key,
              value: value
            });
          }
        }
      });

      if (children.length) {
        tree.children = children;
      }

      return tree;
    }

    function update(source) {
      var nodes = tree.nodes(root).reverse();
      var links = tree.links(nodes);

      nodes.forEach(function(d) { d.y = d.depth * maxLabelWidth; });

      var node = svg.selectAll('g.node')
        .data(nodes, function(d) { return d.id || (d.id = ++i); });

      var nodeEnter = node.enter()
        .append('g')
        .attr('class', 'node')
        .attr('transform', function(d) { return 'translate(' + source.y0 + ',' + source.x0 + ')'; })
        .on('click', click);

      nodeEnter.append('circle')
        .attr('r', 0)
        .style('stroke-width', function(d) {
          return d.isIdNode ? '2px' : '1px';
        })
        .style('stroke', function(d) {
          return d.isIdNode ? '#F7CA18' : '#4ECDC4';
        })
        .style('fill', function(d) {
          if (d.isIdNode) {
            return d._children ? '#F5D76E' : 'white';
          } else {
            return d._children ? '#86E2D5' : 'white';
          }
        })
        .on('mouseover', function(d) { if (d.valueExtended) tip.show(d); })
        .on('mouseout', tip.hide);

      nodeEnter.append('text')
        .attr('x', function(d) {
          var spacing = computeRadius(d) + 5;
          return d.children || d._children ? -spacing : spacing;
        })
        .attr('dy', '4')
        .attr('text-anchor', function(d) { return d.children || d._children ? 'end' : 'start'; })
        .text(function(d) { return d.name + (d.value ? ': ' + d.value : ''); })
        .style('fill-opacity', 0);

      var maxSpan = Math.max.apply(Math, nodes.map(function(d) { return d.y + maxLabelWidth; }));
      if (maxSpan + maxLabelWidth + 20 > w) {
        changeSVGWidth(maxSpan + maxLabelWidth);
        d3.select(selector).node().scrollLeft = source.y0;
      }

      var nodeUpdate = node.transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('transform', function(d) { return 'translate(' + d.y + ',' + d.x + ')'; });

      nodeUpdate.select('circle')
        .attr('r', function(d) { return computeRadius(d); })
        .style('stroke-width', function(d) {
          return d.isIdNode ? '2px' : '1px';
        })
        .style('stroke', function(d) {
          return d.isIdNode ? '#F7CA18' : '#4ECDC4';
        })
        .style('fill', function(d) {
          if (d.isIdNode) {
            return d._children ? '#F5D76E' : 'white';
          } else {
            return d._children ? '#86E2D5' : 'white';
          }
        });

      nodeUpdate.select('text').style('fill-opacity', 1);

      var nodeExit = node.exit().transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('transform', function(d) { return 'translate(' + source.y + ',' + source.x + ')'; })
        .remove();

      nodeExit.select('circle').attr('r', 0);
      nodeExit.select('text').style('fill-opacity', 0);

      var link = svg.selectAll('path.link')
        .data(links, function(d) { return d.target.id; });

      link.enter().insert('path', 'g')
        .attr('class', 'link')
        .attr('d', function(d) {
          var o = { x: source.x0, y: source.y0 };
          return diagonal({ source: o, target: o });
        });

      link.transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('d', diagonal);

      link.exit().transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('d', function(d) {
          var o = { x: source.x, y: source.y };
          return diagonal({ source: o, target: o });
        })
        .remove();

      nodes.forEach(function(d) {
        d.x0 = d.x;
        d.y0 = d.y;
      });
    }

    function computeRadius(d) {
      if (d.children || d._children) {
        return minRadius + (numEndNodes(d) / scalingFactor);
      } else {
        return minRadius;
      }
    }

    function numEndNodes(n) {
      var num = 0;
      if (n.children) {
        n.children.forEach(function(c) {
          num += numEndNodes(c);
        });
      } else if (n._children) {
        n._children.forEach(function(c) {
          num += numEndNodes(c);
        });
      } else {
        num++;
      }
      return num;
    }

    function click(d) {
      if (d.children) {
        d._children = d.children;
        d.children = null;
      } else {
        d.children = d._children;
        d._children = null;
      }

      update(d);


      if (d.children) {
        d.children.forEach(function(child) {
          if (child.isBlankNode && child._children) {
            click(child);
          }
        });
      }
    }

    function collapse(d) {
      if (d.children) {
        d._children = d.children;
        d._children.forEach(collapse);
        d.children = null;
      }
    }

    update(root);
  }

  if (typeof module !== 'undefined' && module.exports) {
    module.exports = jsonldVis;
  } else {
    d3.jsonldVis = jsonldVis;
  }
})();

================
File: raw-src/viz/playground-examples.js
================
(function($) {
  window.playground = window.playground || {};
  var playground = window.playground;


  playground.examples = {};
  playground.frames = {};
  playground.contexts = {};


  playground.examples["Person"] = {
    "@context": "http://schema.org/",
    "@type": "Person",
    "name": "Jane Doe",
    "jobTitle": "Professor",
    "telephone": "(425) 123-4567",
    "url": "http://www.janedoe.com"
  };


  playground.examples["Place"] = {
    "@context": {
      "name": "http://schema.org/name",
      "description": "http://schema.org/description",
      "image": {
        "@id": "http://schema.org/image",
        "@type": "@id"
      },
      "geo": "http://schema.org/geo",
      "latitude": {
        "@id": "http://schema.org/latitude",
        "@type": "xsd:float"
      },
      "longitude": {
        "@id": "http://schema.org/longitude",
        "@type": "xsd:float"
      },
      "xsd": "http://www.w3.org/2001/XMLSchema#"
    },
    "name": "The Empire State Building",
    "description": "The Empire State Building is a 102-story landmark in New York City.",
    "image": "http://www.civil.usherbrooke.ca/cours/gci215a/empire-state-building.jpg",
    "geo": {
       "latitude": "40.75",
       "longitude": "73.98"
    }
  };


  playground.examples["Event"] = {
    "@context": {
      "ical": "http://www.w3.org/2002/12/cal/ical#",
      "xsd": "http://www.w3.org/2001/XMLSchema#",
      "ical:dtstart": {
        "@type": "xsd:dateTime"
      }
    },
    "ical:summary": "Lady Gaga Concert",
    "ical:location": "New Orleans Arena, New Orleans, Louisiana, USA",
    "ical:dtstart": "2011-04-09T20:00:00Z"
  };


  playground.examples["Product"] = {
    "@context": {
      "gr": "http://purl.org/goodrelations/v1#",
      "pto": "http://www.productontology.org/id/",
      "foaf": "http://xmlns.com/foaf/0.1/",
      "xsd": "http://www.w3.org/2001/XMLSchema#",
      "foaf:page": {"@type": "@id"},
      "gr:acceptedPaymentMethods": {"@type": "@id"},
      "gr:hasBusinessFunction": {"@type": "@id"},
       "gr:hasCurrencyValue": {"@type": "xsd:float"}
    },
    "@id": "http://example.org/cars/for-sale#tesla",
    "@type": "gr:Offering",
    "gr:name": "Used Tesla Roadster",
    "gr:description": "Need to sell fast and furiously",
    "gr:hasBusinessFunction": "gr:Sell",
    "gr:acceptedPaymentMethods": "gr:Cash",
    "gr:hasPriceSpecification": {
      "gr:hasCurrencyValue": "85000",
      "gr:hasCurrency": "USD"
    },
    "gr:includes": {
      "@type": ["gr:Individual", "pto:Vehicle"],
      "gr:name": "Tesla Roadster",
      "foaf:page": "http://www.teslamotors.com/roadster"
    }
  };


  playground.examples["Recipe"] = {
    "@context": {
      "name": "http://rdf.data-vocabulary.org/#name",
      "ingredient": "http://rdf.data-vocabulary.org/#ingredients",
      "yield": "http://rdf.data-vocabulary.org/#yield",
      "instructions": "http://rdf.data-vocabulary.org/#instructions",
      "step": {
        "@id": "http://rdf.data-vocabulary.org/#step",
        "@type": "xsd:integer"
      },
      "description": "http://rdf.data-vocabulary.org/#description",
      "xsd": "http://www.w3.org/2001/XMLSchema#"
    },
    "name": "Mojito",
    "ingredient": ["12 fresh mint leaves", "1/2 lime, juiced with pulp",
      "1 tablespoons white sugar", "1 cup ice cubes",
      "2 fluid ounces white rum", "1/2 cup club soda"],
    "yield": "1 cocktail",
    "instructions" : [{
      "step": 1,
      "description": "Crush lime juice, mint and sugar together in glass."
    }, {
      "step": 2,
      "description": "Fill glass to top with ice cubes."
    }, {
      "step": 3,
      "description": "Pour white rum over ice."
    }, {
      "step": 4,
      "description": "Fill the rest of glass with club soda, stir."
    }, {
      "step": 5,
      "description": "Garnish with a lime wedge."
    }]
  };


  playground.examples["Library"] = {
    "@context": {
      "dc": "http://purl.org/dc/elements/1.1/",
      "ex": "http://example.org/vocab#",
      "xsd": "http://www.w3.org/2001/XMLSchema#",
      "ex:contains": {"@type": "@id"}
    },
    "@graph": [{
      "@id": "http://example.org/library",
      "@type": "ex:Library",
      "ex:contains": "http://example.org/library/the-republic"
    }, {
      "@id": "http://example.org/library/the-republic",
      "@type": "ex:Book",
      "dc:creator": "Plato",
      "dc:title": "The Republic",
      "ex:contains": "http://example.org/library/the-republic#introduction"
    }, {
      "@id": "http://example.org/library/the-republic#introduction",
      "@type": "ex:Chapter",
      "dc:description": "An introductory chapter on The Republic.",
      "dc:title": "The Introduction"
    }]
  };


  playground.frames["Library"] = {
    "@context": {
      "dc": "http://purl.org/dc/elements/1.1/",
      "ex": "http://example.org/vocab#"
    },
    "@type": "ex:Library",
    "ex:contains": {
      "@type": "ex:Book",
      "ex:contains": {"@type": "ex:Chapter"}
    }
  };



  playground.examples["Activity"] = {
    "@context": "https://www.w3.org/ns/activitystreams",
    "@type": "Create",
    "actor": {
      "@type": "Person",
      "@id": "acct:sally@example.org",
      "name": "Sally"
    },
    "object": {
      "@type": "Note",
      "content": "This is a simple note"
    },
    "published": "2015-01-25T12:34:56Z"
  }

})(jQuery);

================
File: raw-src/viz/viz-turtle.js
================
import rdf from 'rdf-ext';
import { fromFile } from 'rdf-utils-fs';
import * as d3 from 'd3';
import { JSDOM } from 'jsdom';
import fs from 'fs/promises';


const dom = new JSDOM('<!DOCTYPE html><body></body>');
global.document = dom.window.document;
global.window = dom.window;


function datasetToJsonld(dataset) {
    const jsonld = {};
    for (const quad of dataset) {
        const subject = quad.subject.value;
        const predicate = quad.predicate.value;
        const object = quad.object.termType === 'Literal' ? quad.object.value : { '@id': quad.object.value };

        if (!jsonld[subject]) {
            jsonld[subject] = { '@id': subject };
        }
        if (!jsonld[subject][predicate]) {
            jsonld[subject][predicate] = [];
        }
        jsonld[subject][predicate].push(object);
    }
    return Object.values(jsonld);
}

function jsonldVis(jsonldData, config) {
    config = config || {};
    const h = config.h || 600;
    const w = config.w || 800;
    const maxLabelWidth = config.maxLabelWidth || 250;
    const transitionDuration = config.transitionDuration || 750;
    const minRadius = config.minRadius || 5;
    const scalingFactor = config.scalingFactor || 2;

    const body = d3.select(document.body);
    const svg = body.append('svg')
        .attr('width', w)
        .attr('height', h)
        .append('g')
        .attr('transform', `translate(${maxLabelWidth},0)`);

    const tree = d3.tree().size([h, w]);

    const diagonal = d3.linkHorizontal()
        .x(d => d.y)
        .y(d => d.x);


    const root = d3.hierarchy({
        name: 'RDF Graph',
        children: jsonldData.map(item => ({
            name: item['@id'],
            children: Object.entries(item)
                .filter(([key]) => key !== '@id')
                .map(([key, value]) => ({
                    name: key,
                    children: Array.isArray(value) ? value.map(v => ({ name: v['@id'] || v })) : [{ name: value['@id'] || value }]
                }))
        }))
    });

    root.x0 = h / 2;
    root.y0 = 0;

    update(root);

    function update(source) {
        const treeData = tree(root);
        const nodes = treeData.descendants();
        const links = treeData.links();

        nodes.forEach(d => { d.y = d.depth * maxLabelWidth; });

        const node = svg.selectAll('g.node')
            .data(nodes, d => d.id || (d.id = ++i));

        const nodeEnter = node.enter().append('g')
            .attr('class', 'node')
            .attr('transform', d => `translate(${source.y0},${source.x0})`)
            .on('click', (event, d) => {
                d.children = d.children ? null : d._children;
                update(d);
            });

        nodeEnter.append('circle')
            .attr('r', 1e-6)
            .style('fill', d => d._children ? 'lightsteelblue' : '#fff');

        nodeEnter.append('text')
            .attr('x', d => d.children || d._children ? -13 : 13)
            .attr('dy', '.35em')
            .attr('text-anchor', d => d.children || d._children ? 'end' : 'start')
            .text(d => d.data.name)
            .style('fill-opacity', 1e-6);

        const nodeUpdate = node.merge(nodeEnter).transition()
            .duration(transitionDuration)
            .attr('transform', d => `translate(${d.y},${d.x})`);

        nodeUpdate.select('circle')
            .attr('r', 10)
            .style('fill', d => d._children ? 'lightsteelblue' : '#fff');

        nodeUpdate.select('text')
            .style('fill-opacity', 1);

        const nodeExit = node.exit().transition()
            .duration(transitionDuration)
            .attr('transform', d => `translate(${source.y},${source.x})`)
            .remove();

        nodeExit.select('circle')
            .attr('r', 1e-6);

        nodeExit.select('text')
            .style('fill-opacity', 1e-6);

        const link = svg.selectAll('path.link')
            .data(links, d => d.target.id);

        link.enter().insert('path', 'g')
            .attr('class', 'link')
            .attr('d', d => {
                const o = { x: source.x0, y: source.y0 };
                return diagonal({ source: o, target: o });
            });

        link.merge(link).transition()
            .duration(transitionDuration)
            .attr('d', diagonal);

        link.exit().transition()
            .duration(transitionDuration)
            .attr('d', d => {
                const o = { x: source.x, y: source.y };
                return diagonal({ source: o, target: o });
            })
            .remove();

        nodes.forEach(d => {
            d.x0 = d.x;
            d.y0 = d.y;
        });
    }
    update(root);
    return body.html();
}

function changeSVGWidth(newWidth) {
    if (w !== newWidth) {
        d3.select(selector + ' > svg').attr('width', newWidth);
    }
}

function jsonldTree(source) {
    var tree = {};

    if ('@id' in source) {
        tree.isIdNode = true;
        tree.name = source['@id'];
        if (tree.name.length > maxLabelWidth / 9) {
            tree.valueExtended = tree.name;
            tree.name = '...' + tree.valueExtended.slice(-Math.floor(maxLabelWidth / 9));
        }
    } else {
        tree.isIdNode = true;
        tree.isBlankNode = true;

        tree.name = '_' + Math.random().toString(10).slice(-7);
    }

    var children = [];
    Object.keys(source).forEach(function (key) {
        if (key === '@id' || key === '@context' || source[key] === null) return;

        var valueExtended, value;
        if (typeof source[key] === 'object' && !Array.isArray(source[key])) {
            children.push({
                name: key,
                children: [jsonldTree(source[key])]
            });
        } else if (Array.isArray(source[key])) {
            children.push({
                name: key,
                children: source[key].map(function (item) {
                    if (typeof item === 'object') {
                        return jsonldTree(item);
                    } else {
                        return { name: item };
                    }
                })
            });
        } else {
            valueExtended = source[key];
            value = valueExtended;
            if (value.length > maxLabelWidth / 9) {
                value = value.slice(0, Math.floor(maxLabelWidth / 9)) + '...';
                children.push({
                    name: key,
                    value: value,
                    valueExtended: valueExtended
                });
            } else {
                children.push({
                    name: key,
                    value: value
                });
            }
        }
    });

    if (children.length) {
        tree.children = children;
    }

    return tree;
}

function update(source) {
    var nodes = tree.nodes(root).reverse();
    var links = tree.links(nodes);

    nodes.forEach(function (d) { d.y = d.depth * maxLabelWidth; });

    var node = svg.selectAll('g.node')
        .data(nodes, function (d) { return d.id || (d.id = ++i); });

    var nodeEnter = node.enter()
        .append('g')
        .attr('class', 'node')
        .attr('transform', function (d) { return 'translate(' + source.y0 + ',' + source.x0 + ')'; })
        .on('click', click);

    nodeEnter.append('circle')
        .attr('r', 0)
        .style('stroke-width', function (d) {
            return d.isIdNode ? '2px' : '1px';
        })
        .style('stroke', function (d) {
            return d.isIdNode ? '#F7CA18' : '#4ECDC4';
        })
        .style('fill', function (d) {
            if (d.isIdNode) {
                return d._children ? '#F5D76E' : 'white';
            } else {
                return d._children ? '#86E2D5' : 'white';
            }
        })
        .on('mouseover', function (d) { if (d.valueExtended) tip.show(d); })
        .on('mouseout', tip.hide);

    nodeEnter.append('text')
        .attr('x', function (d) {
            var spacing = computeRadius(d) + 5;
            return d.children || d._children ? -spacing : spacing;
        })
        .attr('dy', '4')
        .attr('text-anchor', function (d) { return d.children || d._children ? 'end' : 'start'; })
        .text(function (d) { return d.name + (d.value ? ': ' + d.value : ''); })
        .style('fill-opacity', 0);

    var maxSpan = Math.max.apply(Math, nodes.map(function (d) { return d.y + maxLabelWidth; }));
    if (maxSpan + maxLabelWidth + 20 > w) {
        changeSVGWidth(maxSpan + maxLabelWidth);
        d3.select(selector).node().scrollLeft = source.y0;
    }

    var nodeUpdate = node.transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('transform', function (d) { return 'translate(' + d.y + ',' + d.x + ')'; });

    nodeUpdate.select('circle')
        .attr('r', function (d) { return computeRadius(d); })
        .style('stroke-width', function (d) {
            return d.isIdNode ? '2px' : '1px';
        })
        .style('stroke', function (d) {
            return d.isIdNode ? '#F7CA18' : '#4ECDC4';
        })
        .style('fill', function (d) {
            if (d.isIdNode) {
                return d._children ? '#F5D76E' : 'white';
            } else {
                return d._children ? '#86E2D5' : 'white';
            }
        });

    nodeUpdate.select('text').style('fill-opacity', 1);

    var nodeExit = node.exit().transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('transform', function (d) { return 'translate(' + source.y + ',' + source.x + ')'; })
        .remove();

    nodeExit.select('circle').attr('r', 0);
    nodeExit.select('text').style('fill-opacity', 0);

    var link = svg.selectAll('path.link')
        .data(links, function (d) { return d.target.id; });

    link.enter().insert('path', 'g')
        .attr('class', 'link')
        .attr('d', function (d) {
            var o = { x: source.x0, y: source.y0 };
            return diagonal({ source: o, target: o });
        });

    link.transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('d', diagonal);

    link.exit().transition()
        .duration(transitionDuration)
        .ease(transitionEase)
        .attr('d', function (d) {
            var o = { x: source.x, y: source.y };
            return diagonal({ source: o, target: o });
        })
        .remove();

    nodes.forEach(function (d) {
        d.x0 = d.x;
        d.y0 = d.y;
    });
}

function computeRadius(d) {
    if (d.children || d._children) {
        return minRadius + (numEndNodes(d) / scalingFactor);
    } else {
        return minRadius;
    }
}

function numEndNodes(n) {
    var num = 0;
    if (n.children) {
        n.children.forEach(function (c) {
            num += numEndNodes(c);
        });
    } else if (n._children) {
        n._children.forEach(function (c) {
            num += numEndNodes(c);
        });
    } else {
        num++;
    }
    return num;
}

function click(d) {
    if (d.children) {
        d._children = d.children;
        d.children = null;
    } else {
        d.children = d._children;
        d._children = null;
    }

    update(d);


    if (d.children) {
        d.children.forEach(function (child) {
            if (child.isBlankNode && child._children) {
                click(child);
            }
        });
    }
}

function collapse(d) {
    if (d.children) {
        d._children = d.children;
        d._children.forEach(collapse);
        d.children = null;
    }
}



export async function visualizeTurtle(turtleFile, outputFile) {
    try {
        const stream = fromFile(turtleFile);
        const dataset = await rdf.dataset().import(stream);
        const jsonldData = datasetToJsonld(dataset);
        const svgContent = jsonldVis(jsonldData);

        await fs.writeFile(outputFile, svgContent);
        console.log(`Visualization saved to ${outputFile}`);
    } catch (error) {
        console.error('Error generating visualization:', error);
    }
}


visualizeTurtle('test-data/foaf-template.ttl', 'output.svg');

================
File: raw-src/containsAny.js
================
function containsAny(contentString, matchers) {
    return matchers.some(matcher => contentString.includes(matcher));
}


const contentString = "Hello, world! Welcome to coding.";
const matchers = ["coding", "javascript"];

console.log(containsAny(contentString, matchers));

const matchers2 = ["python", "java"];
console.log(containsAny(contentString, matchers2));

================
File: raw-src/crawl-fs.js
================
import { readdir } from 'fs/promises'
import { join, extname } from 'path'


const desiredExtensions = ['.html', '.txt']

async function crawlDirectory(dirPath) {

    try {
        const entries = await readdir(dirPath, { withFileTypes: true })
        for (const entry of entries) {
            const fullPath = join(dirPath, entry.name)
            if (entry.isDirectory()) {
                await crawlDirectory(fullPath)
            } else {

                if (desiredExtensions.includes(extname(entry.name))) {
                    console.log(fullPath)
                }
            }
        }
    } catch (error) {
        console.error('An error occurred:', error)
    }
}


crawlDirectory('../data/mail-archive-sample')

================
File: raw-src/grapoi-bits.js
================
const poi = this.getMyPoi()
logger.log('POIPOI')
for (const quad of poi.out().quads()) {
    logger.log(`\t${quad.predicate.value}: ${quad.object.value}`)
}

================
File: raw-src/postcraft-transmission.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

:postcraft a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4 :s5 :s6) . #  :s6  :s10 :s11 :s12 :s13

:s1 a :DatasetReader . # the manifest
:s2 a :ConfigMap . 

### use services.ttl? - defer to RemapContext as possible

# :s3 a :FileReader . # the template for main page
# :s4 a :RemapContext ;
#    trm:configKey :mainTemplateDirMap .

:s3 a :FileReader . # the template for posts

:s4 a :RemapContext ;
    trm:configKey :postTemplateMap .
# context.content -> context.template ## Remaps should go here????
#
:s5 a :DirWalker ;
    trm:configKey :files .

 #:s6 a :ShowMessage .

# process forks here 

:s6 a :FileReader . # the markdown content

:s7 a :PostcraftPrep . # set up title, filenames etc

:s8 a :MarkdownToHTML .
:s9 a :Templater .

:s10 a :RemapContext ;
   trm:configKey :postSaver .

# :s11 a :NOP .
:s11 a :FileWriter .

:s12 a :NOP .
#:s12 a :Unfork ;
 #   trm:configKey :combinePosts .

:s13 a :ShowMessage .

================
File: raw-src/README.md
================
The files here are minimal versions of services etc, to try in isolation before integrating into the main codebase.

================
File: raw-src/regex-play.js
================
const mini = 'keep this\n>remove this\n\n keep this'

const replied = 'for this usage.\n> My real intent is to use karma as a type of point system or game/task\n>\n>\n> I am not sure. \n>and another\nfollowed by normal, then \n>one final quoted line'

const first = '> a quote first line\nthen normal'

const regexMini = /\n>.*\n/gi

const regexFirst = /(^|\n)>.*\n/gi

const regexReplied = /((^|\n)>)+.*\n/gi







console.log('[[[\n' + replied.replace(regexReplied, '\n') + '\n]]]\n\n')

================
File: raw-src/structure-play.js
================
let b = 'B'
let a = 'twat'


let c = a => b

================
File: spec/support/jasmine-browser.json
================
{
  "srcDir": "src",
  "srcFiles": [
    "**/*.js"
  ],
  "specDir": "spec",
  "specFiles": [
    "**/*[sS]pec.js"
  ],
  "helpers": [
    "helpers/**/*.js"
  ],
  "env": {
    "stopSpecOnExpectationFailure": false,
    "stopOnSpecFailure": false,
    "random": true
  },
  "browser": {
    "name": "firefox"
  }
}

================
File: src/applications/dirwalker-test/about.md
================
it originally ran from

transmission.execute('../../data/mail-archive-sample')

================
File: src/applications/dirwalker-test/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:FilePipelineMap a trm:DataMap ;
    trm:sourceFile "input.txt" ;
    trm:destinationFile "output.txt" .

================
File: src/applications/dirwalker-test/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:dirwalkpipe a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a :DirWalker .
:s2 a :AppendProcess .
:s3 a :StringSink .

================
File: src/applications/env-loader-test/about.md
================
```sh
cd ~/github-danny/transmissions
./trans env-loader-test
```

================
File: src/applications/env-loader-test/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/env-loader-test/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:envy a trm:Pipeline ;
    trm:pipe (:s10 :s20 :SM) .
# trm:pipe (:SC) .
:s10 a :EnvLoader .
:s20 a :WhiteboardToMessage .

================
File: src/applications/file-copy-remove-test/data/several-full/one.txt
================
Hello from One

================
File: src/applications/file-copy-remove-test/data/several-full/two.txt
================
Hello from Two

================
File: src/applications/file-copy-remove-test/data/single-full/one.txt
================
Hello from One

================
File: src/applications/file-copy-remove-test/data/start/one.txt
================
Hello from One

================
File: src/applications/file-copy-remove-test/data/start/two.txt
================
Hello from Two

================
File: src/applications/file-copy-remove-test/about.md
================
# file-copy-remove-test

run with :

```
# in transmissions dir

./run file-copy-remove-test
```

or

```
npm test -- tests/integration/file-copy-remove-test.spec.js
```

this should :

- copy `start/one.txt` into `single-empty/`
- copy `single-empty/one.txt` into `single-full/`
- remove `single-empty/one.txt`

- copy everything in `start/` into `several-empty/`
- copy everything in `several-empty/` into `several-full/`
- remove everything in `several-empty/`

Hmm, test services would be helpful to check before and after - or maybe just use regular test runner script from npm?

================
File: src/applications/file-copy-remove-test/init.sh
================
rm -rf data/start
rm -rf data/single-empty
rm -rf data/single-full
rm -rf data/several-empty
rm -rf data/several-full



mkdir -p data/start






echo 'Hello from One' > data/start/one.txt


echo 'Hello from Two' > data/start/two.txt

tree data

================
File: src/applications/file-copy-remove-test/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

#t:copyOneToSingleEmpty a trm:ServiceConfig ;
 #   trm:key t:copyOneToSingleEmpty ;
  #  trm:source "data/start/one.txt" ;
   # trm:destination "data/single-empty/one.txt" .

t:copyOneToSingleEmpty a trm:ServiceConfig ;
    trm:key t:copyOneToSingleEmpty ;
    trm:source "data/start/one.txt" ;
    trm:destination "data/single-empty/one.txt" .

t:copySingleEmptyToSingleFull a trm:ServiceConfig ;
    trm:key t:copySingleEmptyToSingleFull ;
    trm:source "data/single-empty/one.txt" ;
    trm:destination "data/single-full/one.txt" .

t:removeSingleEmpty a trm:ServiceConfig ;
    trm:key t:removeSingleEmpty ;
    trm:target "data/single-empty/one.txt" .

t:copyStartToSeveralEmpty a trm:ServiceConfig ;
    trm:key t:copyStartToSeveralEmpty ;
    trm:source "data/start" ;
    trm:destination "data/several-empty" .

t:copySeveralEmptyToSeveralFull a trm:ServiceConfig ;
    trm:key t:copySeveralEmptyToSeveralFull ;
    trm:source "data/several-empty" ;
    trm:destination "data/several-full" .

t:removeSeveralEmpty a trm:ServiceConfig ;
    trm:key t:removeSeveralEmpty ;
    trm:target "data/several-empty" .

================
File: src/applications/file-copy-remove-test/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:file_copy_remove_test a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4 :s5 :s6) .

:s1 a :FileCopy ;
    trm:configKey :copyOneToSingleEmpty .

:s2 a :FileCopy ;
    trm:configKey :copySingleEmptyToSingleFull .

:s3 a :FileRemove ;
    trm:configKey :removeSingleEmpty .

:s4 a :FileCopy ;
    trm:configKey :copyStartToSeveralEmpty .

:s5 a :FileCopy ;
    trm:configKey :copySeveralEmptyToSeveralFull .

:s6 a :FileRemove ;
    trm:configKey :removeSeveralEmpty .

================
File: src/applications/file-pipeline/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:FilePipelineMap a trm:DataMap ;
    trm:sourceFile "input.txt" ;
    trm:destinationFile "output.txt" .

================
File: src/applications/file-pipeline/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:file_pipeline a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4) .

:s1 a :FileSource .
:s2 a :AppendProcess .
:s3 a :AppendProcess .
:s4 a :FileSink .

================
File: src/applications/globbo/about.md
================
# Globbo

```
./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}'
```

there are more notes under

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-application-walkthrough.md

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-service-walkthrough.md

## Description

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/globbo/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/globbo/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:globbo a trm:Pipeline ;
    trm:pipe (:s10 :s20 :s30 :s40 :SM) .

#:s40 a :RemapContext ;
 #   trm:configKey :walkPrep .

:s10 a :DirWalker .
:s20 a :CaptureAll . # pushes all messages into config.whiteboard
:s30 a :Unfork .
:s40 a :WhiteboardToMessage .

================
File: src/applications/html-to-md/about.md
================
# HTML to Markdown

*a minimal application (that I need) which can also serve as an example in documentation*

```
./run html-to-md -c '{"rootDir": "./test-data/html-to-md", "filename":"webidl.html"}'
```

## Description

---

there are more notes under

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-application-walkthrough.md

/home/danny/github-danny/transmissions/docs/postcraft-site/articles/new-service-walkthrough.md

## Description

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/html-to-md/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/html-to-md/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:h2m a trm:Pipeline ;
    trm:pipe (:s10 :s20 :s30 :s40 :SM) .

#:s40 a :RemapContext ;
 #   trm:configKey :walkPrep .

:s10 a :FileReader .
:s20 a :CaptureAll . # pushes all messages into config.whiteboard
:s30 a :Unfork .
:s40 a :WhiteboardToMessage .

================
File: src/applications/link-lister/about.md
================
run.js had

const here = import.meta.url
const message = { runScript: here }

transmission.execute('', message)

================
File: src/applications/link-lister/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:linklister trm:hasDataMap t:llSourceMap .
t:linklister trm:hasDataMap t:llGotMap .
t:linklister trm:hasDataMap t:llLinkMap .

t:llSourceMap a trm:DataMap ;
    trm:key t:sourceFile ;
    trm:value "starter-links.md" .

t:llGotMap a trm:DataMap ;
    trm:key t:gotFile ;
    trm:value "got.html" .

t:llLinkMap a trm:DataMap ;
    trm:key t:linkFile ;
    trm:value "links.md" .

t:htmlMap a trm:DataMap ;
    trm:key t:htmlFile ;
    trm:value "links.html" .

================
File: src/applications/link-lister/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:linklister a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4 :s5 :s6 :s7 :s8) .

:s1 a :FileReader ;
    trm:configKey :sourceFile .
:s2 a :LineReader .
:s3 a :HttpGet .

:s4 a :LinkFinder .

:s5 a :StringMerger .

:s6 a :FileWriter ;
    trm:configKey :linkFile .
# :s6 a :NOP .
:s7 a :MarkdownToHTML .

:s8 a :FileWriter ;
    trm:configKey :htmlFile .

#:s8 a :StringFilter .
#:s9 a :StringMerger .
#:s10 a :FileWriter 
#        trm:configKey :linkFile .
# :s4 a :NOP .
# :s4 a :FileWriter ;
#     trm:configKey :gotFile .

# :s5 a :NOP .

================
File: src/applications/nop-test/about.md
================
# nop

## Description

minimal for comparing with simple runner

---

- Goal : a tool to recursively read local filesystem directories, checking for files with the `.md` extension to identify collections of such
- Goal : documentation of the app creation process
- Implementation : a #Transmissions application
- SoftGoal : reusability
- _non-goal_ - efficiency

================
File: src/applications/nop-test/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .

================
File: src/applications/nop-test/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:nope a trm:Pipeline ;
    trm:pipe (:N :SC :SM) .

================
File: src/applications/postcraft/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances
@prefix pc: <http://purl.org/stuff/postcraft/> .

### ConfigMap
t:PostcraftMap a trm:ServiceConfig ;
    trm:key t:postcraftMap ;
    trm:group pc:ContentGroup .

### clean 

t:cacheRemove a trm:ServiceConfig ;
    trm:key t:removeCache ;
    trm:target "cache/" .

t:articlesRemove a trm:ServiceConfig ;
    trm:key t:removeArticles ;
    trm:target "public/home/articles" .

t:entriesRemove a trm:ServiceConfig ;
    trm:key t:removeEntries ;
    trm:target "public/home/entries" .

t:journalRemove a trm:ServiceConfig ;
    trm:key t:removeJournal ;
    trm:target "public/home/journal" .

t:todoRemove a trm:ServiceConfig ;
    trm:key t:removeTodo ;
    trm:target "public/home/todo" .

t:indexRemove a trm:ServiceConfig ;
    trm:key t:removeIndex ;
    trm:target "public/home/index.html" .

### copy #####################################

t:copyStatic a trm:ServiceConfig ;
    trm:key t:staticCopy ;
    trm:source "content-static" ;
    trm:destination "public/home/static" .


t:copyMedia a trm:ServiceConfig ;
    trm:key t:mediaCopy ;
    trm:source "media" ;
    trm:destination "public/home/media" .
    
t:copyCSS a trm:ServiceConfig ;
    trm:key t:cssCopy ;
    trm:source "layouts/mediocre/css" ;
    trm:destination "public/home/css" .

t:copyFonts a trm:ServiceConfig ;
    trm:key t:fontsCopy ;
    trm:source "layouts/mediocre/fonts" ;
    trm:destination "public/home/fonts" .

t:copyJS a trm:ServiceConfig ;
    trm:key t:jsCopy ;
    trm:source "layouts/mediocre/js" ;
    trm:destination "public/home/js" .

### render ##################################

trm:Describe  a trm:ServiceConfig ; 
 trm:key trm:describe .

t:phaseOne a trm:ServiceConfig ; 
    trm:key t:markdownToRawPosts ;
    trm:marker "Phase One" .

# TODO IS COPY, not rename!!

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
  #  t:pp2   trm:pre     "filename" ;
   #         trm:post    "filename"  .
 
t:entryRawPrep a trm:ReMap ;
    trm:rename (t:er1 t:er3) . 
   t:er1   trm:pre     "targetFilename" ;
            trm:post    "filepath" . 
    t:er3   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

t:entryPagePrep a trm:ReMap ;
    trm:rename (t:ppp1) . 
    t:ppp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

================
File: src/applications/postcraft/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 
@prefix pc: <http://purl.org/stuff/postcraft/> .

#############################################################
# insert into pipe for debugging - one instance only like this
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:SM2 a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe) 
:UF a :Unfork . # collapses all pipes but one 
#############################################################


## POSTCRAFT.CLEAN ##########################################

:clean a trm:Pipeline ;
    rdfs:label "clean" ;
    rdfs:comment "directory cleaner" ;
    trm:pipe (:r10 :r20 :r30 :r40 :r50 :r60) .

:r10 a :FileRemove ;
    trm:configKey :removeCache .

:r20 a :FileRemove ;
    trm:configKey  :removeArticles .

:r30 a :FileRemove ;
    trm:configKey  :removeJournal .

:r40 a :FileRemove ;
    trm:configKey  :removeEntries .

:r50 a :FileRemove ;
    trm:configKey  :removeTodo .

:r60 a :FileRemove ;
    trm:configKey  :removeIndex .

## POSTCRAFT.COPY ##################################################################

:copy a trm:Pipeline ;
    rdfs:label "copy" ;
    rdfs:comment "dir/file copier" ;
    trm:pipe (:cp10 :cp20 :cp30 :cp40 :cp50) .

### static dirs
:cp10 a :FileCopy ;
    trm:configKey :staticCopy .

### media dirs
:cp20 a :FileCopy ;
    trm:configKey :mediaCopy .
    
### layout dirs
:cp30 a :FileCopy ;
    trm:configKey :cssCopy .

:cp40 a :FileCopy ;
    trm:configKey :jsCopy .

:cp50 a :FileCopy ;
    trm:configKey :fontsCopy .

#####################


## POSTCRAFT.RENDER ###############################################################

:render a trm:Pipeline ;
    rdfs:label "render" ;
    rdfs:comment "render the pages" ;
   trm:pipe (:s10 :s20 :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100 
              :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .
 #  trm:pipe (:s10 :SM :s20 :SM2 :DE  :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100 
  #               :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .

:s10 a :DatasetReader . # read the manifest
# trm:configKey trm:describe .

:s20 a :ConfigMap ;
    trm:configKey :postcraftMap .

:s30 a :FileReader ; # the template for raw entry content
    trm:describe trm:all .

:s40 a :RemapContext ;
    trm:configKey :walkPrep .

:s50 a :DirWalker . # automatically forks

:s60 a :FileReader . # the markdown content

:s70 a :PostcraftPrep . # set up title, filenames etc

:s80 a :MarkdownToHTML . 

:s90 a :RemapContext ;
   trm:configKey :entryRawPrep .

 :s100 a :Templater .

:s110 a :FileWriter .

############### entryContentToEntryPage
:s120 a :EntryContentToPagePrep .
#:s12 a :RemapContext ;
 #  trm:configKey :entryPagePrep .

:s130 a :Templater .

:s140 a :FileWriter .

#######################
:s150  a :Unfork .

:s160 a :FrontPagePrep . 

:s170 a :Templater .

:s180 a :FileWriter .

================
File: src/applications/postcraft-init/about.md
================
./run postcraft-init /home/danny/absolute/path

================
File: src/applications/postcraft-init/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

:copyTemplateToDestination a trm:ServiceConfig ;
    trm:key :copyTemplateToDestination ;
    trm:source "/home/danny/HKMS/postcraft/postcraft-template/" ;
    trm:destination "{{destinationPath}}" .

================
File: src/applications/postcraft-init/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################


:postcraft_init a trm:Pipeline ;
    trm:pipe (:s1) .

:s1 a :FileCopy ;
    trm:configKey :copyTemplateToDestination .

================
File: src/applications/postcraft-previous/processors.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

trm:Describe  a trm:ServiceConfig ; 
 trm:key trm:describe .

t:phaseOne a trm:ServiceConfig ; 
    trm:key t:markdownToRawPosts ;
    trm:marker "Phase One" .

t:copyMedia a trm:ServiceConfig ;
    trm:key t:mediaCopy ;
    trm:source "media" ;
    trm:destination "public/home/media" .
    
t:copyCSS a trm:ServiceConfig ;
    trm:key t:cssCopy ;
    trm:source "layouts/mediocre/css" ;
    trm:destination "public/home/css" .

t:copyFonts a trm:ServiceConfig ;
    trm:key t:fontsCopy ;
    trm:source "layouts/mediocre/fonts" ;
    trm:destination "public/home/fonts" .

t:copyJS a trm:ServiceConfig ;
    trm:key t:jsCopy ;
    trm:source "layouts/mediocre/fonts" ;
    trm:destination "public/home/fonts" .

# TODO IS COPY, not rename!!

t:walkPrep a trm:ReMap ;
    trm:rename (t:pp1 t:pp2) . # consider using blank nodes
    t:pp1   trm:pre     "content" ;
            trm:post    "template"  .
    t:pp2   trm:pre     "entryContentMeta.sourceDir" ;
            trm:post    "sourceDir" .
  #  t:pp2   trm:pre     "filename" ;
   #         trm:post    "filename"  .
 
t:entryRawPrep a trm:ReMap ;
    trm:rename (t:er1 t:er3) . 
   t:er1   trm:pre     "targetFilename" ;
            trm:post    "filepath" . 
    t:er3   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

t:entryPagePrep a trm:ReMap ;
    trm:rename (t:ppp1) . 
    t:ppp1   trm:pre     "content" ;
            trm:post    "contentBlocks.content" .

================
File: src/applications/postcraft-previous/transmission.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:postTemplating a trm:Pipeline ;
    trm:pipe (:s00 :s05 :s10 :s11 :s12 :s20  :s30 :s40  :s50 :s60 :s70 :s80 :s90 :s100 
                :s110 :s120 :s130 :s140 :s150  :s160 :s170 :s180) .

:s00 a :DatasetReader . # read the manifest
# trm:configKey trm:describe .

### Copy media dirs
:s05 a :FileCopy ;
    trm:configKey :mediaCopy .
    
### Copy layout dirs
:s10 a :FileCopy ;
    trm:configKey :cssCopy .

:s11 a :FileCopy ;
    trm:configKey :jsCopy .

:s12 a :FileCopy ;
    trm:configKey :fontsCopy .
#####################

:s20 a :ConfigMap ; ### use services.ttl? - defer to RemapContext as possible
    trm:configKey :markdownToRawPosts .

:s30 a :FileReader ; # the template for raw entry content
    trm:describe trm:all .

:s40 a :RemapContext ;
    trm:configKey :walkPrep .

:s50 a :DirWalker . # automatically forks

:s60 a :FileReader . # the markdown content

:s70 a :PostcraftPrep . # set up title, filenames etc

:s80 a :MarkdownToHTML . 

:s90 a :RemapContext ;
   trm:configKey :entryRawPrep .

 :s100 a :Templater .

:s110 a :FileWriter .

############### entryContentToEntryPage
:s120 a :EntryContentToPagePrep .
#:s12 a :RemapContext ;
 #  trm:configKey :entryPagePrep .

:s130 a :Templater .

:s140 a :FileWriter .

#######################
:s150  a :Unfork .

:s160 a :FrontPagePrep . 

:s170 a :Templater .

:s180 a :FileWriter .

================
File: src/applications/string-pipeline/processors-config.ttl
================
### NOT USED

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dc: <http://purl.org/dc/terms/> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:StringPipeline dc:title "Hello" .

================
File: src/applications/string-pipeline/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances

:stringpipe a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3 :s4) .

:s1 a :StringSource .
:s2 a :AppendProcess .
:s3 a :AppendProcess .
:s4 a :StringSink .

================
File: src/applications/test_foreach/about.md
================
# ForEach processor module for Transmissions

```sh
./trans test_fork
```

Your Goal is to write a processor module for Transmissions that will initiate multiple processing pipelines based on a list provided in the incoming message. First review these instructions as a whole, and then identify the subgoals. Then, taking each subgoal in turn, break it down into a concrete series of tasks. Carry out the sequence of tasks.  
You have plenty of time, so don't rush, try to be as careful in understanding and operation as possible.
Existing source code may be found in the Project Knowledge files.

Two modules are required -

1. `ForEach` located in :

```sh
./transmissions/src/processors/flow/ForEach.js
```

modeled on :

```sh
./transmissions/src/processors/templates/ProcessorTemplate.js
```

2. `FlowProcessorsFactory` located in

```sh
./transmissions/src/processors/flow/FlowProcessorsFactory.js
```

modeled on :

```sh
/transmissions/src/processors/templates/TemplateProcessorsFactory.js
```

The input message will contain the list to be processed in the form of this example :

```json
{
  "foreach": ["item1", "item2", "item3"]
}
```

The behavior will be to emit the message to a subsequent processor using the existing engine infrastructure, like a simpler version of :

```sh
transmissions/src/processors/fs/DirWalker.js
```

Each message emitted will be a structuredClone of the input message.

Once this code is completed, create application definitions in the form of these examples :

```sh
transmissions/src/applications/test_fork/transmissions.ttl
transmissions/src/applications/test_fork/processors-config.ttl
```

After you have finished all these, re-read the high level Goal and taking each of your derived subgoals in turn, review your code to ensure that it fulfils the requirements.
Show me the full source of the implementations.

---

/home/danny/github-danny/postcraft/danny.ayers.name/content-raw/entries/2024-09-27_lively-distractions.md

https://github.com/github/rest-api-description

================
File: src/applications/test_foreach/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

:foreach_test a trm:Pipeline ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a :ForEach .
:s2 a :ShowMessage .
:s3 a :DeadEnd .

================
File: src/applications/test_fork/about.md
================
# Test Fork/Unfork

```
./run test_fork | grep 's2 a NOP'
```

should show the number of forks + 1 (for `message.done`)

```
./run test_fork | grep s1.s2.s10.s11.s12.s13
```

should show just one

================
File: src/applications/test_fork/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:dirWalkerPosts a trm:ServiceConfig ;
    trm:key t:files .

t:postTemplateMap a trm:ReMap ;
   trm:rename (t:rn1) . # consider using blank nodes
     t:rn1    trm:pre     "content" ;
            trm:post    "template"  .

t:postSaver a trm:ReMap ;
    trm:rename (t:rn2) . 
    t:rn2   trm:pre     "targetFilename" ;
            trm:post    "filename" .

================
File: src/applications/test_fork/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SC a :ShowConfig . # verbose report, continues pipe
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:test_fork a :Transmission ;
   trm:contains :pipeA .

:pipeA a trm:Pipeline ;
trm:pipe (:p10 :SM ) .

:p10 a :Fork .

# :s10 a :Unfork .
:s11 a :NOP .

================
File: src/applications/test_multi-pipe/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
# @prefix fs: <http://purl.org/stuff/filesystem/> .
@prefix t: <http://hyperdata.it/transmissions/> . # for custom terms & instances

t:dirWalkerPosts a trm:ServiceConfig ;
    trm:key t:files .

t:postTemplateMap a trm:ReMap ;
   trm:rename (t:rn1) . # consider using blank nodes
     t:rn1    trm:pre     "content" ;
            trm:post    "template"  .

t:postSaver a trm:ReMap ;
    trm:rename (t:rn2) . 
    t:rn2   trm:pre     "targetFilename" ;
            trm:post    "filename" .

================
File: src/applications/test_multi-pipe/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> . # TODO make plural
@prefix : <http://hyperdata.it/transmissions/> . # for custom terms & instances - TODO make one @services s: 

:test_multi-pipes a :Transmission ;
   trm:contains :pipeA, :pipeB, :pipeC .

:pipeA a trm:Pipeline ;
trm:pipe (:s1 :s2 :s3 ) .

:pipeB  a trm:Pipeline ;
 trm:pipe (:s3 :s104 :s105) .

:pipeC a trm:Pipeline ;
trm:pipe (:s3 :s204 :s205) .

# :postcraft a trm:Pipeline ;

:s1 a :NOP .
:s2 a :NOP .
:s3 a :NOP .

:s104 a :NOP .
:s105 a :NOP .

:s204 a :NOP .
:s205 a :ShowTransmission .

================
File: src/applications/two-transmissions/processors-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix t: <http://hyperdata.it/transmissions/> .

#t:copyOneToSingleEmpty a trm:ServiceConfig ;
 #   trm:key t:copyOneToSingleEmpty ;
  #  trm:source "data/start/one.txt" ;
   # trm:destination "data/single-empty/one.txt" .

t:copyOneToSingleEmpty a trm:ServiceConfig ;
    trm:key t:copyOneToSingleEmpty ;
    trm:source "data/start/one.txt" ;
    trm:destination "data/single-empty/one.txt" .

t:copySingleEmptyToSingleFull a trm:ServiceConfig ;
    trm:key t:copySingleEmptyToSingleFull ;
    trm:source "data/single-empty/one.txt" ;
    trm:destination "data/single-full/one.txt" .

t:removeSingleEmpty a trm:ServiceConfig ;
    trm:key t:removeSingleEmpty ;
    trm:target "data/single-empty/one.txt" .

t:copyStartToSeveralEmpty a trm:ServiceConfig ;
    trm:key t:copyStartToSeveralEmpty ;
    trm:source "data/start" ;
    trm:destination "data/several-empty" .

t:copySeveralEmptyToSeveralFull a trm:ServiceConfig ;
    trm:key t:copySeveralEmptyToSeveralFull ;
    trm:source "data/several-empty" ;
    trm:destination "data/several-full" .

t:removeSeveralEmpty a trm:ServiceConfig ;
    trm:key t:removeSeveralEmpty ;
    trm:target "data/several-empty" .

================
File: src/applications/two-transmissions/transmissions.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <http://hyperdata.it/transmissions/> .

#############################################################
# insert into pipe for debugging
:DE a :DeadEnd . # ends the current pipe quietly
:H  a :Halt . # kills everything 
:SM a :ShowMessage . # verbose report, continues pipe
:N  a :NOP . # no operation (except for showing stage in pipe)
:UF a :Unfork . # collapses all pipes but one 
#############################################################

:transmission_one a trm:Pipeline ;
    trm:pipe (:a1 :a2) .

:a1 a :NOP .

:a2 a :NOP .

:transmission_two a trm:Pipeline ;
    trm:pipe (:b1 :b2 :b3) .

:b1 a :NOP .

:b2 a :NOP .

:b3 a :NOP .

================
File: src/engine/A.js
================
import { ModuleLoader } from './ModuleLoader.js';


const cp = ['/over/there/', '/over/here/'];

async function main() {
    const loader = new ModuleLoader(cp);

    try {

        const moduleB = loader.loadModule('B');


        const moduleC = await loader.loadESModule('C');


        moduleB.someFunction();
        moduleC.anotherFunction();


        const moduleBAgain = loader.loadModule('B');

        console.log(moduleB === moduleBAgain);
    } catch (error) {
        console.error('Error:', error.message);
    }
}

main();

================
File: src/engine/AbstractProcessorFactory.js
================
import logger from '../utils/Logger.js'
import ns from '../utils/ns.js'


import SystemProcessorsFactory from '../processors/system/SystemProcessorsFactory.js'
import TestProcessorsFactory from '../processors/test/TestProcessorsFactory.js'
import FsProcessorsFactory from '../processors/fs/FsProcessorsFactory.js'
import MarkupProcessorsFactory from '../processors/markup/MarkupProcessorsFactory.js'
import UtilProcessorsFactory from '../processors/util/UtilProcessorsFactory.js'
import TextProcessorsFactory from '../processors/text/TextProcessorsFactory.js'
import ProtocolsProcessorsFactory from '../processors/protocols/ProtocolsProcessorsFactory.js'
import RDFProcessorsFactory from '../processors/rdf/RDFProcessorsFactory.js'
import PostcraftProcessorsFactory from '../processors/postcraft/PostcraftProcessorsFactory.js'
import FlowProcessorsFactory from '../processors/flow/FlowProcessorsFactory.js'


class AbstractProcessorFactory {




    static createProcessor(type, config) {


        var processor = TestProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        var processor = UtilProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = MarkupProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = TextProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = ProtocolsProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = RDFProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = PostcraftProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = SystemProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        processor = FlowProcessorsFactory.createProcessor(type, config)
        if (processor) return processor

        throw new Error("Unknown processor type: " + type.value)
    }
}

export default AbstractProcessorFactory

================
File: src/engine/B.js
================
import { ModuleLoader } from './ModuleLoader.js';


const cp = ['/some/other/path/', '/over/here/'];

async function someFunction() {
    const loader = new ModuleLoader(cp);

    try {

        const moduleD = loader.loadModule('D');


        const moduleE = await loader.loadESModule('E');


        moduleD.doSomething();
        moduleE.doSomethingElse();
    } catch (error) {
        console.error('Error in B.js:', error.message);
    }
}

export { someFunction };

================
File: src/engine/Connector.js
================
import { EventEmitter } from 'events'
import logger from '../utils/Logger.js'
import footpath from '../utils/footpath.js'

class Connector extends EventEmitter {


    constructor(fromName, toName) {
        super();
        this.fromName = fromName
        this.toName = toName
    }

    connect(processors) {
        let fromProcessor = processors[this.fromName]
        let toProcessor = processors[this.toName]

        fromProcessor.on('message', (message) => {
            var tags = ''
            //     if (toProcessor.message) {
            tags = ' (' + fromProcessor.message.tags + ') '
            toProcessor.tags = tags

            const thisTag = footpath.urlLastPart(this.toName)
            logger.log("| Running : " + tags + thisTag + " a " + toProcessor.constructor.name)

            toProcessor.receive(message)
        })
    }


}

export default Connector

================
File: src/engine/ModuleLoader.js
================
import { createRequire } from 'module';
import { join } from 'path';
import { fileURLToPath } from 'url';

export class ModuleLoader {
    constructor(classpath) {
        this.classpath = classpath;
        this.moduleCache = new Map();
    }

    loadModule(moduleName) {
        if (this.moduleCache.has(moduleName)) {
            console.log(`Module ${moduleName} loaded from cache`);
            return this.moduleCache.get(moduleName);
        }

        for (const path of this.classpath) {
            const require = createRequire(join(path, 'dummy.js'));
            try {
                const module = require(`./${moduleName}`);
                this.moduleCache.set(moduleName, module);
                console.log(`Module ${moduleName} loaded from ${path} and cached`);
                return module;
            } catch (error) {
                if (error.code !== 'MODULE_NOT_FOUND') {
                    throw error;
                }

            }
        }
        throw new Error(`Module ${moduleName} not found in provided classpath`);
    }

    async loadESModule(moduleName) {
        if (this.moduleCache.has(moduleName)) {
            console.log(`ES Module ${moduleName} loaded from cache`);
            return this.moduleCache.get(moduleName);
        }

        for (const path of this.classpath) {
            try {
                const modulePath = new URL(`file://${join(path, moduleName)}.js`).href;
                const module = await import(modulePath);
                this.moduleCache.set(moduleName, module);
                console.log(`ES Module ${moduleName} loaded from ${path} and cached`);
                return module;
            } catch (error) {
                if (error.code !== 'ERR_MODULE_NOT_FOUND') {
                    throw error;
                }

            }
        }
        throw new Error(`ES Module ${moduleName} not found in provided classpath`);
    }
}

================
File: src/engine/Transmission.js
================
import logger from '../utils/Logger.js'
import Connector from './Connector.js'

class Transmission {
  constructor() {
    this.processors = {}
    this.connectors = []

  }

  register(processorName, instance) {
    this.processors[processorName] = instance

  }

  get(processorName) {
    return this.processors[processorName]
  }

  connect(fromProcessorName, toProcessorName) {
    let connector = new Connector(fromProcessorName, toProcessorName)
    this.connectors.push(connector)
    connector.connect(this.processors)
  }




  async execute(message) {
    logger.log('\n+ ***** Execute Transmission : ' + this.label + ' <' + this.id + '>')
    const processorName = this.connectors[0]?.fromName || Object.keys(this.processors)[0]
    let processor = this.get(processorName)
    if (processor) {
      logger.log("| Running : " + processorName + " a " + processor.constructor.name)
      await processor.receive(message)
    } else {
      logger.error("No valid processor found to execute")
    }
  }






















  toString() {
    let description = 'Transmission Structure:\n';


    description += 'Processors:\n';
    Object.keys(this.processors).forEach(processorName => {
      description += `  - ${processorName}\n`;
    });


    description += 'Connectors:\n';
    this.connectors.forEach((connector, index) => {
      description += `  - Connector ${index + 1}: ${connector.fromName} -> ${connector.toName}\n`;
    })

    return description
  }
}

export default Transmission

================
File: src/engine/TransmissionBuilder.js
================
import path from 'path';
import { fileURLToPath } from 'url';

import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'

import ns from '../utils/ns.js'
import GrapoiHelpers from '../utils/GrapoiHelpers.js'
import logger from '../utils/Logger.js'

import AbstractProcessorFactory from "./AbstractProcessorFactory.js";
import Transmission from './Transmission.js'



class TransmissionBuilder {

  static async build(transmissionConfigFile, processorsConfigFile) {

    logger.info('\n+ ***** Load Config ******')
    logger.info('[Transmission : ' + transmissionConfigFile + ']')
    const transmissionConfig = await TransmissionBuilder.readDataset(transmissionConfigFile)
    logger.info('[Processors Config : ' + processorsConfigFile + ']')

    const processorsConfig = await TransmissionBuilder.readDataset(processorsConfigFile)

    const poi = grapoi({ dataset: transmissionConfig })

    const transmissions = []


    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.trm.Pipeline)) {
        const pipelineID = q.subject
        logger.debug('\n+ ' + pipelineID.value)
        transmissions.push(TransmissionBuilder.constructTransmission(transmissionConfig, pipelineID, processorsConfig))
      }
    }
    return transmissions
  }


  static constructTransmission(transmissionConfig, pipelineID, processorsConfig) {
    processorsConfig.whiteboard = {}

    const transmission = new Transmission()
    transmission.id = pipelineID.value
    transmission.label = ''

    const transPoi = grapoi({ dataset: transmissionConfig, term: pipelineID })

    // TODO has grapoi got a first/single property method?
    for (const quad of transPoi.out(ns.rdfs.label).quads()) {
      transmission.label = quad.object.value
    }
    logger.log('\n+ ***** Construct Transmission : ' + transmission.label + ' <' + transmission.id + '>')

    let previousName = "nothing"

    // grapoi probably has a built-in for all this
    const pipenodes = GrapoiHelpers.listToArray(transmissionConfig, pipelineID, ns.trm.pipe)

    this.createNodes(transmission, pipenodes, transmissionConfig, processorsConfig)
    this.connectNodes(transmission, pipenodes)
    return transmission
  }

  static createNodes(transmission, pipenodes, transmissionConfig, processorsConfig) {
    for (let i = 0; i < pipenodes.length; i++) {
      let node = pipenodes[i]
      let processorName = node.value


      if (!transmission.get(processorName)) { // may have been created in earlier pipeline
        let np = rdf.grapoi({ dataset: transmissionConfig, term: node })
        let processorType = np.out(ns.rdf.type).term
        let processorConfig = np.out(ns.trm.configKey).term

        try {

          let name = ns.getShortname(processorName)
          let type = ns.getShortname(processorType.value)

          logger.log("| Create processor :" + name + " of type :" + type)
          //  logger.log("| Create processor <" + processorName + "> of type <" + processorType.value + ">")
        } catch (err) {
          logger.error('-> Can\'t resolve ' + processorName + ' (check transmission.ttl for typos!)\n')
        }
        let processor = AbstractProcessorFactory.createProcessor(processorType, processorsConfig)
        processor.id = processorName
        processor.type = processorType
        processor.transmission = transmission

        if (processorConfig) {

          processor.configKey = processorConfig
        }
        transmission.register(processorName, processor)
      }
    }

  }

  static connectNodes(transmission, pipenodes) {
    for (let i = 0; i < pipenodes.length - 1; i++) {
      let leftNode = pipenodes[i]
      let leftProcessorName = leftNode.value
      let rightNode = pipenodes[i + 1]
      let rightProcessorName = rightNode.value
      logger.log("  > Connect #" + i + " [" + ns.getShortname(leftProcessorName) + "] => [" + ns.getShortname(rightProcessorName) + "]")
      transmission.connect(leftProcessorName, rightProcessorName)
    }
  }






















  static async readDataset(filename) {
    const stream = fromFile(filename)
    const dataset = await rdf.dataset().import(stream)
    return dataset
  }

  static async writeDataset(dataset, filename) {
    await toFile(dataset.toStream(), filename)
  }


}

export default TransmissionBuilder

================
File: src/engine/WorkerPool.js
================
import { Worker } from 'worker_threads'

class WorkerPool {
    constructor(module, size) {
        this.workers = [];
        this.queue = [];
        for (let i = 0; i < size; i++) {
            const worker = new Worker(module);
            worker.on('message', () => {

                this.markWorkerIdle(worker);
            });
            this.workers.push({ worker, busy: false });
        }
    }

    enqueueMessage(message) {
        this.queue.push(message);
        this.dispatch();
    }

    dispatch() {
        const idleWorkerWrapper = this.workers.find(wrapper => !wrapper.busy);
        if (idleWorkerWrapper && this.queue.length) {
            const message = this.queue.shift();
            idleWorkerWrapper.busy = true;
            idleWorkerWrapper.worker.postMessage(message);
        }
    }

    markWorkerIdle(workerWrapper) {
        workerWrapper.busy = false;
        this.dispatch();
    }
}

================
File: src/processors/base/Processor copy.js
================
import logger from '../../utils/Logger.js'
import { EventEmitter } from 'events'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'





class Processor extends EventEmitter {





    constructor(config) {
        super()
        this.config = config
        this.messageQueue = []
        this.processing = false
        this.done = false
    }

    preProcess(message) {
        return







        const processorPoi = rdf.grapoi({ dataset: this.config, term: this.configKey })
        logger.log('this.configKey = ' + this.configKey.value)
        logger.poi(processorPoi)

        logger.log('describe Desc')
        if (this.configKey.value === ns.trm.describe.value) {
            this.describe()
        }



    }

    describe() {
        logger.log('describe')
        const inputs = this.getInputKeys()
        const outputs = this.getOutputKeys()
        for (var input of inputs) {
            logger.log('input = ' + input)
            logger.log(this.message[input] + ' = ' + this.message[input])
        }
        for (var output of outputs) {
            logger.log('output = ' + output)
            logger.log(this.message[output] + ' = ' + this.message[output])
        }
    }





















    getMyConfigNode() {
        const dataset = this.config
        const configNode = grapoi({ dataset, term: this.configKey }).in()
        return configNode.term
    }

    getMyPoi() {
        const dataset = this.config
        const myConfigNode = this.getMyConfigNode()
        const poi = grapoi({ dataset: dataset, term: myConfigNode })
        return poi
    }

    async addPropertyToMyConfig(predicate, value) {
        logger.log('addPropertyToMyConfig predicate = ' + predicate)
        logger.log('addPropertyToMyConfig value = ' + value)
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('addPropertyToMyConfig  myConfigNode.value = ' + myConfigNode.value)
        const dataset = this.config
        dataset.add(myConfigNode, predicate, value)
        this.config = dataset
    }

    showMyConfig() {
        const poi = this.getMyPoi()
        logger.log('POI = ')
        logger.poi(poi)
    }

    getPropertyFromMyConfig(property) {
        const poi = this.getMyPoi()
        try {
            return poi.out(property).term.value
        } catch (err) {
            logger.debug('property not defined : ' + property)
            return rdf.literal('undefined')
        }
    }

    async deletePropertyFromMyConfig(predicate, value) {
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('DELETING FROM ' + s)
        const dataset = this.config
        dataset.delete(myConfigNode, predicate, value)
        this.config = dataset
    }






    async receive(message) {
        await this.enqueue(message)
    }






    async enqueue(message) {
        this.messageQueue.push({ message })
        if (!this.processing) {
            this.executeQueue()
        }
    }

    cloneContext(baseContext) {
        const message = structuredClone(baseContext)
        if (baseContext.dataset) {


            message.dataset = baseContext.dataset
        }
        return message
    }





    async executeQueue() {
        this.processing = true
        while (this.messageQueue.length > 0) {
            let { message } = this.messageQueue.shift()

            message = this.cloneContext(message)
            this.message = message




            this.addTag(message)

            await this.execute(message)
        }
        this.processing = false
    }

    addTag(message) {
        const tag = this.getTag()
        if (!message.tags) {
            message.tags = tag
            return
        }
        message.tags = message.tags + '.' + tag

    }

    getTag() {
        return footpath.urlLastPart(this.id)
    }







    async execute(message) {
        throw new Error('execute method not implemented')
    }







    async doEmit(message) {
        this.emit(message)
    }
}

export default Processor

================
File: src/processors/base/Processor.js
================
import logger from '../../utils/Logger.js'
import { EventEmitter } from 'events'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'





class Processor extends EventEmitter {





    constructor(config) {
        super()
        this.config = config
        this.messageQueue = []
        this.processing = false
        this.done = false
        this.outputs = []
    }

    preProcess(message) {
        return







        const processorPoi = rdf.grapoi({ dataset: this.config, term: this.configKey })
        logger.log('this.configKey = ' + this.configKey.value)
        logger.poi(processorPoi)

        logger.log('describe Desc')
        if (this.configKey.value === ns.trm.describe.value) {
            this.describe()
        }



    }

    describe() {
        logger.log('describe')
        const inputs = this.getInputKeys()
        const outputs = this.getOutputKeys()
        for (var input of inputs) {
            logger.log('input = ' + input)
            logger.log(this.message[input] + ' = ' + this.message[input])
        }
        for (var output of outputs) {
            logger.log('output = ' + output)
            logger.log(this.message[output] + ' = ' + this.message[output])
        }
    }





















    getMyConfigNode() {
        const dataset = this.config
        const configNode = grapoi({ dataset, term: this.configKey }).in()
        return configNode.term
    }

    getMyPoi() {
        const dataset = this.config
        const myConfigNode = this.getMyConfigNode()
        const poi = grapoi({ dataset: dataset, term: myConfigNode })
        return poi
    }

    async addPropertyToMyConfig(predicate, value) {
        logger.log('addPropertyToMyConfig predicate = ' + predicate)
        logger.log('addPropertyToMyConfig value = ' + value)
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('addPropertyToMyConfig  myConfigNode.value = ' + myConfigNode.value)
        const dataset = this.config
        dataset.add(myConfigNode, predicate, value)
        this.config = dataset
    }

    showMyConfig() {
        const poi = this.getMyPoi()
        logger.log('POI = ')
        logger.poi(poi)
    }

    getPropertyFromMyConfig(property) {
        const poi = this.getMyPoi()
        try {
            return poi.out(property).term.value
        } catch (err) {
            logger.debug('property not defined : ' + property)
            return rdf.literal('undefined')
        }
    }

    async deletePropertyFromMyConfig(predicate, value) {
        const myConfigNode = this.getMyConfigNode()
        const s = myConfigNode.value
        logger.log('DELETING FROM ' + s)
        const dataset = this.config
        dataset.delete(myConfigNode, predicate, value)
        this.config = dataset
    }






    async receive(message) {
        await this.enqueue(message)
    }






    async enqueue(message) {
        this.messageQueue.push({ message })
        if (!this.processing) {
            this.executeQueue()
        }
    }

    cloneContext(baseContext) {
        const message = structuredClone(baseContext)
        if (baseContext.dataset) {


            message.dataset = baseContext.dataset
        }
        return message
    }





    async executeQueue() {
        this.processing = true
        while (this.messageQueue.length > 0) {
            let { message } = this.messageQueue.shift()

            message = this.cloneContext(message)
            this.message = message




            this.addTag(message)

            await this.execute(message)
        }
        this.processing = false
    }

    addTag(message) {
        const tag = this.getTag()
        if (!message.tags) {
            message.tags = tag
            return
        }
        message.tags = message.tags + '.' + tag

    }

    getTag() {
        return footpath.urlLastPart(this.id)
    }







    async execute(message) {
        throw new Error('execute method not implemented')
    }







    async doEmit(message) {
        this.emit(message)
    }

















    emit(event, message) {
        if (event === 'message') {
            this.outputs.push(message)
        }
        super.emit(event, message)


    }

    getOutputs() {
        const results = this.outputs
        this.outputs = []
        return results
    }
}

export default Processor

================
File: src/processors/base/ProcessProcessor.js
================
import logger from '../../utils/Logger.js'
import Processor from './Processor.js'

class ProcessProcessor extends Processor {
    constructor(config) {
        super(config)
    }


    async execute(message) {
        this.emit('message', message)
    }
}

export default ProcessProcessor

================
File: src/processors/base/SinkProcessor.js
================
import Processor from './Processor.js'

class SinkProcessor extends Processor {
    constructor(config) {
        super(config)
    }

    async execute(message) {

    }
}

export default SinkProcessor

================
File: src/processors/base/SourceProcessor.js
================
import Processor from './Processor.js'

class SourceProcessor extends Processor {
    constructor(config) {
        super(config);
    }







}

export default SourceProcessor

================
File: src/processors/flow/FlowProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import ForEach from './ForEach.js'

class FlowProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.ForEach)) {
            logger.debug('FlowProcessorsFactory: Creating ForEach processor')
            return new ForEach(config)
        }
        logger.debug('FlowProcessorsFactory: Unknown processor type')
        return false
    }
}

export default FlowProcessorsFactory

================
File: src/processors/flow/ForEach.js
================
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class ForEach extends ProcessProcessor {
    constructor(config) {
        super(config)
    }

    async execute(message) {
        logger.setLogLevel('debug')
        logger.debug('ForEach execute method called')

        if (!message.foreach || !Array.isArray(message.foreach)) {
            logger.error('ForEach: Invalid or missing foreach array in message')
            message.foreach = ["testing-testing", "one", "two", "three"]

        }

        for (const item of message.foreach) {
            const clonedMessage = structuredClone(message)
            clonedMessage.currentItem = item
            delete clonedMessage.foreach

            logger.debug(`ForEach: Emitting message for item: ${item}`)
            this.emit('message', clonedMessage)
        }

        logger.debug('ForEach: Finished processing all items')
    }
}

export default ForEach

================
File: src/processors/fs/DirWalker.js
================
import { readdir } from 'fs/promises'
import { join, extname } from 'path'
import grapoi from 'grapoi'
import logger from '../../utils/Logger.js'
import SourceProcessor from '../base/SourceProcessor.js'
















class DirWalker extends SourceProcessor {





    constructor(config) {
        super(config)
        const dataset = this.config
        const poi = grapoi({ dataset })
        this.includeExtensions = ['.md']
        this.excludePrefixes = ['_']
    }






    async execute(message) {
        logger.setLogLevel('info')
        logger.debug('DirWalker.execute')
        await this.emitThem(message)



        message.done = true

        this.emit('message', message)
    }

    async emitThem(message) {
        message.counter = 0
        message.slugs = []
        message.done = false

        const dirPath = join(message.rootDir, message.sourceDir)
        logger.debug('DirWalker, dirPath = ' + dirPath)


        const entries = await readdir(dirPath, { withFileTypes: true })
        for (const entry of entries) {
            const fullPath = join(dirPath, entry.name)
            if (entry.isDirectory()) {
                await this.execute(entry.name, message)
            } else {
                logger.debug('DirWalker, entry.name = ' + entry.name)

                const includeExtension = this.includeExtensions.includes(extname(entry.name))
                const prefix = entry.name.charAt(0)
                const excludePrefix = this.excludePrefixes.includes(prefix)


                if (!excludePrefix && includeExtension) {

                    message.filename = entry.name
                    message.filepath = message.sourceDir + '/' + entry.name
                    const slug = this.extractSlug(message.filename)
                    message.slugs.push(slug)





                    message.done = false
                    message.counter = message.counter + 1
                    const messageClone = structuredClone(message)
                    this.emit('message', messageClone)
                }
            }
        }




    }

    extractSlug(filepath) {
        var slug = filepath
        if (slug.includes('.')) {
            slug = slug.substr(0, slug.lastIndexOf("."))
        }
        return slug
    }
}
export default DirWalker

================
File: src/processors/fs/FileCopy.js
================
import { copyFile, mkdir, readdir, stat } from 'node:fs/promises'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import Processor from '../base/Processor.js'


class FileCopy extends Processor {
    constructor(config) {
        super(config)
    }





    async execute(message) {



        if (message.rootDir == "") {
            message.rootDir = message.applicationRootDir
        }
        logger.debug("message.rootDir = " + message.rootDir)
        var source, destination


        if (this.configKey === 'undefined') {
            logger.debug('FileCopy: using message.source and message.destination')
            source = message.source
            destination = message.destination
        } else {
            logger.debug(`FileCopy: using configKey ${this.configKey.value}`)
            source = this.getPropertyFromMyConfig(ns.trm.source)
            destination = this.getPropertyFromMyConfig(ns.trm.destination)
            source = path.join(message.rootDir, source)
            destination = path.join(message.rootDir, destination)
        }

        logger.debug(`Source: ${source}`)
        logger.debug(`Destination: ${destination}`)

        try {
            await this.ensureDirectoryExists(path.dirname(destination))
            const sourceStat = await stat(source)

            if (sourceStat.isFile()) {
                logger.debug(`Copying file from ${source} to ${destination}`)
                await copyFile(source, destination)
            } else if (sourceStat.isDirectory()) {
                logger.debug(`Copying directory from ${source} to ${destination}`)
                await this.copyDirectory(source, destination)
            }
        } catch (err) {
            logger.error(`Error in FileCopy: ${err.message}`)
            logger.error(`Source: ${source}`)
            logger.error(`Destination: ${destination}`)
        }

        this.emit('message', message)
    }





    async ensureDirectoryExists(dirPath) {
        logger.debug(`Ensuring directory exists: ${dirPath}`)
        try {
            await mkdir(dirPath, { recursive: true })
            logger.debug(`Directory created/ensured: ${dirPath}`)
        } catch (err) {
            logger.debug(`Error creating directory ${dirPath}: ${err.message}`)
            throw err
        }
    }






    async copyDirectory(source, destination) {
        logger.debug(`Copying directory: ${source} to ${destination}`)
        try {
            await this.ensureDirectoryExists(destination)
            const entries = await readdir(source, { withFileTypes: true })

            for (const entry of entries) {
                const srcPath = path.join(source, entry.name)
                const destPath = path.join(destination, entry.name)

                logger.debug(`Processing: ${srcPath} to ${destPath}`)

                if (entry.isDirectory()) {
                    await this.copyDirectory(srcPath, destPath)
                } else {
                    await copyFile(srcPath, destPath)
                    logger.debug(`File copied: ${srcPath} to ${destPath}`)
                }
            }
        } catch (err) {
            logger.debug(`Error in copyDirectory: ${err.message}`)
            throw err
        }
    }
}

export default FileCopy

================
File: src/processors/fs/FileReader.js
================
import { readFile } from 'node:fs/promises'
import path from 'path'
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'
import SourceProcessor from '../base/SourceProcessor.js'











class FileReader extends SourceProcessor {





    constructor(config) {
        super(config)
    }






    async execute(message) {
        this.preProcess(message)

        var filepath = message.filepath

        if (!filepath) {
            logger.debug(`FileReader: using configKey ${this.configKey.value}`)
            filepath = this.getPropertyFromMyConfig(ns.trm.messageFile)


            logger.log(' - filepath from config : ' + filepath)
        }
        logger.log(' - FileReader reading filepath : ' + filepath)


        var f = path.join(message.dataDir, filepath)
        if (message.rootDir) {
            f = path.join(message.rootDir, filepath)
        }

        const mediaType = this.getPropertyFromMyConfig(ns.trm.mediaType)
        logger.debug('in FileReader, mediaType = ' + mediaType)


        try {
            message.content = (await readFile(f)).toString()

            if (mediaType === 'application/json') {
                message.fromfile = JSON.parse(message.content)
            }
            this.emit('message', message)
        } catch (err) {
            logger.error("FileReader.execute error : " + err.message)
        }
    }
}

export default FileReader

================
File: src/processors/fs/FileRemove.js
================
import { unlink, readdir, stat, rm } from 'node:fs/promises'
import path from 'path'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class FileRemove extends Processor {
    constructor(config) {
        super(config)
    }





    async execute(message) {

        logger.setLogLevel('info')

        this.ignoreDotfiles = true

        var target




        if (this.configKey === 'undefined') {
            logger.debug('FileRemove no configKey from transmission, using message.target')
            target = message.target
        } else {
            logger.debug('FileRemove this.configKey = ' + this.configKey.value)
            target = this.getPropertyFromMyConfig(ns.trm.target)
            target = path.join(message.rootDir, target)
        }

        logger.debug('FileRemove, target = ' + target)
        try {
            const removeStat = await stat(target)

            if (removeStat.isFile()) {
                await this.removeFile(target)
            } else if (removeStat.isDirectory()) {
                await this.removeDirectoryContents(target)
            }
        } catch (err) {

            logger.debug('FileRemove, target stat caused err : ' + target)
        }

        this.emit('message', message)
    }





    async removeFile(filePath) {
        await unlink(filePath)
    }





    async removeDirectoryContents(dirPath) {
        logger.debug('FileRemove, dirPath = ' + dirPath)
        const entries = await readdir(dirPath, { withFileTypes: true })

        for (const entry of entries) {
            if (this.ignoreDotfiles && (entry.name.charAt(0) === ".")) {
                continue
            }
            const entryPath = path.join(dirPath, entry.name)

            if (entry.isDirectory()) {
                await this.removeDirectoryContents(entryPath)
            } else {
                await unlink(entryPath)
            }
        }
    }
}

export default FileRemove

================
File: src/processors/fs/FileWriter.js
================
import { writeFile } from 'node:fs/promises'
import { dirname, join } from 'node:path'
import { mkdir, mkdirSync } from 'node:fs'
import logger from '../../utils/Logger.js'
import SinkProcessor from '../base/SinkProcessor.js'














class FileWriter extends SinkProcessor {





    constructor(config) {
        super(config)
    }

    getInputKeys() {
        return ['filepath, content']
    }




    async execute(message) {
        this.preProcess()
        var filepath = message.filepath

        const content = message.content




        logger.debug("Filewriter.targetFile = " + filepath)

        const dirName = dirname(filepath)
        try {
            await this.mkdirs(dirName)
            logger.log(' - FileWriter writing : ' + filepath)
            await writeFile(filepath, content)

        } catch (err) {
            logger.error("FileWriter.execute error : " + err.message)
        }

        this.emit('message', message)
    }

    async mkdirs(dir) {
        if (!dir) return;
        try {
            mkdir(dir, { recursive: true }, (error) => { })
        } catch (error) {
            console.error(error);
        }
    }
}

export default FileWriter

================
File: src/processors/fs/FsProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import DirWalker from './DirWalker.js'
import FileReader from './FileReader.js'
import FileWriter from './FileWriter.js'
import FileCopy from './FileCopy.js'
import FileRemove from './FileRemove.js'

class FsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.DirWalker)) {
            return new DirWalker(config)
        }
        if (type.equals(ns.t.FileReader)) {
            return new FileReader(config)
        }
        if (type.equals(ns.t.FileWriter)) {
            return new FileWriter(config)
        }
        if (type.equals(ns.t.FileCopy)) {
            return new FileCopy(config)
        }
        if (type.equals(ns.t.FileRemove)) {
            return new FileRemove(config)
        }
        return false
    }
}

export default FsProcessorsFactory

================
File: src/processors/markup/LinkFinder.js
================
import * as cheerio from 'cheerio'

import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class LinkFinder extends ProcessProcessor {

    async execute(message) {

        await this.extractLinks(message)

        if (data === '~~done~~') {
            logger.log('LF DONE*****************')
            this.emitLocal('message', '~~done~~', message)
            return
        }
    }


    relocate(filename, extension) {
        const split = filename.split('.').slice(0, -1)
        return split.join('.') + extension
    }

    async extractLinks(htmlContent, message) {

        const $ = cheerio.load(htmlContent)
        let label = ''

        $('a, h1, h2, h3, h4, h5, h6').each((_, element) => {
            const tagName = element.tagName.toLowerCase()
            if (tagName.startsWith('h')) {
                const level = tagName.substring(1)
                const headerText = $(element).text()
                label = `\n\n${'#'.repeat(parseInt(level))} ${headerText}\n`;
            } else if (tagName === 'a') {
                const linkText = $(element).text()

                let href = $(element).attr('href')

                if (!href || href.startsWith('#')) return

                if (href && !href.includes('://')) {

                    const baseURL = message.sourceURL

                    href = new URL(href, baseURL).toString();
                }
                label = `\n[${linkText}](${href})`

            }
            message.label = label
            this.emit('message', message)
        })
    }
}

export default LinkFinder

================
File: src/processors/markup/MarkdownToHTML.js
================
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

import { marked } from 'marked'


import markedFootnote from 'marked-footnote'
import markedCodeFormat from 'marked-code-format'


class MarkdownToHTML extends ProcessProcessor {


    async execute(message) {
        const input = message.content


        message.content = await
            marked

                .use(markedFootnote())
                .use(
                    markedCodeFormat({

                    })
                )
                .parse(input.toString())

        this.emit('message', message)
    }
}

export default MarkdownToHTML

================
File: src/processors/markup/MarkupProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import MetadataExtractor from './MetadataExtractor.js'
import LinkFinder from './LinkFinder.js'
import MarkdownToHTML from './MarkdownToHTML.js'

class MarkupProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.MetadataExtractor)) {
            return new MetadataExtractor(config)
        }
        if (type.equals(ns.t.MarkdownToHTML)) {
            return new MarkdownToHTML(config)
        }
        if (type.equals(ns.t.LinkFinder)) {
            return new LinkFinder(config)
        }
        return false
    }
}

export default MarkupProcessorsFactory

================
File: src/processors/markup/MetadataExtractor.js
================
import * as cheerio from 'cheerio'

import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class MetadataExtractor extends ProcessProcessor {

    async execute(message) {
        const filename = data.filename
        const content = data.content

        logger.debug("MetadataExtractor input file : " + filename)
        const targetFilename = this.relocate(filename)
        logger.debug("MetadataExtractor outputfile : " + targetFilename)

        const jsonData = this.convertEmailToJSON(content)

        const jsonString = JSON.stringify(jsonData)

        const output = { filename: targetFilename, content: jsonString }

        this.emit('message', output, message)
    }

    relocate(filename) {

        const split = filename.split('.').slice(0, -1)
        var newFileName = split.join('.') + '.json'
        return newFileName
    }

    convertEmailToJSON(htmlContent) {
        const $ = cheerio.load(htmlContent);
        var subjectLine = $('H1').text().trim()
        var fromName = $('B').first().text().trim()
        var nextMessageLink = $('LINK[REL="Next"]').attr('HREF')
        var previousMessageLink = $('LINK[REL="Previous"]').attr('HREF')
        var messageText = $('PRE').text().trim()
        messageText = this.pruneContent(messageText)
        const jsonResult = {
            subjectLine: subjectLine,
            fromName: fromName,
            nextMessageLink: nextMessageLink,
            previousMessageLink: previousMessageLink,
            messageText: messageText

        }























        return jsonResult;
    }

    pruneContent(content) {

        const regex1 = /(^|\n).*?:\n>/s
        content = content.replace(regex1, '$1')

        const regex2 = /\n>.*?\n/g;




        content = content.replace(regex2, '\n')

        return content
    }
}



export default MetadataExtractor

================
File: src/processors/postcraft/EntryContentToPagePrep.js
================
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class EntryContentToPagePrep extends ProcessProcessor {

  constructor(config) {
    super(config)
  }

  async execute(message) {
    if (message.done) {
      this.emit('message', message)
      return
    }

    message.templateFilename = message.rootDir + '/' + message.entryContentToPage.templateFilename

    message.template = false

    message.contentBlocks.content = message.content

    message.filepath = message.rootDir + '/' + message.entryContentToPage.targetDir + '/' + message.slug + '.html'



    this.emit('message', message)
  }

}

export default EntryContentToPagePrep

================
File: src/processors/postcraft/FrontPagePrep copy.js
================
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'
import { readFile } from 'node:fs/promises'

class FrontPagePrep extends ProcessProcessor {

  constructor(config) {
    super(config)
  }

  async execute(message) {
    logger.setLogLevel('debug')
    try {

      message.templateFilename = message.rootDir + '/' + message.indexPage.templateFilename

      logger.debug('Template = ' + message.templateFilename)
      const rawEntryPaths = this.resolveRawEntryPaths(message)
      message.content = ''
      // TODO move this out to template and/or separate processors
      //   for (var f of rawEntryPaths) {
      //   const n = rawEntryPaths.length
      const entryCount = 5
      //    for (var i = entryCount - 1; i >= 0; i--) {
      for (var i = 0; i < entryCount; i++) {
        const rawEntryPath = rawEntryPaths.pop()
        logger.debug('rawEntryPath = ' + rawEntryPath)
        message.content += (await readFile(rawEntryPath)).toString()
      }

      // needed?
      message.contentBlocks.content = message.content
      //  "indexPage": {
      //  "filepath": "public/blog/index.html",
      message.filepath = message.rootDir + '/' + message.indexPage.filepath
      // message.rootDir + '/' + message.entryContentToPage.targetDir + '/' + message.slug + '.html'


      this.emit('message', message)
    } catch (err) {
      logger.error('Error in FrontPagePrep')
      logger.error(err)
    }
  }

  resolveRawEntryPaths(message) {
    var paths = []


    const slugs = message.slugs
    const entryCount = slugs.length
    var path
    for (let i = 0; i < entryCount; i++) {
      var path = slugs[i]

      if (!path) break

      path = message.rootDir + '/' + message.entryContentMeta.targetDir + '/' + path + '.html'
      paths.push(path)

    }
    return paths
  }
}
export default FrontPagePrep

================
File: src/processors/postcraft/FrontPagePrep.js
================
import path from 'path'
import { readFile } from 'node:fs/promises'

import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'


class FrontPagePrep extends ProcessProcessor {
  constructor(config) {
    super(config)
  }


  async execute(message) {

    try {
      message.templateFilename = message.rootDir + '/' + message.indexPage.templateFilename
      logger.debug('Template = ' + message.templateFilename)

      const rawEntryPaths = this.resolveRawEntryPaths(message)
      message.content = ''

      // TODO tidy up
      const entryCount = Math.min(5, rawEntryPaths.length) // Limit to 5 entries or less
      logger.debug('FrontPagePrep, entryCount = ' + entryCount)

      const rangeStart = rawEntryPaths.length - entryCount
      const rangeEnd = rawEntryPaths.length - 1
      //     for (let i = 0; i < entryCount; i++) {
      for (let i = rangeEnd; i >= rangeStart; i--) {
        logger.debug('FrontPagePrep, i = ' + entryCount)
        const rawEntryPath = rawEntryPaths[i]
        if (rawEntryPath) {
          message.content += await readFile(rawEntryPath, 'utf8')
        } else {
          logger.warn(`Skipping undefined entry path at index ${i}`)
        }
      }

      message.contentBlocks.content = message.content

      message.filepath = message.rootDir + '/' + message.indexPage.filepath

      this.emit('message', message)
    } catch (err) {
      logger.error('Error in FrontPagePrep')
      logger.error(err)
    }
  }

  resolveRawEntryPaths(message) {
    const paths = []
    const slugs = message.slugs || []
    const entryCount = slugs.length

    for (let i = 0; i < entryCount; i++) {
      const slug = slugs[i]
      if (slug) {

        const filePath = path.join(message.rootDir, message.entryContentMeta.targetDir, slug + '.html')
        paths.push(filePath)
      }
    }

    return paths
  }
}

export default FrontPagePrep

================
File: src/processors/postcraft/PostcraftDispatcher.js
================
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'











class PostcraftDispatcher extends ProcessProcessor {





  constructor(config) {
    super(config)
  }






  async execute(message) {
    const postcraftConfig = message.dataset
    message.template = data.toString()
    logger.log('PostcraftDispatcherPostcraftDispatcherPostcraftDispatcher ' + data)
    const poi = grapoi({ dataset: postcraftConfig })

    for (const q of poi.out(ns.rdf.type).quads()) {
      if (q.object.equals(ns.pc.ContentGroup)) {
        await this.processContentGroup(message, q.subject)
      }
    }
  }






  async processContentGroup(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })
    const sourceDir = groupPoi.out(ns.fs.sourceDirectory).term.value
    const targetDir = groupPoi.out(ns.fs.targetDirectory).term.value
    const templateFilename = groupPoi.out(ns.pc.template).term.value





    message.sourceDir = sourceDir
    message.targetDir = targetDir
    message.templateFilename = templateFilename
    message.loadContext = 'template'

    this.emit('message', sourceDir, message)
  }
}

export default PostcraftDispatcher

================
File: src/processors/postcraft/PostcraftPrep.js
================
import path from 'path'
import logger from '../../utils/Logger.js';

import ProcessProcessor from '../base/ProcessProcessor.js'

class PostcraftPrep extends ProcessProcessor {

  constructor(config) {
    super(config)
  }

  async execute(message) {





    message.slug = this.extractSlug(message)
    message.targetFilename = this.extractTargetFilename(message)


    message.contentBlocks = {}



    message.contentBlocks.relURL = this.extractRelURL(message)



    message.contentBlocks.link = 'entries/' + message.contentBlocks.relURL



    message.contentBlocks.title = this.extractTitle(message)

    const { created, updated } = this.extractDates(message)
    message.contentBlocks.created = created
    message.contentBlocks.updated = updated




    this.emit('message', message)
  }


  extractSlug(message) {

    var slug = message.filename

    if (slug.includes('.')) {
      slug = slug.substr(0, slug.lastIndexOf("."))
    }
    return slug
  }








  extractTargetFilename(message) {
    return path.join(message.rootDir, message.entryContentMeta.targetDir, this.extractSlug(message) + '.html')
  }

  extractRelURL(message) {

    return this.extractSlug(message) + '.html'




  }

  extractDates(message) {
    const today = (new Date()).toISOString().split('T')[0]
    const dates = { created: today, updated: today }


    const nonExt = message.filename.split('.').slice(0, -1).join()
    const shreds = nonExt.split('_')
    if (Date.parse(shreds[0])) {
      dates.created = shreds[0]
    }
    return dates
  }




  extractTitle(message) {
    let title = 'Title'
    let match = message.content.toString().match(/^#(.*)$/m)
    let contentTitle = match ? match[1].trim() : null
    if (contentTitle) {
      title = contentTitle.replaceAll('#', '') // TODO make nicer
      return title
    }

    // derive from filename
    // eg. 2024-04-19_hello-postcraft.md
    try {
      const nonExt = message.filename.split('.').slice(0, -1).join()
      const shreds = nonExt.split('_')


      title = shreds[1].split('-')
        .map(word => word.charAt(0).toUpperCase() + word.slice(1))
        .join(' ');
    } catch (err) {
      title = message.filename
    }
    return title
  }
}

export default PostcraftPrep

================
File: src/processors/postcraft/PostcraftProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import PostcraftDispatcher from './PostcraftDispatcher.js'
import PostcraftPrep from './PostcraftPrep.js'
import EntryContentToPagePrep from './EntryContentToPagePrep.js'
import FrontPagePrep from './FrontPagePrep.js'

class PostcraftProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.PostcraftDispatcher)) {
            return new PostcraftDispatcher(config)
        }
        if (type.equals(ns.t.PostcraftPrep)) {
            return new PostcraftPrep(config)
        }
        if (type.equals(ns.t.EntryContentToPagePrep)) {
            return new EntryContentToPagePrep(config)
        }
        if (type.equals(ns.t.FrontPagePrep)) {
            return new FrontPagePrep(config)
        }

        return false
    }
}

export default PostcraftProcessorsFactory

================
File: src/processors/protocols/HttpGet.js
================
import axios from 'axios'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'

import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class HttpGet extends ProcessProcessor {

    constructor(config) {
        super(config)
    }

    async execute(url, message) {
        if (url === '~~done~~') {
            logger.log('HG DONE*****************')
            this.emit('message', url, message)
            return
        }
        try {
            logger.log('HG GETTING*****************')
            const response = await axios.get(url)
            const content = response.data

            message.sourceURL = url
            this.emit('message', content, message)
        } catch (error) {
            logger.error("HttpGet.execute error\n" + error)
        }
    }
}

export default HttpGet

================
File: src/processors/protocols/ProtocolsProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import HttpGet from './HttpGet.js'



class ProtocolsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.HttpGet)) {
            return new HttpGet(config)
        }

        return false
    }
}

export default ProtocolsProcessorsFactory

================
File: src/processors/rdf/ConfigMap.js
================
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class ConfigMap extends ProcessProcessor {
  constructor(config) {
    super(config)
  }






  async execute(message) {
    logger.setLogLevel('debug')

    logger.debug(`ConfigMap, Using configKey ${this.configKey.value}`)

    const group = this.getPropertyFromMyConfig(ns.trm.group)
    const targetGroup = rdf.namedNode(group)
    logger.debug(`ConfigMap, group =  ${targetGroup}`)



    this.preProcess(message)
    const dataset = message.dataset
    const poi = grapoi({ dataset, factory: rdf })
    const quads = await poi.out(ns.rdf.type).quads()

    for (const q of quads) {
      const type = q.object


      if (type.equals(targetGroup)) {

        await this.processContentGroup(message, q.subject)
      }
    }

    this.emit('message', message)
  }






  async processContentGroup(message, contentGroupID) {
    logger.debug('contentGroupID = ' + contentGroupID.value)
    switch (contentGroupID.value) {
      case ns.t.PostContent.value:
        await this.markdownToEntryContent(message, contentGroupID)
        break
      case ns.t.PostPages.value:
        await this.entryContentToPostPage(message, contentGroupID)
        break
      case ns.t.IndexPage.value:
        await this.indexPage(message, contentGroupID)
        break
      default:
        logger.log('Group not found in dataset: ' + contentGroupID.value)
    }
  }






  async markdownToEntryContent(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })



    message.filepath = groupPoi.out(ns.pc.template).term.value
    message.template = '§§§ placeholer for debugging §§§'

    message.entryContentMeta = {
      sourceDir: groupPoi.out(ns.fs.sourceDirectory).term.value,
      targetDir: groupPoi.out(ns.fs.targetDirectory).term.value,
      templateFilename: groupPoi.out(ns.pc.template).term.value
    }
  }






  async entryContentToPostPage(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })

    message.entryContentToPage = {
      targetDir: groupPoi.out(ns.fs.targetDirectory).term.value,
      templateFilename: groupPoi.out(ns.pc.template).term.value
    }
  }






  async indexPage(message, contentGroupID) {
    const postcraftConfig = message.dataset
    const groupPoi = rdf.grapoi({ dataset: postcraftConfig, term: contentGroupID })

    message.indexPage = {
      filepath: groupPoi.out(ns.fs.filepath).term.value,
      templateFilename: groupPoi.out(ns.pc.template).term.value
    }
  }
}

export default ConfigMap

================
File: src/processors/rdf/DatasetReader.js
================
import rdf from 'rdf-ext'
import { fromFile } from 'rdf-utils-fs'
import SourceProcessor from '../base/SourceProcessor.js'
import logger from '../../utils/Logger.js'

class DatasetReader extends SourceProcessor {
    constructor(config) {
        super(config)
    }





    getInputKeys() {
        return ['sdfsdf']
    }





    getOutputKeys() {
        return ['sdfsdfsdfdataset']
    }





    async execute(message) {
        this.preProcess(message)
        var datasetName = 'dataset'
        var datasetFilename = message.rootDir + '/manifest.ttl'
        if (message.datasetFilename) {
            datasetFilename = message.datasetFilename
        }
        if (message.datasetName) {
            datasetName = message.datasetName
        }

        const stream = fromFile(datasetFilename)


        message[datasetName] = await rdf.dataset().import(stream)
        this.emit('message', message)
    }
}

export default DatasetReader

================
File: src/processors/rdf/RDFProcessorsFactory.js
================
import ns from '../../utils/ns.js'
import DatasetReader from './DatasetReader.js'
import ConfigMap from './ConfigMap.js'

class RDFProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.DatasetReader)) {
            return new DatasetReader(config)
        }
        if (type.equals(ns.t.ConfigMap)) {
            return new ConfigMap(config)
        }
        return false
    }
}

export default RDFProcessorsFactory

================
File: src/processors/system/EnvLoader.js
================
import 'dotenv/config'



import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'














class EnvLoader extends Processor {





    constructor(config) {
        super(config)
    }





    async execute(message) {
        logger.setLogLevel('debug')


        this.config.whiteboard.env = process.env



        return super.handle(message)

    } catch(err) {
        logger.error("EnvLoader.execute error : " + err.message)
    }
}

export default EnvLoader

================
File: src/processors/system/SystemProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'


import EnvLoader from './EnvLoader.js'



class SystemsProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.EnvLoader)) {
            return new EnvLoader(config)
        }
        return false
    }
}
export default SystemsProcessorsFactory

================
File: src/processors/templates/ProcessorTemplate.js
================
import { readFile } from 'node:fs/promises'

import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'









class ProcessorExample extends Processor {





    constructor(config) {
        super(config)
    }





    async execute(message) {
        logger.setLogLevel('debug')


        this.emit('message', message)
    } catch(err) {
        logger.error("ProcessorExample.execute error : " + err.message)
    }
}

export default ProcessorExample

================
File: src/processors/templates/TemplateProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import Processors from './ProcessorsTemplate.js'




class TemplateProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.ProcessorsTemplate)) {
            return new ProcessorsTemplate(config)
        }
        return false
    }
}
export default TemplateProcessorsFactory

================
File: src/processors/test/AppendProcess.js
================
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class AppendProcess extends ProcessProcessor {


    async execute(message) {
        logger.debug("AppendProcess data : " + message.content)
        message.content = message.content + " world"
        this.emit('message', message)
    }
}

export default AppendProcess

================
File: src/processors/test/FileSink.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import { writeFile } from 'node:fs/promises';
import footpath from '../../utils/footpath.js'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import logger from '../../utils/Logger.js'
import SinkProcessor from '../base/SinkProcessor.js'

class FileSink extends SinkProcessor {

    constructor(config) {
        super(config)
        const dataset = this.config
        const poi = grapoi({ dataset })
        this.destinationFile = poi.out(ns.trm.destinationFile).value
    }


    async execute(message) {
        const toRootDir = '../../../'
        const dataDir = path.join(toRootDir, message.dataDir)
        const df = footpath.resolve(import.meta.url, dataDir, this.destinationFile)
        logger.debug("FileSink to = " + df)
        await writeFile(df, message.content)
        this.emit('message', message)
    }
}

export default FileSink

================
File: src/processors/test/FileSource.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import { readFile } from 'node:fs/promises'

import footpath from '../../utils/footpath.js'
import rdf from 'rdf-ext'

import grapoi from 'grapoi'
import ns from '../../utils/ns.js'

import logger from '../../utils/Logger.js'
import SourceProcessor from '../base/SourceProcessor.js'

class FileSource extends SourceProcessor {

    constructor(config) {
        super(config)
        const dataset = this.config
        const poi = grapoi({ dataset })
        this.sourceFile = poi.out(ns.trm.sourceFile).value
    }


    async execute(message) {
        try {
            const toRootDir = '../../../'
            const dataDir = toRootDir + message.dataDir
            const sf = footpath.resolve(import.meta.url, dataDir, this.sourceFile)
            logger.debug('FileSource file : ' + sf)
            const contents = await readFile(sf, { encoding: 'utf8' })
            logger.debug('FileSource data : ' + contents)
            this.emit('message', { content: contents, ...message })
        } catch (err) {
            logger.error("FileSource.execute error : " + err.message)
        }
    }
}

export default FileSource

================
File: src/processors/test/StringSink.js
================
import logger from '../../utils/Logger.js'
import SinkProcessor from '../base/SinkProcessor.js'

class StringSink extends SinkProcessor {

    execute(message) {
        logger.log("\n\nStringSink outputs : \"" + data + "\"\n\n")
    }
}

export default StringSink

================
File: src/processors/test/StringSource.js
================
import SourceProcessor from '../base/SourceProcessor.js'

class StringSource extends SourceProcessor {

    constructor(config) {
        super(config)
    }

    async execute(message) {
        console.log("message = " + message)
        console.log("data = " + data)
        this.emit('message', message)
    }
}

export default StringSource

================
File: src/processors/test/TestProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import StringSource from './StringSource.js'
import StringSink from './StringSink.js'
import AppendProcess from './AppendProcess.js'
import FileSource from './FileSource.js'
import FileSink from './FileSink.js'



class TestProcessorsFactory {
    static createProcessor(type, config) {



        if (type.equals(ns.t.StringSource)) {
            return new StringSource(config)
        }
        if (type.equals(ns.t.StringSink)) {
            return new StringSink(config)
        }
        if (type.equals(ns.t.AppendProcess)) {
            return new AppendProcess(config)
        }


        if (type.equals(ns.t.FileSource)) {
            return new FileSource(config)
        }
        if (type.equals(ns.t.FileSink)) {
            return new FileSink(config)
        }

        return false
    }
}

export default TestProcessorsFactory

================
File: src/processors/text/LineReader.js
================
import { readFile } from 'node:fs/promises'
import grapoi from 'grapoi'
import ns from '../../utils/ns.js'
import footpath from '../../utils/footpath.js'
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class LineReader extends ProcessProcessor {

    constructor(config) {
        super(config)
    }

    async execute(message) {

        const text = data.toString()









        const lines = text.split('\n')
        for await (let line of lines) {
            if (line.trim() && !line.startsWith('#')) {
                logger.debug('Line = [[[' + line + ']]]')
                this.emit('message', line, message)
            }
        }

        this.emit('message', '~~done~~', message)
    }
}

export default LineReader

================
File: src/processors/text/StringFilter.js
================
import logger from '../../utils/Logger.js';
import ProcessProcessor from '../base/ProcessProcessor.js';

class StringFilter extends ProcessProcessor {
    constructor(config) {
        super(config);
    }








    isAccepted(content, exclude, include) {

        if (content === undefined) {
            return false;
        }


        if ((!exclude || exclude.length === 0) && (!include || include.length === 0)) {
            return true;
        }


        if (this.isExcluded(content, exclude)) {
            return false;
        }


        if (include && include.length > 0) {
            return this.isIncluded(content, include);
        }


        return true;
    }








    isExcluded(content, exclude) {
        if (!exclude || exclude.length === 0) {
            return false;
        }
        const patterns = Array.isArray(exclude) ? exclude : [exclude];
        return patterns.some(pattern => this.matchPattern(content, pattern));
    }







    isIncluded(content, include) {
        if (!include || include.length === 0) {
            return true;
        }
        const patterns = Array.isArray(include) ? include : [include];
        return patterns.some(pattern => this.matchPattern(content, pattern));
    }







    matchPattern(content, pattern) {
        const regexPattern = pattern
            .replace(/\*/g, '.*')
            .replace(/\?/g, '.')
            .replace(/\[([^\]]+)\]/g, '[$1]');
        const regex = new RegExp(`^${regexPattern}$`);
        return regex.test(content);
    }

    async execute(message) {
        logger.debug('\nStringFilter Input : \nmessage.content = ' + message.content);
        logger.debug('message.exclude = ');
        logger.reveal(message.exclude);
        logger.debug('message.include = ');
        logger.reveal(message.include);

        const accepted = this.isAccepted(message.content, message.exclude, message.include);

        if (accepted) {
            logger.debug('\nOutput : \nmessage.content = ' + message.content);
            this.emit('message', message);
        } else {
            logger.debug('\nString filtered out');
        }
    }
}

export default StringFilter;

================
File: src/processors/text/StringMerger.js
================
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'


class StringMerger extends ProcessProcessor {
    constructor(config) {
        super(config)
        this.merged = ''
    }

    async execute(message) {
        logger.log('SMDATA*********************************\n' + data)

        if (data === '~~done~~') {
            logger.log('SM  DONE**********************************\n' + this.merged)
            this.emit('message', this.merged, message)
            return
        }
        this.merged = this.merged + data

    }
}

export default StringMerger

================
File: src/processors/text/StringReplace.js
================
import logger from '../../utils/Logger.js'
import ProcessProcessor from '../base/ProcessProcessor.js'

class StringReplace extends ProcessProcessor {
    constructor(config) {
        super(config)
    }





    async execute(message) {
        logger.debug('StringReplace input: ' + message.content)

        if (message.content && message.match && message.replace) {

            message.content = message.content.split(message.match).join(message.replace)
        } else {
            logger.warn('StringReplace: Missing required properties in message')
        }

        logger.debug('StringReplace output: ' + message.content)
        this.emit('message', message)
    }
}

export default StringReplace

================
File: src/processors/text/Templater.js
================
import ProcessProcessor from '../base/ProcessProcessor.js'
import nunjucks from 'nunjucks'
import logger from '../../utils/Logger.js'

class Templater extends ProcessProcessor {
    constructor(config) {
        super(config)
    }





    async execute(message) {

        if (message.templateFilename) {

            const path = message.templateFilename.substr(0, message.templateFilename.lastIndexOf("/"))
            const filename = message.templateFilename.substr(message.templateFilename.lastIndexOf("/") + 1)

            logger.debug('Templater, path = ' + path)
            logger.debug('Templater, filename = ' + filename)

            nunjucks.configure(path, { autoescape: false })


            message.content = nunjucks.render(filename, message.contentBlocks)



        } else {

            nunjucks.configure({ autoescape: false })


            message.content = nunjucks.renderString(message.template, message.contentBlocks)
        }

        this.emit('message', message)
    }
}
export default Templater

================
File: src/processors/text/TextProcessorsFactory.js
================
import ns from '../../utils/ns.js'

import LineReader from './LineReader.js'
import StringFilter from './StringFilter.js'
import StringMerger from './StringMerger.js'
import Templater from './Templater.js'

class TextProcessorsFactory {
    static createProcessor(type, config) {

        if (type.equals(ns.t.Templater)) {
            return new Templater(config)
        }
        if (type.equals(ns.t.LineReader)) {
            return new LineReader(config)
        }

        if (type.equals(ns.t.StringFilter)) {
            return new StringFilter(config)
        }

        if (type.equals(ns.t.StringMerger)) {
            return new StringMerger(config)
        }

        return false
    }
}

export default TextProcessorsFactory

================
File: src/processors/unsafe/chatgpt.md
================
User
What different ways of creating instances of classes in node js, including the esoteric ones?
ChatGPT
Creating instances of classes in Node.js primarily revolves around using the `new` operator with constructor functions or class definitions. However, there are several techniques, including some less commonly used (or esoteric) methods, to instantiate objects. Here's an overview:

### 1. Standard Instantiation with `new`

The most common method involves defining a class or constructor function and using the `new` keyword to create instances.

```javascript
class MyClass {
  constructor(name) {
    this.name = name;
  }
}

const instance = new MyClass("Example");
```

### 2. Factory Functions

Factory functions return a new object and can encapsulate the use of `new` internally or return object literals directly.

```javascript
function createInstance(name) {
  return new MyClass(name);
}
const instance = createInstance("Example");
```

### 3. Object.create()

`Object.create(proto[, propertiesObject])` creates a new object with the specified prototype object and properties.

```javascript
const prototype = {
  sayName() {
    console.log(this.name);
  },
};
const instance = Object.create(prototype);
instance.name = "Example";
```

### 4. Calling Constructor without `new` (Esoteric)

Using the constructor directly without `new` is not standard for creating instances and usually results in errors or unexpected behavior. However, some constructors might internally manage instantiation to allow this.

```javascript
function MyClass(name) {
  if (!(this instanceof MyClass)) {
    return new MyClass(name);
  }
  this.name = name;
}

const instance = MyClass("Example"); // Works due to internal handling
```

### 5. Clone Existing Instance

Creating a new instance by cloning an existing one, although not creating a fresh instance in the strictest sense, serves to produce a new object.

```javascript
const original = new MyClass("Original");
const clone = Object.assign(
  Object.create(Object.getPrototypeOf(original)),
  original
);
```

### 6. Deserialization

Creating an instance from a string representation, typically used in conjunction with serialization.

```javascript
const serialized = '{"name":"Example"}'; // Could be the result of JSON.stringify(instance)
const instance = Object.assign(new MyClass(), JSON.parse(serialized));
```

### 7. Function.prototype.bind()

You can set the `this` value in the constructor function, allowing for partial application or setting certain initial parameters ahead of time.

```javascript
function MyClass(name, message) {
  this.name = name;
  this.message = message;
}

const PreConfigured = MyClass.bind(null, "PreConfiguredName");
const instance = new PreConfigured("Hello World");
```

### 8. Proxy for Constructor (Esoteric)

Using a `Proxy` to intercept and customize the instantiation process.

```javascript
const MyClassProxy = new Proxy(MyClass, {
  construct(target, args) {
    console.log("MyClass instantiated with args:", args);
    return new target(...args);
  },
});

const instance = new MyClassProxy("Example");
```

These methods showcase the flexibility of object creation in JavaScript and Node.js, ranging from straightforward and common practices to more unusual and esoteric approaches. Each has its use cases, depending on the requirements for flexibility, pattern use, and object initialization specifics.
User
If all the object needs is a distinct name 'MyClass', the ability to set the value of a property 'config' in a constructor-like function where properties will be set, and a function execute(data) which returns a processed version of data, what is the minimum code necessary? Could these objects be create with something like a const instance = create('MyName') kind of function?
ChatGPT
For a minimal implementation that meets the specified requirements, you can use a factory function to create instances. Each instance needs a distinct name (`MyClass`), the ability to set the value of a `config` property in a constructor-like function, and an `execute(data)` method that returns a processed version of the data. Here's a concise way to accomplish this:

```javascript
function createInstance(name) {
  return {
    name: name,
    config: null,
    execute(data) {
      // Process and return the data
      // This example simply returns the data as-is; replace with actual processing logic
      return data;
    },
    setConfig(config) {
      this.config = config;
    },
  };
}

// Example usage:
const instance = createInstance("MyName");
instance.setConfig({ key: "value" }); // Setting the config
console.log(instance.execute("some data")); // Executing and processing data
```

This approach uses a simple object literal inside the `createInstance` factory function to define the necessary properties and methods. Each created instance will have its own `name`, `config`, and `execute` method, as per the requirements.

- `name` is assigned based on the argument passed to `createInstance`.
- `config` can be set using the `setConfig` method, which simulates constructor-like behavior for setting initial properties.
- `execute(data)` method is defined to process and return data. You can replace the placeholder logic with actual data processing as needed.

This solution provides a lightweight and flexible way to create instances with the desired functionality, following your example of a `create('MyName')` style function call.
User
Could the execute function be created from a string?
ChatGPT
Yes, the `execute` function can be created from a string using the `new Function()` constructor. This method allows you to dynamically create a function from a string of JavaScript code. Here's how you could modify the previous example to include an `execute` function created from a string:

```javascript
function createInstance(name, executeFunctionString) {
  return {
    name: name,
    config: null,
    execute: new Function("data", executeFunctionString),
    setConfig(config) {
      this.config = config;
    },
  };
}

// Example usage:
const executeCode = 'return data + " processed";'; // String of code for the execute function
const instance = createInstance("MyName", executeCode);
instance.setConfig({ key: "value" }); // Setting the config

console.log(instance.execute("some data")); // Outputs: "some data processed"
```

In this example, the `executeFunctionString` parameter is passed to `new Function('data', executeFunctionString)`, where `'data'` is the name of the argument that the function will receive, and `executeFunctionString` is the body of the function. This dynamically created function is then assigned to the `execute` property of the object returned by `createInstance`.

**Note of Caution:** Using `new Function()` to create functions from strings poses similar risks to `eval()` in terms of security and performance:

- **Security Risk:** If the string is derived from user input or an untrusted source, it could potentially introduce security vulnerabilities, such as code injection attacks.
- **Debugging Difficulty:** Debugging can be more challenging because the dynamically generated code may not be as readily traceable in debugging tools.
- **Performance:** The JavaScript engine can't optimize these functions as well as it can for statically declared functions.

Always validate and sanitize any input if you choose to use this approach, and consider whether the dynamic nature of function creation is necessary for your use case.
User
In RS6, can imports be done inside functions?
ChatGPT
In ES6 (ECMAScript 2015), the standard import syntax (`import ... from '...'`) cannot be used conditionally or inside functions, blocks, or any other statements that are not at the top level of a module. The ES6 `import` statements are static and must be at the top level of your JavaScript file or module, allowing for these imports to be statically analyzed by JavaScript engines and bundlers.

However, you can dynamically import modules using the `import()` function, which returns a promise. This feature is not part of ES6, but it was added later in the ES2020 (ECMAScript 11) specification. The `import()` function can be used inside functions, conditionals, loops, or anywhere you can execute a function. It is useful for lazy-loading modules only when they are needed, which can improve the performance of your application by reducing the initial load time.

Here's an example of how to use dynamic imports within a function:

```javascript
async function loadModuleDynamically() {
  const moduleName = "./myModule.js"; // The module path
  const module = await import(moduleName);
  module.myFunction(); // Use a function from the imported module
}

loadModuleDynamically().then(() => {
  console.log("Module loaded and function executed");
});
```

In this example, `import(moduleName)` dynamically imports the module specified by `moduleName`, and because `import()` returns a promise, you can use `await` within an async function to wait for the module to be loaded before using it.

This approach provides flexibility for loading modules based on conditions, user actions, or any logic that requires modules to be loaded dynamically at runtime.
User
How could you check at runtime to see if import() was supported?
ChatGPT
To check at runtime whether dynamic `import()` is supported, you can use a feature detection approach. This involves attempting to use `import()` with a data URI or a no-op (no operation) module and catching any errors if the feature is not supported. Since `import()` returns a promise, you can use a try-catch block within an asynchronous function to perform this check. Here's how you could implement it:

```javascript
async function supportsDynamicImport() {
  try {
    // Attempt to dynamically import a module using a no-op or data URI
    // For actual deployment, a more reliable check with a real module path may be preferable
    await new Function('return import("data:text/javascript;base64,Cg==")')();
    return true; // Dynamic import is supported
  } catch (error) {
    return false; // Dynamic import is not supported
  }
}

// Example usage
supportsDynamicImport().then((isSupported) => {
  if (isSupported) {
    console.log("Dynamic import is supported");
  } else {
    console.log("Dynamic import is not supported");
  }
});
```

This function works by dynamically creating a function that attempts to `import` a base64-encoded empty JavaScript module. If the browser or environment supports dynamic imports, this operation will succeed, and the function will return `true`. If not, the operation will throw an exception, and the function will return `false`.

Note: The use of `new Function()` allows this check to not cause a syntax error in environments that do not recognize `import()` syntax at all, as the `import()` call is within a string and not directly in the JavaScript code being parsed.

This method provides a straightforward way to determine the availability of dynamic imports, allowing you to conditionally implement fallback logic or alternative module loading strategies if necessary.
User
Can imports go in eval() strings?
ChatGPT
Using `import` statements directly within `eval()` strings in JavaScript does not work in the same way as using them at the top level of modules or through dynamic `import()` expressions. The static `import` statement is part of the module syntax and expected to be statically analyzable at the top level of module files, not dynamically executed within function bodies, `eval()`, or similar constructs.

However, dynamic imports using `import()` can be used within `eval()` because `import()` is a function that returns a promise. This allows for dynamically importing modules even in messages where static import statements are not syntactically valid. Here's an example:

```javascript
eval('import("./module.js").then(module => { console.log(module); })');
```

This uses `eval()` to dynamically execute a string that contains a dynamic import statement. Note that this approach does indeed work, but it combines two powerful and potentially risky features of JavaScript: `eval()` and dynamic imports. Both should be used cautiously because:

- `eval()` can execute arbitrary code, which poses significant security risks, especially with untrusted input. It's also generally slower and harder to debug compared to other methods of executing JavaScript.
- Dynamic imports can introduce complexity to your module loading strategy and potentially impact performance negatively if not used judyiciously.

Given these considerations, while you can use dynamic `import()` within `eval()` strings, it's essential to carefully assess whether this approach is necessary and ensure that the code being evaluated is from a trusted source to avoid security vulnerabilities.

================
File: src/processors/util/CaptureAll.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class CaptureAll extends Processor {

    constructor(config) {
        if (!config.whiteboard) {
            config.whiteboard = []
        }
        super(config)


        if (CaptureAll.singleInstance) {
            return CaptureAll.singleInstance
        }
        CaptureAll.singleInstance = this;
    }


    async execute(message) {
        logger.log('CaptureAll at (' + message.tags + ') ' + this.getTag())
        this.config.whiteboard.push(message)
        this.emit('message', message)
    }


}

export default CaptureAll

================
File: src/processors/util/DeadEnd.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class DeadEnd extends Processor {

    async execute(message) {

        logger.log('DeadEnd  at (' + message.tags + ') ' + this.getTag())


    }

}

export default DeadEnd

================
File: src/processors/util/Fork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'




class Fork extends Processor {

    constructor(config) {
        super(config)
    }

    async execute(message) {
        const nForks = message.nForks || 2

        logger.debug('forks = ' + nForks)

        for (let i = 0; i < nForks; i++) {
            var messageClone = structuredClone(message)
            messageClone.forkN = i
            logger.debug('--- emit --- ' + i)
            this.emit('message', messageClone)
        }

        message.done = true

        this.emit('message', message)
        return this.getOutputs()
    }

}

export default Fork

================
File: src/processors/util/Halt.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class Halt extends Processor {

    execute(message) {
        logger.log('\n************************************************************************')
        logger.log('*** << Thou Hast Summoned HALT, the Mighty Stopper of All Things  >> ***')
        logger.log('*** <<                   ~~~ ALL IS GOOD ~~~                      >> ***')
        logger.log('*** <<                     Have a nice day!                       >> ***')
        logger.log('************************************************************************\n')
        logger.log('*** Pipeline was : ' + message.tags)
        logger.log('*** Context now : ')
        logger.reveal(message)
        process.exit()
    }
}

export default Halt

================
File: src/processors/util/NOP.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class NOP extends Processor {

    constructor(config) {
        super(config);
    }

    async execute(message) {

        logger.log('NOP at (' + message.tags + ') ' + this.getTag())
        return this.emit('message', message)

    }
}
export default NOP

================
File: src/processors/util/RemapContext.js
================
import logger from '../../utils/Logger.js'
import rdf from 'rdf-ext'

import ns from '../../utils/ns.js'
import GrapoiHelpers from '../../utils/GrapoiHelpers.js'
import Processor from '../base/Processor.js'











class RemapContext extends Processor {




    constructor(config) {
        super(config)
    }





    async execute(message) {
        if (this.preProcess(message)) {
            return
        }

        logger.debug('RemapContext this.configKey = ' + this.configKey.value)

        const renames = GrapoiHelpers.listToArray(this.config, this.configKey, ns.trm.rename)
        const dataset = this.config

        for (let i = 0; i < renames.length; i++) {
            let rename = renames[i]
            let poi = rdf.grapoi({ dataset: dataset, term: rename })
            let pre = poi.out(ns.trm.pre).value
            let post = poi.out(ns.trm.post).value
            var value



            if (pre.includes('.')) {
                const spre = pre.split('.')
                logger.log('pre- split = ' + spre)
                value = message[spre[0]][spre[1]]
            } else {
                value = message[pre]
            }


            value = value.toString()

            if (post.includes('.')) {
                const s = post.split('.')
                logger.log('post split = ' + s)
                message[s[0]][s[1]] = value
            } else {
                message[post] = value
            }
            logger.log(' - Rename : ' + pre + ' to ' + post)
        }
        this.emit('message', message)
    }
}

export default RemapContext

================
File: src/processors/util/ShowConfig.js
================
import logger from '../../utils/Logger.js'
import SinkProcessor from '../base/SinkProcessor.js'

class ShowConfig extends SinkProcessor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async execute(message) {



        if (this.verbose) logger.log("\n***  Show Config ***")


        logger.log("***************************")
        logger.log("***  Config")
        logger.reveal(this.config)
        logger.log("***************************")




        this.emit('message', message)
    }
}

export default ShowConfig

================
File: src/processors/util/ShowMessage.js
================
import logger from '../../utils/Logger.js'
import SinkProcessor from '../base/SinkProcessor.js'

class ShowMessage extends SinkProcessor {

    constructor(config) {
        super(config)
        this.verbose = false
    }

    async execute(message) {



        if (this.verbose) logger.log("\n***  Show Message ***")

        logger.log("***************************")
        logger.log("***  Message")
        logger.reveal(message)
        logger.log("***************************")




        this.emit('message', message)
    }
}

export default ShowMessage

================
File: src/processors/util/ShowTransmission.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class ShowTransmission extends Processor {

    async execute(message) {
        logger.log(this.transmission.toString())
        this.emit('message', message)
    }
}

export default ShowTransmission

================
File: src/processors/util/Stash.js
================
import rdf from 'rdf-ext'
import { fromFile, toFile } from 'rdf-utils-fs'
import SourceProcessor from '../base/SourceProcessor.js'












class Stash extends SourceProcessor {





    constructor(config) {
        super(config)
    }






    async execute(message) {
        const manifestFilename = rootDir + '/manifest.ttl'
        const stream = fromFile(manifestFilename)


        message.rootDir = rootDir
        message.dataset = await rdf.dataset().import(stream)
        this.emit('message', message)
    }
}
export default Stash

================
File: src/processors/util/Unfork.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'
import ns from '../../utils/ns.js'
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import DeadEnd from './DeadEnd.js'




class Unfork extends Processor {

    constructor(config) {
        super(config)






    }

    async execute(message) {

        if (message.done) {
            logger.log(' - Unfork passing message')
            message.done = false
            this.emit('message', message)
        } else {
            logger.log(' - Unfork terminating pipe')
        }
    }

}

export default Unfork

================
File: src/processors/util/UtilProcessorsFactory.js
================
import logger from '../../utils/Logger.js'
import ns from '../../utils/ns.js'

import NOP from './NOP.js'
import DeadEnd from './DeadEnd.js'
import ShowMessage from './ShowMessage.js'
import Halt from './Halt.js'
import RemapContext from './RemapContext.js'
import Unfork from './Unfork.js'
import Fork from './Fork.js'
import ShowTransmission from './ShowTransmission.js'
import CaptureAll from './CaptureAll.js'
import ShowConfig from './ShowConfig.js'
import WhiteboardToMessage from './WhiteboardToMessage.js'

class UtilProcessorsFactory {
    static createProcessor(type, config) {
        if (type.equals(ns.t.NOP)) {
            return new NOP(config)
        }
        if (type.equals(ns.t.DeadEnd)) {
            return new DeadEnd(config)
        }
        if (type.equals(ns.t.ShowMessage)) {
            return new ShowMessage(config)
        }
        if (type.equals(ns.t.Halt)) {
            return new Halt(config)
        }
        if (type.equals(ns.t.RemapContext)) {
            return new RemapContext(config)
        }
        if (type.equals(ns.t.Fork)) {
            return new Fork(config)
        }
        if (type.equals(ns.t.Unfork)) {
            return new Unfork(config)
        }
        if (type.equals(ns.t.ShowTransmission)) {
            return new ShowTransmission(config)
        }
        if (type.equals(ns.t.CaptureAll)) {
            return new CaptureAll(config)
        }
        if (type.equals(ns.t.ShowConfig)) {
            return new ShowConfig(config)
        }
        if (type.equals(ns.t.WhiteboardToMessage)) {
            return new WhiteboardToMessage(config)
        }

        return false

    }
}

export default UtilProcessorsFactory

================
File: src/processors/util/WhiteboardToMessage.js
================
import logger from '../../utils/Logger.js'
import Processor from '../base/Processor.js'

class WhiteboardToMessage extends Processor {

    constructor(config) {
        super(config);
    }
    async execute(message) {

        logger.log('WhiteboardToMessage at (' + message.tags + ') ' + this.getTag())

        const originalArray = this.config.whiteboard

        message.whiteboard = Object.keys(originalArray).reduce((acc, key) => {
            const value = originalArray[key];
            if (value !== undefined && value !== null) {
                Object.keys(value).forEach((prop) => {
                    if (!acc[prop]) {
                        acc[prop] = [];
                    }
                    acc[prop].push(value[prop]);
                });
            }
            return acc;
        }, {});

        return super.handle(message)

    }
}

export default WhiteboardToMessage

================
File: src/processors/about.md
================
# Creating a new Processor

- update repopacks for `transmissions` and `trans-apps`
- create a new chat session in existing Project
- upload repopacks to Claude, with anything else that might be relevant (handover from previous session?)
- follow the prompt model as in `/home/danny/workspaces_hkms-desktop/postcrafts-raw/transmissions/prompts/github-list.md`
- remember additions to `xProcessorsFactory.js` and `transmissions/src/engine/AbstractProcessorFactory.js`

#:todo add comment creation
#:todo check simples & application suitability
#:todo create document creation workflow
#:todo create manifest.ttl creation
#:todo make crossrefs.md, crossrefs.ttl
#:todo create manifest.ttl consumption
#:todo add test creation
#:todo wire to an API, include file creation ops
#:todo add support in #:hyperdata-desktop

#:todo dedicated transmissions model, fine-tuned on relevant docs

#:todo extract todos as something like :

```turtle
<http://hyperdata.it/transmissions/src/processors/about/nid123> a pv:ToDoItem ;
dc:source <http://hyperdata.it/transmissions/src/processors/about.md> ;
pv:semtag "#:todo" ;
dc:line "3" ;
dc:title "tbd" ;
dc:content "extract todos as something like :" .
```

================
File: src/simples/env-loader/about.md
================
node src/apps-simple/env-loader/env-loader.js

from:

:envy a trm:Pipeline ;

# trm:pipe (:SC :s10 :s20 :SM) .

trm:pipe (:p10 :p20 :SC) .
:p10 a :EnvLoader .
:p20 a :WhiteboardToMessage .

================
File: src/simples/env-loader/env-loader.js
================
import logger from '../../utils/Logger.js'
import EnvLoader from '../../processors/system/EnvLoader.js'
import WhiteboardToMessage from '../../processors/util/WhiteboardToMessage.js'

logger.log('EnvLoader simple')

const config = { whiteboard: [] }

const p10 = new EnvLoader(config)
p10.id = 'http://purls.org/stuff/#p10'

const p20 = new WhiteboardToMessage(config)
p10.id = 'http://purls.org/stuff/#p20'

var message = {
    "dataDir": "src/applications/env-loader-test/data",
    "rootDir": "[no key]",
    "tags": "SM"
}

const x = 3

message = await p10.execute(message)

logger.log('p10 output ' + p10.getTag() + message)

message = await p20.execute(message)

logger.log('p20 output ')

logger.reveal(message)

================
File: src/simples/nop/nop copy.js
================
import logger from '../../utils/Logger.js'


import NOP from '../../processors/util/NOP.js'
logger.log('NOP simple')

const config = {
    "runmode": "functions",
    whiteboard: []
}

const p10 = new NOP(config)
p10.id = 'http://purls.org/stuff/#p10'


var message = {
    "test": "test string",
    "dataDir": "src/applications/env-loader-test/data",
    "rootDir": "[no key]",
    "tags": "SM"
}




message = await p10.execute(message)

logger.log('p10 output ' + message)

================
File: src/simples/nop/nop.js
================
import NOP from '../../processors/util/NOP.js'

const config = {
    "runmode": "functions",
    whiteboard: []
}

const nop = new NOP(config)

var message = { 'value': '42' }

message = await nop.execute(message)

console.log('value = ' + message.value)

================
File: src/simples/nop/simple-runner.js
================
import NOP from '../../processors/util/NOP.js'
import Fork from '../../processors/util/Fork.js'








async function main() {
    const config = {}
    const nop = new NOP(config)
    const fork = new Fork(config)

    var message = { 'value': '42' }



    var outputs = await nop.execute(message)
    console.log('NOP outputs:', outputs)


    message.nForks = 3
    outputs = await fork.execute(message)
    console.log('Fork outputs:', outputs)
}

main().catch(console.error)

================
File: src/utils/CommandUtils.js
================
import path from 'path'
import fs from 'fs/promises'

import logger from './Logger.js'
import TransmissionBuilder from '../engine/TransmissionBuilder.js'
import { ModuleLoader } from '../engine/ModuleLoader.js';


const defaultTransmissionsFilename = 'transmissions.ttl'
const defaultProcessorsConfigFile = 'processors-config.ttl'

class CommandUtils {


    static async run(appsDir, application, target, message = {}) {
        logger.setLogLevel('debug')


        logger.debug('\nCommandUtils.run()')

        logger.debug('CommandUtils, appsDir =' + appsDir)
        logger.debug('CommandUtils, application =' + application)
        logger.debug('CommandUtils, target =' + target)

        const appSplit = CommandUtils.splitName(application);
        const appName = appSplit.first;
        const subtask = appSplit.second;

        logger.debug('appName = ' + appName)
        logger.debug('subtask  = ' + subtask)

        const transmissionsDir = path.join(appsDir, appName);
        const modulePath = path.join(transmissionsDir, 'processors');

        logger.debug('transmissionsDir = ' + transmissionsDir)

        const moduleLoader = new ModuleLoader([modulePath]);

        const transmissionsFile = path.join(transmissionsDir, defaultTransmissionsFilename);
        const processorsConfigFile = path.join(transmissionsDir, defaultProcessorsConfigFile);



        logger.debug("transmissionConfigFile = " + transmissionsFile)
        logger.debug("processorsConfigFile = " + processorsConfigFile)



        const defaultDataDir = path.join(transmissionsDir, '/data')
        message = { "dataDir": defaultDataDir }
        message.rootDir = target




        const transmissions = await TransmissionBuilder.build(
            transmissionsFile,
            processorsConfigFile,
            moduleLoader
        );

        if (subtask) {
            for (var i = 0; i < transmissions.length; i++) {
                if (subtask === transmissions[i].label) {
                    transmissions[i].execute(message)
                }
            }
        }
        else {
            for (var i = 0; i < transmissions.length; i++) {
                transmissions[i].execute(message)
            }
        }
    }

    static async parseOrLoadContext(contextArg) {
        let message = ''
        try {
            message = JSON.parse(contextArg)
        } catch (err) {
            logger.log(err)
            // If it fails, assume it's a filename and try to load the file
            const filePath = path.resolve(contextArg);
            const fileContent = await fs.readFile(filePath, 'utf8');
            message = JSON.parse(fileContent)
            logger.log('message from file = ' + message)
        }
        return message

    }

    static async listSubdirectories(currentDirectory) {
        try {
            const entries = await fs.readdir(currentDirectory, { withFileTypes: true });

            const subdirChecks = entries.filter(dirent => dirent.isDirectory()).map(async (dirent) => {
                const subdirPath = path.join(currentDirectory, dirent.name);
                const files = await fs.readdir(subdirPath);
                return files.includes('about.md') ? dirent.name : null;
            })


            const subdirsContainingAboutMd = (await Promise.all(subdirChecks)).filter(name => name !== null);

            subdirsContainingAboutMd.forEach(dir => console.log(dir));
        } catch (err) {
            console.error('Error listing subdirectories:', err);
        }
    }

    static splitName(input) {
        if (input.includes('.')) {
            let parts = input.split('.');
            return {
                first: parts[0],
                second: parts[1]
            };
        } else {
            return {
                first: input,
                second: false
            };
        }
    }
}

export default CommandUtils

================
File: src/utils/footpath.js
================
import path from 'path'
import { fileURLToPath } from 'url'

import logger from './Logger.js'







let footpath = {}

footpath.resolve = function footpath(here, relative, start) {

    const loggy = false
    if (loggy) {
        logger.debug("\n*** start footpath.resolve ***")
        logger.debug("process.cwd() = " + process.cwd())
        logger.debug("here = " + here)
        logger.debug("relative = " + relative)
        logger.debug("start = " + start)
    }

    const __filename = fileURLToPath(here)
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, relative)
    const filePath = path.join(rootDir, start)

    if (loggy) {
        logger.debug("__filename = " + __filename)
        logger.debug("__dirname = " + __dirname)
        logger.debug("rootDir = " + rootDir)
        logger.debug("filePath = " + filePath)
        logger.debug("*** end footpath.resolve ***\n")
    }

    return filePath
}

footpath.urlLastPart = function footpath(url = 'http://example.org/not-a-url') {


    const urlObj = new URL(url);
    const hash = urlObj.hash;
    const path = urlObj.pathname;
    const lastPart = hash ? hash.replace(/^#/, '') : path.split('/').pop();
    // } catch {
    //  return 'not-a-url'

    return lastPart;
}

export default footpath

================
File: src/utils/GrapoiHelpers.js
================
import rdf from 'rdf-ext'
import grapoi from 'grapoi'
import { fromFile, toFile } from 'rdf-utils-fs'
import ns from './ns.js'
import logger from './Logger.js'



class GrapoiHelpers {


    static async readDataset(filename) {
        const stream = fromFile(filename)
        const dataset = await rdf.dataset().import(stream)
        return dataset
    }

    static async writeDataset(dataset, filename) {
        await toFile(dataset.toStream(), filename)
    }


    static listToArray(dataset, term, property) {
        const poi = rdf.grapoi({ dataset: dataset, term: term })
        const first = poi.out(property).term

        let p = rdf.grapoi({ dataset, term: first })
        let object = p.out(ns.rdf.first).term

        const result = [object]

        while (true) {
            let restHead = p.out(ns.rdf.rest).term
            let p2 = rdf.grapoi({ dataset, term: restHead })
            let object = p2.out(ns.rdf.first).term

            if (restHead.equals(ns.rdf.nil)) break
            result.push(object)
            p = rdf.grapoi({ dataset, term: restHead })
        }
        return result
    }





    static listObjects(dataset, subjectList, predicate) {
        const objects = []
        for (const subject of subjectList) {
            logger.log("subject = " + subject.value)
            let p = rdf.grapoi({ dataset, term: subject })
            let object = p.out(predicate).term
            logger.log("object = " + object.value)
            objects.push(object)
        }
        return objects
    }
}
export default GrapoiHelpers

================
File: src/utils/Logger.js
================
import fs from 'fs';


let logger = {};

logger.logfile = 'latest.log';



const LOG_LEVELS = [
    "debug",
    "info",
    "log",
    "warn",
    "error",
];
const logComponent = "api.logger";

logger.appendLogToFile = function (message) {
    if (logger.logfile) {
        fs.appendFileSync(logger.logfile, message + '\n', 'utf8');
    }
};

logger.setLogLevel = function (logLevel = "warn") {

    console[logLevel]('', logComponent, logLevel);
    logger.currentLogLevel = logLevel;
};

logger.timestampISO = function () {
    let now = new Date();
    return now.toISOString();
};

logger.log = function (msg, level = "log") {
    const currentLevelIndex = LOG_LEVELS.indexOf(logger.currentLogLevel);
    const messageLevelIndex = LOG_LEVELS.indexOf(level);

    if (messageLevelIndex >= currentLevelIndex) {
        console[level](msg);
        const logMessage = `[${logger.timestampISO()}] [${level.toUpperCase()}] - ${msg}`;
        logger.appendLogToFile(logMessage);
    }
};

logger.reveal = function (instance) {
    const serialized = {};

    for (const key in instance) {

        if (key === 'dataset') {

        } else {
            if (instance.hasOwnProperty(key)) {
                let kiki = instance[key];

                if (kiki) {
                    if (Buffer.isBuffer(kiki)) {
                        kiki = kiki.toString();
                    }
                    if (kiki.length > 100) {
                        kiki = kiki.substring(0, 100) + '...';
                    }
                    serialized[key] = kiki
                } else {
                    serialized[key] = '[no key]'
                }
            }
        }
    }
    const props = JSON.stringify(serialized, null, 2);
    logger.log(`Instance of ${instance.constructor.name} with properties - \n${props}`);
};

logger.debug = function (msg) {
    logger.log(msg, "debug");
};

logger.info = function (msg) {
    logger.log(msg, "info");
};

logger.warn = function (msg) {
    logger.log(msg, "warn");
};

logger.error = function (msg) {
    logger.log(msg, "error");
};

logger.poi = function exploreGrapoi(grapoi, predicates, objects, subjects) {

    console.log('Properties of the Grapoi object:');
    for (const prop in grapoi) {
        console.log(`\t${prop}: ${grapoi[prop]}`);
    }


    console.log('\nPath:');
    const path = grapoi.out(predicates, objects).in(predicates, subjects);
    for (const quad of path.quads()) {
        console.log(`\t${quad.predicate.value}: ${quad.object.value}`);
    }
};

function handleExit(options, exitCode) {
    if (options.cleanup) {

    }
    if (exitCode || exitCode === 0) console.log(exitCode);
    if (options.exit) process.exit();
}


process.on('exit', handleExit.bind(null, { cleanup: true }));
process.on('SIGINT', handleExit.bind(null, { exit: true }));
process.on('SIGUSR1', handleExit.bind(null, { exit: true }));
process.on('SIGUSR2', handleExit.bind(null, { exit: true }));
process.on('uncaughtException', handleExit.bind(null, { exit: true }));

export default logger;

================
File: src/utils/ns.js
================
import rdf from 'rdf-ext'

const ns = {
    rdf: rdf.namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#'),
    rdfs: rdf.namespace('http://www.w3.org/2000/01/rdf-schema#'),
    dc: rdf.namespace('http://purl.org/dc/terms/'),
    schema: rdf.namespace('http://schema.org/'),
    xsd: rdf.namespace('http://www.w3.org/2001/XMLSchema#'),
    trm: rdf.namespace('http://purl.org/stuff/transmission/'),
    t: rdf.namespace('http://hyperdata.it/transmissions/'),
    fs: rdf.namespace('http://purl.org/stuff/filesystem/'),
    pc: rdf.namespace('http://purl.org/stuff/postcraft/')
}





ns.getShortname = function (url) {

    if (!url) return
    const lastSlashIndex = url.lastIndexOf('/');
    const lastHashIndex = url.lastIndexOf('#');
    const path = url.slice(lastSlashIndex + 1);
    return path.split('#')[0].split('?')[0];
}
export default ns

================
File: src/utils/t2j.js
================
import { Readable } from 'readable-stream'
import rdf from '@rdfjs/data-model'
import SerializerJsonld from '@rdfjs/serializer-jsonld'
import Serializer from '@rdfjs/serializer-turtle'
import N3Parser from '@rdfjs/parser-n3'
import { fromFile } from 'rdf-utils-fs'
import { toFile } from 'rdf-utils-fs'

const testTurtle = `
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix trm: <http://purl.org/stuff/transmission/> .
@prefix : <https://hyperdata.it/transmissions/> . # for custom terms & instances

:simplepipe a trm:PipelineTransmission ;
    trm:pipe (:s1 :s2 :s3) .

:s1 a trm:StringSource .
:s2 a trm:AppendProcess .
:s3 a trm:StringSink .
`
export class Turtle2JSONLD {
    static async convert(turtle) {

        let parser = new N3Parser({ factory: rdf })



        const input = Readable.from(turtle)

        const output = parser.import(input)

        const serializerJsonld = new SerializerJsonld()
        const jsonStream = serializerJsonld.import(output)





        const outputJson = await Turtle2JSONLD.streamToString(jsonStream)
        return outputJson
    }

    static stringToStream(str) {
        const stream = new Readable();
        stream.push(str);
        stream.push(null);
        return stream;
    }

    static streamToString(stream) {
        const chunks = [];
        return new Promise((resolve, reject) => {
            stream.on('data', (chunk) => {
                chunks.push(Buffer.from(chunk))
                console.log('chunk:', chunk)
            }
            );
            stream.on('error', (err) => reject(err));
            stream.on('end', () => {
                const result = Buffer.concat(chunks).toString('utf8')
                resolve(result)
                console.log('****************** result:', result)
            });
        })
    }
}



const testJson = await Turtle2JSONLD.convert(testTurtle)
console.log('àààààààààààààààààààààà')
console.log(testJson)

================
File: src/utils/test_runner.js
================
import fs from 'fs';
import path from 'path';

const testFiles = fs.readdirSync(__dirname).filter(file => file.startsWith('test_'));

testFiles.forEach(testFile => {
    console.log(`Running ${testFile}`);
    require(path.join(__dirname, testFile));
});

================
File: tests/helpers/reporter.js
================
import { SpecReporter } from 'jasmine-spec-reporter';

class CustomReporter {
    constructor() {
        this.specReporter = new SpecReporter({
            spec: {
                displayPending: true
            }
        });
    }

    jasmineStarted() {
        this.specReporter.jasmineStarted.apply(this.specReporter, arguments);
    }

    suiteStarted() {
        this.specReporter.suiteStarted.apply(this.specReporter, arguments);
    }

    specStarted() {
        this.specReporter.specStarted.apply(this.specReporter, arguments);
    }

    specDone() {
        this.specReporter.specDone.apply(this.specReporter, arguments);
    }

    suiteDone() {
        this.specReporter.suiteDone.apply(this.specReporter, arguments);
    }

    jasmineDone() {
        this.specReporter.jasmineDone.apply(this.specReporter, arguments);
    }
}

export default CustomReporter;

================
File: tests/integration/file-copy-remove-test.spec.js
================
import { expect } from 'chai';
import { exec } from 'child_process';
import * as fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const testDir = path.resolve(__dirname, '../../src/applications/file-copy-remove-test/data');

async function setupInitialStructure() {
    const startDir = path.join(testDir, 'start');
    await fs.mkdir(startDir, { recursive: true });
    await fs.writeFile(path.join(startDir, 'one.txt'), 'Hello from One');
    await fs.writeFile(path.join(startDir, 'two.txt'), 'Hello from Two');
}

async function checkFileExists(filePath) {
    try {
        await fs.access(filePath);
        return true;
    } catch {
        return false;
    }
}

describe('file-copy-remove-test', function () {

    jasmine.DEFAULT_TIMEOUT_INTERVAL = 30000;

    beforeAll(async function () {
        console.log('Starting beforeAll...');
        const sourceFile = path.join(testDir, 'start', 'one.txt');
        const sourceExists = await checkFileExists(sourceFile);
        console.log(`Source file exists: ${sourceExists}`);
        await setupInitialStructure();
        await Promise.all([
            clearDir('single-empty'),
            clearDir('single-full'),
            clearDir('several-empty'),
            clearDir('several-full')
        ]);
        console.log('Finished beforeAll');
    });

    async function getDirContents(dir) {
        try {
            return await fs.readdir(path.join(testDir, dir));
        } catch (error) {
            if (error.code === 'ENOENT') return [];
            throw error;
        }
    }

    async function clearDir(dir) {
        try {
            await fs.rm(path.join(testDir, dir), { recursive: true, force: true });
        } catch (error) {
            if (error.code !== 'ENOENT') throw error;
        }
    }



    it('performs file operations correctly', function (done) {
        console.log('Starting test...');
        exec('node run.js file-copy-remove-test', async (error, stdout, stderr) => {
            console.log('Exec completed');
            if (error) {
                console.error(`exec error: ${error}`);
                return done(error);
            }
            if (stderr) {
                console.error(`stderr: ${stderr}`);
            }
            console.log(`stdout: ${stdout}`);
            try {
                const singleEmpty = await getDirContents('single-empty');
                console.log('single-empty contents:', singleEmpty);
                expect(singleEmpty).to.be.empty;

                const singleFull = await getDirContents('single-full');
                console.log('single-full contents:', singleFull);
                expect(singleFull).to.deep.equal(['one.txt']);

                const severalEmpty = await getDirContents('several-empty');
                console.log('several-empty contents:', severalEmpty);
                expect(severalEmpty).to.be.empty;

                const severalFull = await getDirContents('several-full');
                console.log('several-full contents:', severalFull);
                expect(severalFull).to.have.members(['one.txt', 'two.txt']);

                console.log('All assertions passed');
                done();
            } catch (err) {
                console.error('Error in assertions:', err);
                done(err);
            }
        });
    });
});

================
File: tests/integration/file-pipeline.spec.js
================
import footpath from '../../src/utils/footpath.js'

import path from 'path'
import { fileURLToPath } from 'url'
import { expect } from 'chai'
import { exec } from 'child_process'
import fs from 'fs'


describe('file-pipeline', function () {
    const __filename = fileURLToPath(import.meta.url);
    const __dirname = path.dirname(__filename)
    const rootDir = path.resolve(__dirname, '../../')

    const dataFile = footpath.resolve(import.meta.url, '../../src/applications/file-pipeline/data/', 'output.txt')
    console.log('in file-pipeline.spec.js dataFile = ' + dataFile)
    it('produces correct output', function (done) {

        if (fs.existsSync(dataFile)) {
            fs.unlinkSync(dataFile);
        }

        exec('node run.js file-pipeline', (error, stdout, stderr) => {
            if (error) {
                console.error(`exec error: ${error}`);
                return;
            }


            fs.readFile(dataFile, 'utf8', (err, data) => {
                if (err) {
                    console.error(`readFile error: ${err}`);
                    return;
                }


                expect(data.trim()).to.equal('Hello world world');
                done();
            });
        });
    });
});

================
File: tests/integration/string-pipeline.speeec.js
================
import { expect } from 'chai'
import { exec } from 'child_process'






describe('string-pipeline', function () {
    it('produces correct output', function (done) {
        exec('node run.js string-pipeline Hello', (error, stdout, stderr) => {
            if (error) {
                console.error(`exec error: ${error}`);
                return;
            }

            const match = stdout.match(/"([^"]+)"/)
            const output = match ? match[1] : ''

            expect(output).to.equal('Hello world world')

            done();
        });
    });
});

================
File: tests/unit/NOP.spec.js
================
import NOP from '../../src/processors/util/NOP.js'
import { expect } from 'chai'

describe('NOP', function () {
    it('double() should return the input string concatenated with itself', function () {
        const nop = new NOP()
        const input = 'test'
        const expectedOutput = 'testtest'
        const output = nop.double(input)
        expect(output).to.equal(expectedOutput)
    })
})

================
File: tests/unit/PostcraftPrep.spec.js
================
import PostcraftPrep from '../../src/processors/postcraft/PostcraftPrep.js'
import { expect } from 'chai'

describe('PostcraftPrep', function () {
    beforeEach(function () {
        this.context = {
            content: 'only text',
            filename: 'minimal-filename.md'
        }
    })

    it('extractTitle(context) should lift the title from the filename', function () {
        this.context.filename = '2024-05-10_this-thing.md'
        const input = this.context
        const expectedOutput = 'This Thing'
        const pp = new PostcraftPrep()
        const output = pp.extractTitle(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractSlug(context) should return filename without path and extension', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        const input = this.context
        const expectedOutput = '2024-05-10_hello-postcraft'
        const pp = new PostcraftPrep()
        const output = pp.extractSlug(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractTargetFilename(context) should return the correct target filename', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        this.context.rootDir = '/root'
        this.context.entryContentMeta = {
            targetDir: 'target'
        }
        const input = this.context
        const expectedOutput = '/root/target/2024-05-10_hello-postcraft.html'
        const pp = new PostcraftPrep()
        const output = pp.extractTargetFilename(input)
        expect(output).to.equal(expectedOutput)
    })

    it('extractDates(context) should return the correct dates', function () {
        this.context.filename = '2024-05-10_hello-postcraft.md'
        const input = this.context
        const expectedOutput = { created: '2024-05-10', updated: (new Date()).toISOString().split('T')[0] }
        const pp = new PostcraftPrep()
        const output = pp.extractDates(input)
        expect(output).to.deep.equal(expectedOutput)
    })
})

================
File: tests/unit/StringFilter.spec.js
================
import StringFilter from '../../src/processors/text/StringFilter.js';
import { expect } from 'chai';

describe('StringFilter', function () {

    function compose(content, include, exclude) {
        return { content, include, exclude };
    }


    const contentSamples = [
        '/home/user/documents/',
        '/home/user/documents/file.txt',
        '/var/log/',
        '/etc/config.conf',
        '/usr/local/bin/app',
        '/home/user/pictures/vacation/',
        '/home/user/pictures/vacation/photo.jpg',
        '/opt/',
        '/tmp/temp.file',
        '/home/user/.config/',
        '',
        undefined
    ];

    const patternSamples = [
        '*.txt',
        '*.jpg',
        '/home/user/*',
        '/var/*',
        '*/bin/*',
        ['*.txt', '*.jpg'],
        ['/home/user/*', '/var/*'],
        ['*/bin/*', '*.conf'],
        ['*.file', '/tmp/*'],
        ['/opt/*', '/etc/*'],
        '',
        [],
        undefined
    ];

    describe('isAccepted()', function () {
        it('should accept all content when include and exclude are empty', function () {
            const filter = new StringFilter();
            contentSamples.forEach(content => {
                if (content !== undefined) {
                    const message = compose(content, '', '');
                    expect(filter.isAccepted(message.content, message.exclude, message.include)).to.be.true;
                }
            });
        });

        it('should reject undefined content', function () {
            const filter = new StringFilter();
            const message = compose(undefined, '', '');
            expect(filter.isAccepted(message.content, message.exclude, message.include)).to.be.false;
        });

        it('should correctly apply include patterns', function () {
            const filter = new StringFilter();
            const includeTests = [
                { content: '/home/user/documents/file.txt', include: '*.txt', expected: true },
                { content: '/home/user/pictures/vacation/photo.jpg', include: '*.jpg', expected: true },
                { content: '/var/log/', include: '/var/*', expected: true },
                { content: '/home/user/documents/', include: '/home/user/*', expected: true },
                { content: '/usr/local/bin/app', include: '*/bin/*', expected: true },
                { content: '/etc/config.conf', include: ['*.conf', '*.txt'], expected: true },
                { content: '/opt/', include: ['/var/*', '/opt/*'], expected: true },
                { content: '/tmp/temp.file', include: '*.doc', expected: false }
            ];

            includeTests.forEach(test => {
                const message = compose(test.content, test.include, '');
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });

        it('should correctly apply exclude patterns', function () {
            const filter = new StringFilter();
            const excludeTests = [
                { content: '/home/user/documents/file.txt', exclude: '*.txt', expected: false },
                { content: '/home/user/pictures/vacation/photo.jpg', exclude: '*.jpg', expected: false },
                { content: '/var/log/', exclude: '/var/*', expected: false },
                { content: '/home/user/documents/', exclude: '/home/user/*', expected: false },
                { content: '/usr/local/bin/app', exclude: '*/bin/*', expected: false },
                { content: '/etc/config.conf', exclude: ['*.conf', '*.txt'], expected: false },
                { content: '/opt/', exclude: ['/var/*', '/tmp/*'], expected: true },
                { content: '/tmp/temp.file', exclude: '*.doc', expected: true }
            ];

            excludeTests.forEach(test => {
                const message = compose(test.content, '', test.exclude);
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });

        it('should correctly apply both include and exclude patterns', function () {
            const filter = new StringFilter();
            const combinedTests = [
                { content: '/home/user/documents/file.txt', include: '*.txt', exclude: '/var/*', expected: true },
                { content: '/var/log/system.log', include: '*.log', exclude: '/var/*', expected: false },
                { content: '/home/user/pictures/vacation/photo.jpg', include: ['/home/user/*', '*.jpg'], exclude: '*.png', expected: true },
                { content: '/etc/config.conf', include: ['*.conf', '*.txt'], exclude: ['/home/*', '/var/*'], expected: true },
                { content: '/usr/local/bin/app', include: '*/bin/*', exclude: '*/local/*', expected: false }
            ];

            combinedTests.forEach(test => {
                const message = compose(test.content, test.include, test.exclude);
                expect(filter.isAccepted(message.content, message.exclude, message.include)).to.equal(test.expected);
            });
        });
    });
});

================
File: tests/unit/StringReplace.spec.js
================
import StringReplace from '../../src/processors/text/StringReplace.js'
import { expect } from 'chai'




describe('StringReplace', function () {



    it('execute() should replace all occurrences of the match string with the replace string', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: 'Hello world! Hello universe!',
            match: 'Hello',
            replace: 'Hi'
        }


        stringReplace.execute(message)


        const expectedOutput = 'Hi world! Hi universe!'
        expect(message.content).to.equal(expectedOutput)
    })




    it('execute() should not modify the content if the match string is not found', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: 'Hello world!',
            match: 'Goodbye',
            replace: 'Hi'
        }


        stringReplace.execute(message)


        const expectedOutput = 'Hello world!'
        expect(message.content).to.equal(expectedOutput)
    })




    it('execute() should handle empty content string', function () {

        const stringReplace = new StringReplace()
        const message = {
            content: '',
            match: 'Hello',
            replace: 'Hi'
        }


        stringReplace.execute(message)


        const expectedOutput = ''
        expect(message.content).to.equal(expectedOutput)
    })
})

================
File: tests/grapoi-raw-tests.js
================
import { namedNode, literal } from '@rdfjs/data-model';
import { Grapoi } from '@grapoi';
import { Term } from '@rdfjs/types';

describe('Grapoi', () => {
    let grapoi;

    beforeEach(() => {
        grapoi = new Grapoi();
    });

    describe('.addList', () => {
        it('should return a Grapoi instance', () => {

            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const items: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = grapoi.addList(predicates, items);

            expect(result).toBeInstanceOf(Grapoi);
        });
    });


});



import { namedNode, literal } from '@rdfjs/data-model';
import { Grapoi } from '@grapoi';
import { Term } from '@rdfjs/types';

describe('Grapoi', () => {
    let grapoi;

    beforeEach(() => {
        grapoi = new Grapoi();
    });

    describe('.addList', () => {
        it('should return a Grapoi instance', () => {
            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const items: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = grapoi.addList(predicates, items);

            expect(result).toBeInstanceOf(Grapoi);
        });
    });

    describe('.addOut', () => {
        it('should return a Grapoi instance', () => {
            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const objects: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = grapoi.addOut(predicates, objects);

            expect(result).toBeInstanceOf(Grapoi);
        });
    });


});




describe('.base', () => {
    it('should return a Grapoi instance', () => {
        const base: Term = namedNode('https://housemd.rdf-ext.org/person/allison-cameron');

        const result = grapoi.base(base);

        expect(result).toBeInstanceOf(Grapoi);
    });
});



import { namedNode, literal } from '@rdfjs/data-model';
import { Path } from '@grapoi';
import { Term } from '@rdfjs/types';

describe('Path', () => {
    let path;

    beforeEach(() => {
        path = new Path();
    });

    describe('.addList', () => {
        it('should return a Path instance', () => {
            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const items: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = path.addList(predicates, items);

            expect(result).toBeInstanceOf(Path);
        });
    });


});



import { namedNode, literal } from '@rdfjs/data-model';
import { PathList } from '@grapoi';
import { Term } from '@rdfjs/types';

describe('PathList', () => {
    let pathList;

    beforeEach(() => {
        pathList = new PathList();
    });

    describe('.addList', () => {
        it('should return a PathList instance', () => {
            const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
            const items: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

            const result = pathList.addList(predicates, items);

            expect(result).toBeInstanceOf(PathList);
        });
    });


});





describe('.addOut', () => {
    it('should return a Path instance', () => {
        const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
        const objects: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

        const result = path.addOut(predicates, objects);

        expect(result).toBeInstanceOf(Path);
    });
});





describe('.addOut', () => {
    it('should return a PathList instance', () => {
        const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];
        const objects: Term[] = [namedNode('https://housemd.rdf-ext.org/person/robert-chase')];

        const result = pathList.addOut(predicates, objects);

        expect(result).toBeInstanceOf(PathList);
    });
});





describe('.deleteIn', () => {
    it('should return a Path instance', () => {
        const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];

        const result = path.deleteIn(predicates);

        expect(result).toBeInstanceOf(Path);
    });
});





describe('.deleteIn', () => {
    it('should return a PathList instance', () => {
        const predicates: Term[] = [namedNode('https://housemd.rdf-ext.org/person/allison-cameron'), namedNode('http://schema.org/knows')];

        const result = pathList.deleteIn(predicates);

        expect(result).toBeInstanceOf(PathList);
    });
});

================
File: types/grapoi.d.ts
================
import { DatasetCore, Quad, Term } from "@rdfjs/types";


interface Grapoi extends PathList {
    addList(predicates: Term | Term[], items: Term | Term[]): Grapoi;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): Grapoi;
    base(base: Term | Term[]): Grapoi;
}


interface Edge {
    dataset: DatasetCore;
    end: string;
    quad: Quad;
    start: string;
    term: Term;
    graph: Term;
    startTerm: Term;
}


interface Instruction {
    operation?: string;
    quantifier?: string;
    start?: string;
    end?: string;
    subjects?: Term[];
    predicates?: Term[];
    objects?: Term[];
    graphs?: DatasetCore[];
    items?: Term[];
    callback?: (edge: Edge, ptr: Path | PathList) => Path | PathList;
}


interface Path {
    addList(predicates: Term | Term[], items: Term | Term[]): Path;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): Path;
    deleteIn(predicates: Term | Term[], subjects: Term | Term[]): Path;
    deleteList(predicates: Term | Term[]): Path;
    deleteOut(predicates: Term | Term[], objects: Term | Term[]): Path;
    extend(edge: Edge): Path;
    execute(instruction: Instruction): Path;
}


interface PathList {
    addList(predicates: Term | Term[], items: Term | Term[]): PathList;
    addOut(predicates: Term | Term[], objects: Term | Term[], callback?: Function): PathList;
    deleteIn(predicates: Term | Term[], subjects: Term | Term[]): PathList;
    deleteList(predicates: Term | Term[]): PathList;
    deleteOut(predicates: Term | Term[], objects: Term | Term[]): PathList;
    distinct(): PathList;
    in(predicates: Term | Term[], subjects: Term | Term[]): PathList;
    isAny(): boolean;
    isList(): boolean;
    list(): Iterator<Term> | undefined;
    map(callback: Function): PathList[];
    out(predicates: Term | Term[], objects: Term | Term[]): PathList;
    quads(): Iterator<Quad>;
    execute(instruction: Instruction): PathList[];
}

================
File: vocabs/dbooth-pipeline.n3.txt
================
# Ontology and inference rules used in describing an RDF data pipeline.
# See http://dbooth.org/2011/pipeline/

@prefix p: <http://purl.org/pipeline/ont#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

######### Basic Node types ##########
p:Node		a rdfs:Class .
p:CommandNode	rdfs:subClassOf p:Node .  # Default Node type
p:JenaNode      rdfs:subClassOf p:Node .
p:SesameNode    rdfs:subClassOf p:Node .
p:PerlNode      rdfs:subClassOf p:Node .
p:MysqlNode	rdfs:subClassOf p:Node .
p:OracleNode	rdfs:subClassOf p:Node .

######### Node properties ##########
p:inputs 	rdfs:domain p:Node .
p:parameters 	rdfs:domain p:Node .
p:dependsOn 	rdfs:domain p:Node .

# p:output specifies the output cache for a node.
# It is node-type-specific, e.g., filename for FileNode .
# It may be set explicitly, otherwise a default will be used.
p:output	rdfs:domain p:Node .

# p:updater specifies the updater method for a Node.
# It is node-type-specific, e.g., a script for CommandNode .
p:updater	rdfs:domain p:Node .

# p:updaterType specifies the type of updater used.
# It is node-type-specific.
p:updaterType	rdfs:domain p:Node .

######### Rules ##########
# A Node dependsOn its inputs and parameters:
{ ?a p:inputs ?b . }     => { ?a p:dependsOn ?b . } .
{ ?a p:parameters ?b . } => { ?a p:dependsOn ?b . } .

================
File: vocabs/info-store.rdf
================
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" xmlns:owl="http://www.w3.org/2002/07/owl#">

    <owl:Ontology rdf:about="http://rdflib.net/2002/InformationStore">
        <rdfs:label>InformationStore</rdfs:label>
        <rdfs:comment>A document defining a vocabulary used by an InformationStore, a TripleStore
            with support for multiple contexts.</rdfs:comment>
        <rdfs:isDefinedBy rdf:resource="http://rdflib.net/2002/InformationStore" />
    </owl:Ontology>

    <rdfs:Class rdf:about="http://rdflib.net/2002/InformationStore#Context">
        <rdfs:label>Context</rdfs:label>
        <rdfs:subClassOf rdf:resource="http://www.w3.org/2000/01/rdf-schema#Resource" />
        <rdfs:isDefinedBy rdf:resource="http://rdflib.net/2002/InformationStore" />
    </rdfs:Class>

    <rdfs:Class rdf:about="http://rdflib.net/2002/InformationStore#UpdateEvent">
        <rdfs:label>An Event for updating a Context</rdfs:label>
        <rdfs:subClassOf rdf:resource="http://www.w3.org/2000/01/rdf-schema#Resource" />
        <rdfs:isDefinedBy rdf:resource="http://rdflib.net/2002/InformationStore" />
    </rdfs:Class>

    <rdf:Property rdf:about="http://rdflib.net/2002/InformationStore#source">
        <rdfs:label>source</rdfs:label>
        <rdfs:domain rdf:resource="http://www.w3.org/2000/01/rdf-schema#Resource" />
        <rdfs:range rdf:resource="http://www.w3.org/2000/01/rdf-schema#Resource" />
        <rdfs:isDefinedBy rdf:resource="http://rdflib.net/2002/InformationStore" />
    </rdf:Property>

    <rdf:Property rdf:about="http://rdflib.net/2002/InformationStore#updateEvent">
        <rdfs:label>update event</rdfs:label>
        <rdfs:domain rdf:resource="http://www.w3.org/2000/01/rdf-schema#Context" />
        <rdfs:range rdf:resource="http://www.w3.org/2000/01/rdf-schema#UpdateEvent" />
        <rdfs:isDefinedBy rdf:resource="http://rdflib.net/2002/InformationStore" />
    </rdf:Property>

    <rdf:Property rdf:about="http://rdflib.net/2002/InformationStore#error">
        <rdfs:label>Error</rdfs:label>
        <rdfs:comment>This property is used to capture information about errors that occured while
            updating the context.</rdfs:comment>
        <rdfs:domain rdf:resource="http://www.w3.org/2000/01/rdf-schema#UpdateEvent" />
        <rdfs:range rdf:resource="http://www.w3.org/2000/01/rdf-schema#Literal" />
        <rdfs:isDefinedBy rdf:resource="http://rdflib.net/2002/InformationStore" />
    </rdf:Property>

    <rdf:Property rdf:about="http://rdflib.net/2002/InformationStore#http_status">
        <rdfs:label>http status</rdfs:label>
        <rdfs:comment>This property is used for the http status that was returned when updating the
            context.</rdfs:comment>
        <rdfs:domain rdf:resource="http://www.w3.org/2000/01/rdf-schema#UpdateEvent" />
        <rdfs:range rdf:resource="http://www.w3.org/2000/01/rdf-schema#Literal" />
        <rdfs:isDefinedBy rdf:resource="http://rdflib.net/2002/InformationStore" />
    </rdf:Property>

</rdf:RDF>

================
File: vocabs/info-store.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .

<http://rdflib.net/2002/InformationStore>
    a owl:Ontology ;
    rdfs:comment """A document defining a vocabulary used by an InformationStore, a TripleStore
            with support for multiple contexts.""" ;
    rdfs:isDefinedBy <http://rdflib.net/2002/InformationStore> ;
    rdfs:label "InformationStore" .

<http://rdflib.net/2002/InformationStore#Context>
    a rdfs:Class ;
    rdfs:isDefinedBy <http://rdflib.net/2002/InformationStore> ;
    rdfs:label "Context" ;
    rdfs:subClassOf rdfs:Resource .

<http://rdflib.net/2002/InformationStore#UpdateEvent>
    a rdfs:Class ;
    rdfs:isDefinedBy <http://rdflib.net/2002/InformationStore> ;
    rdfs:label "An Event for updating a Context" ;
    rdfs:subClassOf rdfs:Resource .

<http://rdflib.net/2002/InformationStore#error>
    a rdf:Property ;
    rdfs:comment """This property is used to capture information about errors that occured while
            updating the context.""" ;
    rdfs:domain rdfs:UpdateEvent ;
    rdfs:isDefinedBy <http://rdflib.net/2002/InformationStore> ;
    rdfs:label "Error" ;
    rdfs:range rdfs:Literal .

<http://rdflib.net/2002/InformationStore#http_status>
    a rdf:Property ;
    rdfs:comment """This property is used for the http status that was returned when updating the
            context.""" ;
    rdfs:domain rdfs:UpdateEvent ;
    rdfs:isDefinedBy <http://rdflib.net/2002/InformationStore> ;
    rdfs:label "http status" ;
    rdfs:range rdfs:Literal .

<http://rdflib.net/2002/InformationStore#source>
    a rdf:Property ;
    rdfs:domain rdfs:Resource ;
    rdfs:isDefinedBy <http://rdflib.net/2002/InformationStore> ;
    rdfs:label "source" ;
    rdfs:range rdfs:Resource .

<http://rdflib.net/2002/InformationStore#updateEvent>
    a rdf:Property ;
    rdfs:domain rdfs:Context ;
    rdfs:isDefinedBy <http://rdflib.net/2002/InformationStore> ;
    rdfs:label "update event" ;
    rdfs:range rdfs:UpdateEvent .

================
File: vocabs/notes.md
================
---

- [specgen-ng](https://github.com/danja/specgen-ng)

rapper -o turtle email.rdf > email.ttl

cd /home/danny/HKMS/specgen-ng

mkdir spec/email

danny@danny-desktop:~/HKMS/transmissions/src$ cd /home/danny/foaf-archive/vocabs/
danny@danny-desktop:~/foaf-archive/vocabs$ mkdir email

python src/specgenng.py \
 --indir=/home/danny/foaf-archive/vocabs/email/ \
 --ns=http://www.w3.org/2000/04/maillog2rdf/email# \
 --prefix=email \
 --ontofile=email.ttl \
 --templatedir=template \
 --outdir=/home/danny/foaf-archive/vocabs/email/ \
 --outfile=email.html

grr, not enough input

email_danja_2024-03-16.ttl

rapper -i turtle -o rdfxml email_danja_2024-03-16.ttl > email_danja_2024-03-16.rdf

python src/specgenng.py \
 --indir=/home/danny/foaf-archive/vocabs/email/ \
 --ns=http://www.w3.org/2000/04/maillog2rdf/email# \
 --prefix=email \
 --ontofile=email_danja_2024-03-16.rdf \
 --templatedir=template \
 --outdir=/home/danny/foaf-archive/vocabs/email/ \
 --outfile=email.html

================
File: vocabs/slug-config.rdf
================
<?xml version='1.0'?>
<rdf:RDF  
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:scutter="http://www.hackdiary.com/scutter/" 
  xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
  xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:slug="http://purl.org/NET/schemas/slug/config/">  

   <rdf:Description rdf:about="">
      <dc:title>Slug Configuration File</dc:title>
      <dc:description>
Configuration File for the Slug Semantic Web Crawler. Used to define crawler profiles 
with different worker, memory, consumer and filter configurations.
      </dc:description>      
   </rdf:Description>
   
   <!-- select config from cmd-line parameter -->
   
   <slug:Scutter rdf:about="default">
	 <dc:description>A default Scutter configuration</dc:description>
	 
	 <!-- configure global memory -->
     <slug:hasMemory rdf:resource="memory"/>

	 <!-- how many worker threads -->
     <slug:workers>10</slug:workers>

     <!-- configures consumers for incoming data -->
	 <slug:consumers>
	   <rdf:Seq>
	    <rdf:li rdf:resource="storer"/>
	    <rdf:li rdf:resource="rdf-consumer"/>	    
	   </rdf:Seq>	   
	 </slug:consumers>
	
	 <!-- configures filter pipeline for controller -->
	 <slug:filters>
	   <rdf:Seq>
	     <rdf:li rdf:resource="single-fetch-filter"/>	     	   
	     <rdf:li rdf:resource="depth-filter"/>
	     <rdf:li rdf:resource="regex-filter"/>	     
	   </rdf:Seq>
	 </slug:filters> 	 
	 
   </slug:Scutter>

   <slug:Scutter rdf:about="shallow-slow-scutter">
	 <dc:description>A Scutter which crawls only a shallow depth, with 
	 fewer workers</dc:description>

     <slug:hasMemory rdf:resource="memory"/>
     <!-- fewer workers, so slower -->
     <slug:workers>3</slug:workers>

	 <!-- standard consumer components -->
	 <slug:consumers>
	   <rdf:Seq>
	    <rdf:li rdf:resource="storer"/>
	    <rdf:li rdf:resource="rdf-consumer"/>	    
	   </rdf:Seq>	   
	 </slug:consumers>
	
	 <!-- alternate filters, using shallow-depth-filter -->
	 <slug:filters>
	   <rdf:Seq>
	     <rdf:li rdf:resource="single-fetch-filter"/>	     	   
	     <rdf:li rdf:resource="shallow-depth-filter"/>
	     <rdf:li rdf:resource="regex-filter"/>	     
	   </rdf:Seq>
	 </slug:filters> 	 
	 
   </slug:Scutter>

   <slug:Scutter rdf:about="mapping-scutter">
	 <dc:description>A Scutter which simply discovers and maps 
	 connections between files using source/origin properties in 
	 its memory</dc:description>

     <slug:hasMemory rdf:resource="memory"/>
     <slug:workers>10</slug:workers>

	 <!-- standard consumer components -->
	 <slug:consumers>
	   <rdf:Seq>
	    <rdf:li rdf:resource="rdf-consumer"/>	    
	   </rdf:Seq>	   
	 </slug:consumers>
	
	 <!-- alternate filters, using shallow-depth-filter -->
	 <slug:filters>
	   <rdf:Seq>
	     <rdf:li rdf:resource="single-fetch-filter"/>	     	   
	     <rdf:li rdf:resource="deep-depth-filter"/> 
	   </rdf:Seq>
	 </slug:filters> 	 
	 
   </slug:Scutter>

   <slug:Scutter rdf:about="persistent-scutter">
	 <dc:description>A Scutter that includes writing incoming data into 
	 a persistent memory. Note that the memory is different to that 
	 holdings Scutter persistent state.</dc:description>
	 
     <slug:hasMemory rdf:resource="memory"/>

	 <!-- how many worker threads? -->
     <slug:workers>10</slug:workers>

     <!-- configures consumers for incoming data. Added persistent-storer -->
	 <slug:consumers>
	   <rdf:Seq>
	    <rdf:li rdf:resource="storer"/>
	    <rdf:li rdf:resource="rdf-consumer"/>	    
	    <rdf:li rdf:resource="persistent-storer"/>	    	    
	   </rdf:Seq>	   
	 </slug:consumers>
	
	 <!-- configures filter pipeline for controller -->
	 <slug:filters>
	   <rdf:Seq>
	     <rdf:li rdf:resource="single-fetch-filter"/>	     	   
	     <rdf:li rdf:resource="depth-filter"/>
	     <rdf:li rdf:resource="regex-filter"/>	     
	   </rdf:Seq>
	 </slug:filters> 	 
	 
   </slug:Scutter>
   
   <slug:Scutter rdf:about="cache-builder">
	 <dc:description>Builds a local cache of fetched data, doesn't 
	 traverse RDF links to discover new resources.</dc:description>
	 
     <slug:hasMemory rdf:resource="memory"/>

	 <!-- how many worker threads? -->
     <slug:workers>10</slug:workers>

     <!-- configures consumers for incoming data -->
	 <slug:consumers>
	   <rdf:Seq>
	    <rdf:li rdf:resource="storer"/>
	   </rdf:Seq>	   
	 </slug:consumers>
		 
   </slug:Scutter>   
   
   <slug:Consumer rdf:about="storer">
     <dc:title>ResponseStorer</dc:title>
     <dc:description>Stores HTTP responses in a file system cache</dc:description>
   
     <slug:impl>com.ldodds.slug.http.ResponseStorer</slug:impl>
     <!-- must be a directory, will get created automatically if it doesn't exist -->
     <slug:cache>c:\temp\slug-cache</slug:cache>
   </slug:Consumer>

   <slug:Consumer rdf:about="persistent-storer">
     <dc:title>Database-backed ResponseStorer</dc:title>
     <dc:description>Stores HTTP responses in Jena persistent model</dc:description>
     <slug:impl>com.ldodds.slug.http.PersistentResponseStorer</slug:impl>
     <!-- must be reference to a Memory -->
     <slug:hasMemory rdf:resource="db-memory"/>
   </slug:Consumer>

   <slug:Consumer rdf:about="rdf-consumer">
     <dc:title>RDFConsumer</dc:title>
     <dc:description>Discovers seeAlso links in RDF models and adds them to task list</dc:description>
     <slug:impl>com.ldodds.slug.http.RDFConsumer</slug:impl>
   </slug:Consumer>

   <slug:Filter rdf:about="depth-filter">
     <dc:title>Limit Depth of Crawling</dc:title>
	 <slug:impl>com.ldodds.slug.http.DepthFilter</slug:impl>
	 <!-- if depth >= this then url not included. Initial depth is 0 -->
	 <slug:depth>3</slug:depth>
   </slug:Filter>

   <slug:Filter rdf:about="deep-depth-filter">
     <dc:title>Limit Depth of Crawling</dc:title>
	 <slug:impl>com.ldodds.slug.http.DepthFilter</slug:impl>
	 <!-- if depth >= this then url not included. Initial depth is 0 -->
	 <slug:depth>5</slug:depth>
   </slug:Filter>
   
   <slug:Filter rdf:about="shallow-depth-filter">
     <dc:title>Limit Depth of Crawling</dc:title>
	 <slug:impl>com.ldodds.slug.http.DepthFilter</slug:impl>
	 <!-- if depth >= this then url not included. Initial depth is 0 -->
	 <slug:depth>1</slug:depth>
   </slug:Filter>
   
   <slug:Filter rdf:about="regex-filter">
     <dc:title>Block URLs based on Regex</dc:title>
	 <slug:impl>com.ldodds.slug.http.RegexFilter</slug:impl>
	 <!-- regular expression, if matches, then url not included -->
	 <slug:filter>livejournal</slug:filter>
   </slug:Filter>

   <slug:Filter rdf:about="single-fetch-filter">
     <dc:title>Avoid Loops</dc:title>
	 <slug:impl>com.ldodds.slug.http.SingleFetchFilter</slug:impl>
   </slug:Filter>         
   
   <slug:Memory rdf:about="memory">
     <slug:file>memory.rdf</slug:file>
   </slug:Memory>

   <slug:Memory rdf:about="db-memory">
     <!-- name of the persistent model -->
     <slug:modelURI rdf:resource="CHANGEME"/>
     
     <!-- JDBC URL for database -->     
     <slug:dbURL>jdbc:mysql://localhost/CHANGEME</slug:dbURL>
     
     <!-- database username and password -->
     <slug:user>CHANGEME</slug:user>
     <slug:pass>CHANGEME</slug:pass>
     
     <!-- Jena Database Engine name -->
     <slug:dbName>MySQL</slug:dbName>
     
     <!-- JDBC Driver -->
     <slug:driver>com.mysql.jdbc.Driver</slug:driver>
     
   </slug:Memory>
   
</rdf:RDF>

================
File: vocabs/slug-config.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix scutter: <http://www.hackdiary.com/scutter/> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .
@prefix slug: <http://purl.org/NET/schemas/slug/config/> .

<file:///home/danny/HKMS/transmissions/vocabs/cache-builder>
    slug:consumers [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/storer> ;
        a rdf:Seq
    ] ;
    slug:hasMemory <file:///home/danny/HKMS/transmissions/vocabs/memory> ;
    slug:workers "10" ;
    dc:description """Builds a local cache of fetched data, doesn't 
	 traverse RDF links to discover new resources.""" ;
    a slug:Scutter .

<file:///home/danny/HKMS/transmissions/vocabs/db-memory>
    slug:dbName "MySQL" ;
    slug:dbURL "jdbc:mysql://localhost/CHANGEME" ;
    slug:driver "com.mysql.jdbc.Driver" ;
    slug:modelURI <file:///home/danny/HKMS/transmissions/vocabs/CHANGEME> ;
    slug:pass "CHANGEME" ;
    slug:user "CHANGEME" ;
    a slug:Memory .

<file:///home/danny/HKMS/transmissions/vocabs/deep-depth-filter>
    slug:depth "5" ;
    slug:impl "com.ldodds.slug.http.DepthFilter" ;
    dc:title "Limit Depth of Crawling" ;
    a slug:Filter .

<file:///home/danny/HKMS/transmissions/vocabs/default>
    slug:consumers [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/storer> ;
        rdf:_2 <file:///home/danny/HKMS/transmissions/vocabs/rdf-consumer> ;
        a rdf:Seq
    ] ;
    slug:filters [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/single-fetch-filter> ;
        rdf:_2 <file:///home/danny/HKMS/transmissions/vocabs/depth-filter> ;
        rdf:_3 <file:///home/danny/HKMS/transmissions/vocabs/regex-filter> ;
        a rdf:Seq
    ] ;
    slug:hasMemory <file:///home/danny/HKMS/transmissions/vocabs/memory> ;
    slug:workers "10" ;
    dc:description "A default Scutter configuration" ;
    a slug:Scutter .

<file:///home/danny/HKMS/transmissions/vocabs/depth-filter>
    slug:depth "3" ;
    slug:impl "com.ldodds.slug.http.DepthFilter" ;
    dc:title "Limit Depth of Crawling" ;
    a slug:Filter .

<file:///home/danny/HKMS/transmissions/vocabs/mapping-scutter>
    slug:consumers [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/rdf-consumer> ;
        a rdf:Seq
    ] ;
    slug:filters [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/single-fetch-filter> ;
        rdf:_2 <file:///home/danny/HKMS/transmissions/vocabs/deep-depth-filter> ;
        a rdf:Seq
    ] ;
    slug:hasMemory <file:///home/danny/HKMS/transmissions/vocabs/memory> ;
    slug:workers "10" ;
    dc:description """A Scutter which simply discovers and maps 
	 connections between files using source/origin properties in 
	 its memory""" ;
    a slug:Scutter .

<file:///home/danny/HKMS/transmissions/vocabs/memory>
    slug:file "memory.rdf" ;
    a slug:Memory .

<file:///home/danny/HKMS/transmissions/vocabs/persistent-scutter>
    slug:consumers [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/storer> ;
        rdf:_2 <file:///home/danny/HKMS/transmissions/vocabs/rdf-consumer> ;
        rdf:_3 <file:///home/danny/HKMS/transmissions/vocabs/persistent-storer> ;
        a rdf:Seq
    ] ;
    slug:filters [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/single-fetch-filter> ;
        rdf:_2 <file:///home/danny/HKMS/transmissions/vocabs/depth-filter> ;
        rdf:_3 <file:///home/danny/HKMS/transmissions/vocabs/regex-filter> ;
        a rdf:Seq
    ] ;
    slug:hasMemory <file:///home/danny/HKMS/transmissions/vocabs/memory> ;
    slug:workers "10" ;
    dc:description """A Scutter that includes writing incoming data into 
	 a persistent memory. Note that the memory is different to that 
	 holdings Scutter persistent state.""" ;
    a slug:Scutter .

<file:///home/danny/HKMS/transmissions/vocabs/persistent-storer>
    slug:hasMemory <file:///home/danny/HKMS/transmissions/vocabs/db-memory> ;
    slug:impl "com.ldodds.slug.http.PersistentResponseStorer" ;
    dc:description "Stores HTTP responses in Jena persistent model" ;
    dc:title "Database-backed ResponseStorer" ;
    a slug:Consumer .

<file:///home/danny/HKMS/transmissions/vocabs/rdf-consumer>
    slug:impl "com.ldodds.slug.http.RDFConsumer" ;
    dc:description "Discovers seeAlso links in RDF models and adds them to task list" ;
    dc:title "RDFConsumer" ;
    a slug:Consumer .

<file:///home/danny/HKMS/transmissions/vocabs/regex-filter>
    slug:filter "livejournal" ;
    slug:impl "com.ldodds.slug.http.RegexFilter" ;
    dc:title "Block URLs based on Regex" ;
    a slug:Filter .

<file:///home/danny/HKMS/transmissions/vocabs/shallow-depth-filter>
    slug:depth "1" ;
    slug:impl "com.ldodds.slug.http.DepthFilter" ;
    dc:title "Limit Depth of Crawling" ;
    a slug:Filter .

<file:///home/danny/HKMS/transmissions/vocabs/shallow-slow-scutter>
    slug:consumers [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/storer> ;
        rdf:_2 <file:///home/danny/HKMS/transmissions/vocabs/rdf-consumer> ;
        a rdf:Seq
    ] ;
    slug:filters [
        rdf:_1 <file:///home/danny/HKMS/transmissions/vocabs/single-fetch-filter> ;
        rdf:_2 <file:///home/danny/HKMS/transmissions/vocabs/shallow-depth-filter> ;
        rdf:_3 <file:///home/danny/HKMS/transmissions/vocabs/regex-filter> ;
        a rdf:Seq
    ] ;
    slug:hasMemory <file:///home/danny/HKMS/transmissions/vocabs/memory> ;
    slug:workers "3" ;
    dc:description """A Scutter which crawls only a shallow depth, with 
	 fewer workers""" ;
    a slug:Scutter .

<file:///home/danny/HKMS/transmissions/vocabs/single-fetch-filter>
    slug:impl "com.ldodds.slug.http.SingleFetchFilter" ;
    dc:title "Avoid Loops" ;
    a slug:Filter .

<file:///home/danny/HKMS/transmissions/vocabs/slug-config.rdf>
    dc:description """
Configuration File for the Slug Semantic Web Crawler. Used to define crawler profiles 
with different worker, memory, consumer and filter configurations.
      """ ;
    dc:title "Slug Configuration File" .

<file:///home/danny/HKMS/transmissions/vocabs/storer>
    slug:cache "c:\\temp\\slug-cache" ;
    slug:impl "com.ldodds.slug.http.ResponseStorer" ;
    dc:description "Stores HTTP responses in a file system cache" ;
    dc:title "ResponseStorer" ;
    a slug:Consumer .

================
File: .babelrc
================
{
  "plugins": ["@babel/syntax-dynamic-import"],
  "presets": [
    [
      "@babel/preset-env",
      {
        "modules": false
      }
    ]
  ]
}

================
File: .gitignore
================
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

================
File: jasmine.json
================
{
    "spec_dir": "tests",
    "spec_files": [
        "**/*[sS]pec.js"
    ],
    "helpers": [
        "helpers/reporter.js"
    ],
    "stopSpecOnExpectationFailure": true,
    "random": false
}

================
File: jsconfig.json
================
{
    "compilerOptions": {
      "target": "ES6",
      "module": "commonjs",
      "allowSyntheticDefaultImports": true,
      "baseUrl": "./",
      "paths": {
        "*": ["node_modules/*", "types/*"]
      }
    },
    "include": ["src/**/*", "run.js", "../trans-apps/applications/git-apps/github"],
    "exclude": ["node_modules", "**/node_modules/*"],
    "typeAcquisition": {
      "include": ["@rdfjs/types", "grapoi"]
    }
  }

================
File: jsdoc.json
================
{
    "source": {
        "include": [
            "src"
        ],
        "exclude": [
            "node_modules"
        ],
        "includePattern": ".+\\.js(doc|x)?$",
        "excludePattern": "(^|\\/|\\\\)_"
    },
    "opts": {
        "verbose": true,
        "recurse": true,
        "destination": "./docs/jsdoc"
    },
    "plugins": [
        "plugins/markdown"
    ]
}

================
File: kiki.txt
================
[api.logger] log level: info

+ ***** Load Config ******
[Transmission : src/applications/globbo/transmission.ttl]
[Services Config : src/applications/globbo/services.ttl]

+ ***** Construct *****
| Create service <http://hyperdata.it/transmissions/s10> of type <http://hyperdata.it/transmissions/DirWalker>
| Create service <http://hyperdata.it/transmissions/s20> of type <http://hyperdata.it/transmissions/CaptureAll>
| Create service <http://hyperdata.it/transmissions/s30> of type <http://hyperdata.it/transmissions/Unfork>
| Create service <http://hyperdata.it/transmissions/s40> of type <http://hyperdata.it/transmissions/WhiteboardToMessage>
| Create service <http://hyperdata.it/transmissions/SM> of type <http://hyperdata.it/transmissions/ShowMessage>
  > Connect #0 [http://hyperdata.it/transmissions/s10] => [http://hyperdata.it/transmissions/s20]
  > Connect #1 [http://hyperdata.it/transmissions/s20] => [http://hyperdata.it/transmissions/s30]
  > Connect #2 [http://hyperdata.it/transmissions/s30] => [http://hyperdata.it/transmissions/s40]
  > Connect #3 [http://hyperdata.it/transmissions/s40] => [http://hyperdata.it/transmissions/SM]

+ http://hyperdata.it/transmissions/globbo

+ ***** Execute *****
| Running : http://hyperdata.it/transmissions/s10 a DirWalker
| Running :  (s10) s20 a CaptureAll
CaptureAll at (s10.s20) s20
| Running :  (s10.s20) s30 a Unfork
 - Unfork terminating pipe
| Running :  (s10) s20 a CaptureAll
| Running :  (s10) s20 a CaptureAll
| Running :  (s10) s20 a CaptureAll
| Running :  (s10) s20 a CaptureAll
CaptureAll at (s10.s20) s20
| Running :  (s10.s20) s30 a Unfork
 - Unfork terminating pipe
CaptureAll at (s10.s20) s20
| Running :  (s10.s20) s30 a Unfork
 - Unfork terminating pipe
CaptureAll at (s10.s20) s20
| Running :  (s10.s20) s30 a Unfork
 - Unfork terminating pipe
| Running :  (s10) s20 a CaptureAll
CaptureAll at (s10.s20) s20
| Running :  (s10.s20) s30 a Unfork
 - Unfork terminating pipe
CaptureAll at (s10.s20) s20
| Running :  (s10.s20) s30 a Unfork
 - Unfork passing message
| Running :  (s10.s20.s30) s40 a WhiteboardToMessage
WhiteboardToMessage at (s10.s20.s30.s40) s40
TypeError: Cannot convert undefined or null to object
    at Function.keys (<anonymous>)
    at file:///home/danny/github-danny/transmissions/src/services/util/WhiteboardToMessage.js:16:20
    at Array.reduce (<anonymous>)
    at WhiteboardToMessage.execute (file:///home/danny/github-danny/transmissions/src/services/util/WhiteboardToMessage.js:14:57)
    at WhiteboardToMessage.executeQueue (file:///home/danny/github-danny/transmissions/src/services/base/Service.js:175:24)
    at WhiteboardToMessage.enqueue (file:///home/danny/github-danny/transmissions/src/services/base/Service.js:144:18)
    at WhiteboardToMessage.receive (file:///home/danny/github-danny/transmissions/src/services/base/Service.js:133:20)
    at Unfork.<anonymous> (file:///home/danny/github-danny/transmissions/src/engine/Connector.js:27:23)
    at Unfork.emit (node:events:518:28)
    at Unfork.execute (file:///home/danny/github-danny/transmissions/src/services/util/Unfork.js:28:18)
0

================
File: LICENSE
================
MIT License

Copyright (c) 2024 Danny Ayers

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: links.md
================
[Main page](https://en.wikipedia.org/wiki/Main_Page)[Contents](https://en.wikipedia.org/wiki/Wikipedia:Contents)[Current events](https://en.wikipedia.org/wiki/Portal:Current_events)[Random article](https://en.wikipedia.org/wiki/Special:Random)[About Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:About)[Contact us](https://en.wikipedia.org/wiki/Wikipedia:Contact_us)[Donate](https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en)[Help](https://en.wikipedia.org/wiki/Help:Contents)[Learn to edit](https://en.wikipedia.org/wiki/Help:Introduction)[Community portal](https://en.wikipedia.org/wiki/Wikipedia:Community_portal)[Recent changes](https://en.wikipedia.org/wiki/Special:RecentChanges)[Upload file](https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard)[
	
	
		
		
	
](https://en.wikipedia.org/wiki/Main_Page)[

Search
	](https://en.wikipedia.org/wiki/Special:Search)[Create account](https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=FOAF)[Log in](https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=FOAF)[ Create account](https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=FOAF)[ Log in](https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=FOAF)[learn more](https://en.wikipedia.org/wiki/Help:Introduction)[Contributions](https://en.wikipedia.org/wiki/Special:MyContributions)[Talk](https://en.wikipedia.org/wiki/Special:MyTalk)[Català](https://ca.wikipedia.org/wiki/FOAF)[Deutsch](https://de.wikipedia.org/wiki/FOAF)[Español](https://es.wikipedia.org/wiki/FOAF)[فارسی](https://fa.wikipedia.org/wiki/%D8%A7%D9%81%E2%80%8C%D8%A7%D9%88%D8%A7%DB%8C%E2%80%8C%D8%A7%D9%81_(%D9%87%D8%B3%D8%AA%DB%8C%E2%80%8C%D8%B4%D9%86%D8%A7%D8%B3%DB%8C))[Français](https://fr.wikipedia.org/wiki/FOAF)[Italiano](https://it.wikipedia.org/wiki/FOAF)[Latviešu](https://lv.wikipedia.org/wiki/FOAF)[Nederlands](https://nl.wikipedia.org/wiki/FOAF)[日本語](https://ja.wikipedia.org/wiki/Friend_of_a_Friend)[Norsk bokmål](https://no.wikipedia.org/wiki/FOAF)[Polski](https://pl.wikipedia.org/wiki/FOAF_(format))[Português](https://pt.wikipedia.org/wiki/FOAF)[Русский](https://ru.wikipedia.org/wiki/FOAF)[Українська](https://uk.wikipedia.org/wiki/FOAF)[Edit links](https://www.wikidata.org/wiki/Special:EntityPage/Q1389366#sitelinks-wikipedia)[Article](https://en.wikipedia.org/wiki/FOAF)[Talk](https://en.wikipedia.org/wiki/Talk:FOAF)[Read](https://en.wikipedia.org/wiki/FOAF)[Edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit)[View history](https://en.wikipedia.org/w/index.php?title=FOAF&action=history)[Read](https://en.wikipedia.org/wiki/FOAF)[Edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit)[View history](https://en.wikipedia.org/w/index.php?title=FOAF&action=history)[What links here](https://en.wikipedia.org/wiki/Special:WhatLinksHere/FOAF)[Related changes](https://en.wikipedia.org/wiki/Special:RecentChangesLinked/FOAF)[Upload file](https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard)[Special pages](https://en.wikipedia.org/wiki/Special:SpecialPages)[Permanent link](https://en.wikipedia.org/w/index.php?title=FOAF&oldid=1165941964)[Page information](https://en.wikipedia.org/w/index.php?title=FOAF&action=info)[Cite this page](https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=FOAF&id=1165941964&wpFormIdentifier=titleform)[Get shortened URL](https://en.wikipedia.org/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFOAF)[Download QR code](https://en.wikipedia.org/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFOAF)[Wikidata item](https://www.wikidata.org/wiki/Special:EntityPage/Q1389366)[Download as PDF](https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf&page=FOAF&action=show-download-screen)[Printable version](https://en.wikipedia.org/w/index.php?title=FOAF&printable=yes)[Wikimedia Commons](https://commons.wikimedia.org/wiki/Category:FOAF)[friend of a friend](https://en.wikipedia.org/wiki/Friend_of_a_friend)[Friend of a Friend (disambiguation)](https://en.wikipedia.org/wiki/Friend_of_a_Friend_(disambiguation))[](https://en.wikipedia.org/wiki/File:FoafLogo.svg)[RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)[OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language)[Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web)[CC BY 1.0](https://en.wikipedia.org/wiki/Creative_Commons_license)[xmlns.com/foaf/spec/](http://xmlns.com/foaf/spec/)[friend of a friend](https://en.wikipedia.org/wiki/Friend_of_a_friend)[machine-readable](https://en.wikipedia.org/wiki/Machine-readable_data)[ontology](https://en.wikipedia.org/wiki/Ontology_(information_science))[persons](https://en.wikipedia.org/wiki/Person)[social networks](https://en.wikipedia.org/wiki/Social_networks)[Resource Description Framework](https://en.wikipedia.org/wiki/Resource_Description_Framework)[Web Ontology Language](https://en.wikipedia.org/wiki/Web_Ontology_Language)[e-mail addresses](https://en.wikipedia.org/wiki/E-mail_address)[telephone number](https://en.wikipedia.org/wiki/Telephone_number)[Facebook](https://en.wikipedia.org/wiki/Facebook)[Jabber ID](https://en.wikipedia.org/wiki/Extensible_Messaging_and_Presence_Protocol)[URI](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier)[Social Semantic Web](https://en.wikipedia.org/wiki/Social_Semantic_Web)[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)[RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)[social web](https://en.wikipedia.org/wiki/Social_web)[clarification needed](https://en.wikipedia.org/wiki/Wikipedia:Please_clarify)[Tim Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee)[semantic web](https://en.wikipedia.org/wiki/Semantic_web)[Giant Global Graph](https://en.wikipedia.org/wiki/Giant_Global_Graph)[Internet](https://en.wikipedia.org/wiki/Internet)[World Wide Web](https://en.wikipedia.org/wiki/World_Wide_Web)[edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit&section=1)[WebID](https://en.wikipedia.org/wiki/WebID)[edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit&section=2)[Live Journal](https://en.wikipedia.org/wiki/Live_Journal)[DeadJournal](https://en.wikipedia.org/wiki/DeadJournal)[My Opera](https://en.wikipedia.org/wiki/My_Opera)[Identi.ca](https://en.wikipedia.org/wiki/Identi.ca)[FriendFeed](https://en.wikipedia.org/wiki/FriendFeed)[WordPress](https://en.wikipedia.org/wiki/WordPress)[TypePad](https://en.wikipedia.org/wiki/TypePad)[Yandex](https://en.wikipedia.org/wiki/Yandex)[Safari](https://en.wikipedia.org/wiki/Safari_(web_browser))[Firefox](https://en.wikipedia.org/wiki/Firefox_(web_browser))[Semantic MediaWiki](https://en.wikipedia.org/wiki/Semantic_MediaWiki)[semantic annotation](https://en.wikipedia.org/wiki/Semantic_annotation)[linked data](https://en.wikipedia.org/wiki/Linked_data)[MediaWiki](https://en.wikipedia.org/wiki/MediaWiki)[content management systems](https://en.wikipedia.org/wiki/Content_management_systems)[edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit&section=3)[Turtle](https://en.wikipedia.org/wiki/Turtle_(syntax))[web resources](https://en.wikipedia.org/wiki/Web_resource)[edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit&section=4)[edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit&section=5)[http://xmlns.com/foaf/0.1/](http://xmlns.com/foaf/0.1/)[http://xmlns.com/foaf/0.1/](http://xmlns.com/foaf/0.1/)[edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit&section=6)[Resource Description Framework](https://en.wikipedia.org/wiki/Resource_Description_Framework)[Web Ontology Language](https://en.wikipedia.org/wiki/Web_Ontology_Language)[Social web](https://en.wikipedia.org/wiki/Social_web)[Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web)[Description of a Career](https://en.wiktionary.org/wiki/DOAC)[Description of a Project](https://en.wikipedia.org/wiki/DOAP)[Semantically-Interlinked Online Communities](https://en.wikipedia.org/wiki/Semantically-Interlinked_Online_Communities)[hCard](https://en.wikipedia.org/wiki/HCard)[vCard](https://en.wikipedia.org/wiki/VCard)[XHTML Friends Network](https://en.wikipedia.org/wiki/XHTML_Friends_Network)[edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit&section=7)[XML Watch: Finding friends with XML and RDF](https://web.archive.org/web/20091223003446/http://www.ibm.com/developerworks/xml/library/x-foaf.html)[IBM DeveloperWorks](https://en.wikipedia.org/wiki/IBM_DeveloperWorks)[XML Watch: Support online communities with FOAF](https://web.archive.org/web/20100307223814/http://www.ibm.com/developerworks/xml/library/x-foaf2.html)[IBM DeveloperWorks](https://en.wikipedia.org/wiki/IBM_DeveloperWorks)["Giant Global Graph"](https://web.archive.org/web/20160713021037/http://dig.csail.mit.edu/breadcrumbs/node/215)[the original](http://dig.csail.mit.edu/breadcrumbs/node/215)["LiveJournal FOAF"](https://web.archive.org/web/20100118151037/http://community.livejournal.com/ljfoaf)[the original](http://community.livejournal.com/ljfoaf)["Known FOAF data providers"](https://web.archive.org/web/20100226072731/http://wiki.foaf-project.org/w/DataSources)[the original](http://wiki.foaf-project.org/w/DataSources)["press release on the social networking support"](http://company.yandex.com/press_center/press_releases/2008/2008-08-15.xml)["FOAF Support in Safari RSS"](http://ejohn.org/blog/foaf-support-in-safari-rss/)["Semantic Radar plugin for the Firefox browser"](https://web.archive.org/web/20140108014347/https://addons.mozilla.org/en-US/firefox/addon/semantic-radar/)[the original](https://addons.mozilla.org/en-US/firefox/addon/semantic-radar/)["FOAF support module for Perl"](https://metacpan.org/pod/XML::FOAF)["FOAF+SSL authentication support for Perl"](https://metacpan.org/pod/Web::ID)[http://drupal.org/project/foaf](https://drupal.org/project/foaf)[Drupal](https://en.wikipedia.org/wiki/Drupal)["FOAF Vocabulary Specification 0.99"](http://xmlns.com/foaf/spec/20140114.html)[Archived](https://web.archive.org/web/20220303180551/http://xmlns.com/foaf/spec/20140114.html)[edit](https://en.wikipedia.org/w/index.php?title=FOAF&action=edit&section=8)[Official website](http://www.foaf-project.org)[Archived](https://web.archive.org/web/20211023122305/http://www.foaf-project.org/)[Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine)[FOAF dataset](http://ebiquity.umbc.edu/resource/html/id/82/)[FOAF-search - a search engine for FOAF data](https://web.archive.org/web/20181130195340/https://www.foaf-search.net/)[Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine)[v](https://en.wikipedia.org/wiki/Template:Semantic_Web)[t](https://en.wikipedia.org/wiki/Template_talk:Semantic_Web)[e](https://en.wikipedia.org/wiki/Special:EditPage/Template:Semantic_Web)[Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web)[Databases](https://en.wikipedia.org/wiki/Database)[Hypertext](https://en.wikipedia.org/wiki/Hypertext)[Internet](https://en.wikipedia.org/wiki/Internet)[Ontologies](https://en.wikipedia.org/wiki/Ontology_(computer_science))[Semantics](https://en.wikipedia.org/wiki/Semantics_(computer_science))[Semantic networks](https://en.wikipedia.org/wiki/Semantic_network)[World Wide Web](https://en.wikipedia.org/wiki/World_Wide_Web)[Dataspaces](https://en.wikipedia.org/wiki/Dataspaces)[Hyperdata](https://en.wikipedia.org/wiki/Hyperdata)[Linked data](https://en.wikipedia.org/wiki/Linked_data)[Rule-based systems](https://en.wikipedia.org/wiki/Rule-based_system)[Semantic analytics](https://en.wikipedia.org/wiki/Semantic_analytics)[Semantic broker](https://en.wikipedia.org/wiki/Semantic_broker)[Semantic computing](https://en.wikipedia.org/wiki/Semantic_computing)[Semantic mapper](https://en.wikipedia.org/wiki/Semantic_mapper)[Semantic matching](https://en.wikipedia.org/wiki/Semantic_matching)[Semantic publishing](https://en.wikipedia.org/wiki/Semantic_publishing)[Semantic reasoner](https://en.wikipedia.org/wiki/Semantic_reasoner)[Semantic search](https://en.wikipedia.org/wiki/Semantic_search)[Semantic service-oriented architecture](https://en.wikipedia.org/wiki/Semantic_service-oriented_architecture)[Semantic wiki](https://en.wikipedia.org/wiki/Semantic_wiki)[Solid](https://en.wikipedia.org/wiki/Solid_(web_decentralization_project))[Collective intelligence](https://en.wikipedia.org/wiki/Collective_intelligence)[Description logic](https://en.wikipedia.org/wiki/Description_logic)[Folksonomy](https://en.wikipedia.org/wiki/Folksonomy)[Geotagging](https://en.wikipedia.org/wiki/Geotagging)[Information architecture](https://en.wikipedia.org/wiki/Information_architecture)[Knowledge extraction](https://en.wikipedia.org/wiki/Knowledge_extraction)[Knowledge management](https://en.wikipedia.org/wiki/Knowledge_management)[Knowledge representation and reasoning](https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning)[Library 2.0](https://en.wikipedia.org/wiki/Library_2.0)[Digital library](https://en.wikipedia.org/wiki/Digital_library)[Digital humanities](https://en.wikipedia.org/wiki/Digital_humanities)[Metadata](https://en.wikipedia.org/wiki/Metadata)[References](https://en.wikipedia.org/wiki/Reference_(computer_science))[Topic map](https://en.wikipedia.org/wiki/Topic_map)[Web 2.0](https://en.wikipedia.org/wiki/Web_2.0)[Web engineering](https://en.wikipedia.org/wiki/Web_engineering)[Web Science Trust](https://en.wikipedia.org/wiki/Web_Science_Trust)[HTTP](https://en.wikipedia.org/wiki/HTTP)[IRI](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier)[URI](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier)[RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)[triples](https://en.wikipedia.org/wiki/Semantic_triple)[RDF/XML](https://en.wikipedia.org/wiki/RDF/XML)[JSON-LD](https://en.wikipedia.org/wiki/JSON-LD)[Turtle](https://en.wikipedia.org/wiki/Turtle_(syntax))[TriG](https://en.wikipedia.org/wiki/TriG_(syntax))[Notation3](https://en.wikipedia.org/wiki/Notation3)[N-Triples](https://en.wikipedia.org/wiki/N-Triples)[TriX](https://en.wikipedia.org/wiki/TriX_(serialization_format))[RRID](https://en.wikipedia.org/wiki/Research_Resource_Identifier)[SPARQL](https://en.wikipedia.org/wiki/SPARQL)[XML](https://en.wikipedia.org/wiki/XML)[Semantic HTML](https://en.wikipedia.org/wiki/Semantic_HTML)[Common Logic](https://en.wikipedia.org/wiki/Common_Logic)[OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language)[RDFS](https://en.wikipedia.org/wiki/RDF_Schema)[Rule Interchange Format](https://en.wikipedia.org/wiki/Rule_Interchange_Format)[Semantic Web Rule Language](https://en.wikipedia.org/wiki/Semantic_Web_Rule_Language)[ALPS](https://en.wikipedia.org/w/index.php?title=Application-Level_Profile_Semantics_(ALPS)&action=edit&redlink=1)[SHACL](https://en.wikipedia.org/wiki/SHACL)[eRDF](https://en.wikipedia.org/wiki/Embedded_RDF)[GRDDL](https://en.wikipedia.org/wiki/GRDDL)[Microdata](https://en.wikipedia.org/wiki/Microdata_(HTML))[Microformats](https://en.wikipedia.org/wiki/Microformat)[RDFa](https://en.wikipedia.org/wiki/RDFa)[SAWSDL](https://en.wikipedia.org/wiki/SAWSDL)[Facebook Platform](https://en.wikipedia.org/wiki/Facebook_Platform)[DOAP](https://en.wikipedia.org/wiki/DOAP)[Dublin Core](https://en.wikipedia.org/wiki/Dublin_Core)[Schema.org](https://en.wikipedia.org/wiki/Schema.org)[SIOC](https://en.wikipedia.org/wiki/Semantically_Interlinked_Online_Communities)[SKOS](https://en.wikipedia.org/wiki/Simple_Knowledge_Organization_System)[hAtom](https://en.wikipedia.org/wiki/HAtom)[hCalendar](https://en.wikipedia.org/wiki/HCalendar)[hCard](https://en.wikipedia.org/wiki/HCard)[hProduct](https://en.wikipedia.org/wiki/HProduct)[hRecipe](https://en.wikipedia.org/wiki/HRecipe)[hReview](https://en.wikipedia.org/wiki/HReview)[v](https://en.wikipedia.org/wiki/Template:Social_networking)[t](https://en.wikipedia.org/wiki/Template_talk:Social_networking)[e](https://en.wikipedia.org/wiki/Special:EditPage/Template:Social_networking)[Social networks](https://en.wikipedia.org/wiki/Social_network)[social media](https://en.wikipedia.org/wiki/Social_media)[Personal](https://en.wikipedia.org/wiki/Personal_network)[Professional](https://en.wikipedia.org/wiki/Professional_network_service)[Sexual](https://en.wikipedia.org/wiki/Sexual_network)[Value](https://en.wikipedia.org/wiki/Value_network)[Clique](https://en.wikipedia.org/wiki/Clique)[Adolescent](https://en.wikipedia.org/wiki/Adolescent_clique)[Corporate social media](https://en.wikipedia.org/wiki/Corporate_social_media)[Distributed social network](https://en.wikipedia.org/wiki/Distributed_social_network)[list](https://en.wikipedia.org/wiki/Comparison_of_software_and_protocols_for_distributed_social_networking)[Enterprise social networking](https://en.wikipedia.org/wiki/Enterprise_social_networking)[Enterprise social software](https://en.wikipedia.org/wiki/Enterprise_social_software)[Mobile social network](https://en.wikipedia.org/wiki/Mobile_social_network)[Personal knowledge networking](https://en.wikipedia.org/wiki/Personal_knowledge_networking)[Services](https://en.wikipedia.org/wiki/Social_networking_service)[List of social networking services](https://en.wikipedia.org/wiki/List_of_social_networking_services)[List of virtual communities with more than 1 million users](https://en.wikipedia.org/wiki/List_of_virtual_communities_with_more_than_1_million_users)[Ambient awareness](https://en.wikipedia.org/wiki/Ambient_awareness)[Assortative mixing](https://en.wikipedia.org/wiki/Assortative_mixing)[Attention inequality](https://en.wikipedia.org/wiki/Attention_inequality)[Interpersonal bridge](https://en.wikipedia.org/wiki/Bridge_(interpersonal))[Organizational network analysis](https://en.wikipedia.org/wiki/Organizational_network_analysis)[Small-world experiment](https://en.wikipedia.org/wiki/Small-world_experiment)[Social aspects of television](https://en.wikipedia.org/wiki/Social_aspects_of_television)[Social capital](https://en.wikipedia.org/wiki/Social_capital)[Social data revolution](https://en.wikipedia.org/wiki/Social_data_revolution)[Social exchange theory](https://en.wikipedia.org/wiki/Social_exchange_theory)[Social identity theory](https://en.wikipedia.org/wiki/Social_identity_theory)[Social media and psychology](https://en.wikipedia.org/wiki/Social_media_and_psychology)[Social media intelligence](https://en.wikipedia.org/wiki/Social_media_intelligence)[Social media mining](https://en.wikipedia.org/wiki/Social_media_mining)[Social media optimization](https://en.wikipedia.org/wiki/Social_media_optimization)[Social network analysis](https://en.wikipedia.org/wiki/Social_network_analysis)[Social web](https://en.wikipedia.org/wiki/Social_web)[Structural endogamy](https://en.wikipedia.org/wiki/Structural_endogamy)[Virtual collective consciousness](https://en.wikipedia.org/wiki/Virtual_collective_consciousness)[Account verification](https://en.wikipedia.org/wiki/Account_verification)[Aggregation](https://en.wikipedia.org/wiki/Social_network_aggregation)[Change detection](https://en.wikipedia.org/wiki/Social_network_change_detection)[Blockmodeling](https://en.wikipedia.org/wiki/Blockmodeling)[Collaboration graph](https://en.wikipedia.org/wiki/Collaboration_graph)[Collaborative consumption](https://en.wikipedia.org/wiki/Collaborative_consumption)[Giant Global Graph](https://en.wikipedia.org/wiki/Giant_Global_Graph)[Lateral communication](https://en.wikipedia.org/wiki/Lateral_communication)[Reputation system](https://en.wikipedia.org/wiki/Reputation_system)[Social bot](https://en.wikipedia.org/wiki/Social_bot)[Social graph](https://en.wikipedia.org/wiki/Social_graph)[Social media analytics](https://en.wikipedia.org/wiki/Social_media_analytics)[Social network analysis software](https://en.wikipedia.org/wiki/Social_network_analysis_software)[Social networking potential](https://en.wikipedia.org/wiki/Social_networking_potential)[Social television](https://en.wikipedia.org/wiki/Social_television)[Structural cohesion](https://en.wikipedia.org/wiki/Structural_cohesion)[Affinity fraud](https://en.wikipedia.org/wiki/Affinity_fraud)[Attention economy](https://en.wikipedia.org/wiki/Attention_economy)[Collaborative finance](https://en.wikipedia.org/wiki/Collaborative_finance)[Creator economy](https://en.wikipedia.org/wiki/Creator_economy)[Influencer marketing](https://en.wikipedia.org/wiki/Influencer_marketing)[Narrowcasting](https://en.wikipedia.org/wiki/Narrowcasting)[Sharing economy](https://en.wikipedia.org/wiki/Sharing_economy)[Social commerce](https://en.wikipedia.org/wiki/Social_commerce)[Social sorting](https://en.wikipedia.org/wiki/Social_sorting)[Viral marketing](https://en.wikipedia.org/wiki/Viral_marketing)[Algorithmic radicalization](https://en.wikipedia.org/wiki/Algorithmic_radicalization)[Community recognition](https://en.wikipedia.org/wiki/Community_recognition)[Complex contagion](https://en.wikipedia.org/wiki/Complex_contagion)[Computer addiction](https://en.wikipedia.org/wiki/Computer_addiction)[Consequential strangers](https://en.wikipedia.org/wiki/Consequential_strangers)[Friend of a friend](https://en.wikipedia.org/wiki/Friend_of_a_friend)[Friending and following](https://en.wikipedia.org/wiki/Friending_and_following)[Friendship paradox](https://en.wikipedia.org/wiki/Friendship_paradox)[Influence-for-hire](https://en.wikipedia.org/wiki/Influence-for-hire)[Internet addiction](https://en.wikipedia.org/wiki/Internet_addiction)[Information overload](https://en.wikipedia.org/wiki/Information_overload)[Overchoice](https://en.wikipedia.org/wiki/Overchoice)[Six degrees of separation](https://en.wikipedia.org/wiki/Six_degrees_of_separation)[Social media addiction](https://en.wikipedia.org/wiki/Social_media_addiction)[Social media and suicide](https://en.wikipedia.org/wiki/Social_media_and_suicide)[Social invisibility](https://en.wikipedia.org/wiki/Social_invisibility)[Social network game](https://en.wikipedia.org/wiki/Social_network_game)[Suicide and the Internet](https://en.wikipedia.org/wiki/Suicide_and_the_Internet)[Tribe](https://en.wikipedia.org/wiki/Tribe_(internet))[Viral phenomenon](https://en.wikipedia.org/wiki/Viral_phenomenon)[Friendship recession](https://en.wikipedia.org/wiki/Friendship_recession)[Peer pressure](https://en.wikipedia.org/wiki/Peer_pressure)[Researchers](https://en.wikipedia.org/wiki/List_of_social_network_researchers)[User profile](https://en.wikipedia.org/wiki/User_profile)[Online identity](https://en.wikipedia.org/wiki/Online_identity)[Persona](https://en.wikipedia.org/wiki/Persona_(user_experience))[Social profiling](https://en.wikipedia.org/wiki/Social_profiling)[Viral messages](https://en.wikipedia.org/wiki/Viral_messages)[Virtual community](https://en.wikipedia.org/wiki/Virtual_community)[https://en.wikipedia.org/w/index.php?title=FOAF&oldid=1165941964](https://en.wikipedia.org/w/index.php?title=FOAF&oldid=1165941964)[Categories](https://en.wikipedia.org/wiki/Help:Category)[Ontology (information science)](https://en.wikipedia.org/wiki/Category:Ontology_(information_science))[Semantic Web](https://en.wikipedia.org/wiki/Category:Semantic_Web)[2000 software](https://en.wikipedia.org/wiki/Category:2000_software)[Articles with short description](https://en.wikipedia.org/wiki/Category:Articles_with_short_description)[Short description matches Wikidata](https://en.wikipedia.org/wiki/Category:Short_description_matches_Wikidata)[All articles with unsourced statements](https://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements)[Articles with unsourced statements from April 2017](https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_April_2017)[Wikipedia articles needing clarification from April 2017](https://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_clarification_from_April_2017)[Webarchive template wayback links](https://en.wikipedia.org/wiki/Category:Webarchive_template_wayback_links)[Creative Commons Attribution-ShareAlike License 4.0](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License)[](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License)[Terms of Use](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use)[Privacy Policy](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy)[Wikimedia Foundation, Inc.](https://www.wikimediafoundation.org/)[Privacy policy](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy)[About Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:About)[Disclaimers](https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer)[Contact Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Contact_us)[Code of Conduct](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct)[Developers](https://developer.wikimedia.org)[Statistics](https://stats.wikimedia.org/#/en.wikipedia.org)[Cookie statement](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement)[Mobile view](https://en.m.wikipedia.org/w/index.php?title=FOAF&mobileaction=toggle_view_mobile)[](https://wikimediafoundation.org/)[](https://www.mediawiki.org/)~done~[http://xmlns.com/foaf/spec/20140114.html](http://xmlns.com/foaf/spec/20140114.html)[rdf](http://xmlns.com/foaf/spec/20140114.rdf)[http://xmlns.com/foaf/spec/](http://xmlns.com/foaf/spec/)[rdf](http://xmlns.com/foaf/spec/index.rdf)[http://xmlns.com/foaf/spec/20100809.html](http://xmlns.com/foaf/spec/20100809.html)[rdf](http://xmlns.com/foaf/spec/20100809.rdf)[Dan Brickley](mailto:danbri@danbri.org)[Libby Miller](mailto:libby@nicecupoftea.org)[foaf-dev@lists.foaf-project.org](http://lists.foaf-project.org/)[RDF
    and Semantic Web developer community](http://www.w3.org/2001/sw/interest/)[](http://creativecommons.org/licenses/by/1.0/)[Creative Commons Attribution License](http://creativecommons.org/licenses/by/1.0/)[RDF](http://www.w3.org/RDF/)[FOAF project](http://www.foaf-project.org/)[RDFS/OWL](http://xmlns.com/foaf/spec/index.rdf)[per-term](http://xmlns.com/foaf/doc/)[multilingual translations](http://svn.foaf-project.org/foaftown/foaf18n/)[direct link](http://xmlns.com/foaf/spec/index.rdf)[content negotiation](http://en.wikipedia.org/wiki/Content_negotiation)[namespace URI](http://xmlns.com/foaf/0.1/)[foaf-dev@lists.foaf-project.org](mailto:foaf-dev@lists.foaf-project.org)[public archives](http://lists.foaf-project.org)[FOAF mailing list](mailto:foaf-dev@lists.foaf-project.org)[FOAF website](http://www.foaf-project.org/)[Dublin Core](http://www.dublincore.org/)[SKOS](http://www.w3.org/2004/02/skos/)[DOAP](http://trac.usefulinc.com/doap)[SIOC](http://sioc-project.org/)[Org vocabulary](http://www.epimorphics.com/public/vocabulary/org.html)[Bio vocabulary](http://vocab.org/bio/0.1/.html)[Portable Contacts](http://portablecontacts.net/)[W3C Social Web group](http://www.w3.org/2005/Incubator/socialweb/)[W3 future directions](http://www.w3.org/Talks/WWW94Tim/)[Giant Global Graph](http://dig.csail.mit.edu/breadcrumbs/node/215)[foaf](http://www.w3.org/People/Berners-Lee/card)[Semantic Web](http://www.w3.org/2001/sw/)[SPARQL](http://www.w3.org/TR/rdf-sparql-query/)[SKOS](http://www.w3.org/2004/02/skos/)[GRDDL](http://www.w3.org/2001/sw/grddl-wg/)[RDFa](http://www.w3.org/TR/xhtml-rdfa-primer/)[Linked 
  Data](http://www.w3.org/DesignIssues/LinkedData.html)[FOAF namespace
  document](http://xmlns.com/foaf/0.1/)[XML
  Watch: Finding friends with XML and RDF](http://www-106.ibm.com/developerworks/xml/library/x-foaf.html)[with image metadata](http://rdfweb.org/2002/01/photo/)[co-depiction](http://rdfweb.org/2002/01/photo/)[FOAF-a-matic](http://www.ldodds.com/foaf/foaf-a-matic.html)[FOAF project home page](http://www.foaf-project.org)[alt.folklore.urban archive](http://www.urbanlegends.com/)[snopes.com](http://www.snopes.com/)[ISO
  Standardisation](http://www.iso.ch/iso/en/ISOOnline.openerpage)[W3C](http://www.w3.org/)[Process](http://www.w3.org/Consortium/Process/)[Open Source](http://www.opensource.org/)[Free Software](http://www.gnu.org/philosophy/free-sw.html)[Jabber
  JEPs](http://www.jabber.org/jeps/jep-0001.html)[Resource Description Framework](http://www.w3.org/RDF/)[RDF](http://www.w3.org/RDF/)[Semantic Web](http://www.w3.org/2001/sw/)[Semantic Web](http://www.w3.org/2001/sw/)[Dublin Core](http://dublincore.org/)[foaf-a-matic](http://www.ldodds.com/foaf/foaf-a-matic.html)[FOAF
  autodiscovery](http://web.archive.org/web/20040416181630/rdfweb.org/mt/foaflog/archives/000041.html)[RDF/XML](http://xmlns.com/foaf/spec/index.rdf)[issue tracker](http://wiki.foaf-project.org/w/IssueTracker)[OWL](http://www.w3.org/2001/sw/WebOnt)[GRDDL](http://www.w3.org/2004/01/rdxh/spec)[Jabber](http://www.jabber.org/)[AIM](http://www.aim.com/)[MSN](http://chat.msn.com/)[ICQ](http://web.icq.com/icqchat/)[Yahoo!](http://chat.yahoo.com/)[MSN](http://chat.msn.com/)[Freenode](http://www.freenode.net/)[Amazon](http://www.amazon.com/)[eBay](http://www.ebay.com/)[PayPal](http://www.paypal.com/)[thinkgeek](http://www.thinkgeek.com/)[EverQuest](http://everquest.station.sony.com/)[Xbox live](http://www.xbox.com/live/)[Neverwinter Nights](http://nwn.bioware.com/)[Relationship module](http://www.perceive.net/schemas/20021119/relationship/)[scutters](http://wiki.foaf-project.org/w/ScutterSpec)[UsingDublinCoreCreator](http://wiki.foaf-project.org/w/UsingDublinCoreCreator)[RFC 2368](http://ftp.ics.uci.edu/pub/ietf/uri/rfc2368.txt)[Wikipedia](http://www.wikipedia.org/)[NNDB](http://www.nndb.com/)[AIM](http://www.aim.com/)[iChat](http://www.apple.com/macosx/what-is-macosx/ichat.html)[Apple](http://www.apple.com/)[geo-positioning vocabulary](http://www.w3.org/2003/01/geo/wgs84_pos#)[GeoInfo](http://esw.w3.org/topic/GeoInfo)[GeoOnion vocab](http://esw.w3.org/topic/GeoOnion)[UsingContactNearestAirport](http://wiki.foaf-project.org/w/UsingContactNearestAirport)[Co-Depiction](http://rdfweb.org/2002/01/photo/)['Annotating Images With SVG'](http://www.jibbering.com/svg/AnnotateImage.html)[issue tracker](http://wiki.foaf-project.org/w/NameVocabIssue)[issue tracker](http://wiki.foaf-project.org/w/NameVocabIssue)[SKOS](http://www.w3.org/2004/02/skos/)[In SKOS](http://www.w3.org/TR/2005/WD-swbp-skos-core-guide-20050510/#secmodellingrdf)[2005 discussion](http://www.w3.org/TR/2005/WD-swbp-skos-core-guide-20051102/#secopen)[TDB URI scheme](http://larry.masinter.net/duri.html)[original goals](http://www.foaf-project.org/original-intro)[foaf-dev](http://lists.foaf-project.org/mailman/listinfo/foaf-dev)[issue tracker](http://wiki.foaf-project.org/w/NameVocabIssue)[icq chat](http://web.icq.com/icqchat/)[What is ICQ?](http://www.icq.com/products/whatisicq.html)[About Us](http://company.icq.com/info/)[RDF](http://www.w3.org/RDF/)[CPAN](http://www.cpan.org/)[Jabber](http://www.jabber.org/)[Jabber](http://www.jabber.org/)[issue tracker](http://wiki.foaf-project.org/w/NameVocabIssue)[Edd Dumbill's 
documentation](http://usefulinc.com/foaf/)[FOAF-based whitelists](http://www.w3.org/2001/12/rubyrdf/util/foafwhite/intro.html)[in Sam Ruby's 
weblog entry](http://www.intertwingly.net/blog/1545.html)[Windows Live Messenger](http://en.wikipedia.org/wiki/Windows_Live_Messenger)[Microsoft mesenger](http://download.live.com/messenger)[Windows Live ID](http://en.wikipedia.org/wiki/Windows_Live_ID)[this article](http://www.teamtechnology.co.uk/tt/t-articl/mb-simpl.htm)[Cory Caplinger's summary table](http://webspace.webring.com/people/cl/lifexplore/mbintro.htm)[FOAF Myers Briggs addition](http://web.archive.org/web/20080802184922/http://rdfweb.org/mt/foaflog/archives/000004.html)[issue tracker](http://wiki.foaf-project.org/w/IssueTracker)[indirect identifier](http://www.w3.org/TR/webarch/#indirect-identification)[OpenID](http://openid.net/specs/openid-authentication-1_1.html)[delegation model](http://openid.net/specs/openid-authentication-1_1.html#delegating_authentication)[technique](http://xmlns.com/foaf/spec/#sec-autodesc)[History of the 
Finger Protocol](http://www.rajivshah.com/Case_Studies/Finger/Finger.htm)[discussions](http://rdfweb.org/mt/foaflog/archives/2004/02/12/20.07.32/)[PayPal](http://www.paypal.com/)[FOAF Issue Tracker](http://wiki.foaf-project.org/w/IssueTracker)[Yahoo! Chat](http://chat.yahoo.com/)[Yahoo! Groups](http://www.yahoogroups.com/)[BirthdayIssue](http://wiki.foaf-project.org/w/BirthdayIssue)[Wikipedia entry](http://en.wikipedia.org/wiki/Geek_Code)[issue tracker](http://wiki.foaf-project.org/w/NameVocabIssue)[issue 
tracker](http://wiki.foaf-project.org/w/IssueTracker)[SemWeb Vocab Status Ontology](http://www.w3.org/2003/06/sw-vocab-status/note)[W3CBasic Geo Vocabulary](http://www.w3.org/2003/01/geo/)[W3C's site](http://www.w3.org/2001/sw/)[more background on URIs](http://www.w3.org/TR/webarch/#identification)[linked data](http://www.w3.org/DesignIssues/LinkedData)[SKOS](http://www.w3.org/2004/02/skos/)[Dublin Core terms](http://dublincore.org/documents/dcmi-terms/)[dct:Agent](http://dublincore.org/documents/dcmi-terms/#classes-Agent)[dct:creator](http://dublincore.org/documents/dcmi-terms/#terms-creator)[recent](http://www.w3.org/TR/wordnet-rdf/)[SIOC](http://rdfs.org/sioc/ns#)[SIOC](http://www.sioc-project.org/)[rdfweb-dev](http://rdfweb.org/pipermail/rdfweb-dev/)[#foaf](http://rdfweb.org/irc/)[FoafExplorer](http://xml.mfd-consult.dk/foaf/explorer/)[Web View](http://eikeon.com/foaf/)[foaf-a-matic](http://www.ldodds.com/foaf/foaf-a-matic.html)[Ecademy](http://www.ecademy.com/)[TypePad](http://www.typepad.com/)[many](http://conferences.oreillynet.com/cs/et2003/view/e_sess/3633)[explaining](http://hackdiary.com/)[in Japanese](http://kanzaki.com/docs/sw/foaf.html)[Spanish](http://f14web.com.ar/inkel/2003/01/27/foaf.html)[Chris Schmidt](http://crschmidt.net/)[spec generation](http://xmlns.com/foaf/0.1/specgen.py)[cool hacks](http://crschmidt.net/semweb/)[FOAF Logo](http://iandavis.com/2006/foaf-icons/)[years ago](http://www.w3.org/History/1989/proposal.html)[schema.org](http://schema.org/)[Person](http://schema.org/Person)[ImageObject](http://schema.org/ImageObject)[CreativeWork](http://schema.org/CreativeWork)[Bio vocabulary](http://vocab.org/bio/0.1/.html)[0.97](http://xmlns.com/foaf/spec/20100101.html)[0.96](http://xmlns.com/foaf/spec/20091215.html)[Portable Contacts](http://portablecontacts.net/)

================
File: package.json
================
{
  "type": "module",
  "version": "1.0.0",
  "description": "Transmissions",
  "name": "transmissions",
  "scripts": {
    "test": "jasmine --config=jasmine.json --reporter=tests/helpers/reporter.js",
    "docs": "jsdoc -c jsdoc.json",
    "build": "webpack --mode=production --node-env=production",
    "build:dev": "webpack --mode=development",
    "build:prod": "webpack --mode=production --node-env=production",
    "rp": "repopack --verbose -c repopack.config.json .",
    "watch": "webpack --watch",
    "serve": "webpack serve"
  },
  "devDependencies": {
    "@babel/core": "^7.23.7",
    "@babel/preset-env": "^7.23.8",
    "autoprefixer": "^10.4.17",
    "babel-loader": "^9.1.3",
    "chai": "^5.0.3",
    "css-loader": "^6.9.1",
    "html-webpack-plugin": "^5.6.0",
    "jasmine": "^5.1.0",
    "jasmine-browser-runner": "^2.3.0",
    "jasmine-core": "^5.1.1",
    "jasmine-spec-reporter": "^7.0.0",
    "jsdoc": "^4.0.2",
    "mini-css-extract-plugin": "^2.7.7",
    "postcss": "^8.4.33",
    "postcss-loader": "^8.0.0",
    "prettier": "^3.2.4",
    "style-loader": "^3.3.4",
    "webpack": "^5.90.0",
    "webpack-cli": "^5.1.4",
    "webpack-dev-server": "^4.15.1",
    "workbox-webpack-plugin": "^7.0.0"
  },
  "dependencies": {
    "@dotenvx/dotenvx": "^1.14.2",
    "@rdfjs/formats": "^4.0.0",
    "axios": "^1.6.8",
    "cheerio": "^1.0.0-rc.12",
    "d3": "^7.9.0",
    "jsdom": "^25.0.0",
    "marked": "^12.0.1",
    "marked-code-format": "^1.1.6",
    "marked-custom-heading-id": "^2.0.10",
    "marked-footnote": "^1.2.4",
    "markmap-lib": "^0.17.0",
    "markmap-render": "^0.17.0",
    "markmap-toolbar": "^0.17.0",
    "markmap-view": "^0.17.0",
    "nunjucks": "^3.2.4",
    "queue": "^7.0.0",
    "rdf-ext": "^2.5.2",
    "rdf-utils-fs": "^3.0.0",
    "yargs": "^17.7.2"
  }
}

================
File: postcss.config.js
================
module.exports = {


  plugins: [["autoprefixer"]],
};

================
File: README.md
================
# transmissions

After _No Code_ and _Lo Code_ comes _Marginally Less Code_

**Transmissions** is a micro-framework intended to simplify construction of small pipeliney data processing applications in JavaScript (assuming you are already familiar with JavaScript and RDF).

The code is in active development, ie. **not stable**, subject to arbitrary changes.

A bit like `make` or a `package.json` builder. But much harder work (and fun).

Applications are defined in several places, the bits of interest are eg. Postcraft's [transmissions.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/transmissions.ttl) and [services.ttl](https://github.com/danja/transmissions/blob/main/src/applications/postcraft/services.ttl).
The former defines the flow, the latter config of the services (under [src/services](https://github.com/danja/transmissions/tree/main/src/services)). The runtime instance of the application is given in the target [manifest.ttl](https://github.com/danja/postcraft/blob/main/danny.ayers.name/manifest.ttl).

### Installation etc.

This is not ready yet. But if you really must...

Make a fresh dir. Clone this repo and [Postcraft](https://github.com/danja/postcraft) into it.

```
cd transmissions
npm i
```

This may or may not work :

```
npm run test
```

Then if you do :

```
./trans postcraft /home/danny/github-danny/postcraft/danny.ayers.name
```

it may build a site (my blog - this is dogfooding to the max) under `public/home`

```
./trans
```

on its own should list the applications available. Most of these won't work, the code has been shapeshifting a lot.

### Status

**2024-09-02** Getting used as a serrrrriously over-engineered, feature-lacking static site builder, proof of concept is [Postcraft](https://github.com/danja/postcraft), as evinced by my [blog](https://danny.ayers.name/) (where, for now at least you will find update on this). But it mostly works as intended. Docs lagging. But now I have a documentation engine...

Documentation will be lagging behind code, be incomplete and out of date.

**2024-03-24** : a couple of simple data processing pipelines working and wired up as Jasmine e2e tests in place; started to develop actually useful pipelines for [foaf-archive](https://github.com/danja/foaf-archive) project

## Motivation

This was prompted by something of a ~~perfect storm~~ serendipitous convergence of ideas, requirements and environment.

- I had the realization that much of the software I want to write in the near future has the same basic high level **architecture** : pipeline-like, ie. a series of steps each of which transform data from one form into another ([Wikipedia](<https://en.wikipedia.org/wiki/Pipeline_(software)>)).
- **Pipelines** are approachable using concepts from Design Patterns like [Dependency Injection](https://en.wikipedia.org/wiki/Dependency_injection), [Inversion of control](https://en.wikipedia.org/wiki/Inversion_of_control) etc. A key benefit of such patterns is that, because the components are loosely coupled, they can largely be developed in isolation as small standalone units before integration into a larger application.
- **Large Language Models** currently struggle with large codebases and complex architectures (mostly due to [context window](https://en.wikipedia.org/wiki/Large_language_model#Prompt_engineering,_attention_mechanism,_and_context_window) limitations). But they are very good at building small, simple, self-contained units of code.

reused in different contexts.

_I'm writing this with GitHub Copilot enabled for markdown, which can be helpful, but sometimes it goes off on one... After the first point above it offered :_

```
This is true of a blog engine, a personal knowledge management system, a chatbot, a mail archive miner, a web scraper, a data visualizer, a data importer/exporter, a data transformer, a data validator, a data filter, a data aggregator, a data splitter, a data joiner, a data normalizer, a data summarizer, a data classifier, a data indexer, a data search engine, a data storage system, a data retrieval system, a data synchronization system, a data backup system, a data migration system, a data versioning system, a data sharing system, a data access control system, a data privacy system, a data security system, a data integrity system, a data quality system, a data governance system, a data provenance system, a data lineage system, a data lifecycle system, a data archiving system, a data retention system, a data deletion system, a data recovery system, a data discovery system, a data exploration system, a data analysis system, a data visualization system, a data reporting system, a data monitoring system, a data alerting system, a data forecasting system, a data prediction system, a data recommendation system, a data personalization system, a data optimization system, a data simulation system, a data modeling system, a data inference system, a data learning system, a data reasoning system, a data decision system, a data planning system, a data execution system, a data monitoring system, a data feedback system, a data control system, a data automation system, a data orchestration system, a data integration system, a data federation system, a data interoperability system, a data exchange system, a data transformation system, a data migration system, a data synchronization system, a data replication system, a data federation system, a data virtualization system, a data streaming system, a data batch system, a data real-time system, a data near-real-time system, a data historical system, a data temporal system, a data spatial system, a data geospatial system, a data network system, a data graph system, a data tree system, a data list system, a data set system, a data table system, a data document system, a data record system, a data entity system, a data attribute system, a data value system, a data type system, a data format system, a data encoding system, a data serialization
```

_... which is a little over the top, but it's a fair list of things that might be helped by a micro-framework like this. Remarkably the first 5 are pretty much my immediate intended uses for this._

**Docs below need updating**

for later :

The [Community Solid Server](https://github.com/CommunitySolidServer/CommunitySolidServer) uses [Components.js](https://componentsjs.readthedocs.io/en/latest/) to specify how modules and components need to be wired together at runtime. Components.js is a dependency injection framework for JavaScript applications.

## What it is

A low-level system for data processing pipelines/networks. Wherever convenient functionality will be defined declaratively with JSON-LD configuration files.

Dependency injection is used internally to allow loose coupling of components.

## What it isn't

There are several sophisticated frameworks for building interfaces between software applications and creating data processing networks. NodeRed, NoFlo etc. This is not one of them. This is much more basic and bare bones, down in the details.

See also [David Booth](https://github.com/dbooth-boston)'s [RDF Pipeline Framework](https://github.com/rdf-pipeline)

_I do eventually want to use this with NodeRed or whatever, but the entities created by transmissions will be at the level of nodes in such networks, not the network itself._

## Motivation

I'm in the process of writing yet another blog engine (Postcraft). I've also started working on a playground for interconnecting intelligent agents in an XMPP multiuser chat environment (Kia). I'm also revising a system for managing a personal knowledge base in the world of LLMs (HKMS). These all share functionality around connectivity to external data/messaging systems and internal data transformation. Might as well write this bit once only, and avoid thinking about software architecture more than I have to.

### Goals

To facilate :

- rapid development of small applications
- reuse of components in a loosely-couple environment
- versatility

### Soft Goals

- performance - low on the list
- scalability - ditto
- security - ditto

================
File: run.js
================
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

import path from 'path'
import fs from 'fs/promises'
import { fileURLToPath } from 'url';





import logger from './src/utils/Logger.js'
import CommandUtils from './src/utils/CommandUtils.js'

var applicationsDir = './src/applications'







let message = {}

logger.setLogLevel("info")






if (process.argv.length <= 2) {

    console.log('Available applications :');
    CommandUtils.listSubdirectories(applicationsDir)

} else {
    await yargs(hideBin(process.argv))
        .usage('Usage: ./trans <application>[.subtask] [options] [target]')
        .option('message', {
            alias: 'm',
            describe: 'message as a JSON string or a path to a JSON file',
            type: 'string',
        })
        .option('dir', {
            alias: 'd',
            describe: 'application directory',
            type: 'string',
        })
        .command('$0 <application> [target]', 'runs the specified application', (yargs) => {
            return yargs.positional('application', {
                describe: 'the application to run'
            })
                .positional('target', {
                    describe: 'the target of the application'

                })
        }, async (argv) => {
            const { dir, application, target, message: contextArg } = argv
            if (dir) {
                applicationsDir = dir
            }
            logger.setLogLevel("info")
            logger.debug('\n**** run.js, async (args)')
            logger.debug('applicationsDir = ' + applicationsDir)
            logger.debug('application = ' + application)
            logger.debug('target = ' + target)


            const modulePath = path.join(applicationsDir, application, 'processors');
            await CommandUtils.run(applicationsDir, application, target, message, modulePath);
        })
        .help('h')
        .alias('h', 'help')

        .fail((msg, err, yargs) => {
            if (err) throw err;
            console.error(msg);
            process.exit(1);
        })
        .argv
}

================
File: runs.txt
================
41  ./run file-copy-remove-test
   43  ./run file-copy-remove-test
   56  client = docker.DockerClient(base_url='unix://var/run/docker.sock', version='auto')
  241  ./run file-copy-remove-test
  245  ./run file-copy-remove-test
  266  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  267  ./run file-copy-remove-test
  268  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  269  ./run file-copy-remove-test
  271  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  272  ./run two-transmissions
  308  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  310  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  312  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  314  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  318  ./run file-pipeline
  321  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  322  ./run postcraft-init /absolute/path
  323  ./run postcraft-init /home/danny/absolute/path
  325  ./run postcraft-init /home/danny/absolute/path
  326  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  329  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  334  ./run postcraft /home/danny/HKMS/postcraft/danny.ayers.name
  338  ./run postcraft ../elfquake.org
  340  ./run postcraft ../elfquake.org
  383  ./run postcraft ../elfquake.org
  403  ./run postcraft ../elfquake.org
  404  ./run postcraft ../postcraft/danny.ayers.name
  406  ./run postcraft ../postcraft/danny.ayers.name
  463  ../transmissions/run postcraft ./
  465  ./run postcraft ../strandz/docs/strandz.it
  466  ./run postcraft ../foaf-retro/foaf-retro.hyperdata.it
  467  ./run postcraft ../hyperdata/docs/ps.hyperdata.it
  501  ./run globbo
  502  ./run globbo something
  503  ./run globbo
  504  ./run globbo something
  505  ./run globbo ./
  506  ./run globbo something
  507  ./run
  508  ./run --help
  509  ./run globbo something
  510  ./run globbo -c something
  511  ./run globbo -c "something"
  512  ./run globbo -c "{ something }"
  513  ./run globbo -c "{ a:something }"
  514  ./run globbo -c "{a:something}"
  515  ./run globbo -c '{a:something}'
  516  ./run globbo -c '{"a":"something"}'
  517  ./run globbo
  518  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}
  519  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}'
  521  ./run postcraft /home/danny/github-danny/postcraft/danny.ayers.name
  523  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}'
  524  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}' > kiki.txt
  525  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}' 
  526  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}' > kiki.txt
  527  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}' 
  528  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}' > kiki.txt
  529  ./run globbo -c '{"rootDir": "./", "sourceDir":"docs"}' 
  549  ./run.sh 
  554  history|grep ./run > runs.txt

================
File: trans
================
#!/bin/bash

# run
# use 'chmod +x run' to make this executable


# Execute the Node.js script with Node
node run.js "$@"

================
File: webpack.config.js
================
const path = require('path');
const HtmlWebpackPlugin = require('html-webpack-plugin');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');
const WorkboxWebpackPlugin = require('workbox-webpack-plugin');

const isProduction = process.env.NODE_ENV == 'production';


const stylesHandler = MiniCssExtractPlugin.loader;



const config = {
    entry: './src/index.js',
    output: {
        path: path.resolve(__dirname, 'dist'),
    },
    devServer: {
        open: true,
        host: 'localhost',
    },
    plugins: [
        new HtmlWebpackPlugin({
            template: 'index.html',
        }),

        new MiniCssExtractPlugin(),



    ],
    module: {
        rules: [
            {
                test: /\.(js|jsx)$/i,
                loader: 'babel-loader',
            },
            {
                test: /\.css$/i,
                use: [stylesHandler, 'css-loader', 'postcss-loader'],
            },
            {
                test: /\.(eot|svg|ttf|woff|woff2|png|jpg|gif)$/i,
                type: 'asset',
            },



        ],
    },
};

module.exports = () => {
    if (isProduction) {
        config.mode = 'production';


        config.plugins.push(new WorkboxWebpackPlugin.GenerateSW());

    } else {
        config.mode = 'development';
    }
    return config;
};
